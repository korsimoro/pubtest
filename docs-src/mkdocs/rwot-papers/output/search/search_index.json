{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"rwot1/","text":"Rebooting the Web of Trust I: San Francisco (November 2015) This repository contains documents related to RWOT1, the first Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on November 3rd & 4th, 2015. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop successfully completed its five papers, which are now all available online: Rebranding the Web of Trust by Shannon Appelcline, Dave Crocker, Randall Farmer, and Justin Newton A history of the Web of Trust and a look at what the term could mean for the future. Opportunities Created by the Web of Trust for Controlling and Leveraging Personal Data by du5t, Kaliya \"Identity Woman\" Young (@identitywoman), John Edge, Drummond Reed, and Noah Thorp Five use cases, from two relatively simple cases of managing selective disclosure to the most extreme case of establishing government-verifiable credentials from nothing for a stateless refugee. Decentralized Public Key Infrastructure by Christopher Allen, Arthur Brock, Vitalik Buterin, Jon Callas, Duke Dorje, Christian Lundkvist, Pavel Kravchenko, Jude Nelson, Drummond Reed, Markus Sabadello, Greg Slepak, Noah Thorp, and Harlan T Wood A massive overview of a decentralized public-key infrastructure (DPKI). Smart Signatures by Christopher Allen, Greg Maxwell, Peter Todd, Ryan Shea, Pieter Wuille, Joseph Bonneau, Joseph Poon, and Tyler Close A system to explicitly outline and fully program conditions for verification, inspired by Bitcoin Script. Creating the New World of Trust by Shannon Appelcline A summary of the next step for the Rebooting the Web of Trust group. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Rwot1"},{"location":"rwot1/#rebooting-the-web-of-trust-i-san-francisco-november-2015","text":"This repository contains documents related to RWOT1, the first Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on November 3rd & 4th, 2015. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust I: San Francisco (November 2015)"},{"location":"rwot1/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"rwot1/#completed-papers","text":"The design workshop successfully completed its five papers, which are now all available online:","title":"Completed Papers"},{"location":"rwot1/#rebranding-the-web-of-trust","text":"","title":"Rebranding the Web of Trust"},{"location":"rwot1/#by-shannon-appelcline-dave-crocker-randall-farmer-and-justin-newton","text":"A history of the Web of Trust and a look at what the term could mean for the future.","title":"by Shannon Appelcline, Dave Crocker, Randall Farmer, and Justin Newton"},{"location":"rwot1/#opportunities-created-by-the-web-of-trust-for-controlling-and-leveraging-personal-data","text":"","title":"Opportunities Created by the Web of Trust for Controlling and Leveraging Personal Data"},{"location":"rwot1/#by-du5t-kaliya-identity-woman-young-identitywoman-john-edge-drummond-reed-and-noah-thorp","text":"Five use cases, from two relatively simple cases of managing selective disclosure to the most extreme case of establishing government-verifiable credentials from nothing for a stateless refugee.","title":"by du5t, Kaliya \"Identity Woman\" Young (@identitywoman), John Edge, Drummond Reed, and Noah Thorp"},{"location":"rwot1/#decentralized-public-key-infrastructure","text":"","title":"Decentralized Public Key Infrastructure"},{"location":"rwot1/#by-christopher-allen-arthur-brock-vitalik-buterin-jon-callas-duke-dorje-christian-lundkvist-pavel-kravchenko-jude-nelson-drummond-reed-markus-sabadello-greg-slepak-noah-thorp-and-harlan-t-wood","text":"A massive overview of a decentralized public-key infrastructure (DPKI).","title":"by Christopher Allen, Arthur Brock, Vitalik Buterin, Jon Callas, Duke Dorje, Christian Lundkvist, Pavel Kravchenko, Jude Nelson, Drummond Reed, Markus Sabadello, Greg Slepak, Noah Thorp, and Harlan T Wood"},{"location":"rwot1/#smart-signatures","text":"","title":"Smart Signatures"},{"location":"rwot1/#by-christopher-allen-greg-maxwell-peter-todd-ryan-shea-pieter-wuille-joseph-bonneau-joseph-poon-and-tyler-close","text":"A system to explicitly outline and fully program conditions for verification, inspired by Bitcoin Script.","title":"by Christopher Allen, Greg Maxwell, Peter Todd, Ryan Shea, Pieter Wuille, Joseph Bonneau, Joseph Poon, and Tyler Close"},{"location":"rwot1/#creating-the-new-world-of-trust","text":"","title":"Creating the New World of Trust"},{"location":"rwot1/#by-shannon-appelcline","text":"A summary of the next step for the Rebooting the Web of Trust group.","title":"by Shannon Appelcline"},{"location":"rwot1/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"rwot1/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"rwot2/","text":"Rebooting the Web of Trust II: ID2020 (May 2016) This repository contains documents related to RWOT2, the second Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on May 21st & 22nd, 2016. It was run in conjunction with the UN ID2020 Summit on Identity, which occurred at the UN in New York on May 20th, 2016. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the ID 2020 Design DesignShop, all participants were requested to submit a 1 or 2 page topics paper to be shared with other attendees on either: A specific decentralized identity use case related to the topic of the UN ID 2020 Summit A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop successfully completed its five papers, which are now all available online: Identity Crisis: Clearer Identity Through Correlation by Joe Andrieu, Kevin Gannon, Igor Kruiper, Ajit Tripathi, and Gary Zimmerman A new way to look at identity, as correlation over time. Powering the Physician-Patient Relationship with HIE of One Blockchain Health IT by Adrian Gropper, MD Using Blockchains and DIDs for physician-patient interactions. Protecting Digital Identities in Developing Countries by Wayne Hennessy-Barrett A real-world use case, describing issues of identity in the developing world. Requirements for DIDs (Decentralized Identifiers) by Drummond Reed and Les Chasen The first of a series of papers abou tproducing a concrete DID system. Smarter Signatures: Experiments in Verifications by Christopher Allen and Shannon Appelcline A look at uses and requirements of next-generation smart signature systems. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Rwot2"},{"location":"rwot2/#rebooting-the-web-of-trust-ii-id2020-may-2016","text":"This repository contains documents related to RWOT2, the second Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on May 21st & 22nd, 2016. It was run in conjunction with the UN ID2020 Summit on Identity, which occurred at the UN in New York on May 20th, 2016. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust II: ID2020 (May 2016)"},{"location":"rwot2/#topics-advance-readings","text":"In advance of the ID 2020 Design DesignShop, all participants were requested to submit a 1 or 2 page topics paper to be shared with other attendees on either: A specific decentralized identity use case related to the topic of the UN ID 2020 Summit A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"rwot2/#completed-papers","text":"The design workshop successfully completed its five papers, which are now all available online:","title":"Completed Papers"},{"location":"rwot2/#identity-crisis-clearer-identity-through-correlation","text":"","title":"Identity Crisis: Clearer Identity Through Correlation"},{"location":"rwot2/#by-joe-andrieu-kevin-gannon-igor-kruiper-ajit-tripathi-and-gary-zimmerman","text":"A new way to look at identity, as correlation over time.","title":"by Joe Andrieu, Kevin Gannon, Igor Kruiper, Ajit Tripathi, and Gary Zimmerman"},{"location":"rwot2/#powering-the-physician-patient-relationship-with-hie-of-one-blockchain-health-it","text":"","title":"Powering the Physician-Patient Relationship with HIE of One Blockchain Health IT"},{"location":"rwot2/#by-adrian-gropper-md","text":"Using Blockchains and DIDs for physician-patient interactions.","title":"by Adrian Gropper, MD"},{"location":"rwot2/#protecting-digital-identities-in-developing-countries","text":"","title":"Protecting Digital Identities in Developing Countries"},{"location":"rwot2/#by-wayne-hennessy-barrett","text":"A real-world use case, describing issues of identity in the developing world.","title":"by Wayne Hennessy-Barrett"},{"location":"rwot2/#requirements-for-dids-decentralized-identifiers","text":"","title":"Requirements for DIDs (Decentralized Identifiers)"},{"location":"rwot2/#by-drummond-reed-and-les-chasen","text":"The first of a series of papers abou tproducing a concrete DID system.","title":"by Drummond Reed and Les Chasen"},{"location":"rwot2/#smarter-signatures-experiments-in-verifications","text":"","title":"Smarter Signatures: Experiments in Verifications"},{"location":"rwot2/#by-christopher-allen-and-shannon-appelcline","text":"A look at uses and requirements of next-generation smart signature systems.","title":"by Christopher Allen and Shannon Appelcline"},{"location":"rwot2/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"rwot2/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"rwot3/","text":"Rebooting the Web of Trust III: San Francisco (October 2016) This repository contains documents related to RWOT3, the third Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on October 19th-21st, 2016. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop exceeded its mandate by producing seven papers, which are now all available online: DID (Decentralized Identifier) Data Model and Generic Syntax 1.0 Implementer\u2019s Draft 01 by Drummond Reed, Les Chasen, Christopher Allen, and Ryan Grant The complete draft of the Decentralized IDentifier (DID) model and syntac, a project that has run through the RWOT workshops to date. Digital Verification Advancements at RWoT III by Manu Sporny with Christopher Allen, Harlan Wood, and Jason Law A short overview of enhancements to Digital Verification that came out of RWOT III. Embedding Human Wisdom in Our Digital Tomorrow by Daniel Hardman, Kaliya \u201cIdentity Woman\u201d Young, and Matthew Schutte A discussion of the dangers of transferring wisdom into the digital world, seen through the lenses of vulnerability, shadows, healing, tensions, complexity and gestalt, and organizational choices. Hubs by Daniel Buchner, Wayne Vaughan, and Ryan Shea An overview of the hubs datastore system. Joram 1.0.0 by Joe Andrieu and Bob Clint An Information Lifecycle Engagement Model that offers a use case for a Syrian refugee. Portable Reputation Toolkit Use Cases by Christopher Allen, Tim Daubensch\u00fctz, Manu Sporny, Noah Thorp, Harlan Wood, Glenn Willen, and Alessandro Voto A model and proof-of-concept implementation for decentralized verification. Smart Consent Protocol by Dr. Shaun Conway, Lohan Spies, Jonathan Endersby, and Tim Daubensch\u00fctz Bringing together COALA IP and Consent to deal with digital intellectual property. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Rwot3"},{"location":"rwot3/#rebooting-the-web-of-trust-iii-san-francisco-october-2016","text":"This repository contains documents related to RWOT3, the third Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on October 19th-21st, 2016. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust III: San Francisco (October 2016)"},{"location":"rwot3/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"rwot3/#completed-papers","text":"The design workshop exceeded its mandate by producing seven papers, which are now all available online:","title":"Completed Papers"},{"location":"rwot3/#did-decentralized-identifier-data-model-and-generic-syntax-10-implementers-draft-01","text":"","title":"DID (Decentralized Identifier) Data Model and Generic Syntax 1.0 Implementer\u2019s Draft 01"},{"location":"rwot3/#by-drummond-reed-les-chasen-christopher-allen-and-ryan-grant","text":"The complete draft of the Decentralized IDentifier (DID) model and syntac, a project that has run through the RWOT workshops to date.","title":"by Drummond Reed, Les Chasen, Christopher Allen, and Ryan Grant"},{"location":"rwot3/#digital-verification-advancements-at-rwot-iii","text":"","title":"Digital Verification Advancements at RWoT III"},{"location":"rwot3/#by-manu-sporny-with-christopher-allen-harlan-wood-and-jason-law","text":"A short overview of enhancements to Digital Verification that came out of RWOT III.","title":"by Manu Sporny with Christopher Allen, Harlan Wood, and Jason Law"},{"location":"rwot3/#embedding-human-wisdom-in-our-digital-tomorrow","text":"","title":"Embedding Human Wisdom in Our Digital Tomorrow"},{"location":"rwot3/#by-daniel-hardman-kaliya-identity-woman-young-and-matthew-schutte","text":"A discussion of the dangers of transferring wisdom into the digital world, seen through the lenses of vulnerability, shadows, healing, tensions, complexity and gestalt, and organizational choices.","title":"by Daniel Hardman, Kaliya \u201cIdentity Woman\u201d Young, and Matthew Schutte"},{"location":"rwot3/#hubs","text":"","title":"Hubs"},{"location":"rwot3/#by-daniel-buchner-wayne-vaughan-and-ryan-shea","text":"An overview of the hubs datastore system.","title":"by Daniel Buchner, Wayne Vaughan, and Ryan Shea"},{"location":"rwot3/#joram-100","text":"","title":"Joram 1.0.0"},{"location":"rwot3/#by-joe-andrieu-and-bob-clint","text":"An Information Lifecycle Engagement Model that offers a use case for a Syrian refugee.","title":"by Joe Andrieu and Bob Clint"},{"location":"rwot3/#portable-reputation-toolkit-use-cases","text":"","title":"Portable Reputation Toolkit Use Cases"},{"location":"rwot3/#by-christopher-allen-tim-daubenschutz-manu-sporny-noah-thorp-harlan-wood-glenn-willen-and-alessandro-voto","text":"A model and proof-of-concept implementation for decentralized verification.","title":"by Christopher Allen, Tim Daubensch\u00fctz, Manu Sporny, Noah Thorp, Harlan Wood, Glenn Willen, and Alessandro Voto"},{"location":"rwot3/#smart-consent-protocol","text":"","title":"Smart Consent Protocol"},{"location":"rwot3/#by-dr-shaun-conway-lohan-spies-jonathan-endersby-and-tim-daubenschutz","text":"Bringing together COALA IP and Consent to deal with digital intellectual property.","title":"by Dr. Shaun Conway, Lohan Spies, Jonathan Endersby, and Tim Daubensch\u00fctz"},{"location":"rwot3/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"rwot3/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"rwot4/","text":"Rebooting the Web of Trust IV: Paris (April 2017) This repository contains documents related to RWOT4, the fourth Rebooting the Web of Trust design workshop, which ran in Paris, France, on April 19th-21st, 2017. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop met its goal of three to five white papers with three total. Design Considerations for Decentralized Reputation Systems by Angus Champion de Crespigny, Dmitry Khovratovich, Florent Blondeau, Klara Sok, Philippe Honigman, Nikolaos Alexopoulos, Fabien Petitcolas, and Shaun Conway Ten design considerations for the creation of decentralized reputation systems. LD Signature Format Alignment by Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello, and Manu Sporny Bringing JSON-LD signatures into alignment with JOSE JSON Web Signature (JWS) standards. Re-Imagining What Users Really Want by Joe Andrieu, Frederic Engel, Adam Lake, Moses Ma, Olivier Maas, and Mark van der Waal Five people, five opportunities for self-sovereign identity. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Rwot4"},{"location":"rwot4/#rebooting-the-web-of-trust-iv-paris-april-2017","text":"This repository contains documents related to RWOT4, the fourth Rebooting the Web of Trust design workshop, which ran in Paris, France, on April 19th-21st, 2017. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust IV: Paris (April 2017)"},{"location":"rwot4/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"rwot4/#completed-papers","text":"The design workshop met its goal of three to five white papers with three total.","title":"Completed Papers"},{"location":"rwot4/#design-considerations-for-decentralized-reputation-systems","text":"","title":"Design Considerations for Decentralized Reputation Systems"},{"location":"rwot4/#by-angus-champion-de-crespigny-dmitry-khovratovich-florent-blondeau-klara-sok-philippe-honigman-nikolaos-alexopoulos-fabien-petitcolas-and-shaun-conway","text":"Ten design considerations for the creation of decentralized reputation systems.","title":"by Angus Champion de Crespigny, Dmitry Khovratovich, Florent Blondeau, Klara Sok, Philippe Honigman, Nikolaos Alexopoulos, Fabien Petitcolas, and Shaun Conway"},{"location":"rwot4/#ld-signature-format-alignment","text":"","title":"LD Signature Format Alignment"},{"location":"rwot4/#by-kim-hamilton-duffy-rodolphe-marques-markus-sabadello-and-manu-sporny","text":"Bringing JSON-LD signatures into alignment with JOSE JSON Web Signature (JWS) standards.","title":"by Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello, and Manu Sporny"},{"location":"rwot4/#re-imagining-what-users-really-want","text":"","title":"Re-Imagining What Users Really Want"},{"location":"rwot4/#by-joe-andrieu-frederic-engel-adam-lake-moses-ma-olivier-maas-and-mark-van-der-waal","text":"Five people, five opportunities for self-sovereign identity.","title":"by Joe Andrieu, Frederic Engel, Adam Lake, Moses Ma, Olivier Maas, and Mark van der Waal"},{"location":"rwot4/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"rwot4/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"rwot5/","text":"Rebooting the Web of Trust V: Boston (October 2017) This repository contains documents related to RWOT5, the fifth Rebooting the Web of Trust design workshop, which is to run in Boston, Massachusetts, on October 3rd-5th, 2017. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop exceeded its goal of three to five white papers with a total of eight publications: ActivityPub: From Decentralized to Distributed Social Networks (Text) by Christopher Lemmer Webber & Manu Sporny An overview of the W3C ActivityPub protocol. Amira 1.0.0 (Text) by Joe Andrieu & Team This paper reinterprets Christopher Allen\u2019s Rebooting the Web of Trust user story through the lens of the Information Lifecycle Engagement Model. It presents a human-centric illustration of an individual\u2019s experience in a self-sovereign, decentralized realization of the Web of Trust as originally conceived by Phil Zimmerman for PGP. The DCS Theorem by Greg Slepak & Anya Petrova A probability proof of the DCS Triangle. Why can't decentralized consensus systems have all three of decentralization, consensus, and scale? Plus, two methods for getting around these limitations. A Decentralized Approach to Blockcerts Credential Revocation (Text) by Jo\u00e3o Santos & Kim Hamilton Duffy Blockcerts are blockchain-anchored credentials with a verification process designed to be decentralized and trustless. This proposal describes an alternate method of issuing Blockcerts using Ethereum, which allows for a new form of revocation by either the issuer or the recipient. Engineering Privacy for Verified Credentials: In Which We Describe Data Minimization, Selective Disclosure, and Progressive Trust (Text) by Lionel Wolberger, Brent Zundel, Zachary Larson, Irene Hernandez & Katryna Dow We often share information on the World Wide Web, though some of it is private. The W3C Credentials Community Group focuses on how privacy can be enhanced when attributes are shared electronically. In the course of our work, we have identified three related but distinct privacy enhancing strategies: \"data minimization,\" \"selective disclosure,\" and \"progressive trust.\" These enhancements are enabled with cryptography. The goal of this paper is to enable decision makers, particularly non-technical ones, to gain a nuanced grasp of these enhancements along with some idea of how their enablers work. We describe them below in plain English, but with some rigor. This knowledge will enable readers of this paper to be better able to know when they need privacy enhancements, to select the type of enhancement needed, to assess techniques that enable those enhancements, and to adopt the correct enhancement for the correct use case. Identity Hubs Capabilities Perspective (Text) by Adrian Gropper, Drummond Reed & Mark S. Miller Identity Hubs as currently proposed in the Decentralized Identity Foundation (DIF) are a subset of a general Decentralized Identifier (DID) based user-controlled agent, based on ACLs rather than an object-capabilities (ocap) architecture. Transitioning the Hubs design to an ocap model can be achieved by introducing an UMA authorization server as the control endpoint. Linked Data Capabilities (Text) Christopher Lemmer Webber & Mark S. Miller Linked Data Signatures enable a method of asserting the integrity of linked data documents that are passed throughout the web. The object capability model is a powerful system for ensuring the security of computing systems. Veres One DID Method (Text) by Manu Sporny & Dave Longley The Veres One Ledger is a permissionless public ledger designed specifically for the creation and management of decentralized identifiers (DIDs). This specification defines how a developer may create and update DIDs in the Veres One Ledger. When GDPR becomes real, and Blockchain is no longer Fairy Dust (Text) by Marta Piekarska, Michael Lodder, Zachary Larson & Kaliya Young (Identity Woman) This document describes the GDPR requirements and the different approaches to digital identity solutions and finally explains why distributed ledger technology may offer an opportunity for enterprises to simplify data management solutions that are GDPR compliant. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Rwot5"},{"location":"rwot5/#rebooting-the-web-of-trust-v-boston-october-2017","text":"This repository contains documents related to RWOT5, the fifth Rebooting the Web of Trust design workshop, which is to run in Boston, Massachusetts, on October 3rd-5th, 2017. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust V: Boston (October 2017)"},{"location":"rwot5/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"rwot5/#completed-papers","text":"The design workshop exceeded its goal of three to five white papers with a total of eight publications:","title":"Completed Papers"},{"location":"rwot5/#activitypub-from-decentralized-to-distributed-social-networks-text","text":"","title":"ActivityPub: From Decentralized to Distributed Social Networks (Text)"},{"location":"rwot5/#by-christopher-lemmer-webber-manu-sporny","text":"An overview of the W3C ActivityPub protocol.","title":"by Christopher Lemmer Webber &amp; Manu Sporny"},{"location":"rwot5/#amira-100-text","text":"","title":"Amira 1.0.0 (Text)"},{"location":"rwot5/#by-joe-andrieu-team","text":"This paper reinterprets Christopher Allen\u2019s Rebooting the Web of Trust user story through the lens of the Information Lifecycle Engagement Model. It presents a human-centric illustration of an individual\u2019s experience in a self-sovereign, decentralized realization of the Web of Trust as originally conceived by Phil Zimmerman for PGP.","title":"by Joe Andrieu &amp; Team"},{"location":"rwot5/#the-dcs-theorem","text":"","title":"The DCS Theorem"},{"location":"rwot5/#by-greg-slepak-anya-petrova","text":"A probability proof of the DCS Triangle. Why can't decentralized consensus systems have all three of decentralization, consensus, and scale? Plus, two methods for getting around these limitations.","title":"by Greg Slepak &amp; Anya Petrova"},{"location":"rwot5/#a-decentralized-approach-to-blockcerts-credential-revocation-text","text":"","title":"A Decentralized Approach to Blockcerts Credential Revocation (Text)"},{"location":"rwot5/#by-joao-santos-kim-hamilton-duffy","text":"Blockcerts are blockchain-anchored credentials with a verification process designed to be decentralized and trustless. This proposal describes an alternate method of issuing Blockcerts using Ethereum, which allows for a new form of revocation by either the issuer or the recipient.","title":"by Jo\u00e3o Santos &amp; Kim Hamilton Duffy"},{"location":"rwot5/#engineering-privacy-for-verified-credentials-in-which-we-describe-data-minimization-selective-disclosure-and-progressive-trust-text","text":"","title":"Engineering Privacy for Verified Credentials: In Which We Describe Data Minimization, Selective Disclosure, and Progressive Trust (Text)"},{"location":"rwot5/#by-lionel-wolberger-brent-zundel-zachary-larson-irene-hernandez-katryna-dow","text":"We often share information on the World Wide Web, though some of it is private. The W3C Credentials Community Group focuses on how privacy can be enhanced when attributes are shared electronically. In the course of our work, we have identified three related but distinct privacy enhancing strategies: \"data minimization,\" \"selective disclosure,\" and \"progressive trust.\" These enhancements are enabled with cryptography. The goal of this paper is to enable decision makers, particularly non-technical ones, to gain a nuanced grasp of these enhancements along with some idea of how their enablers work. We describe them below in plain English, but with some rigor. This knowledge will enable readers of this paper to be better able to know when they need privacy enhancements, to select the type of enhancement needed, to assess techniques that enable those enhancements, and to adopt the correct enhancement for the correct use case.","title":"by Lionel Wolberger, Brent Zundel, Zachary Larson, Irene Hernandez &amp; Katryna Dow"},{"location":"rwot5/#identity-hubs-capabilities-perspective-text","text":"","title":"Identity Hubs Capabilities Perspective (Text)"},{"location":"rwot5/#by-adrian-gropper-drummond-reed-mark-s-miller","text":"Identity Hubs as currently proposed in the Decentralized Identity Foundation (DIF) are a subset of a general Decentralized Identifier (DID) based user-controlled agent, based on ACLs rather than an object-capabilities (ocap) architecture. Transitioning the Hubs design to an ocap model can be achieved by introducing an UMA authorization server as the control endpoint.","title":"by Adrian Gropper, Drummond Reed &amp; Mark S. Miller"},{"location":"rwot5/#linked-data-capabilities-text","text":"","title":"Linked Data Capabilities (Text)"},{"location":"rwot5/#christopher-lemmer-webber-mark-s-miller","text":"Linked Data Signatures enable a method of asserting the integrity of linked data documents that are passed throughout the web. The object capability model is a powerful system for ensuring the security of computing systems.","title":"Christopher Lemmer Webber &amp; Mark S. Miller"},{"location":"rwot5/#veres-one-did-method-text","text":"","title":"Veres One DID Method (Text)"},{"location":"rwot5/#by-manu-sporny-dave-longley","text":"The Veres One Ledger is a permissionless public ledger designed specifically for the creation and management of decentralized identifiers (DIDs). This specification defines how a developer may create and update DIDs in the Veres One Ledger.","title":"by Manu Sporny &amp; Dave Longley"},{"location":"rwot5/#when-gdpr-becomes-real-and-blockchain-is-no-longer-fairy-dust-text","text":"","title":"When GDPR becomes real, and Blockchain is no longer Fairy Dust (Text)"},{"location":"rwot5/#by-marta-piekarska-michael-lodder-zachary-larson-kaliya-young-identity-woman","text":"This document describes the GDPR requirements and the different approaches to digital identity solutions and finally explains why distributed ledger technology may offer an opportunity for enterprises to simplify data management solutions that are GDPR compliant.","title":"by Marta Piekarska, Michael Lodder, Zachary Larson &amp; Kaliya Young (Identity Woman)"},{"location":"rwot5/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"rwot5/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"rwot6/","text":"Rebooting the Web of Trust VI: Santa Barbara (March 2018) This repository contains documents related to RWOT6, the sixth Rebooting the Web of Trust design workshop, which ran in Santa Barbara, California, on March 6th to 8th, 2018. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Topics & Advance Readings directory for a listing of all the papers. Complete Papers The goal for each Rebooting the Web of Trust workshop is publication of three to five white papers: BTCR DID Resolver Specification (Text) Kim Hamilton Duffy, Christopher Allen, Ryan Grant, and Dan Pape This describes the process of resolving a BTCR DID into a DID Document. The draft reference implementation is available at https://github.com/WebOfTrustInfo/btcr-did-tools-js (see didFormatter.js). Note that not all steps described in this document are implemented yet. Decentralized Autonomic Data (DAD) and the three R's of Key Management (Text) by Samuel M. Smith Ph.D. with Vishal Gupta This paper proposes a new class of data called decentralized autonomic data (DAD). The term decentralized means that the governance of the data may not reside with a single party. A related concept is that the trust in the data provenance is diffuse in nature. Central to the approach is leveraging the emerging DID (decentralized identifier) standard. The term autonomic means self-managing or self-regulating. In the context of data, we crystalize the meaning of self-managing to include cryptographic techniques for maintaining data provenance that make the data self-identifying, self-certifying, and self-securing. Implied thereby is the use of cryptographic keys and signatures to provide a root of trust for data integrity and to maintain that trust over transformation of that data, e.g. provenance. Thus key management must be a first order property of DADs. This includes key reproduction, rotation, and recovery. The pre-rotation and hybrid recovery methods presented herein are somewhat novel. Decentralized Identifiers v1.0 (Text) A Status Note The Decentralized Identifiers specification editors and implementers spent some time at Rebooting the Web of Trust 6 processing the remaining issues in the issue tracker. This document summarizes the proposed resolutions that the group has put forward to resolve all of the DID specification issues that were submitted before 2018-03-05. Exploring Sustainable Technology Commons using Appreciative Inquiry (Text) by Heather Vescent, Kaliya \u201cIdentity Woman\u201d Young, Adrian Gropper, and Juan Caballero Technology commons come in a variety of flavors and have achieved varying levels of financial success. For-profit corporate activities have in few historical cases been set up with a financial feedback mechanism to support the commons upon which they depend and capitalize. Why do the commons and the technology sectors\u2019 available forms of capitalism act as incompatible as oil and water, even though they support each other\u2019s aims? When capitalist benefactors support the technology commons that they utilize, it creates a sustainable and thriving commons which enables and supports additional capitalistic technology innovation. Having worked on both sides of the equation, the authors of this piece propose a vocabulary to nourish these interactions between the two sides; identified characteristics of a sustainable technology commons; identified commons models and variations; applied Appreciative Inquiry principles to one commons model; and identified future research areas. Identity Hub Attestation Flows and Components (Text) by Daniel Buchner, Cherie Duncan, John Toohey, Ron Kreutzer, and Stephen Curran In this document, we define a set of user flows and describe the associated Action Objects that support a Hub-centric approach to the request, issuance, presentation, verification, and revocation of interoperable attestations. This document extends the Identity Hub Explainer . Introduction to DID Auth (Text) by Markus Sabadello, Kyle Den Hartog, Christian Lundkvist, Cedric Franz, Alberto Elias, Andrew Hughes, John Jordan & Dmitri Zagidulin The term DID Auth has been used in different ways and is currently not well-defined. We define DID Auth as a ceremony where an identity owner, with the help of various components such as web browsers, mobile devices, and other agents, proves to a relying party that they are in control of a DID. This means demonstrating control of the DID using the mechanism specified in the DID Document's \"authentication\" object. This could take place using a number of different data formats, protocols, and flows. DID Auth includes the ability to establish mutually authenticated communication channels and to authenticate to web sites and applications. Authorization, Verifiable Credentials, and Capabilities are built on top of DID Auth and are out of scope for this document. This paper gives on overview of the scope of DID Auth, supported protocols and flows, and the use of components of the DID Documents that are relevant to authentication, as well as formats for challenges and responses. Open Badges are Verifiable Credentials (Text) By Nate Otto & Kim Hamilton Duffy We identify use cases and requirements that connect threads of work happening in the Rebooting Web of Trust community around: educational achievement claims (particularly using the Open Badges vocabulary); use of decentralized identifiers (DIDs) within web services where educational claims circulate; and integrating blockchain-reliant verification layers. We illustrate each of these cases with a set of example documents and describe user stories for Open Badges ecosystem software in the roles of Issuer, Host/Backpack, Displayer, and Verifier that need to be implemented in order to enable the capabilities described. SSI: A Roadmap for Adoption (Text) By Moses Ma, Claire Rumore, Dan Gisolfi, Wes Kussmaul & Dan Greening (Senex Rex) This document proposes the formation of a short-term team to develop consistent messaging for the Self-Sovereign Identity (SSI) market. It will target key stakeholders who would actively promote SSI adoption. The goal is to create an SSI market roadmap. This roadmap will help SSI leaders, standards bodies, developers, academics, media, and investors coordinate and clarify their messaging for the market, to accelerate the SSI adoption. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 - New York City (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Rwot6"},{"location":"rwot6/#rebooting-the-web-of-trust-vi-santa-barbara-march-2018","text":"This repository contains documents related to RWOT6, the sixth Rebooting the Web of Trust design workshop, which ran in Santa Barbara, California, on March 6th to 8th, 2018. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community.","title":"Rebooting the Web of Trust VI: Santa Barbara (March 2018)"},{"location":"rwot6/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Topics & Advance Readings directory for a listing of all the papers.","title":"Topics &amp; Advance Readings"},{"location":"rwot6/#complete-papers","text":"The goal for each Rebooting the Web of Trust workshop is publication of three to five white papers:","title":"Complete Papers"},{"location":"rwot6/#btcr-did-resolver-specification-text","text":"","title":"BTCR DID Resolver Specification (Text)"},{"location":"rwot6/#kim-hamilton-duffy-christopher-allen-ryan-grant-and-dan-pape","text":"This describes the process of resolving a BTCR DID into a DID Document. The draft reference implementation is available at https://github.com/WebOfTrustInfo/btcr-did-tools-js (see didFormatter.js). Note that not all steps described in this document are implemented yet.","title":"Kim Hamilton Duffy, Christopher Allen, Ryan Grant, and Dan Pape"},{"location":"rwot6/#decentralized-autonomic-data-dad-and-the-three-rs-of-key-management-text","text":"","title":"Decentralized Autonomic Data (DAD) and the three R's of Key Management (Text)"},{"location":"rwot6/#by-samuel-m-smith-phd-with-vishal-gupta","text":"This paper proposes a new class of data called decentralized autonomic data (DAD). The term decentralized means that the governance of the data may not reside with a single party. A related concept is that the trust in the data provenance is diffuse in nature. Central to the approach is leveraging the emerging DID (decentralized identifier) standard. The term autonomic means self-managing or self-regulating. In the context of data, we crystalize the meaning of self-managing to include cryptographic techniques for maintaining data provenance that make the data self-identifying, self-certifying, and self-securing. Implied thereby is the use of cryptographic keys and signatures to provide a root of trust for data integrity and to maintain that trust over transformation of that data, e.g. provenance. Thus key management must be a first order property of DADs. This includes key reproduction, rotation, and recovery. The pre-rotation and hybrid recovery methods presented herein are somewhat novel.","title":"by Samuel M. Smith Ph.D. with Vishal Gupta"},{"location":"rwot6/#decentralized-identifiers-v10-text","text":"","title":"Decentralized Identifiers v1.0 (Text)"},{"location":"rwot6/#a-status-note","text":"The Decentralized Identifiers specification editors and implementers spent some time at Rebooting the Web of Trust 6 processing the remaining issues in the issue tracker. This document summarizes the proposed resolutions that the group has put forward to resolve all of the DID specification issues that were submitted before 2018-03-05.","title":"A Status Note"},{"location":"rwot6/#exploring-sustainable-technology-commons-using-appreciative-inquiry-text","text":"","title":"Exploring Sustainable Technology Commons using Appreciative Inquiry (Text)"},{"location":"rwot6/#by-heather-vescent-kaliya-identity-woman-young-adrian-gropper-and-juan-caballero","text":"Technology commons come in a variety of flavors and have achieved varying levels of financial success. For-profit corporate activities have in few historical cases been set up with a financial feedback mechanism to support the commons upon which they depend and capitalize. Why do the commons and the technology sectors\u2019 available forms of capitalism act as incompatible as oil and water, even though they support each other\u2019s aims? When capitalist benefactors support the technology commons that they utilize, it creates a sustainable and thriving commons which enables and supports additional capitalistic technology innovation. Having worked on both sides of the equation, the authors of this piece propose a vocabulary to nourish these interactions between the two sides; identified characteristics of a sustainable technology commons; identified commons models and variations; applied Appreciative Inquiry principles to one commons model; and identified future research areas.","title":"by Heather Vescent, Kaliya \u201cIdentity Woman\u201d Young, Adrian Gropper, and Juan Caballero"},{"location":"rwot6/#identity-hub-attestation-flows-and-components-text","text":"","title":"Identity Hub Attestation Flows and Components (Text)"},{"location":"rwot6/#by-daniel-buchner-cherie-duncan-john-toohey-ron-kreutzer-and-stephen-curran","text":"In this document, we define a set of user flows and describe the associated Action Objects that support a Hub-centric approach to the request, issuance, presentation, verification, and revocation of interoperable attestations. This document extends the Identity Hub Explainer .","title":"by Daniel Buchner, Cherie Duncan, John Toohey, Ron Kreutzer, and Stephen Curran"},{"location":"rwot6/#introduction-to-did-auth-text","text":"","title":"Introduction to DID Auth (Text)"},{"location":"rwot6/#by-markus-sabadello-kyle-den-hartog-christian-lundkvist-cedric-franz-alberto-elias-andrew-hughes-john-jordan-dmitri-zagidulin","text":"The term DID Auth has been used in different ways and is currently not well-defined. We define DID Auth as a ceremony where an identity owner, with the help of various components such as web browsers, mobile devices, and other agents, proves to a relying party that they are in control of a DID. This means demonstrating control of the DID using the mechanism specified in the DID Document's \"authentication\" object. This could take place using a number of different data formats, protocols, and flows. DID Auth includes the ability to establish mutually authenticated communication channels and to authenticate to web sites and applications. Authorization, Verifiable Credentials, and Capabilities are built on top of DID Auth and are out of scope for this document. This paper gives on overview of the scope of DID Auth, supported protocols and flows, and the use of components of the DID Documents that are relevant to authentication, as well as formats for challenges and responses.","title":"by Markus Sabadello, Kyle Den Hartog, Christian Lundkvist, Cedric Franz, Alberto Elias, Andrew Hughes, John Jordan &amp; Dmitri Zagidulin"},{"location":"rwot6/#open-badges-are-verifiable-credentials-text","text":"","title":"Open Badges are Verifiable Credentials (Text)"},{"location":"rwot6/#by-nate-otto-kim-hamilton-duffy","text":"We identify use cases and requirements that connect threads of work happening in the Rebooting Web of Trust community around: educational achievement claims (particularly using the Open Badges vocabulary); use of decentralized identifiers (DIDs) within web services where educational claims circulate; and integrating blockchain-reliant verification layers. We illustrate each of these cases with a set of example documents and describe user stories for Open Badges ecosystem software in the roles of Issuer, Host/Backpack, Displayer, and Verifier that need to be implemented in order to enable the capabilities described.","title":"By Nate Otto &amp; Kim Hamilton Duffy"},{"location":"rwot6/#ssi-a-roadmap-for-adoption-text","text":"","title":"SSI: A Roadmap for Adoption (Text)"},{"location":"rwot6/#by-moses-ma-claire-rumore-dan-gisolfi-wes-kussmaul-dan-greening-senex-rex","text":"This document proposes the formation of a short-term team to develop consistent messaging for the Self-Sovereign Identity (SSI) market. It will target key stakeholders who would actively promote SSI adoption. The goal is to create an SSI market roadmap. This roadmap will help SSI leaders, standards bodies, developers, academics, media, and investors coordinate and clarify their messaging for the market, to accelerate the SSI adoption.","title":"By Moses Ma, Claire Rumore, Dan Gisolfi, Wes Kussmaul &amp; Dan Greening (Senex Rex)"},{"location":"rwot6/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 - New York City (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"rwot6/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"rwot7/","text":"Rebooting the Web of Trust VII: Toronto (September 2018) This repository contains documents related to RWOT7, the seventh Rebooting the Web of Trust design workshop, which ran near Toronto, Canada, on September 26th to 28th, 2018. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community. Watch for our next event March 1st-3rd in Barcelona, Spain. Final Papers BTCR v0.1 Decisions (Text) Kim Hamilton Duffy, Christopher Allen, and Dan Pape The Bitcoin Reference (BTCR) DID method supports DIDs using the Bitcoin blockchain. This method has been under development through Rebooting Web of Trust events and hackathons over the past year. The BTCR method's reliance on the Bitcoin blockchain presents both advantages and design challenges. During RWOT7, the authors made a number of design and implementation decisions -- largely scope-cutting in nature -- in order to lock down a Minimum Viable Product (MVP) version, which we'll refer to as v0.1. This paper documents those decisions, which will apply to the upcoming v0.1 BTCR method specification and associated v0.1 BTCR reference implementation. A DID for Everything (Text) Shaun Conway, Andrew Hughes, Moses Ma, Jack Poole, Martin Riedel, Samuel M. Smith Ph.D., and Carsten St\u00f6cker The decentralized identifier (DID) is a new and open standard type of globally unique identifier that offers a model for lifetime-scope portable digital identity that does not depend on any centralized authority and that can never be taken away by third-parties. DIDs are supported by the W3C community and the Decentralized Identity Foundation (DIF). They are the \"atomic units\" of a new layer of decentralized identity infrastructure. However, DIDs can be extended from identifiers for people to any entity, thus identifying everything. We can use DIDs to help us identify and manage objects, machines, or agents through their digital twins; we can expand them to locations, to events, and even to pure data objects, which we refer to as decentralized autonomic data (DAD) items. The paper will present novel use-cases for DIDs and DADs and propose a new cryptographic data structure that is a self-contained blockchain of DADs. This enables the verification of the provenance of a given data flow. It builds on a prior paper and an associated reading. How to Convince Dad* of the Importance of Self-Sovereign Identity (Text) Shannon Appelcline, Kenneth Bok, Lucas Parker, Peter Scott, and Matthew Wong One of the major problems with bootstrapping self-sovereign identity is that it requires adoption by a large number of people. Pushing self-sovereign identity from the top-down is most likely to result in a technology that\u2019s not actually used, but instead encouraging the average person to demand self-sovereign identity from the bottom-up will result in the organic development of a vibrant, well-utilized decentralized web-of-trust ecosystem. This paper addresses that need by offering arguments to a variety of people who might be reluctant to use self-sovereign identity, uninterested in its possibilities, or oblivious to the dangers of centralization. By focusing on the needs of real people, we hope to also encourage developers, engineers, and software business owners to create the apps that will address their reluctance and fulfill their needs, making self-sovereign identity a reality. IPLD as a general pattern for DID documents and Verifiable Claims (Text) jonnycrunch, Anthony Ronning, Kim Duffy, Christian Lundkvist Since the emergence of the Decentralized Identifier (DID) specification at the Fall 2016 Rebooting the Web of Trust [1], numerous DID method specifications have appeared. Each DID method specification defines how to resolve a cryptographically-tied DID document given a method-specific identifier. In this paper, we describe a way to represent the DID document as a content-addressed Merkle Directed Acyclic Graph (DAG) using Interplanetary Linked Data (IPLD). This technique enables more cost-efficient, scaleable creation of DIDs and can be applied across different DID method specifications. Peer to Peer Degrees of Trust (Text) Harrison Stahl, Titus Capilnean, Peter Snyder, and Tyler Yasaka Aunthenticity is a challenge for any identity solution. In the physical world, at least in America, it is not difficult to change one's identity. In the digital world, there is the problem of bots. The botnet detection market is expected to be worth over one billion USD by 2023, in a landscape where most digital activity is still heavily centralized. These centralized digital solutions have the advantage of being able to track IP addresses, request phone verification, and present CAPTCHAs to users in order to authenticate them. If this problem is so difficult to solve in the centralized world, how much more challenging will it be in the decentralized world, where none of these techniques are available? In this paper, we explore the idea of using a web of trust as a tool to add authenticity to decentralized identifiers (DIDs). We define a framework for deriving relative trust degrees using a given trust metric: a \"trustworthiness\" score for a given identity from the perspective of another identity. It is our intent that this framework may be used as a starting point for an ongoing exploration of graph-based, decentralized trust. We believe this approach may ultimately be used as a foundation for decentralized reputation. Resource Integrity Proofs (Text) Ganesh Annan and Kim Hamilton Duffy Currently, the Web provides a simple yet powerful mechanism for the dissemination of information via links. Unfortunately, there is no generalized mechanism that enables verifying that a fetched resource has been delivered without unexpected manipulation. Would it be possible to create an extensible and multipurpose cryptographic link that provides discoverability, integrity, and scheme agility? This paper proposes a linking solution that decouples integrity information from link and resource syntaxes, enabling verification of any representation of a resource from any type of link. We call this approach Resource Integrity Proofs (RIPs). RIPs provide a succinct way to link to resources with cryptographically verifiable content integrity. RIPs can be combined with blockchain technology to create discoverable proofs of existence to off-chain resources. Use Cases and Proposed Solutions for Verifiable Offline Credentials (Text) Michael Lodder, Samantha Mathews Chase, and Wolf McNally In this paper we cover various scenarios where some or all parties have intermittent, unreliable, untrusted, insecure, or no network access, but require cryptographic verification (message protection and/or proofs). Furthermore, communications between the parties may be only via legacy voice channels. Applicable situations include marine, subterranean, remote expeditions, disaster areas, refugee camps, and high-security installations. This paper then recommends solutions for addressing offline deployments. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Here are the advanced readings to date: Addressing Global/Local Barriers to Adoption of Decentralized Identity Systems by Eric Brown Agent to Agent Communication Protocol Overview by Kyle Den Hartog Blockcerts -- Where we are and what's next by Kim Hamilton Duffy, Anthony Ronning, Lucas Parker, and Peter Scott Can Curation Markets Establish a Sustainable Technology Commons by Sam Chase CapAuth by Manu Sporny, Dave Longley, Chris Webber, and Ganesh Annan A Concept Diagram For RWOT Identity Terms by Andrew Hughes Cryptocurrency Wallets as a Form of Functional Identity by Mikerah Quintyne-Collins and Abdulwasay Mehar Decentralized Error Reporting by Jack Poole Decentralized Identities and eIDAS by Oliver Terbu Decentralized Identity: Hub Authentication & Message Encryption by Daniel Buchner DIDDoc Conventions for Interoperability by Stephen Curran & Olena Mitovska DIDs In DPKI by Greg Slepak DID Resolution Topics by Markus Sabadello Digital Identity for the Homeless by Matthew Wong, T. Tian & CG Chen Exploring Browser Web of Trust Use Cases by Peter Snyder and Ben Livshits Five Mental Models of Identity by Joe Andrieu Identity Hub Permissions / Authorization by Daniel Buchner IPLD as a general pattern for DID Documents by Christian Lundkvist Is a Decentralized Collective Identity Possible? by Heather Vescent Magenc Magnet URIs: Secure Object Permanence for the Web by Christopher Lemmer Webber Measuring Trust by Tyler Yasaka More Control for Identity Holders by Arturo Manzaneda and Ismenia Galvao Nobody REALLY Trusts the Blockchain by Dan Burnett, Shahan Khatchadourian, and Chaals Nevile Not-a-Bot: A Use Case for Decentralized Identity using Proximity Verification to generate a Web of Trust by Moses Ma & Claire Rumore The Political Economy of Naming by Kate Sills A Public Web of Trust of Public Identities by Ouri Poupko and Ehud Shapiro Resource Integrity Proofs by Ganesh Annan, Manu Sporny, Dave Longley, and David Lehn RWoT Tribal Knowledge: Cryptographic and Data Model Requirements by Manu Sporny, Dave Longley, and Chris Webber The Role of Standards in Accelerating Innovation by Michael B. Jones Scoped Presentation Request on Verifiable Credentials by Martin Riedel Secure Crypto-Wallet Introductions by Wolf McNally, Ryan Grant Standards for Agency and Decentralized Information Governance - Early Experience by Adrian Gropper, MD, Michael Chen, MD, and Lydia Fazzio, MD Towards Proof of Person by Peter Watts A Trustless Web-of-Trust by Ouri Poupko The United Humans by Bohdan Andriyiv Verifiable Displays by Kim Hamilton Duffy, Bohdan Andriyiv, and Lucas Parker Verifiable Offline Credentials by Michael Lodder What (and Who) Is In Your Wallet by Darrell O'Donnell Digital Identity for the Homeless by Matthew Wong, T. Tian & CG Chen Zero Trust Computing with DIDs and DADs by Samuel M. Smith Primers These primers overview major topics which are likely to be discussed at the design workshop. If you read nothing else, read these. (But really, read as much as you can!) DID Primer \u2014 Decentralized Identifiers ( extended version also available) Functional Identity Primer \u2014 A different way to look at identity Verifiable Credentials Primer \u2014 the project formerly known as Verifiable Claims DIDs In DPKI - how DIDs fit into Decentralized Public-key Infrastructure","title":"Rebooting the Web of Trust VII: Toronto (September 2018)"},{"location":"rwot7/#rebooting-the-web-of-trust-vii-toronto-september-2018","text":"This repository contains documents related to RWOT7, the seventh Rebooting the Web of Trust design workshop, which ran near Toronto, Canada, on September 26th to 28th, 2018. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community. Watch for our next event March 1st-3rd in Barcelona, Spain.","title":"Rebooting the Web of Trust VII: Toronto (September 2018)"},{"location":"rwot7/#final-papers","text":"","title":"Final Papers"},{"location":"rwot7/#btcr-v01-decisions-text","text":"","title":"BTCR v0.1 Decisions (Text)"},{"location":"rwot7/#kim-hamilton-duffy-christopher-allen-and-dan-pape","text":"The Bitcoin Reference (BTCR) DID method supports DIDs using the Bitcoin blockchain. This method has been under development through Rebooting Web of Trust events and hackathons over the past year. The BTCR method's reliance on the Bitcoin blockchain presents both advantages and design challenges. During RWOT7, the authors made a number of design and implementation decisions -- largely scope-cutting in nature -- in order to lock down a Minimum Viable Product (MVP) version, which we'll refer to as v0.1. This paper documents those decisions, which will apply to the upcoming v0.1 BTCR method specification and associated v0.1 BTCR reference implementation.","title":"Kim Hamilton Duffy, Christopher Allen, and Dan Pape"},{"location":"rwot7/#a-did-for-everything-text","text":"","title":"A DID for Everything (Text)"},{"location":"rwot7/#shaun-conway-andrew-hughes-moses-ma-jack-poole-martin-riedel-samuel-m-smith-phd-and-carsten-stocker","text":"The decentralized identifier (DID) is a new and open standard type of globally unique identifier that offers a model for lifetime-scope portable digital identity that does not depend on any centralized authority and that can never be taken away by third-parties. DIDs are supported by the W3C community and the Decentralized Identity Foundation (DIF). They are the \"atomic units\" of a new layer of decentralized identity infrastructure. However, DIDs can be extended from identifiers for people to any entity, thus identifying everything. We can use DIDs to help us identify and manage objects, machines, or agents through their digital twins; we can expand them to locations, to events, and even to pure data objects, which we refer to as decentralized autonomic data (DAD) items. The paper will present novel use-cases for DIDs and DADs and propose a new cryptographic data structure that is a self-contained blockchain of DADs. This enables the verification of the provenance of a given data flow. It builds on a prior paper and an associated reading.","title":"Shaun Conway, Andrew Hughes, Moses Ma, Jack Poole, Martin Riedel, Samuel M. Smith Ph.D., and Carsten St\u00f6cker"},{"location":"rwot7/#how-to-convince-dad42-of-the-importance-of-self-sovereign-identity-text","text":"","title":"How to Convince Dad* of the Importance of Self-Sovereign Identity (Text)"},{"location":"rwot7/#shannon-appelcline-kenneth-bok-lucas-parker-peter-scott-and-matthew-wong","text":"One of the major problems with bootstrapping self-sovereign identity is that it requires adoption by a large number of people. Pushing self-sovereign identity from the top-down is most likely to result in a technology that\u2019s not actually used, but instead encouraging the average person to demand self-sovereign identity from the bottom-up will result in the organic development of a vibrant, well-utilized decentralized web-of-trust ecosystem. This paper addresses that need by offering arguments to a variety of people who might be reluctant to use self-sovereign identity, uninterested in its possibilities, or oblivious to the dangers of centralization. By focusing on the needs of real people, we hope to also encourage developers, engineers, and software business owners to create the apps that will address their reluctance and fulfill their needs, making self-sovereign identity a reality.","title":"Shannon Appelcline, Kenneth Bok, Lucas Parker, Peter Scott, and Matthew Wong"},{"location":"rwot7/#ipld-as-a-general-pattern-for-did-documents-and-verifiable-claims-text","text":"","title":"IPLD as a general pattern for DID documents and Verifiable Claims (Text)"},{"location":"rwot7/#jonnycrunch-anthony-ronning-kim-duffy-christian-lundkvist","text":"Since the emergence of the Decentralized Identifier (DID) specification at the Fall 2016 Rebooting the Web of Trust [1], numerous DID method specifications have appeared. Each DID method specification defines how to resolve a cryptographically-tied DID document given a method-specific identifier. In this paper, we describe a way to represent the DID document as a content-addressed Merkle Directed Acyclic Graph (DAG) using Interplanetary Linked Data (IPLD). This technique enables more cost-efficient, scaleable creation of DIDs and can be applied across different DID method specifications.","title":"jonnycrunch, Anthony Ronning, Kim Duffy, Christian Lundkvist"},{"location":"rwot7/#peer-to-peer-degrees-of-trust-text","text":"","title":"Peer to Peer Degrees of Trust (Text)"},{"location":"rwot7/#harrison-stahl-titus-capilnean-peter-snyder-and-tyler-yasaka","text":"Aunthenticity is a challenge for any identity solution. In the physical world, at least in America, it is not difficult to change one's identity. In the digital world, there is the problem of bots. The botnet detection market is expected to be worth over one billion USD by 2023, in a landscape where most digital activity is still heavily centralized. These centralized digital solutions have the advantage of being able to track IP addresses, request phone verification, and present CAPTCHAs to users in order to authenticate them. If this problem is so difficult to solve in the centralized world, how much more challenging will it be in the decentralized world, where none of these techniques are available? In this paper, we explore the idea of using a web of trust as a tool to add authenticity to decentralized identifiers (DIDs). We define a framework for deriving relative trust degrees using a given trust metric: a \"trustworthiness\" score for a given identity from the perspective of another identity. It is our intent that this framework may be used as a starting point for an ongoing exploration of graph-based, decentralized trust. We believe this approach may ultimately be used as a foundation for decentralized reputation.","title":"Harrison Stahl, Titus Capilnean, Peter Snyder, and Tyler Yasaka"},{"location":"rwot7/#resource-integrity-proofs-text","text":"","title":"Resource Integrity Proofs (Text)"},{"location":"rwot7/#ganesh-annan-and-kim-hamilton-duffy","text":"Currently, the Web provides a simple yet powerful mechanism for the dissemination of information via links. Unfortunately, there is no generalized mechanism that enables verifying that a fetched resource has been delivered without unexpected manipulation. Would it be possible to create an extensible and multipurpose cryptographic link that provides discoverability, integrity, and scheme agility? This paper proposes a linking solution that decouples integrity information from link and resource syntaxes, enabling verification of any representation of a resource from any type of link. We call this approach Resource Integrity Proofs (RIPs). RIPs provide a succinct way to link to resources with cryptographically verifiable content integrity. RIPs can be combined with blockchain technology to create discoverable proofs of existence to off-chain resources.","title":"Ganesh Annan and Kim Hamilton Duffy"},{"location":"rwot7/#use-cases-and-proposed-solutions-for-verifiable-offline-credentials-text","text":"","title":"Use Cases and Proposed Solutions for Verifiable Offline Credentials (Text)"},{"location":"rwot7/#michael-lodder-samantha-mathews-chase-and-wolf-mcnally","text":"In this paper we cover various scenarios where some or all parties have intermittent, unreliable, untrusted, insecure, or no network access, but require cryptographic verification (message protection and/or proofs). Furthermore, communications between the parties may be only via legacy voice channels. Applicable situations include marine, subterranean, remote expeditions, disaster areas, refugee camps, and high-security installations. This paper then recommends solutions for addressing offline deployments.","title":"Michael Lodder, Samantha Mathews Chase, and Wolf McNally"},{"location":"rwot7/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Here are the advanced readings to date: Addressing Global/Local Barriers to Adoption of Decentralized Identity Systems by Eric Brown Agent to Agent Communication Protocol Overview by Kyle Den Hartog Blockcerts -- Where we are and what's next by Kim Hamilton Duffy, Anthony Ronning, Lucas Parker, and Peter Scott Can Curation Markets Establish a Sustainable Technology Commons by Sam Chase CapAuth by Manu Sporny, Dave Longley, Chris Webber, and Ganesh Annan A Concept Diagram For RWOT Identity Terms by Andrew Hughes Cryptocurrency Wallets as a Form of Functional Identity by Mikerah Quintyne-Collins and Abdulwasay Mehar Decentralized Error Reporting by Jack Poole Decentralized Identities and eIDAS by Oliver Terbu Decentralized Identity: Hub Authentication & Message Encryption by Daniel Buchner DIDDoc Conventions for Interoperability by Stephen Curran & Olena Mitovska DIDs In DPKI by Greg Slepak DID Resolution Topics by Markus Sabadello Digital Identity for the Homeless by Matthew Wong, T. Tian & CG Chen Exploring Browser Web of Trust Use Cases by Peter Snyder and Ben Livshits Five Mental Models of Identity by Joe Andrieu Identity Hub Permissions / Authorization by Daniel Buchner IPLD as a general pattern for DID Documents by Christian Lundkvist Is a Decentralized Collective Identity Possible? by Heather Vescent Magenc Magnet URIs: Secure Object Permanence for the Web by Christopher Lemmer Webber Measuring Trust by Tyler Yasaka More Control for Identity Holders by Arturo Manzaneda and Ismenia Galvao Nobody REALLY Trusts the Blockchain by Dan Burnett, Shahan Khatchadourian, and Chaals Nevile Not-a-Bot: A Use Case for Decentralized Identity using Proximity Verification to generate a Web of Trust by Moses Ma & Claire Rumore The Political Economy of Naming by Kate Sills A Public Web of Trust of Public Identities by Ouri Poupko and Ehud Shapiro Resource Integrity Proofs by Ganesh Annan, Manu Sporny, Dave Longley, and David Lehn RWoT Tribal Knowledge: Cryptographic and Data Model Requirements by Manu Sporny, Dave Longley, and Chris Webber The Role of Standards in Accelerating Innovation by Michael B. Jones Scoped Presentation Request on Verifiable Credentials by Martin Riedel Secure Crypto-Wallet Introductions by Wolf McNally, Ryan Grant Standards for Agency and Decentralized Information Governance - Early Experience by Adrian Gropper, MD, Michael Chen, MD, and Lydia Fazzio, MD Towards Proof of Person by Peter Watts A Trustless Web-of-Trust by Ouri Poupko The United Humans by Bohdan Andriyiv Verifiable Displays by Kim Hamilton Duffy, Bohdan Andriyiv, and Lucas Parker Verifiable Offline Credentials by Michael Lodder What (and Who) Is In Your Wallet by Darrell O'Donnell Digital Identity for the Homeless by Matthew Wong, T. Tian & CG Chen Zero Trust Computing with DIDs and DADs by Samuel M. Smith","title":"Topics &amp; Advance Readings"},{"location":"rwot7/#primers","text":"These primers overview major topics which are likely to be discussed at the design workshop. If you read nothing else, read these. (But really, read as much as you can!) DID Primer \u2014 Decentralized Identifiers ( extended version also available) Functional Identity Primer \u2014 A different way to look at identity Verifiable Credentials Primer \u2014 the project formerly known as Verifiable Claims DIDs In DPKI - how DIDs fit into Decentralized Public-key Infrastructure","title":"Primers"},{"location":"rwot8/","text":"Rebooting the Web of Trust VIII: Barcelona (March 2019) This repository contains documents related to RWOT8, the eighth Rebooting the Web of Trust design workshop, which ran in Barcelona, Spain on March 1st to 3rd, 2019. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Visit http://rwot8.eventbrite.com for more information and to purchase tickets. Event details for attendees (schedule, hotels, transportation) (pdf 14MB) Draft Documents These are the documents currently in process from RWOT8. Paper Lead CRUD and multi-device for IPID Andr\u00e9 Cruz Crypto Jurisdiction Toni DID Key Management in the Browser Alberto DID Spec Work Manu The Digital Citizen Chris C. Driving Adoption Based on Basic Human Needs Sam Enabling a Web of Trust with Self-Signed and Self-Issued Credentials Nader Establishing the Identity of the Issuer by the Verifier in Verifiable Credentials Matt How SSI Will Survive Capitalism Adrian G. Human-Meaningful Names for SSI Boyma Identity Containers Alex P. JORAM Illustrations Katie JORAM 2 Joe LibP2P for DID Auth jonnycrunch Peer DID Method Spec Report Brent Satyrn Joe Social Key Recovery #1: Shamir Secret Sharing Best Practices Christopher A. Social Key Recovery #1a: New SSS Library Daan & Mark Social Key Recovery #2: Evaluating Social Schemes for Recovering Control of an Identifier Peg Terminology Reiks Sociolegal frameworks for the phygital age Elizabeth Understanding DIDs Drummond Use Cases and Research Directions for self-sovereign publication and journalism Juan C. Using OpenID Connect Self-Issued to Achieve DID Auth Dmitri Z Using OpenID Connect Self-Issued to achieve Verifiable Credentials Presentation Dmitri Z Verifiable Credential Museum/Playground Ryan Topics & Advance Readings Please see Topics & Advance Readings for a list of readings prepared in advance of the conference.","title":"Rebooting the Web of Trust VIII: Barcelona (March 2019)"},{"location":"rwot8/#rebooting-the-web-of-trust-viii-barcelona-march-2019","text":"This repository contains documents related to RWOT8, the eighth Rebooting the Web of Trust design workshop, which ran in Barcelona, Spain on March 1st to 3rd, 2019. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Visit http://rwot8.eventbrite.com for more information and to purchase tickets. Event details for attendees (schedule, hotels, transportation) (pdf 14MB)","title":"Rebooting the Web of Trust VIII: Barcelona (March 2019)"},{"location":"rwot8/#draft-documents","text":"These are the documents currently in process from RWOT8. Paper Lead CRUD and multi-device for IPID Andr\u00e9 Cruz Crypto Jurisdiction Toni DID Key Management in the Browser Alberto DID Spec Work Manu The Digital Citizen Chris C. Driving Adoption Based on Basic Human Needs Sam Enabling a Web of Trust with Self-Signed and Self-Issued Credentials Nader Establishing the Identity of the Issuer by the Verifier in Verifiable Credentials Matt How SSI Will Survive Capitalism Adrian G. Human-Meaningful Names for SSI Boyma Identity Containers Alex P. JORAM Illustrations Katie JORAM 2 Joe LibP2P for DID Auth jonnycrunch Peer DID Method Spec Report Brent Satyrn Joe Social Key Recovery #1: Shamir Secret Sharing Best Practices Christopher A. Social Key Recovery #1a: New SSS Library Daan & Mark Social Key Recovery #2: Evaluating Social Schemes for Recovering Control of an Identifier Peg Terminology Reiks Sociolegal frameworks for the phygital age Elizabeth Understanding DIDs Drummond Use Cases and Research Directions for self-sovereign publication and journalism Juan C. Using OpenID Connect Self-Issued to Achieve DID Auth Dmitri Z Using OpenID Connect Self-Issued to achieve Verifiable Credentials Presentation Dmitri Z Verifiable Credential Museum/Playground Ryan","title":"Draft Documents"},{"location":"rwot8/#topics-advance-readings","text":"Please see Topics & Advance Readings for a list of readings prepared in advance of the conference.","title":"Topics &amp; Advance Readings"},{"location":"RWoT1/","text":"Rebooting the Web of Trust I: San Francisco (November 2015) This repository contains documents related to RWOT1, the first Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on November 3rd & 4th, 2015. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop successfully completed its five papers, which are now all available online: Rebranding the Web of Trust by Shannon Appelcline, Dave Crocker, Randall Farmer, and Justin Newton A history of the Web of Trust and a look at what the term could mean for the future. Opportunities Created by the Web of Trust for Controlling and Leveraging Personal Data by du5t, Kaliya \"Identity Woman\" Young (@identitywoman), John Edge, Drummond Reed, and Noah Thorp Five use cases, from two relatively simple cases of managing selective disclosure to the most extreme case of establishing government-verifiable credentials from nothing for a stateless refugee. Decentralized Public Key Infrastructure by Christopher Allen, Arthur Brock, Vitalik Buterin, Jon Callas, Duke Dorje, Christian Lundkvist, Pavel Kravchenko, Jude Nelson, Drummond Reed, Markus Sabadello, Greg Slepak, Noah Thorp, and Harlan T Wood A massive overview of a decentralized public-key infrastructure (DPKI). Smart Signatures by Christopher Allen, Greg Maxwell, Peter Todd, Ryan Shea, Pieter Wuille, Joseph Bonneau, Joseph Poon, and Tyler Close A system to explicitly outline and fully program conditions for verification, inspired by Bitcoin Script. Creating the New World of Trust by Shannon Appelcline A summary of the next step for the Rebooting the Web of Trust group. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Index"},{"location":"RWoT1/#rebooting-the-web-of-trust-i-san-francisco-november-2015","text":"This repository contains documents related to RWOT1, the first Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on November 3rd & 4th, 2015. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust I: San Francisco (November 2015)"},{"location":"RWoT1/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"RWoT1/#completed-papers","text":"The design workshop successfully completed its five papers, which are now all available online:","title":"Completed Papers"},{"location":"RWoT1/#rebranding-the-web-of-trust","text":"","title":"Rebranding the Web of Trust"},{"location":"RWoT1/#by-shannon-appelcline-dave-crocker-randall-farmer-and-justin-newton","text":"A history of the Web of Trust and a look at what the term could mean for the future.","title":"by Shannon Appelcline, Dave Crocker, Randall Farmer, and Justin Newton"},{"location":"RWoT1/#opportunities-created-by-the-web-of-trust-for-controlling-and-leveraging-personal-data","text":"","title":"Opportunities Created by the Web of Trust for Controlling and Leveraging Personal Data"},{"location":"RWoT1/#by-du5t-kaliya-identity-woman-young-identitywoman-john-edge-drummond-reed-and-noah-thorp","text":"Five use cases, from two relatively simple cases of managing selective disclosure to the most extreme case of establishing government-verifiable credentials from nothing for a stateless refugee.","title":"by du5t, Kaliya \"Identity Woman\" Young (@identitywoman), John Edge, Drummond Reed, and Noah Thorp"},{"location":"RWoT1/#decentralized-public-key-infrastructure","text":"","title":"Decentralized Public Key Infrastructure"},{"location":"RWoT1/#by-christopher-allen-arthur-brock-vitalik-buterin-jon-callas-duke-dorje-christian-lundkvist-pavel-kravchenko-jude-nelson-drummond-reed-markus-sabadello-greg-slepak-noah-thorp-and-harlan-t-wood","text":"A massive overview of a decentralized public-key infrastructure (DPKI).","title":"by Christopher Allen, Arthur Brock, Vitalik Buterin, Jon Callas, Duke Dorje, Christian Lundkvist, Pavel Kravchenko, Jude Nelson, Drummond Reed, Markus Sabadello, Greg Slepak, Noah Thorp, and Harlan T Wood"},{"location":"RWoT1/#smart-signatures","text":"","title":"Smart Signatures"},{"location":"RWoT1/#by-christopher-allen-greg-maxwell-peter-todd-ryan-shea-pieter-wuille-joseph-bonneau-joseph-poon-and-tyler-close","text":"A system to explicitly outline and fully program conditions for verification, inspired by Bitcoin Script.","title":"by Christopher Allen, Greg Maxwell, Peter Todd, Ryan Shea, Pieter Wuille, Joseph Bonneau, Joseph Poon, and Tyler Close"},{"location":"RWoT1/#creating-the-new-world-of-trust","text":"","title":"Creating the New World of Trust"},{"location":"RWoT1/#by-shannon-appelcline","text":"A summary of the next step for the Rebooting the Web of Trust group.","title":"by Shannon Appelcline"},{"location":"RWoT1/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"RWoT1/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"RWoT1/LICENSE-CC-BY-4.0/","text":"LICENSE-CC-BY-4.0.md UNLESS OTHERWISE NOTED, THE CONTENTS OF THIS DIRECTORY ARE LICENSED UNDER THE CREATIVE COMMONS ATTRIBUTION 4.0 INTERNATIONAL LICENSE License Summary of CC-BY 4.0 International This section is a human-readable summary of (and not a substitute for) the full license included below. You are free to: Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution \u2014 You must give appropriate credit , provide a link to the license , and indicate if changes were made . You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation . No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. Creative Commons Attribution 4.0 International License FROM: https://creativecommons.org/licenses/by/4.0/ Creative Commons Corporation (\u201cCreative Commons\u201d) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \u201cas-is\u201d basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible. Using Creative Commons Public Licenses Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses. Considerations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. More considerations for licensors . Considerations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor\u2019s permission is not necessary for any reason\u2013for example, because of any applicable exception or limitation to copyright\u2013then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More considerations for the public . Creative Commons Attribution 4.0 International Public License By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions. Section 1 \u2013 Definitions. a. Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image. b. Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License. c. Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights. d. Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements. e. Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material. f. Licensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License. g. Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license. h. Licensor means the individual(s) or entity(ies) granting rights under this Public License. i. Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them. j. Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world. k. You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning. Section 2 \u2013 Scope. a. License grant. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to: A. reproduce and Share the Licensed Material, in whole or in part; and B. produce, reproduce, and Share Adapted Material. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions. Term. The term of this Public License is specified in Section 6(a). Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material. Downstream recipients. A. Offer from the Licensor \u2013 Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License. B. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i). b. Other rights. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise. Patent and trademark rights are not licensed under this Public License. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties. Section 3 \u2013 License Conditions. Your exercise of the Licensed Rights is expressly made subject to the following conditions. a. Attribution. If You Share the Licensed Material (including in modified form), You must: A. retain the following if it is supplied by the Licensor with the Licensed Material: > i. identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated); > ii. a copyright notice; > iii. a notice that refers to this Public License; > iv. a notice that refers to the disclaimer of warranties; > v. a URI or hyperlink to the Licensed Material to the extent reasonably practicable; B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable. If You Share Adapted Material You produce, the Adapter's License You apply must not prevent recipients of the Adapted Material from complying with this Public License. Section 4 \u2013 Sui Generis Database Rights. Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material: a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database; b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database. For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights. Section 5 \u2013 Disclaimer of Warranties and Limitation of Liability. a. Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You. b. To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You. c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability. Section 6 \u2013 Term and Termination. a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically. b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates: automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or upon express reinstatement by the Licensor. For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License. c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License. d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License. Section 7 \u2013 Other Terms and Conditions. a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed. b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License. Section 8 \u2013 Interpretation. a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License. b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions. c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor. d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority. Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the \u201cLicensor.\u201d Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies , Creative Commons does not authorize the use of the trademark \u201cCreative Commons\u201d or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses. Creative Commons may be contacted at creativecommons.org","title":"LICENSE-CC-BY-4.0.md"},{"location":"RWoT1/LICENSE-CC-BY-4.0/#license-cc-by-40md","text":"UNLESS OTHERWISE NOTED, THE CONTENTS OF THIS DIRECTORY ARE LICENSED UNDER THE CREATIVE COMMONS ATTRIBUTION 4.0 INTERNATIONAL LICENSE","title":"LICENSE-CC-BY-4.0.md"},{"location":"RWoT1/LICENSE-CC-BY-4.0/#license-summary-of-cc-by-40-international","text":"This section is a human-readable summary of (and not a substitute for) the full license included below.","title":"License Summary of CC-BY 4.0 International"},{"location":"RWoT1/LICENSE-CC-BY-4.0/#you-are-free-to","text":"Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms.","title":"You are free to:"},{"location":"RWoT1/LICENSE-CC-BY-4.0/#under-the-following-terms","text":"Attribution \u2014 You must give appropriate credit , provide a link to the license , and indicate if changes were made . You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"Under the following terms:"},{"location":"RWoT1/LICENSE-CC-BY-4.0/#notices","text":"You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation . No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Notices:"},{"location":"RWoT1/LICENSE-CC-BY-4.0/#creative-commons-attribution-40-international-license","text":"FROM: https://creativecommons.org/licenses/by/4.0/ Creative Commons Corporation (\u201cCreative Commons\u201d) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an \u201cas-is\u201d basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.","title":"Creative Commons Attribution 4.0 International License"},{"location":"RWoT1/LICENSE-CC-BY-4.0/#using-creative-commons-public-licenses","text":"Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses. Considerations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. More considerations for licensors . Considerations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor\u2019s permission is not necessary for any reason\u2013for example, because of any applicable exception or limitation to copyright\u2013then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More considerations for the public .","title":"Using Creative Commons Public Licenses"},{"location":"RWoT1/LICENSE-CC-BY-4.0/#creative-commons-attribution-40-international-public-license","text":"By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.","title":"Creative Commons Attribution 4.0 International Public License"},{"location":"RWoT1/LICENSE-CC-BY-4.0/#section-1-definitions","text":"a. Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image. b. Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License. c. Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights. d. Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements. e. Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material. f. Licensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License. g. Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license. h. Licensor means the individual(s) or entity(ies) granting rights under this Public License. i. Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them. j. Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world. k. You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.","title":"Section 1 \u2013 Definitions."},{"location":"RWoT1/LICENSE-CC-BY-4.0/#section-2-scope","text":"a. License grant. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to: A. reproduce and Share the Licensed Material, in whole or in part; and B. produce, reproduce, and Share Adapted Material. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions. Term. The term of this Public License is specified in Section 6(a). Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material. Downstream recipients. A. Offer from the Licensor \u2013 Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License. B. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i). b. Other rights. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise. Patent and trademark rights are not licensed under this Public License. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.","title":"Section 2 \u2013 Scope."},{"location":"RWoT1/LICENSE-CC-BY-4.0/#section-3-license-conditions","text":"Your exercise of the Licensed Rights is expressly made subject to the following conditions. a. Attribution. If You Share the Licensed Material (including in modified form), You must: A. retain the following if it is supplied by the Licensor with the Licensed Material: > i. identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated); > ii. a copyright notice; > iii. a notice that refers to this Public License; > iv. a notice that refers to the disclaimer of warranties; > v. a URI or hyperlink to the Licensed Material to the extent reasonably practicable; B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable. If You Share Adapted Material You produce, the Adapter's License You apply must not prevent recipients of the Adapted Material from complying with this Public License.","title":"Section 3 \u2013 License Conditions."},{"location":"RWoT1/LICENSE-CC-BY-4.0/#section-4-sui-generis-database-rights","text":"Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material: a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database; b. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and c. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database. For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.","title":"Section 4 \u2013 Sui Generis Database Rights."},{"location":"RWoT1/LICENSE-CC-BY-4.0/#section-5-disclaimer-of-warranties-and-limitation-of-liability","text":"a. Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You. b. To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You. c. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.","title":"Section 5 \u2013 Disclaimer of Warranties and Limitation of Liability."},{"location":"RWoT1/LICENSE-CC-BY-4.0/#section-6-term-and-termination","text":"a. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically. b. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates: automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or upon express reinstatement by the Licensor. For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License. c. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License. d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License.","title":"Section 6 \u2013 Term and Termination."},{"location":"RWoT1/LICENSE-CC-BY-4.0/#section-7-other-terms-and-conditions","text":"a. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed. b. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.","title":"Section 7 \u2013 Other Terms and Conditions."},{"location":"RWoT1/LICENSE-CC-BY-4.0/#section-8-interpretation","text":"a. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License. b. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions. c. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor. d. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority. Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the \u201cLicensor.\u201d Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies , Creative Commons does not authorize the use of the trademark \u201cCreative Commons\u201d or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses. Creative Commons may be contacted at creativecommons.org","title":"Section 8 \u2013 Interpretation."},{"location":"RWoT2/","text":"Rebooting the Web of Trust II: ID2020 (May 2016) This repository contains documents related to RWOT2, the second Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on May 21st & 22nd, 2016. It was run in conjunction with the UN ID2020 Summit on Identity, which occurred at the UN in New York on May 20th, 2016. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the ID 2020 Design DesignShop, all participants were requested to submit a 1 or 2 page topics paper to be shared with other attendees on either: A specific decentralized identity use case related to the topic of the UN ID 2020 Summit A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop successfully completed its five papers, which are now all available online: Identity Crisis: Clearer Identity Through Correlation by Joe Andrieu, Kevin Gannon, Igor Kruiper, Ajit Tripathi, and Gary Zimmerman A new way to look at identity, as correlation over time. Powering the Physician-Patient Relationship with HIE of One Blockchain Health IT by Adrian Gropper, MD Using Blockchains and DIDs for physician-patient interactions. Protecting Digital Identities in Developing Countries by Wayne Hennessy-Barrett A real-world use case, describing issues of identity in the developing world. Requirements for DIDs (Decentralized Identifiers) by Drummond Reed and Les Chasen The first of a series of papers abou tproducing a concrete DID system. Smarter Signatures: Experiments in Verifications by Christopher Allen and Shannon Appelcline A look at uses and requirements of next-generation smart signature systems. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Index"},{"location":"RWoT2/#rebooting-the-web-of-trust-ii-id2020-may-2016","text":"This repository contains documents related to RWOT2, the second Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on May 21st & 22nd, 2016. It was run in conjunction with the UN ID2020 Summit on Identity, which occurred at the UN in New York on May 20th, 2016. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust II: ID2020 (May 2016)"},{"location":"RWoT2/#topics-advance-readings","text":"In advance of the ID 2020 Design DesignShop, all participants were requested to submit a 1 or 2 page topics paper to be shared with other attendees on either: A specific decentralized identity use case related to the topic of the UN ID 2020 Summit A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"RWoT2/#completed-papers","text":"The design workshop successfully completed its five papers, which are now all available online:","title":"Completed Papers"},{"location":"RWoT2/#identity-crisis-clearer-identity-through-correlation","text":"","title":"Identity Crisis: Clearer Identity Through Correlation"},{"location":"RWoT2/#by-joe-andrieu-kevin-gannon-igor-kruiper-ajit-tripathi-and-gary-zimmerman","text":"A new way to look at identity, as correlation over time.","title":"by Joe Andrieu, Kevin Gannon, Igor Kruiper, Ajit Tripathi, and Gary Zimmerman"},{"location":"RWoT2/#powering-the-physician-patient-relationship-with-hie-of-one-blockchain-health-it","text":"","title":"Powering the Physician-Patient Relationship with HIE of One Blockchain Health IT"},{"location":"RWoT2/#by-adrian-gropper-md","text":"Using Blockchains and DIDs for physician-patient interactions.","title":"by Adrian Gropper, MD"},{"location":"RWoT2/#protecting-digital-identities-in-developing-countries","text":"","title":"Protecting Digital Identities in Developing Countries"},{"location":"RWoT2/#by-wayne-hennessy-barrett","text":"A real-world use case, describing issues of identity in the developing world.","title":"by Wayne Hennessy-Barrett"},{"location":"RWoT2/#requirements-for-dids-decentralized-identifiers","text":"","title":"Requirements for DIDs (Decentralized Identifiers)"},{"location":"RWoT2/#by-drummond-reed-and-les-chasen","text":"The first of a series of papers abou tproducing a concrete DID system.","title":"by Drummond Reed and Les Chasen"},{"location":"RWoT2/#smarter-signatures-experiments-in-verifications","text":"","title":"Smarter Signatures: Experiments in Verifications"},{"location":"RWoT2/#by-christopher-allen-and-shannon-appelcline","text":"A look at uses and requirements of next-generation smart signature systems.","title":"by Christopher Allen and Shannon Appelcline"},{"location":"RWoT2/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"RWoT2/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"RWoT3/","text":"Rebooting the Web of Trust III: San Francisco (October 2016) This repository contains documents related to RWOT3, the third Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on October 19th-21st, 2016. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop exceeded its mandate by producing seven papers, which are now all available online: DID (Decentralized Identifier) Data Model and Generic Syntax 1.0 Implementer\u2019s Draft 01 by Drummond Reed, Les Chasen, Christopher Allen, and Ryan Grant The complete draft of the Decentralized IDentifier (DID) model and syntac, a project that has run through the RWOT workshops to date. Digital Verification Advancements at RWoT III by Manu Sporny with Christopher Allen, Harlan Wood, and Jason Law A short overview of enhancements to Digital Verification that came out of RWOT III. Embedding Human Wisdom in Our Digital Tomorrow by Daniel Hardman, Kaliya \u201cIdentity Woman\u201d Young, and Matthew Schutte A discussion of the dangers of transferring wisdom into the digital world, seen through the lenses of vulnerability, shadows, healing, tensions, complexity and gestalt, and organizational choices. Hubs by Daniel Buchner, Wayne Vaughan, and Ryan Shea An overview of the hubs datastore system. Joram 1.0.0 by Joe Andrieu and Bob Clint An Information Lifecycle Engagement Model that offers a use case for a Syrian refugee. Portable Reputation Toolkit Use Cases by Christopher Allen, Tim Daubensch\u00fctz, Manu Sporny, Noah Thorp, Harlan Wood, Glenn Willen, and Alessandro Voto A model and proof-of-concept implementation for decentralized verification. Smart Consent Protocol by Dr. Shaun Conway, Lohan Spies, Jonathan Endersby, and Tim Daubensch\u00fctz Bringing together COALA IP and Consent to deal with digital intellectual property. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Index"},{"location":"RWoT3/#rebooting-the-web-of-trust-iii-san-francisco-october-2016","text":"This repository contains documents related to RWOT3, the third Rebooting the Web of Trust design workshop, which ran in San Francisco, CA, on October 19th-21st, 2016. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust III: San Francisco (October 2016)"},{"location":"RWoT3/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"RWoT3/#completed-papers","text":"The design workshop exceeded its mandate by producing seven papers, which are now all available online:","title":"Completed Papers"},{"location":"RWoT3/#did-decentralized-identifier-data-model-and-generic-syntax-10-implementers-draft-01","text":"","title":"DID (Decentralized Identifier) Data Model and Generic Syntax 1.0 Implementer\u2019s Draft 01"},{"location":"RWoT3/#by-drummond-reed-les-chasen-christopher-allen-and-ryan-grant","text":"The complete draft of the Decentralized IDentifier (DID) model and syntac, a project that has run through the RWOT workshops to date.","title":"by Drummond Reed, Les Chasen, Christopher Allen, and Ryan Grant"},{"location":"RWoT3/#digital-verification-advancements-at-rwot-iii","text":"","title":"Digital Verification Advancements at RWoT III"},{"location":"RWoT3/#by-manu-sporny-with-christopher-allen-harlan-wood-and-jason-law","text":"A short overview of enhancements to Digital Verification that came out of RWOT III.","title":"by Manu Sporny with Christopher Allen, Harlan Wood, and Jason Law"},{"location":"RWoT3/#embedding-human-wisdom-in-our-digital-tomorrow","text":"","title":"Embedding Human Wisdom in Our Digital Tomorrow"},{"location":"RWoT3/#by-daniel-hardman-kaliya-identity-woman-young-and-matthew-schutte","text":"A discussion of the dangers of transferring wisdom into the digital world, seen through the lenses of vulnerability, shadows, healing, tensions, complexity and gestalt, and organizational choices.","title":"by Daniel Hardman, Kaliya \u201cIdentity Woman\u201d Young, and Matthew Schutte"},{"location":"RWoT3/#hubs","text":"","title":"Hubs"},{"location":"RWoT3/#by-daniel-buchner-wayne-vaughan-and-ryan-shea","text":"An overview of the hubs datastore system.","title":"by Daniel Buchner, Wayne Vaughan, and Ryan Shea"},{"location":"RWoT3/#joram-100","text":"","title":"Joram 1.0.0"},{"location":"RWoT3/#by-joe-andrieu-and-bob-clint","text":"An Information Lifecycle Engagement Model that offers a use case for a Syrian refugee.","title":"by Joe Andrieu and Bob Clint"},{"location":"RWoT3/#portable-reputation-toolkit-use-cases","text":"","title":"Portable Reputation Toolkit Use Cases"},{"location":"RWoT3/#by-christopher-allen-tim-daubenschutz-manu-sporny-noah-thorp-harlan-wood-glenn-willen-and-alessandro-voto","text":"A model and proof-of-concept implementation for decentralized verification.","title":"by Christopher Allen, Tim Daubensch\u00fctz, Manu Sporny, Noah Thorp, Harlan Wood, Glenn Willen, and Alessandro Voto"},{"location":"RWoT3/#smart-consent-protocol","text":"","title":"Smart Consent Protocol"},{"location":"RWoT3/#by-dr-shaun-conway-lohan-spies-jonathan-endersby-and-tim-daubenschutz","text":"Bringing together COALA IP and Consent to deal with digital intellectual property.","title":"by Dr. Shaun Conway, Lohan Spies, Jonathan Endersby, and Tim Daubensch\u00fctz"},{"location":"RWoT3/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"RWoT3/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"RWoT4/","text":"Rebooting the Web of Trust IV: Paris (April 2017) This repository contains documents related to RWOT4, the fourth Rebooting the Web of Trust design workshop, which ran in Paris, France, on April 19th-21st, 2017. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop met its goal of three to five white papers with three total. Design Considerations for Decentralized Reputation Systems by Angus Champion de Crespigny, Dmitry Khovratovich, Florent Blondeau, Klara Sok, Philippe Honigman, Nikolaos Alexopoulos, Fabien Petitcolas, and Shaun Conway Ten design considerations for the creation of decentralized reputation systems. LD Signature Format Alignment by Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello, and Manu Sporny Bringing JSON-LD signatures into alignment with JOSE JSON Web Signature (JWS) standards. Re-Imagining What Users Really Want by Joe Andrieu, Frederic Engel, Adam Lake, Moses Ma, Olivier Maas, and Mark van der Waal Five people, five opportunities for self-sovereign identity. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Index"},{"location":"RWoT4/#rebooting-the-web-of-trust-iv-paris-april-2017","text":"This repository contains documents related to RWOT4, the fourth Rebooting the Web of Trust design workshop, which ran in Paris, France, on April 19th-21st, 2017. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust IV: Paris (April 2017)"},{"location":"RWoT4/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"RWoT4/#completed-papers","text":"The design workshop met its goal of three to five white papers with three total.","title":"Completed Papers"},{"location":"RWoT4/#design-considerations-for-decentralized-reputation-systems","text":"","title":"Design Considerations for Decentralized Reputation Systems"},{"location":"RWoT4/#by-angus-champion-de-crespigny-dmitry-khovratovich-florent-blondeau-klara-sok-philippe-honigman-nikolaos-alexopoulos-fabien-petitcolas-and-shaun-conway","text":"Ten design considerations for the creation of decentralized reputation systems.","title":"by Angus Champion de Crespigny, Dmitry Khovratovich, Florent Blondeau, Klara Sok, Philippe Honigman, Nikolaos Alexopoulos, Fabien Petitcolas, and Shaun Conway"},{"location":"RWoT4/#ld-signature-format-alignment","text":"","title":"LD Signature Format Alignment"},{"location":"RWoT4/#by-kim-hamilton-duffy-rodolphe-marques-markus-sabadello-and-manu-sporny","text":"Bringing JSON-LD signatures into alignment with JOSE JSON Web Signature (JWS) standards.","title":"by Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello, and Manu Sporny"},{"location":"RWoT4/#re-imagining-what-users-really-want","text":"","title":"Re-Imagining What Users Really Want"},{"location":"RWoT4/#by-joe-andrieu-frederic-engel-adam-lake-moses-ma-olivier-maas-and-mark-van-der-waal","text":"Five people, five opportunities for self-sovereign identity.","title":"by Joe Andrieu, Frederic Engel, Adam Lake, Moses Ma, Olivier Maas, and Mark van der Waal"},{"location":"RWoT4/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"RWoT4/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"RWoT4/ld-signatures/","text":"LD Signature Format Alignment An Abstract from Rebooting the Web of Trust IV Design Workshop By Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello, Manu Sporny Abstract The goal of the \"LD Signature Format Alignment\" Working Group at Rebooting the Web of Trust IV was to investigate the feasibility and impact of the proposed 2017 RSA Signature Suite spec, which brings JSON-LD signatures into alignment with the JOSE JSON Web Signature (JWS) standards. The 2017 RSA Signature Suite is based on RFC 7797 , the JSON Web Signature (JWS) Unencoded Payload Option specification. This approach avoids past concerns about JWT raised in the LD signature adopters , including: Increased space consumption associated with base-64 encoding. Difficulty of nesting or chaining signatures, leading to data duplication. Use of a format that is not a JSON object, preventing ability to rely exclusively on a JSON document-based storage engine (while preserving the signature). Using unencoded payloads with detached content, as described in the introduction of RFC 7797 , addresses these concerns and helps in cases in which \"...the payload may be very large and where means are already in place to enable the payload to be communicated between the parties without modifications.\" This avoids unnecessary copying and transformations which can result \"significant space and time improvements\" when working with large payloads. Our working group had two primary questions about the proposed 2017 RSA Signature Suite: Is the specification sufficiently clear for implementors? Is there a negative usability impact to LD signature implementations using this signature suite? To answer these questions, we developed prototypes for the suite in several key programming languages to assess: Availability of library support for JWS unencoded payload options Impact to existing LD signature implementations, e.g. jsonld-signatures library Impact to usability of Verifiable Claims (and others) using JSON-LD signatures with this signature suite. Status We accomplished our goals as follows: We delivered prototypes for the 2017 RSA Signature Suite that provided sufficient confidence to move forward with the proposed aligned signature approach. We verified that there was no significant impact to existing LD signature implementations and usability in general. Specifically, unencoded payloads with detached content allows LD signatures to be compatible with JWS while avoiding the concerns raised in the summary of past concerns described above. The major obstacle we encountered while performing this work was the lack of JSON Web Signature library support for unencoded payloads, which is addressed in \"Next Steps\". Implementations of LD JWS signing The following prototypes were developed: For Javascript/Node.js: https://github.com/ WebOfTrustInfo/ld-signatures-js (this is a fork of JSON-LD signatures official library) For Python: https://github.com/ WebOfTrustInfo/ld-signatures-python For Java: https://github.com/ WebOfTrustInfo/ld-signatures-java JSON-LD JWS Implementation Guidance (cross-platform) A white paper, which follows, describes the precise differences between existing LD signatures and the new approach. Next Steps The primary gap in developing these prototypes, which accounted for most of our development work, was lack of library support for JWS unencoded payloads. To work around this limitation, our implementations mirrored the only implementation we found, available in the JOSE PHP library . A cleaner solution that we propose is to recraft our prototypes as JWS unencoded payload libraries. Such a library would expose simple sign-and-verify APIs, for example: signature = sign(headers: JSON, payload: STRING); In this example, payload is assumed to be a detached payload, as described in RFC 7797 . This library would facilitate minimal changes to existing JSON-LD signature implementations. Detailed List of Next Steps Determine how to address problem that JWS implementations lack support for RFC 7797: Recraft prototypes as JWS unencoded-signature libraries to provide a RFC 7797 implementation (with a least RS256) to either be merged into official JWS libraries or to act as standalone bridges until official support is provided. Double-check end-to-end samples with RS256 algorithm (not provided in RFC 7797 or PHP tests). Add 2017 RSA Signature suite to JSON-LD signature libraries, consuming JWS unencoded payload implementation. Implementing the 2017 RSA Signature Suite in a LD Signature Library A White Paper from Rebooting the Web of Trust IV Design Workshop By Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello This document describes specific steps and issues with implementing the 2017 RSA Signature Suite in an existing LD signature library. Source of Truth RFC 7797 does not include an RS256 example, so we obtained a source of truth using the JOSE library, which is the only library we located implementing the RFC 7797 spec. We used the test testCompactJSONWithUnencodedDetachedPayload from tests/Functional/SignerTest.php, which uses the algorithm HMAC-SHA256, as the basis for a new test using the RSA-SHA256 algorithm. The resulting unit test is shown in the included testCompactJSONWithUnencodedDetachedPayloadRS256. public function testCompactJSONWithUnencodedDetachedPayloadRS256() { $payload = '$.02'; $protected_header = [ 'alg' =&gt; 'RS256', 'b64' =&gt; false, 'crit' =&gt; ['b64'], ]; $key = JWKFactory::createFromKeyFile( __DIR__.'/../Unit/Keys/RSA/private.encrypted.key', 'tests', // password for key [ 'kid' =&gt; 'My Private RSA key', 'use' =&gt; 'sig', ] // these options do not affect outcome of this test ); $jws = JWSFactory::createJWSWithDetachedPayloadToCompactJSON($payload, $key, $protected_header); $this\u2011&gt;assertEquals('eyJhbGciOiJSUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..fZRkjTTrcXdUovHjghM6JvlMhJuR1s8X1F4Uy_F4oMhZ9KtF2Zp78lYSOI7OxB5uoTu8FpQHvy\u2011dz3N4nLhoSWAi2_HrxZG_2DyctUUB_8pRKYBmIdIgpOlEMjIreOvXyM6A32gR\u2011PdbzoQq14yQbbfxk12jyZSwcaNu29gXnW_uO7ku1GSV_juWE5E_yIstvEB1GG8ApUGIuzRJDrAAa8KBkHN7Rdfhc8rJMOeSZI0dc_A\u2011Y7t0M0RtrgvV_FhzM40K1pwr1YUZ5y1N4QV13M5u5qJ_lBK40WtWYL5MbJ58Qqk_\u2011Q8l1dp6OCmoMvwdc7FmMsPigmyklqo46uyjjw', $jws); $loader = new Loader(); $loaded = $loader-&gt;loadAndVerifySignatureUsingKeyAndDetachedPayload( $jws, $key, ['RS256'], $payload, $index ); $this-&gt;assertInstanceOf(JWSInterface::class, $loaded); $this-&gt;assertEquals(0, $index); $this-&gt;assertEquals($protected_header, $loaded-&gt;getSignature(0)-&gt;getProtectedHeaders()); } Note this test uses the {\"alg\":\"RS256\",\"b64\":false,\"crit\":[\"b64\"]} header and $.02 as the unencoded payload. The test asserts that the input to the signing function, including the protected headers, should match: eyJhbGciOiJSUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19.$.02 And the resulting signature should match: eyJhbGciOiJSUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..fZRkjTTrcXdUovHjghM6JvlMhJuR1s8X1F4Uy_F4oMhZ9KtF2Zp78lYSOI7OxB5uoTu8FpQHvy\u2011dz3N4nLhoSWAi2_HrxZG_2DyctUUB_8pRKYBmIdIgpOlEMjIreOvXyM6A32gR\u2011PdbzoQq14yQbbfxk12jyZSwcaNu29gXnW_uO7ku1GSV_juWE5E_yIstvEB1GG8ApUGIuzRJDrAAa8KBkHN7Rdfhc8rJMOeSZI0dc_A\u2011Y7t0M0RtrgvV_FhzM40K1pwr1YUZ5y1N4QV13M5u5qJ_lBK40WtWYL5MbJ58Qqk_\u2011Q8l1dp6OCmoMvwdc7FmMsPigmyklqo46uyjjw Our prototypes successfully matched this testcase, and matched results on JSON-LD claim inputs. LD signature flow Overview The signing flow for the 2017 RSA Signature Suite is identical to other signature suites in the JSON-LD signature library ; the processing required to implement 2017 RSA Signatures is confined to step 5 bolded below (all other steps are unchanged). A new algorithm, RsaSignature2017, was added to implement this signature suite. The LD signature algorithm works as follows: Inputs: - JSON-LD headers (nonce, created, creator, ...) same as before, algorithm should be RsaSignature2017 - JSON-LD document JSON-LD Signing Algorithm: Ensure algorithm is in accepted set Add created date of now, if not supplied Canonicalize using the GCA2015 algorithm, as specified in the 2017 RSA Signature Suite specification (NOTE: GCA2015 was formerly called URDNA2015) Prepend the JSON-LD signature options date, domain, nonce to the input to sign, as implemented in the _getDataToHash method of the JSON-LD signature library Sign with the 2017 RSA Signature Suite (details in next section) Compact the signature Outputs: - JSON-LD document with the signature block added We'll refer to steps 2-4 of the JSON-LD signing algorithm as the \"JSON-LD canonicalized form\" of the JSON-LD document. Signing with the 2017 RSA Signature Suite This section drills into step #5 above. To extend the JSON-LD signature library to support the 2017 RSA Signature Suite, we added a new algorithm type -- RsaSignature2017 -- and a new processing case for this type in the function createSignature. First, suppose a JWS library with unencoded payload support were available. If so, then the steps would be: Form the JWS Headers Per RFC 7797 , creating a JWS signature using the unencoded payload option requires the JWS Header parameters \"b64\":false and \"crit\":[\"b64\"]. In addition to these parameters, RsaSignature2017 specifies using RSA Signatures with SHA-256. This corresponds to a JWS signing algorithm of RS256. In sum the complete set of JWS headers used for a 2017 RSA Signature is: { \"alg\":\"RS256\", \"b64\":false, \"crit\":[\"b64\"] } Call the JWS library with headers from #1 (parameter 1: headers) and the JSON-LD canonicalized payload (parameter 2: payload) result = sign(headers: JSON, payload: STRING); Update the LD signature block to contain signatureValue=<result> Implementing JWS unencoded payload signing In step #2 above, we assumed the availability of a JWS library supporting unencoded payloads. Because we only found a PHP library supporting unencoded payloads, we needed to implement those steps in the language we chose. Per RFC 7797 , when the b64 header parameter is used, it must be integrity protected. Therefore it must occur within the JWS Protected Header (meaning it is part of the input that is signed). Also, per RFC7797, the expected input to sign is formatted as follows: ASCII(BASE64URL(UTF8(JWS Protected Header)) || '.' || JWS Payload In our case, JWS Payload is the JSON-LD canonicalized form. This yields the following steps: Format the input to sign: a. Stringify the JWS headers, sorting the keys. - Note: sorting the header parameters is an implementation choice to allow predictability in the sorting order of the protected headers. Since the original JWS header can be obtained from the JWS signature prefix, verification could simply ensure it encodes the JWS headers in the same order. b. Encode the stringified header, referred to as <header> below, as follows: - utf-8 encode - base64 url encode - ascii encode c. Form the JWS input to sign as <header> + \".\" + <payload>, where <payload> is the JSON-LD canonicalized form. - The critical distinction here is that payload is not base64 encoded, per the b64=false argument. Sign: a. RSASHA256-sign the JWS input b. base64-url-encode the signature value Return the signature result <header> + \"..\" + <base64Signature>: a. The .. indicates a JWS detached payload. Note that typically in JWS, the encoded payload is between the middle 2 dots. Steps to Verify The verification algorithm uses the following steps: Record the 'signatureValue' field from the 'signature' section of the JSON-LD document, then remove the entire 'signature' section. Recall the signature value is the 'base64Signature' portion of the JWS signature, i.e. excluding \"<header>..\" in: <header> + \"..\" + <base64Signature> Follow the same steps as in signing listed in \"1. Format the input to sign\", yielding the JWS input. Using a RSA256 signature library, call its \"verify\" method with the JWS input and the expected signature from step 1. This returns a boolean indicating whether the signature matches. Problems Encountered Lack of JWS detached payload library support As described above, the only library we found that supports detached payloads was the PHP JOSE library. Inconsistent ordering of JWS headers To our knowledge the JOSE specs do not specify how JSON headers should be ordered. In our implementations, we ensured consistent lexicographical sorting of JWS headers. This is not critical since the encoded header is included in the signature, but our goal was to produce consistent signatures (similar to what's done in _getDataToHash. Specifying the sorting of the keys, the separators and the encoding should be enough for any implementation to be able to produce the same signature. Example in python: import json header = {'alg': 'RS256', 'b64': False, 'crit': ['b64']} # stringify json # there are no guarantees about the ordering of the keys and the separators use # a whitespace between the keys json.dumps(header) '{\"crit\": [\"b64\"], \"alg\": \"RS256\", \"b64\": false}' # we can specify the separators. In this case we say we don't want whitespaces json.dumps(header, separators=(',', ':')) '{\"crit\":[\"b64\"],\"alg\":\"RS256\",\"b64\":false}' # and we can specify the ordering of the keys json.dumps(header, separators=(',', ':'), sort_keys=True) '{\"alg\":\"RS256\",\"b64\":false,\"crit\":[\"b64\"]}' # ultimately we can specify the encoding to use and return a bytestring that can then be used to base64 encode / sign / hash json.dumps(header, separators=(',', ':'), sort_keys=True).encode('utf-8') b'{\"alg\":\"RS256\",\"b64\":false,\"crit\":[\"b64\"]}' Reference: Modifications to javascript JSON-LD signature library to support 2017 RSA Signature Suite The ld-signatures-js repo contains the 2017 RSA Signature Suite prototype The modifications are: - Add new algorithm type RsaSignature2017 - Add new paths to _createSignature to support RsaSignature2017 (Node.js and Javascript environments) - Add new paths to _verifySignature to support RsaSignature2017 (Node.js and Javascript environments) For example, the inlined implementation of _createSignature with algorithm RsaSignature2017 (Node.js environment) is: var crypto = api.use('crypto'); var signer = crypto.createSign('RSA-SHA256'); // detached signature headers for JWS var protectedHeader = {\"alg\":\"RS256\",\"b64\":false,\"crit\":[\"b64\"]}; var stringifiedHeader = JSON.stringify(protectedHeader, Object.keys(protectedHeader).sort()); var b64UrlEncodedHeader = base64url.encode(stringifiedHeader); // jws input to sign var to_sign = b64UrlEncodedHeader + \".\" + _getDataToHash(input, options); // sign signer.update(to_sign, 'utf-8'); var signaturePart = signer.sign(options.privateKeyPem, 'base64'); // JWS signature for unencoded payload is: &lt;b64UrlEncodedHeader&gt; + '..' + &lt;signaturePath&gt; var signature = b64UrlEncodedHeader + \"..\" + signaturePart; Reminder: This inlined version is to demonstrate the computations performed. It includes steps that should be performed by a JWS library supporting unencoded payloads. The ld-signatures-js repo factors these parts out as separate functions, but should ultimately be replaced by a proper JWS library supporting unencoded payloads, when a javascript implementation exists. Additional Credits Abstract Authors: Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello, Manu Sporny White Paper Authors: Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello Lead Editor: Kim Hamilton Duffy Related Papers: Signature Format Alignment About Rebooting the Web of Trust This paper was produced as part of the Rebooting the Web of Trust IV design workshop. On April 19th through April 21st, 2017, over 40 tech visionaries came together in Paris, France to talk about the future of decentralized trust on the internet with the goal of writing 3-5 white papers and specs. This is one of them. Workshop Sponsors: \u00c6vatar, Blockstream, Digital Contract Design, Microsoft, Protocol Labs, U Change Workshop Producer: Christopher Allen Workshop Facilitators: Christopher Allen and Betty Dhamers, graphic recording by Benoit Pacaud, additional paper editorial & layout by Shannon Appelcline. What\u2019s Next? The design workshop and this paper are just starting points for Rebooting the Web of Trust. If you have any comments, thoughts, or expansions on this paper, please post them to our GitHub issues page: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2017/issues The next Rebooting the Web of Trust design workshop is scheduled for Fall 2017 in Boston, Massachusetts. If you\u2019d like to be involved or would like to help sponsor these events, email: ChristopherA@LifeWithAlacrity.com","title":"Ld signatures"},{"location":"RWoT4/ld-signatures/#ld-signature-format-alignment","text":"An Abstract from Rebooting the Web of Trust IV Design Workshop By Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello, Manu Sporny","title":"LD Signature Format Alignment"},{"location":"RWoT4/ld-signatures/#abstract","text":"The goal of the \"LD Signature Format Alignment\" Working Group at Rebooting the Web of Trust IV was to investigate the feasibility and impact of the proposed 2017 RSA Signature Suite spec, which brings JSON-LD signatures into alignment with the JOSE JSON Web Signature (JWS) standards. The 2017 RSA Signature Suite is based on RFC 7797 , the JSON Web Signature (JWS) Unencoded Payload Option specification. This approach avoids past concerns about JWT raised in the LD signature adopters , including: Increased space consumption associated with base-64 encoding. Difficulty of nesting or chaining signatures, leading to data duplication. Use of a format that is not a JSON object, preventing ability to rely exclusively on a JSON document-based storage engine (while preserving the signature). Using unencoded payloads with detached content, as described in the introduction of RFC 7797 , addresses these concerns and helps in cases in which \"...the payload may be very large and where means are already in place to enable the payload to be communicated between the parties without modifications.\" This avoids unnecessary copying and transformations which can result \"significant space and time improvements\" when working with large payloads. Our working group had two primary questions about the proposed 2017 RSA Signature Suite: Is the specification sufficiently clear for implementors? Is there a negative usability impact to LD signature implementations using this signature suite? To answer these questions, we developed prototypes for the suite in several key programming languages to assess: Availability of library support for JWS unencoded payload options Impact to existing LD signature implementations, e.g. jsonld-signatures library Impact to usability of Verifiable Claims (and others) using JSON-LD signatures with this signature suite.","title":"Abstract"},{"location":"RWoT4/ld-signatures/#status","text":"We accomplished our goals as follows: We delivered prototypes for the 2017 RSA Signature Suite that provided sufficient confidence to move forward with the proposed aligned signature approach. We verified that there was no significant impact to existing LD signature implementations and usability in general. Specifically, unencoded payloads with detached content allows LD signatures to be compatible with JWS while avoiding the concerns raised in the summary of past concerns described above. The major obstacle we encountered while performing this work was the lack of JSON Web Signature library support for unencoded payloads, which is addressed in \"Next Steps\".","title":"Status"},{"location":"RWoT4/ld-signatures/#implementations-of-ld-jws-signing","text":"The following prototypes were developed: For Javascript/Node.js: https://github.com/ WebOfTrustInfo/ld-signatures-js (this is a fork of JSON-LD signatures official library) For Python: https://github.com/ WebOfTrustInfo/ld-signatures-python For Java: https://github.com/ WebOfTrustInfo/ld-signatures-java","title":"Implementations of LD JWS signing"},{"location":"RWoT4/ld-signatures/#json-ld-jws-implementation-guidance-cross-platform","text":"A white paper, which follows, describes the precise differences between existing LD signatures and the new approach.","title":"JSON-LD JWS Implementation Guidance (cross-platform)"},{"location":"RWoT4/ld-signatures/#next-steps","text":"The primary gap in developing these prototypes, which accounted for most of our development work, was lack of library support for JWS unencoded payloads. To work around this limitation, our implementations mirrored the only implementation we found, available in the JOSE PHP library . A cleaner solution that we propose is to recraft our prototypes as JWS unencoded payload libraries. Such a library would expose simple sign-and-verify APIs, for example: signature = sign(headers: JSON, payload: STRING); In this example, payload is assumed to be a detached payload, as described in RFC 7797 . This library would facilitate minimal changes to existing JSON-LD signature implementations.","title":"Next Steps"},{"location":"RWoT4/ld-signatures/#detailed-list-of-next-steps","text":"Determine how to address problem that JWS implementations lack support for RFC 7797: Recraft prototypes as JWS unencoded-signature libraries to provide a RFC 7797 implementation (with a least RS256) to either be merged into official JWS libraries or to act as standalone bridges until official support is provided. Double-check end-to-end samples with RS256 algorithm (not provided in RFC 7797 or PHP tests). Add 2017 RSA Signature suite to JSON-LD signature libraries, consuming JWS unencoded payload implementation.","title":"Detailed List of Next Steps"},{"location":"RWoT4/ld-signatures/#implementing-the-2017-rsa-signature-suite-in-a-ld-signature-library","text":"A White Paper from Rebooting the Web of Trust IV Design Workshop By Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello This document describes specific steps and issues with implementing the 2017 RSA Signature Suite in an existing LD signature library.","title":"Implementing the 2017 RSA Signature Suite in a LD Signature Library"},{"location":"RWoT4/ld-signatures/#source-of-truth","text":"RFC 7797 does not include an RS256 example, so we obtained a source of truth using the JOSE library, which is the only library we located implementing the RFC 7797 spec. We used the test testCompactJSONWithUnencodedDetachedPayload from tests/Functional/SignerTest.php, which uses the algorithm HMAC-SHA256, as the basis for a new test using the RSA-SHA256 algorithm. The resulting unit test is shown in the included testCompactJSONWithUnencodedDetachedPayloadRS256. public function testCompactJSONWithUnencodedDetachedPayloadRS256() { $payload = '$.02'; $protected_header = [ 'alg' =&gt; 'RS256', 'b64' =&gt; false, 'crit' =&gt; ['b64'], ]; $key = JWKFactory::createFromKeyFile( __DIR__.'/../Unit/Keys/RSA/private.encrypted.key', 'tests', // password for key [ 'kid' =&gt; 'My Private RSA key', 'use' =&gt; 'sig', ] // these options do not affect outcome of this test ); $jws = JWSFactory::createJWSWithDetachedPayloadToCompactJSON($payload, $key, $protected_header); $this\u2011&gt;assertEquals('eyJhbGciOiJSUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..fZRkjTTrcXdUovHjghM6JvlMhJuR1s8X1F4Uy_F4oMhZ9KtF2Zp78lYSOI7OxB5uoTu8FpQHvy\u2011dz3N4nLhoSWAi2_HrxZG_2DyctUUB_8pRKYBmIdIgpOlEMjIreOvXyM6A32gR\u2011PdbzoQq14yQbbfxk12jyZSwcaNu29gXnW_uO7ku1GSV_juWE5E_yIstvEB1GG8ApUGIuzRJDrAAa8KBkHN7Rdfhc8rJMOeSZI0dc_A\u2011Y7t0M0RtrgvV_FhzM40K1pwr1YUZ5y1N4QV13M5u5qJ_lBK40WtWYL5MbJ58Qqk_\u2011Q8l1dp6OCmoMvwdc7FmMsPigmyklqo46uyjjw', $jws); $loader = new Loader(); $loaded = $loader-&gt;loadAndVerifySignatureUsingKeyAndDetachedPayload( $jws, $key, ['RS256'], $payload, $index ); $this-&gt;assertInstanceOf(JWSInterface::class, $loaded); $this-&gt;assertEquals(0, $index); $this-&gt;assertEquals($protected_header, $loaded-&gt;getSignature(0)-&gt;getProtectedHeaders()); } Note this test uses the {\"alg\":\"RS256\",\"b64\":false,\"crit\":[\"b64\"]} header and $.02 as the unencoded payload. The test asserts that the input to the signing function, including the protected headers, should match: eyJhbGciOiJSUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19.$.02 And the resulting signature should match: eyJhbGciOiJSUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..fZRkjTTrcXdUovHjghM6JvlMhJuR1s8X1F4Uy_F4oMhZ9KtF2Zp78lYSOI7OxB5uoTu8FpQHvy\u2011dz3N4nLhoSWAi2_HrxZG_2DyctUUB_8pRKYBmIdIgpOlEMjIreOvXyM6A32gR\u2011PdbzoQq14yQbbfxk12jyZSwcaNu29gXnW_uO7ku1GSV_juWE5E_yIstvEB1GG8ApUGIuzRJDrAAa8KBkHN7Rdfhc8rJMOeSZI0dc_A\u2011Y7t0M0RtrgvV_FhzM40K1pwr1YUZ5y1N4QV13M5u5qJ_lBK40WtWYL5MbJ58Qqk_\u2011Q8l1dp6OCmoMvwdc7FmMsPigmyklqo46uyjjw Our prototypes successfully matched this testcase, and matched results on JSON-LD claim inputs.","title":"Source of Truth"},{"location":"RWoT4/ld-signatures/#ld-signature-flow","text":"","title":"LD signature flow"},{"location":"RWoT4/ld-signatures/#overview","text":"The signing flow for the 2017 RSA Signature Suite is identical to other signature suites in the JSON-LD signature library ; the processing required to implement 2017 RSA Signatures is confined to step","title":"Overview"},{"location":"RWoT4/ld-signatures/#5-bolded-below-all-other-steps-are-unchanged-a-new-algorithm","text":"RsaSignature2017, was added to implement this signature suite. The LD signature algorithm works as follows: Inputs: - JSON-LD headers (nonce, created, creator, ...) same as before, algorithm should be RsaSignature2017 - JSON-LD document JSON-LD Signing Algorithm: Ensure algorithm is in accepted set Add created date of now, if not supplied Canonicalize using the GCA2015 algorithm, as specified in the 2017 RSA Signature Suite specification (NOTE: GCA2015 was formerly called URDNA2015) Prepend the JSON-LD signature options date, domain, nonce to the input to sign, as implemented in the _getDataToHash method of the JSON-LD signature library Sign with the 2017 RSA Signature Suite (details in next section) Compact the signature Outputs: - JSON-LD document with the signature block added We'll refer to steps 2-4 of the JSON-LD signing algorithm as the \"JSON-LD canonicalized form\" of the JSON-LD document.","title":"5 bolded below (all other steps are unchanged). A new algorithm,"},{"location":"RWoT4/ld-signatures/#signing-with-the-2017-rsa-signature-suite","text":"This section drills into step #5 above. To extend the JSON-LD signature library to support the 2017 RSA Signature Suite, we added a new algorithm type -- RsaSignature2017 -- and a new processing case for this type in the function createSignature. First, suppose a JWS library with unencoded payload support were available. If so, then the steps would be: Form the JWS Headers Per RFC 7797 , creating a JWS signature using the unencoded payload option requires the JWS Header parameters \"b64\":false and \"crit\":[\"b64\"]. In addition to these parameters, RsaSignature2017 specifies using RSA Signatures with SHA-256. This corresponds to a JWS signing algorithm of RS256. In sum the complete set of JWS headers used for a 2017 RSA Signature is: { \"alg\":\"RS256\", \"b64\":false, \"crit\":[\"b64\"] } Call the JWS library with headers from #1 (parameter 1: headers) and the JSON-LD canonicalized payload (parameter 2: payload) result = sign(headers: JSON, payload: STRING); Update the LD signature block to contain signatureValue=<result>","title":"Signing with the 2017 RSA Signature Suite"},{"location":"RWoT4/ld-signatures/#implementing-jws-unencoded-payload-signing","text":"In step #2 above, we assumed the availability of a JWS library supporting unencoded payloads. Because we only found a PHP library supporting unencoded payloads, we needed to implement those steps in the language we chose. Per RFC 7797 , when the b64 header parameter is used, it must be integrity protected. Therefore it must occur within the JWS Protected Header (meaning it is part of the input that is signed). Also, per RFC7797, the expected input to sign is formatted as follows: ASCII(BASE64URL(UTF8(JWS Protected Header)) || '.' || JWS Payload In our case, JWS Payload is the JSON-LD canonicalized form. This yields the following steps: Format the input to sign: a. Stringify the JWS headers, sorting the keys. - Note: sorting the header parameters is an implementation choice to allow predictability in the sorting order of the protected headers. Since the original JWS header can be obtained from the JWS signature prefix, verification could simply ensure it encodes the JWS headers in the same order. b. Encode the stringified header, referred to as <header> below, as follows: - utf-8 encode - base64 url encode - ascii encode c. Form the JWS input to sign as <header> + \".\" + <payload>, where <payload> is the JSON-LD canonicalized form. - The critical distinction here is that payload is not base64 encoded, per the b64=false argument. Sign: a. RSASHA256-sign the JWS input b. base64-url-encode the signature value Return the signature result <header> + \"..\" + <base64Signature>: a. The .. indicates a JWS detached payload. Note that typically in JWS, the encoded payload is between the middle 2 dots.","title":"Implementing JWS unencoded payload signing"},{"location":"RWoT4/ld-signatures/#steps-to-verify","text":"The verification algorithm uses the following steps: Record the 'signatureValue' field from the 'signature' section of the JSON-LD document, then remove the entire 'signature' section. Recall the signature value is the 'base64Signature' portion of the JWS signature, i.e. excluding \"<header>..\" in: <header> + \"..\" + <base64Signature> Follow the same steps as in signing listed in \"1. Format the input to sign\", yielding the JWS input. Using a RSA256 signature library, call its \"verify\" method with the JWS input and the expected signature from step 1. This returns a boolean indicating whether the signature matches.","title":"Steps to Verify"},{"location":"RWoT4/ld-signatures/#problems-encountered","text":"","title":"Problems Encountered"},{"location":"RWoT4/ld-signatures/#lack-of-jws-detached-payload-library-support","text":"As described above, the only library we found that supports detached payloads was the PHP JOSE library.","title":"Lack of JWS detached payload library support"},{"location":"RWoT4/ld-signatures/#inconsistent-ordering-of-jws-headers","text":"To our knowledge the JOSE specs do not specify how JSON headers should be ordered. In our implementations, we ensured consistent lexicographical sorting of JWS headers. This is not critical since the encoded header is included in the signature, but our goal was to produce consistent signatures (similar to what's done in _getDataToHash. Specifying the sorting of the keys, the separators and the encoding should be enough for any implementation to be able to produce the same signature.","title":"Inconsistent ordering of JWS headers"},{"location":"RWoT4/ld-signatures/#example-in-python","text":"import json header = {'alg': 'RS256', 'b64': False, 'crit': ['b64']} # stringify json # there are no guarantees about the ordering of the keys and the separators use # a whitespace between the keys json.dumps(header) '{\"crit\": [\"b64\"], \"alg\": \"RS256\", \"b64\": false}' # we can specify the separators. In this case we say we don't want whitespaces json.dumps(header, separators=(',', ':')) '{\"crit\":[\"b64\"],\"alg\":\"RS256\",\"b64\":false}' # and we can specify the ordering of the keys json.dumps(header, separators=(',', ':'), sort_keys=True) '{\"alg\":\"RS256\",\"b64\":false,\"crit\":[\"b64\"]}' # ultimately we can specify the encoding to use and return a bytestring that can then be used to base64 encode / sign / hash json.dumps(header, separators=(',', ':'), sort_keys=True).encode('utf-8') b'{\"alg\":\"RS256\",\"b64\":false,\"crit\":[\"b64\"]}'","title":"Example in python:"},{"location":"RWoT4/ld-signatures/#reference-modifications-to-javascript-json-ld-signature-library-to-support-2017-rsa-signature-suite","text":"The ld-signatures-js repo contains the 2017 RSA Signature Suite prototype The modifications are: - Add new algorithm type RsaSignature2017 - Add new paths to _createSignature to support RsaSignature2017 (Node.js and Javascript environments) - Add new paths to _verifySignature to support RsaSignature2017 (Node.js and Javascript environments) For example, the inlined implementation of _createSignature with algorithm RsaSignature2017 (Node.js environment) is: var crypto = api.use('crypto'); var signer = crypto.createSign('RSA-SHA256'); // detached signature headers for JWS var protectedHeader = {\"alg\":\"RS256\",\"b64\":false,\"crit\":[\"b64\"]}; var stringifiedHeader = JSON.stringify(protectedHeader, Object.keys(protectedHeader).sort()); var b64UrlEncodedHeader = base64url.encode(stringifiedHeader); // jws input to sign var to_sign = b64UrlEncodedHeader + \".\" + _getDataToHash(input, options); // sign signer.update(to_sign, 'utf-8'); var signaturePart = signer.sign(options.privateKeyPem, 'base64'); // JWS signature for unencoded payload is: &lt;b64UrlEncodedHeader&gt; + '..' + &lt;signaturePath&gt; var signature = b64UrlEncodedHeader + \"..\" + signaturePart; Reminder: This inlined version is to demonstrate the computations performed. It includes steps that should be performed by a JWS library supporting unencoded payloads. The ld-signatures-js repo factors these parts out as separate functions, but should ultimately be replaced by a proper JWS library supporting unencoded payloads, when a javascript implementation exists.","title":"Reference: Modifications to javascript JSON-LD signature library to support 2017 RSA Signature Suite"},{"location":"RWoT4/ld-signatures/#additional-credits","text":"Abstract Authors: Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello, Manu Sporny White Paper Authors: Kim Hamilton Duffy, Rodolphe Marques, Markus Sabadello Lead Editor: Kim Hamilton Duffy Related Papers: Signature Format Alignment","title":"Additional Credits"},{"location":"RWoT4/ld-signatures/#about-rebooting-the-web-of-trust","text":"This paper was produced as part of the Rebooting the Web of Trust IV design workshop. On April 19th through April 21st, 2017, over 40 tech visionaries came together in Paris, France to talk about the future of decentralized trust on the internet with the goal of writing 3-5 white papers and specs. This is one of them. Workshop Sponsors: \u00c6vatar, Blockstream, Digital Contract Design, Microsoft, Protocol Labs, U Change Workshop Producer: Christopher Allen Workshop Facilitators: Christopher Allen and Betty Dhamers, graphic recording by Benoit Pacaud, additional paper editorial & layout by Shannon Appelcline.","title":"About Rebooting the Web of Trust"},{"location":"RWoT4/ld-signatures/#whats-next","text":"The design workshop and this paper are just starting points for Rebooting the Web of Trust. If you have any comments, thoughts, or expansions on this paper, please post them to our GitHub issues page: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2017/issues The next Rebooting the Web of Trust design workshop is scheduled for Fall 2017 in Boston, Massachusetts. If you\u2019d like to be involved or would like to help sponsor these events, email: ChristopherA@LifeWithAlacrity.com","title":"What\u2019s Next?"},{"location":"RWoT4/reputation-design/","text":"Design Considerations for Decentralized Reputation Systems Authors Lead: Angus Champion de Crespigny \u00ad\u2013 anguschampion@gmail.com Contributors: Dmitry Khovratovich \u2013 khovratovich@gmail.com Florent Blondeau \u2013 florent.blondeau@nameshield.net Klara Sok \u2013 klarasok@gmail.com Philippe Honigman \u2013 philh@ftopia.com Nikolaos Alexopoulos \u2013 alexopoulos@tk.tu-darmstadt.de Fabien Petitcolas \u2013 fabienpe@outlook.com Shaun Conway \u2013 shaun@consent.global Abstract Reputation systems provide an effective way to build a web of trust on the Internet. They consider the history of interactions between peers to establish a measure for a reputation that can itself be used to support a trust decision. Decentralised reputations systems (DRS) rely on a decentralised computer architecture and a distributed ledger to store and maintain reputation information, so that no single entity has control over that information. While there have been numerous analyses of how reputation may be used, there has to date been no systematic definition of the various aspects that should be considered when a reputation system is being designed. By defining these design considerations, we can come to a consensus about what is and is not important in a system. We can discuss the different ways in which they can be built and we can conduct further research and analysis into specific factors in a structured way. We identified ten design considerations for all decentralized reputations should address. These are: Context : What is the reputation value applicable to? What can be understood about an entity by seeing their reputation value(s)? Participation : How is participation defined? Who can and can\u2019t participate? Who can and can\u2019t have a reputation value assigned? User consent : Is consent required by a user to issue claims or a reputation value against the user? Is consent required to reveal claims or a reputation value of a user? Confidentiality : To meet consent requirements, how is data that calculates a reputation value kept private? Can it be derived? Value generation : How is the reputation value calculated or generated? How are claims contributing to the reputation value normalized? Performance : How does the system manage the performance and behavior of the users? How does it manage the performance of the network for speed, reliability, and data integrity? How do users have confidence in this? Sustainability : How does the system stay relevant over time? Claim lifecycle : How are claims valued over time? Can they be revoked and under what conditions? Resilience : How does the system protect against attacks that reduce the integrity of the reputation value? Legal : What is the legal environment in which the system sits? Are there potential violations of \u2018natural\u2019 law? The rest of this paper will further define these considerations and populate each with examples and considerations for their design. We will continue to develop and refine to establish language standards for discussing reputation systems. We have not defined what is and isn\u2019t required for each consideration, as particular implementations may have differing reasons for each. However, we anticipate that best practices for these considerations will be topics for future analysis. Previous Work Resnick et al.[^1] detail three high-level properties that reputations systems require and highlight challenges related to the capture (difficulty of enticing users to provide feedback; eliciting negative feedback; and ensuring honest reports), distribution (problems with name changes of users; and the lack of portability between different systems) and aggregation of feedback. Kumar et al.[^2] look at design considerations that are specific to establishing the reputation of computer nodes in a peer-to-peer network. Koutrouli et al.[^3] look at the basic element and design issues of reputation-based trust models in peer-to-peer systems, so that each peer can make autonomous trust decisions based on other peers\u2019 reputations. 1. Context Dependence Definition: The formal set of hypotheses used to define the value scale of reputation statements in the system and the scope to which the reputation value applies. Every reputation system should clearly define the context to which a reputation applies. For example, a high reputation on StackOverflow may correlate with someone being a strong developer, but the reputation context in fact is more aligned with quickly providing useful information; a person\u2019s ability to architect a project or to make design trade-offs in time-limited projects, which may be considered valuable traits for a developer, may not align with this. In a decentralized reputation system, care should be taken when defining rules that determine the context of use for reputation claims. Designers should pay attention to implicit rules that could be unclear to users. For example, a 4-out-of-5-star rating is considered excellent on some platforms but poor on others. To improve on the precision of the specific value to be measured, granularity may be increased, but traded off for usability. When you want a user to be more precise in the reputation value he gives to others, you can as a designer make him be more precise by increasing the granularity of the fields he fills. You don\u2019t ask for a single five-star rating anymore, but instead split the response into different categories for which the user can give a rating independently. Systems with more granularity will be less usable by users, but may be able to provide more information with a proper analysis. Different contexts can exist in the same reputation system or in different ones. Reputation system operators could be tempted to merge numerical values from different contexts, either in the same system or in different systems, but matching between contexts should be made with explicit rules that are carefully targeted at this precise matching. This creates a new context, with its own rules and guides of conduct. 2. Participation Definition: The rules by which entities can determine whether or not they will partake or be considered by the network or by which the network determines the participation of entities. The network should clearly lay out the rationale and implementation of two aspects of participation: 1. Who is allowed to join the network? Membership in the network may bring with it different capabilities, so this may not be a binary decision. One member may be a passive member with little rights beyond viewing traffic on the system, while another may be able to submit claims as they reach a higher level of membership. The rules for each role, the purposes of each role, and how they will be enforced should be clearly defined. 2. Who is allowed to have a reputation assigned to them by the network? This question is closely related to context: who is eligible to have a reputation in this system? Clear rules need to define who can receive a reputation and to balance such requirements against entities\u2019 rights to privacy. Note that this only defines which entities can have a reputation linked to them: it is separate from whether or not that entity chooses to reveal that reputation. 3. Consent Definition: the rules by which entities accept claims against them or allow the viewing of claims or reputation values. Once an entity is participating in a network, a system design needs to consider to what extent the entity has control over the claims made against them and the reputation information that is associated with them. Consent considerations fall into a few categories. Not all of these requirements will be necessarily be present in any system. Consent to reveal : to what extent can an entity who has received a reputation value reveal in whole, reveal in part, or decline to reveal their reputation value? Consent to inbound claims : does an entity have the right to accept in whole, accept in part, or reject in whole a claim made against them? Consent to outbound claims : can an entity define who can see a claim that they have submitted against another entity or any information related to that claim? Right to be forgotten : can the recipient of a reputation value delete or hide that reputation? A legal base for the right to be forgotten is given by Article 12 of the Directive 95/46/EC of the European Union ^4 . It provides for the \u201cerasure or blocking of data processing\u201d. In the context of DRS, the right to be forgotten may involve the full deletion of all data used for computing the reputation value, or a restriction to aggregate such data, or a restriction to associate such data with the related emitting or receiving individual. 4. Confidentiality Definition: How to ensure that no data is leaked and that other considerations are not violated by derivation of metadata or analysis. While an entity may choose their level of participation, choose what claims are made against them, and choose to whom their reputation is revealed through considerations of \u201cParticipation\u201d and \u201cConsent\u201d, the system needs to be designed so that the method of achieving each of these attributes is secure and does not leak information nor even data that enables information to be derived. This information falls into a number of categories, for a case where Alice is sending a claim against Bob. Privacy of Sender (\u201cAlice\u201d) This may include: Sender Unlinkability. Alice limits the set who knows she vouched for Bob. Connections Unlinkability. Alice prohibits exposure of the fact that her two connections were endorsed by the same person. Uncountability. Alice limits knowledge of how many claims she issued over any period of time. Grade Privacy. Alice prohibits exposure of her submitted claim not only by itself (through Consent) but as a whole through which it could be derived, for example the most popular, average, or empty values. Context Privacy. Alice prohibits exposure of the context she endorses not only individually (through Consent) but as a whole, for example the most popular context or unused contexts. Time Privacy. Alice prohibits exposure of the time when she sends claims not only individually (through Consent) but as a whole, for example the most active time, passive time, etc. Revocation Privacy. Alice prohibits exposure of revocation-specific data: connections with revoked endorsements, without revoked endorsements, validity time, etc. Privacy of Recipient (Bob) This includes: Sender Unlinkability. Bob limits the set who knows Alice sent a claim against him. Connections Unlinkability. Bob prohibits exposure of the fact that his two connections endorsed the same person (Bob). Uncountability. Bob limits knowledge of how many endorsements he received over any period of time. Grade Privacy. Bob prohibits learning of his claim values not only individually (through Consent) but as a whole, for example the most popular claim, average claim, empty claim. Context Privacy. Bob prohibits learning of the context in which a claim was submitted, not only individually (through Consent) but as a whole, for example the most popular context, unused context. Time Privacy. Bob prohibits exposure of the time when he received claims, not only individually (through Consent) but as a whole, for example the most active years and months or inactive years and months. Group Privacy This includes: Group Unlinkability. Groups whose members endorse each other much more often than others (classmates, colleagues) may not be detected by design. The above definitions are examples. However, each design should balance the need for metadata that may assist in analysis and identification of bad actors against the potential for network attack. 5. Value Generation Definition: the process to establish the reputation value of an entity on the reputation network based on the required inputs. The value-generation process is the ultimate utility of a DRS, and consequently requires significant design and protections to ensure it accurately represents the context it has been defined to evaluate. The value may not necessarily be numeric. Various factors that may need to be defined in the generation of such a value include: Value factors \u2013 What are the factors that contribute to the overall value? Initialization of information \u2013 Do the factors need initialization? Are there default values? Do all need to be included? Aggregation and transformation \u2013 What process brings these factors to the ultimate value? This may include sums, convolutions, or more complex transformations. Claim threshold \u2013 Are there a minimum number of claims that need to be submitted against an entity before a value can be generated? Context \u2013 What assumptions are being made about the factors? Do they align with the context? Ranking/normalization \u2013 Are some factors or claims worth more than others? Timeliness \u2013 Do some factors carry less weight due to time elapsed since they were set or defined? Behavioral \u2013 Does a reputation value change depending on how it has been used or an entity\u2019s behavior? 6. Performance Definition: how to ensure the network and its participants perform as expected. System performance is a key aspect to consider, as perceived reputation as conveyed by any reputation score is intimately linked to the legitimacy of the system producing reputation artefacts (scores, ranking, color, category, etc.). While legitimacy is a function of much more than pure performance, we focus here strictly on performance. Performance of decentralized reputation networks can be considered to fall into two categories: System performance as an aggregation of individual node performances. System performance as a function of architectural design choices, or at the network level. Node Performance The requirements for nodes on a DRS should be clearly defined to ensure that they can contribute effectively to the network, in addition to enabling the rapid identification of errors or bad actors the and mitigation of flow on effects. Some key factors of performance of connected nodes are: Availability Reliability External and internal consistency Capabilities Identification of bad actors, who through corruption, collusion, gaming, or otherwise are maliciously altering the intended utility of the network Node performance can be measured by : Liveliness (availability) Error rates (reliability) Distribution functions (consistency) Corrections (capabilities) Measurable node performance can in turn be leveraged in order to improve the performance of the whole system, through incentivizing good performance via monetary and non-monetary means, and/or punishment of bad performance via monetary and non-monetary means, up to exclusion from the network. Network performance An effective distributed network is scalable, with maximum uptime, and coordinates communication between nodes in a rapid, efficient manner. Network performance can be monitored using different indicators , such as: Number of active nodes Node activity Node failure rate Network power and topology may also need to be defined in advance, depending on the needs of the network. Is there a minimum number of nodes needed to effectively function? What is the consensus mechanism? Are its speed and mechanics suitable for the context of the system? What is the degree of decentralization inherent in the system? How might clusters of nodes impact the performance of the reputation network? Built-in rules regarding responsiveness of the network will likely be required, and these should be defined in correlation with the defined performance considerations. 7. Sustainability Definition: The system\u2019s ability to evolve and remain accurate over time. Being distributed and self-governing, a DRS will consequently be difficult to modify on a regular basis. As a result, designing the system to be consistent and valuable over time will likely require considerable design. It is likely that peers of the network themselves, rather than a central authority, will define and enforce the shared ethics and desires of the user population, however the ethics and desires to be enforced would need to be incorporated into the system\u2019s design from the start. Such a design may allow for nodes or entities on the network to signal for such changes when required, or may construct incentives in such a way that the market naturally corrects any diversions with time. These desires and ethics may include any aspect of any one of the design considerations. 8. Claim Life-cycle Definition: how to manage claims made on the network and the impacts they may have over time. The network should define the conditions whereby claims that contribute to a reputation score are considered applicable or not applicable to the score over time. These conditions may include: Time to live \u2013 Alice may submit a claim that Bob is up to date with his rent payments, with a time to live of one month. Decay \u2013 Alice may submit a claim that Bob is untrustworthy. Over time Bob may change his behavior, so the claim loses its value gradually over time. Validity. Dispute resolution/adjudication. 9. Resilience Definition: The ability of the system to tolerate malicious behaviour. Reputation systems need to be resilient to attacks to be of any use in the real world. Attacks against reputation systems in general aim at distorting the utility of the network \u2014 that is, the reputation of a set of participants. Some attacks are well studied in literature and we refer to them as \u201ctraditional\u201d attacks on reputation systems. Decentralization, while alleviating the need for a single point of failure (SPOF) raises additional concerns, documented below: Traditional attacks on reputation systems: Self-Promoting \u2013 Raise reputation of one\u2019s self through false feedback, which can be facilitated via a Sybil attack Whitewashing \u2013 Leave the system and re-enter with a new \u201cname\u201d if reputation is low Slandering (Bad-mouthing) \u2013 Lower the reputation of a competitor via false feedback Ballot stuffing[^5] \u2013 Collusion between the recipient and sender of the reputation claim Mixed (orchestrated, byzantine) \u2013 combination of the above Denial of Service (DoS) Censorship Single Point of Failure (SPoF) These attacks have been documented and analyzed in several academic papers[^6]^,^[^7]^,^[^8]. Special concerns for decentralized systems: Codebase development and maintenance \u2013 Code on the nodes does not need to be uniform but the interfaces must match Information withholding \u2013 A recipient of reputation only discloses partial information about himself Stale information \u2013 A recipient of information discloses outdated information The choice of defenses is interdependent with design decisions of other sections. For example, the participation mechanism is very important to the mitigation of Sybil attacks that in turn facilitate many of the traditional attacks mentioned above. Blockchain constructs can mitigate some of the attacks outlined above. For some other attacks, additional measures are necessary. For example, many designs do not implement negative reputation, as this is notoriously difficult to secure. Others rely on a limited endorsement budget or tie an endorsement to a financial transaction. Ultimately, there may be an inherent Security and Privacy trade-off. For example, when Alice assesses the reputation of Bob, she may wish to learn as much information as possible in order to avoid attacks. 10. Legal Definition: the legal environment in which the network may operate. All technology fits within some form of society, and society has a strong interest in preventing attacks upon a person\u2019s identity and reputation. It also seeks to redress them while maintaining the ability for people to express their opinion. Consequently, it is wise to consider any state-bound or natural law when implementing a DRS to ensure there is limited exposure for the participants and the creators in such a network. For example, reputation is the respect or esteem which a person (the trustee) enjoys in Society or what people (the trustors) think of him/her. An important element in the protection of reputation is the wrong of defamation. Designers of DRS should therefore bear in mind some of the remedies that law generally provides for defamation. Aside from compensatory damages there are also motions to identify the defamatory party. injunctions to prevent further publication of defamatory information. Conclusion The authors believe that the above ten design considerations can be used as a framework to design and implement effective decentralized reputation systems. While the decisions for each consideration have been left open in this paper, each can be analyzed further to establish industry best practices to set a benchmark for a human-driven future web of trust. [^1]: Paul Resnick, Richard Zeckhauser, Eric Friedman, and Ko Kuwabara, \u2018Reputation systems\u2019, Communications of the ACM , vol. 43, no. 12, pp. 45\u201348, 2000. [^2]: Sandeep Kumar, Chander Diwaker, and Amit Chaudhary, \u2018Reputation System in Peer-To-Peer Network: Design and Classification\u2019, Journal of Global Research in Computer Science , vol. 2, no. 8, pp. 1\u20133, 2011. [^3]: Eleni Koutrouli and Aphrodite Tsalgatidou, \u2018Reputation-based trust systems for P2P applications: design issues and comparison framework\u2019, in International Conference on Trust, Privacy and Security in Digital Business , 2006, pp. 152\u2013161. [^5]: C Dellarocas, \" Immunizing online reputation reporting systems against unfair ratings and discriminatory behavior \", EC'00, Proceedings of the 2nd ACM conference on Electronic commerce [^6]: Hoffman, K., Zage, D. and Nita-Rotaru, C., 2009. A survey of attack and defense techniques for reputation systems. ACM Computing Surveys (CSUR) , 42 (1), p.1. [^7]: J\u00f8sang, A., Ismail, R. and Boyd, C., 2007. A survey of trust and reputation systems for online service provision. Decision support systems , 43 (2), pp.618-644. [^8]: Koutrouli, E. and Tsalgatidou, A., 2012. Taxonomy of attacks and defense mechanisms in P2P reputation systems\u2014Lessons for reputation system designers. Computer Science Review , 6 (2), pp.47-70.","title":"Design Considerations for Decentralized Reputation Systems"},{"location":"RWoT4/reputation-design/#design-considerations-for-decentralized-reputation-systems","text":"","title":"Design Considerations for Decentralized Reputation Systems"},{"location":"RWoT4/reputation-design/#authors","text":"Lead: Angus Champion de Crespigny \u00ad\u2013 anguschampion@gmail.com Contributors: Dmitry Khovratovich \u2013 khovratovich@gmail.com Florent Blondeau \u2013 florent.blondeau@nameshield.net Klara Sok \u2013 klarasok@gmail.com Philippe Honigman \u2013 philh@ftopia.com Nikolaos Alexopoulos \u2013 alexopoulos@tk.tu-darmstadt.de Fabien Petitcolas \u2013 fabienpe@outlook.com Shaun Conway \u2013 shaun@consent.global","title":"Authors"},{"location":"RWoT4/reputation-design/#abstract","text":"Reputation systems provide an effective way to build a web of trust on the Internet. They consider the history of interactions between peers to establish a measure for a reputation that can itself be used to support a trust decision. Decentralised reputations systems (DRS) rely on a decentralised computer architecture and a distributed ledger to store and maintain reputation information, so that no single entity has control over that information. While there have been numerous analyses of how reputation may be used, there has to date been no systematic definition of the various aspects that should be considered when a reputation system is being designed. By defining these design considerations, we can come to a consensus about what is and is not important in a system. We can discuss the different ways in which they can be built and we can conduct further research and analysis into specific factors in a structured way. We identified ten design considerations for all decentralized reputations should address. These are: Context : What is the reputation value applicable to? What can be understood about an entity by seeing their reputation value(s)? Participation : How is participation defined? Who can and can\u2019t participate? Who can and can\u2019t have a reputation value assigned? User consent : Is consent required by a user to issue claims or a reputation value against the user? Is consent required to reveal claims or a reputation value of a user? Confidentiality : To meet consent requirements, how is data that calculates a reputation value kept private? Can it be derived? Value generation : How is the reputation value calculated or generated? How are claims contributing to the reputation value normalized? Performance : How does the system manage the performance and behavior of the users? How does it manage the performance of the network for speed, reliability, and data integrity? How do users have confidence in this? Sustainability : How does the system stay relevant over time? Claim lifecycle : How are claims valued over time? Can they be revoked and under what conditions? Resilience : How does the system protect against attacks that reduce the integrity of the reputation value? Legal : What is the legal environment in which the system sits? Are there potential violations of \u2018natural\u2019 law? The rest of this paper will further define these considerations and populate each with examples and considerations for their design. We will continue to develop and refine to establish language standards for discussing reputation systems. We have not defined what is and isn\u2019t required for each consideration, as particular implementations may have differing reasons for each. However, we anticipate that best practices for these considerations will be topics for future analysis.","title":"Abstract"},{"location":"RWoT4/reputation-design/#previous-work","text":"Resnick et al.[^1] detail three high-level properties that reputations systems require and highlight challenges related to the capture (difficulty of enticing users to provide feedback; eliciting negative feedback; and ensuring honest reports), distribution (problems with name changes of users; and the lack of portability between different systems) and aggregation of feedback. Kumar et al.[^2] look at design considerations that are specific to establishing the reputation of computer nodes in a peer-to-peer network. Koutrouli et al.[^3] look at the basic element and design issues of reputation-based trust models in peer-to-peer systems, so that each peer can make autonomous trust decisions based on other peers\u2019 reputations.","title":"Previous Work"},{"location":"RWoT4/reputation-design/#1-context-dependence","text":"Definition: The formal set of hypotheses used to define the value scale of reputation statements in the system and the scope to which the reputation value applies. Every reputation system should clearly define the context to which a reputation applies. For example, a high reputation on StackOverflow may correlate with someone being a strong developer, but the reputation context in fact is more aligned with quickly providing useful information; a person\u2019s ability to architect a project or to make design trade-offs in time-limited projects, which may be considered valuable traits for a developer, may not align with this. In a decentralized reputation system, care should be taken when defining rules that determine the context of use for reputation claims. Designers should pay attention to implicit rules that could be unclear to users. For example, a 4-out-of-5-star rating is considered excellent on some platforms but poor on others. To improve on the precision of the specific value to be measured, granularity may be increased, but traded off for usability. When you want a user to be more precise in the reputation value he gives to others, you can as a designer make him be more precise by increasing the granularity of the fields he fills. You don\u2019t ask for a single five-star rating anymore, but instead split the response into different categories for which the user can give a rating independently. Systems with more granularity will be less usable by users, but may be able to provide more information with a proper analysis. Different contexts can exist in the same reputation system or in different ones. Reputation system operators could be tempted to merge numerical values from different contexts, either in the same system or in different systems, but matching between contexts should be made with explicit rules that are carefully targeted at this precise matching. This creates a new context, with its own rules and guides of conduct.","title":"1. Context Dependence"},{"location":"RWoT4/reputation-design/#2-participation","text":"Definition: The rules by which entities can determine whether or not they will partake or be considered by the network or by which the network determines the participation of entities. The network should clearly lay out the rationale and implementation of two aspects of participation: 1. Who is allowed to join the network? Membership in the network may bring with it different capabilities, so this may not be a binary decision. One member may be a passive member with little rights beyond viewing traffic on the system, while another may be able to submit claims as they reach a higher level of membership. The rules for each role, the purposes of each role, and how they will be enforced should be clearly defined. 2. Who is allowed to have a reputation assigned to them by the network? This question is closely related to context: who is eligible to have a reputation in this system? Clear rules need to define who can receive a reputation and to balance such requirements against entities\u2019 rights to privacy. Note that this only defines which entities can have a reputation linked to them: it is separate from whether or not that entity chooses to reveal that reputation.","title":"2. Participation"},{"location":"RWoT4/reputation-design/#3-consent","text":"Definition: the rules by which entities accept claims against them or allow the viewing of claims or reputation values. Once an entity is participating in a network, a system design needs to consider to what extent the entity has control over the claims made against them and the reputation information that is associated with them. Consent considerations fall into a few categories. Not all of these requirements will be necessarily be present in any system. Consent to reveal : to what extent can an entity who has received a reputation value reveal in whole, reveal in part, or decline to reveal their reputation value? Consent to inbound claims : does an entity have the right to accept in whole, accept in part, or reject in whole a claim made against them? Consent to outbound claims : can an entity define who can see a claim that they have submitted against another entity or any information related to that claim? Right to be forgotten : can the recipient of a reputation value delete or hide that reputation? A legal base for the right to be forgotten is given by Article 12 of the Directive 95/46/EC of the European Union ^4 . It provides for the \u201cerasure or blocking of data processing\u201d. In the context of DRS, the right to be forgotten may involve the full deletion of all data used for computing the reputation value, or a restriction to aggregate such data, or a restriction to associate such data with the related emitting or receiving individual.","title":"3. Consent"},{"location":"RWoT4/reputation-design/#4-confidentiality","text":"Definition: How to ensure that no data is leaked and that other considerations are not violated by derivation of metadata or analysis. While an entity may choose their level of participation, choose what claims are made against them, and choose to whom their reputation is revealed through considerations of \u201cParticipation\u201d and \u201cConsent\u201d, the system needs to be designed so that the method of achieving each of these attributes is secure and does not leak information nor even data that enables information to be derived. This information falls into a number of categories, for a case where Alice is sending a claim against Bob.","title":"4. Confidentiality"},{"location":"RWoT4/reputation-design/#privacy-of-sender-alice","text":"This may include: Sender Unlinkability. Alice limits the set who knows she vouched for Bob. Connections Unlinkability. Alice prohibits exposure of the fact that her two connections were endorsed by the same person. Uncountability. Alice limits knowledge of how many claims she issued over any period of time. Grade Privacy. Alice prohibits exposure of her submitted claim not only by itself (through Consent) but as a whole through which it could be derived, for example the most popular, average, or empty values. Context Privacy. Alice prohibits exposure of the context she endorses not only individually (through Consent) but as a whole, for example the most popular context or unused contexts. Time Privacy. Alice prohibits exposure of the time when she sends claims not only individually (through Consent) but as a whole, for example the most active time, passive time, etc. Revocation Privacy. Alice prohibits exposure of revocation-specific data: connections with revoked endorsements, without revoked endorsements, validity time, etc.","title":"Privacy of Sender (\u201cAlice\u201d)"},{"location":"RWoT4/reputation-design/#privacy-of-recipient-bob","text":"This includes: Sender Unlinkability. Bob limits the set who knows Alice sent a claim against him. Connections Unlinkability. Bob prohibits exposure of the fact that his two connections endorsed the same person (Bob). Uncountability. Bob limits knowledge of how many endorsements he received over any period of time. Grade Privacy. Bob prohibits learning of his claim values not only individually (through Consent) but as a whole, for example the most popular claim, average claim, empty claim. Context Privacy. Bob prohibits learning of the context in which a claim was submitted, not only individually (through Consent) but as a whole, for example the most popular context, unused context. Time Privacy. Bob prohibits exposure of the time when he received claims, not only individually (through Consent) but as a whole, for example the most active years and months or inactive years and months.","title":"Privacy of Recipient (Bob)"},{"location":"RWoT4/reputation-design/#group-privacy","text":"This includes: Group Unlinkability. Groups whose members endorse each other much more often than others (classmates, colleagues) may not be detected by design. The above definitions are examples. However, each design should balance the need for metadata that may assist in analysis and identification of bad actors against the potential for network attack.","title":"Group Privacy"},{"location":"RWoT4/reputation-design/#5-value-generation","text":"Definition: the process to establish the reputation value of an entity on the reputation network based on the required inputs. The value-generation process is the ultimate utility of a DRS, and consequently requires significant design and protections to ensure it accurately represents the context it has been defined to evaluate. The value may not necessarily be numeric. Various factors that may need to be defined in the generation of such a value include: Value factors \u2013 What are the factors that contribute to the overall value? Initialization of information \u2013 Do the factors need initialization? Are there default values? Do all need to be included? Aggregation and transformation \u2013 What process brings these factors to the ultimate value? This may include sums, convolutions, or more complex transformations. Claim threshold \u2013 Are there a minimum number of claims that need to be submitted against an entity before a value can be generated? Context \u2013 What assumptions are being made about the factors? Do they align with the context? Ranking/normalization \u2013 Are some factors or claims worth more than others? Timeliness \u2013 Do some factors carry less weight due to time elapsed since they were set or defined? Behavioral \u2013 Does a reputation value change depending on how it has been used or an entity\u2019s behavior?","title":"5. Value Generation"},{"location":"RWoT4/reputation-design/#6-performance","text":"Definition: how to ensure the network and its participants perform as expected. System performance is a key aspect to consider, as perceived reputation as conveyed by any reputation score is intimately linked to the legitimacy of the system producing reputation artefacts (scores, ranking, color, category, etc.). While legitimacy is a function of much more than pure performance, we focus here strictly on performance. Performance of decentralized reputation networks can be considered to fall into two categories: System performance as an aggregation of individual node performances. System performance as a function of architectural design choices, or at the network level.","title":"6. Performance"},{"location":"RWoT4/reputation-design/#node-performance","text":"The requirements for nodes on a DRS should be clearly defined to ensure that they can contribute effectively to the network, in addition to enabling the rapid identification of errors or bad actors the and mitigation of flow on effects. Some key factors of performance of connected nodes are: Availability Reliability External and internal consistency Capabilities Identification of bad actors, who through corruption, collusion, gaming, or otherwise are maliciously altering the intended utility of the network Node performance can be measured by : Liveliness (availability) Error rates (reliability) Distribution functions (consistency) Corrections (capabilities) Measurable node performance can in turn be leveraged in order to improve the performance of the whole system, through incentivizing good performance via monetary and non-monetary means, and/or punishment of bad performance via monetary and non-monetary means, up to exclusion from the network.","title":"Node Performance"},{"location":"RWoT4/reputation-design/#network-performance","text":"An effective distributed network is scalable, with maximum uptime, and coordinates communication between nodes in a rapid, efficient manner. Network performance can be monitored using different indicators , such as: Number of active nodes Node activity Node failure rate Network power and topology may also need to be defined in advance, depending on the needs of the network. Is there a minimum number of nodes needed to effectively function? What is the consensus mechanism? Are its speed and mechanics suitable for the context of the system? What is the degree of decentralization inherent in the system? How might clusters of nodes impact the performance of the reputation network? Built-in rules regarding responsiveness of the network will likely be required, and these should be defined in correlation with the defined performance considerations.","title":"Network performance"},{"location":"RWoT4/reputation-design/#7-sustainability","text":"Definition: The system\u2019s ability to evolve and remain accurate over time. Being distributed and self-governing, a DRS will consequently be difficult to modify on a regular basis. As a result, designing the system to be consistent and valuable over time will likely require considerable design. It is likely that peers of the network themselves, rather than a central authority, will define and enforce the shared ethics and desires of the user population, however the ethics and desires to be enforced would need to be incorporated into the system\u2019s design from the start. Such a design may allow for nodes or entities on the network to signal for such changes when required, or may construct incentives in such a way that the market naturally corrects any diversions with time. These desires and ethics may include any aspect of any one of the design considerations.","title":"7. Sustainability"},{"location":"RWoT4/reputation-design/#8-claim-life-cycle","text":"Definition: how to manage claims made on the network and the impacts they may have over time. The network should define the conditions whereby claims that contribute to a reputation score are considered applicable or not applicable to the score over time. These conditions may include: Time to live \u2013 Alice may submit a claim that Bob is up to date with his rent payments, with a time to live of one month. Decay \u2013 Alice may submit a claim that Bob is untrustworthy. Over time Bob may change his behavior, so the claim loses its value gradually over time. Validity. Dispute resolution/adjudication.","title":"8. Claim Life-cycle"},{"location":"RWoT4/reputation-design/#9-resilience","text":"Definition: The ability of the system to tolerate malicious behaviour. Reputation systems need to be resilient to attacks to be of any use in the real world. Attacks against reputation systems in general aim at distorting the utility of the network \u2014 that is, the reputation of a set of participants. Some attacks are well studied in literature and we refer to them as \u201ctraditional\u201d attacks on reputation systems. Decentralization, while alleviating the need for a single point of failure (SPOF) raises additional concerns, documented below: Traditional attacks on reputation systems: Self-Promoting \u2013 Raise reputation of one\u2019s self through false feedback, which can be facilitated via a Sybil attack Whitewashing \u2013 Leave the system and re-enter with a new \u201cname\u201d if reputation is low Slandering (Bad-mouthing) \u2013 Lower the reputation of a competitor via false feedback Ballot stuffing[^5] \u2013 Collusion between the recipient and sender of the reputation claim Mixed (orchestrated, byzantine) \u2013 combination of the above Denial of Service (DoS) Censorship Single Point of Failure (SPoF) These attacks have been documented and analyzed in several academic papers[^6]^,^[^7]^,^[^8]. Special concerns for decentralized systems: Codebase development and maintenance \u2013 Code on the nodes does not need to be uniform but the interfaces must match Information withholding \u2013 A recipient of reputation only discloses partial information about himself Stale information \u2013 A recipient of information discloses outdated information The choice of defenses is interdependent with design decisions of other sections. For example, the participation mechanism is very important to the mitigation of Sybil attacks that in turn facilitate many of the traditional attacks mentioned above. Blockchain constructs can mitigate some of the attacks outlined above. For some other attacks, additional measures are necessary. For example, many designs do not implement negative reputation, as this is notoriously difficult to secure. Others rely on a limited endorsement budget or tie an endorsement to a financial transaction. Ultimately, there may be an inherent Security and Privacy trade-off. For example, when Alice assesses the reputation of Bob, she may wish to learn as much information as possible in order to avoid attacks.","title":"9. Resilience"},{"location":"RWoT4/reputation-design/#10-legal","text":"Definition: the legal environment in which the network may operate. All technology fits within some form of society, and society has a strong interest in preventing attacks upon a person\u2019s identity and reputation. It also seeks to redress them while maintaining the ability for people to express their opinion. Consequently, it is wise to consider any state-bound or natural law when implementing a DRS to ensure there is limited exposure for the participants and the creators in such a network. For example, reputation is the respect or esteem which a person (the trustee) enjoys in Society or what people (the trustors) think of him/her. An important element in the protection of reputation is the wrong of defamation. Designers of DRS should therefore bear in mind some of the remedies that law generally provides for defamation. Aside from compensatory damages there are also motions to identify the defamatory party. injunctions to prevent further publication of defamatory information.","title":"10. Legal"},{"location":"RWoT4/reputation-design/#conclusion","text":"The authors believe that the above ten design considerations can be used as a framework to design and implement effective decentralized reputation systems. While the decisions for each consideration have been left open in this paper, each can be analyzed further to establish industry best practices to set a benchmark for a human-driven future web of trust. [^1]: Paul Resnick, Richard Zeckhauser, Eric Friedman, and Ko Kuwabara, \u2018Reputation systems\u2019, Communications of the ACM , vol. 43, no. 12, pp. 45\u201348, 2000. [^2]: Sandeep Kumar, Chander Diwaker, and Amit Chaudhary, \u2018Reputation System in Peer-To-Peer Network: Design and Classification\u2019, Journal of Global Research in Computer Science , vol. 2, no. 8, pp. 1\u20133, 2011. [^3]: Eleni Koutrouli and Aphrodite Tsalgatidou, \u2018Reputation-based trust systems for P2P applications: design issues and comparison framework\u2019, in International Conference on Trust, Privacy and Security in Digital Business , 2006, pp. 152\u2013161. [^5]: C Dellarocas, \" Immunizing online reputation reporting systems against unfair ratings and discriminatory behavior \", EC'00, Proceedings of the 2nd ACM conference on Electronic commerce [^6]: Hoffman, K., Zage, D. and Nita-Rotaru, C., 2009. A survey of attack and defense techniques for reputation systems. ACM Computing Surveys (CSUR) , 42 (1), p.1. [^7]: J\u00f8sang, A., Ismail, R. and Boyd, C., 2007. A survey of trust and reputation systems for online service provision. Decision support systems , 43 (2), pp.618-644. [^8]: Koutrouli, E. and Tsalgatidou, A., 2012. Taxonomy of attacks and defense mechanisms in P2P reputation systems\u2014Lessons for reputation system designers. Computer Science Review , 6 (2), pp.47-70.","title":"Conclusion"},{"location":"RWoT4/what-users-really-want/","text":"Re-Imagining What Users Really Want A paper of the Rebooting the Web of Trust workshop, Paris, 2017 By Joe Andrieu <joe@legendaryrequirements.com>, Frederic Engel <market.engel@gmail.com>, Adam Lake <creatinglake@gmail.com>, Moses Ma <moses.ma@futurelabconsulting.com>, Olivier Maas <olivier.maas@worldline.com>, Mark van der Waal <mark.vanderwaal@sovalacc.com> We consider five fictional personas to explore the obstacles and opportunities for self-sovereign identity. For one, Rutu Shah, we describe an illustrative day in her life. Five Personas, Five Opportunities The following five individuals represent a diverse sampling of normal, everyday people whose lives could be improved by self-sovereign identity. Today, these five aren\u2019t using enhanced digital identity. For each, we describe their current lack of engagement and provide at least one opportunity that could reframe self-sovereign identity as a clear win for them. Ren\u00e9e Moreau Mme. Ren\u00e9e Moreau is a seventy-eight-year-old grandmother living in the town of Amiens, France ^1 . She has a low level of understanding about information technology, but fears being hacked and losing what meager savings she has left. To the extent that she has discussed it with her children, Ren\u00e9e Moreau feels \u201cfancy\u201d identity solutions are just too complicated for her to use. Reframing: FamilyPix secure picture sharing enables Renee to receive photos of her grandchildren without exposing them to pedophiles or marketers. Bonus Reframing: A friends-of-family web of trust enables support when emergencies arise. Family-of-friends and friends-of-family are discretely recruited for real-time on-demand response to minor emergencies and quality-of-life interactions such as getting locked out of the house, trip-and-fall incidents, and needing a ride to a doctor visit. Jack Smith Mr. Jack Smith is the fifty-seven-year-old Chief Information Officer of a Fortune 1000 company in the United States. He lives in Portland, Oregon with his wife and ten-year-old son. He has earned a reputation as a shrewd debunker of silver bullet initiatives that promise miracle results but rarely deliver. As a rule, he has found identity solutions too burdensome for integrated deployment throughout the hundreds of applications deployed across his company. Instead, core corporate applications are managed through a centralized Active Directory service and departmental applications are free to implement their own identity approach. He underestimates his company risk for regulatory compliance and tends to avoid technical topics where his aging expertise might appear dated. Employees spend as much as 15 minutes per day dealing with logins at both intranet and Internet sites. Phishing is a significant, but overlooked security risk. Reframing: Super Sign-On brings corporate identity data into compliance while unifying secure logins for applications across the enterprise and beyond the corporate boundary at online services like Trello, Atlassian (Jira, Confluence, etc), and Dropbox. Mayumi Takamasa Miss Mayumi Takamasa is a seventeen-year-old female student in Tokyo, Japan. In conversations about privacy risks, she is dismissive and doesn\u2019t care about advanced identity management. Online she has seen cyberbullying among her peers and goes to some effort to create boundaries between what she and her friends post for each other and what her mother can see. Reframing: The MySociety social game creates independent identity across social network platforms \u2014 without exposing content to uninvited eyes. The game creates pseudonymous social spaces with built-in privacy, disappearing messages, and points, levels, and badges based on social interactions. Joram Hadad Mr. Joram Hadad is a twenty-five-year-old Syrian refugee from Damascus, traveling through Greece on his way to resettlement in France. He fears reprisal against his family should the Syrian government discover he made it out of the country. He has no technology to speak of and has provided authorities with a fake name. He needs health care and basic human services. Reframing: MyID , a biometric-and-PIN-secured digital datastore in a USB thumb drive, gives Joram physical control over identity data that only he has the power to share. Rutu Shah Ms. Rutu Shah is a thirty-eight-year-old Mayor of a modest city in India, Ahmedabad ^2 . In her daily life she is wary of phishing attacks and security risks, but social and economic issues have taken priority over pushing for better identity across city services. She understands the value, but struggles to push for change, as commitments to drinkable water, better education, and sustainable jobs got her elected. She is particularly concerned about online predators and children\u2019s safety. Reframing: SafetyNet creates personalized peer-to-peer, independent trust networks where parents, educators, mentors, and social workers vet and connect trusted resources for children. The same service also allows professional networks to flourish through phish-proof, trusted introductions, connecting entrepreneurs with business development, mentors, and funding sources. The Magic of Reframing Reframing is a powerful tool, based on the work of George Lakoff ^3 , linguist, former Distinguished Professor of Cognitive Science and Linguistics at the University of California at Berkeley, and current Director of the Center for the Neural Mind & Society ^4 . We use it here to avoid the common pitfall of point-by-point refutation of obstacles and misconceptions, to instead discover a transformative, indisputable win for individuals from self-sovereign identity systems. The process of reframing is based on finding blind spots. When you detect blind spots, either in the user or in the self, you unveil the potential for flipping the system. Useful techniques include ethnography, empathy, design-thinking and design sprints. Rather than fighting existing references, create a new one, framed indisputably in terms of something individuals clearly want. Eschew obscure technical jargon for emotionally impactful and concise trigger phrases that immediately resonate. When you illuminate a blind spot with emotionally desirable terminology, listeners naturally align their mental model and find it hard to challenge the excitement and possibility. By focusing on a single win for each individual, we further simplify the message and focus positive resonance. Reframing greatly increases the possibility for substantive engagement and adoption. A Day in the Life of Rutu Shah A fictional persona, Ms. Rutu Shah is the mayor of a modest city, Ahmedabad, India ^5 . Here we envision a day in her live, enhanced by self-sovereign identity.","title":"Re-Imagining What Users Really Want"},{"location":"RWoT4/what-users-really-want/#re-imagining-what-users-really-want","text":"","title":"Re-Imagining What Users Really Want"},{"location":"RWoT4/what-users-really-want/#a-paper-of-the-rebooting-the-web-of-trust-workshop-paris-2017","text":"","title":"A paper of the Rebooting the Web of Trust workshop, Paris, 2017"},{"location":"RWoT4/what-users-really-want/#by-joe-andrieu-joelegendaryrequirementscom-frederic-engel-marketengelgmailcom-adam-lake-creatinglakegmailcom-moses-ma-mosesmafuturelabconsultingcom-olivier-maas-oliviermaasworldlinecom-mark-van-der-waal-markvanderwaalsovalacccom","text":"We consider five fictional personas to explore the obstacles and opportunities for self-sovereign identity. For one, Rutu Shah, we describe an illustrative day in her life.","title":"By Joe Andrieu &lt;joe@legendaryrequirements.com&gt;, Frederic Engel &lt;market.engel@gmail.com&gt;, Adam Lake &lt;creatinglake@gmail.com&gt;, Moses Ma &lt;moses.ma@futurelabconsulting.com&gt;, Olivier Maas &lt;olivier.maas@worldline.com&gt;, Mark van der Waal &lt;mark.vanderwaal@sovalacc.com&gt;"},{"location":"RWoT4/what-users-really-want/#five-personas-five-opportunities","text":"The following five individuals represent a diverse sampling of normal, everyday people whose lives could be improved by self-sovereign identity. Today, these five aren\u2019t using enhanced digital identity. For each, we describe their current lack of engagement and provide at least one opportunity that could reframe self-sovereign identity as a clear win for them.","title":"Five Personas, Five Opportunities"},{"location":"RWoT4/what-users-really-want/#renee-moreau","text":"Mme. Ren\u00e9e Moreau is a seventy-eight-year-old grandmother living in the town of Amiens, France ^1 . She has a low level of understanding about information technology, but fears being hacked and losing what meager savings she has left. To the extent that she has discussed it with her children, Ren\u00e9e Moreau feels \u201cfancy\u201d identity solutions are just too complicated for her to use. Reframing: FamilyPix secure picture sharing enables Renee to receive photos of her grandchildren without exposing them to pedophiles or marketers. Bonus Reframing: A friends-of-family web of trust enables support when emergencies arise. Family-of-friends and friends-of-family are discretely recruited for real-time on-demand response to minor emergencies and quality-of-life interactions such as getting locked out of the house, trip-and-fall incidents, and needing a ride to a doctor visit.","title":"Ren\u00e9e Moreau"},{"location":"RWoT4/what-users-really-want/#jack-smith","text":"Mr. Jack Smith is the fifty-seven-year-old Chief Information Officer of a Fortune 1000 company in the United States. He lives in Portland, Oregon with his wife and ten-year-old son. He has earned a reputation as a shrewd debunker of silver bullet initiatives that promise miracle results but rarely deliver. As a rule, he has found identity solutions too burdensome for integrated deployment throughout the hundreds of applications deployed across his company. Instead, core corporate applications are managed through a centralized Active Directory service and departmental applications are free to implement their own identity approach. He underestimates his company risk for regulatory compliance and tends to avoid technical topics where his aging expertise might appear dated. Employees spend as much as 15 minutes per day dealing with logins at both intranet and Internet sites. Phishing is a significant, but overlooked security risk. Reframing: Super Sign-On brings corporate identity data into compliance while unifying secure logins for applications across the enterprise and beyond the corporate boundary at online services like Trello, Atlassian (Jira, Confluence, etc), and Dropbox.","title":"Jack Smith"},{"location":"RWoT4/what-users-really-want/#mayumi-takamasa","text":"Miss Mayumi Takamasa is a seventeen-year-old female student in Tokyo, Japan. In conversations about privacy risks, she is dismissive and doesn\u2019t care about advanced identity management. Online she has seen cyberbullying among her peers and goes to some effort to create boundaries between what she and her friends post for each other and what her mother can see. Reframing: The MySociety social game creates independent identity across social network platforms \u2014 without exposing content to uninvited eyes. The game creates pseudonymous social spaces with built-in privacy, disappearing messages, and points, levels, and badges based on social interactions.","title":"Mayumi Takamasa"},{"location":"RWoT4/what-users-really-want/#joram-hadad","text":"Mr. Joram Hadad is a twenty-five-year-old Syrian refugee from Damascus, traveling through Greece on his way to resettlement in France. He fears reprisal against his family should the Syrian government discover he made it out of the country. He has no technology to speak of and has provided authorities with a fake name. He needs health care and basic human services. Reframing: MyID , a biometric-and-PIN-secured digital datastore in a USB thumb drive, gives Joram physical control over identity data that only he has the power to share.","title":"Joram Hadad"},{"location":"RWoT4/what-users-really-want/#rutu-shah","text":"Ms. Rutu Shah is a thirty-eight-year-old Mayor of a modest city in India, Ahmedabad ^2 . In her daily life she is wary of phishing attacks and security risks, but social and economic issues have taken priority over pushing for better identity across city services. She understands the value, but struggles to push for change, as commitments to drinkable water, better education, and sustainable jobs got her elected. She is particularly concerned about online predators and children\u2019s safety. Reframing: SafetyNet creates personalized peer-to-peer, independent trust networks where parents, educators, mentors, and social workers vet and connect trusted resources for children. The same service also allows professional networks to flourish through phish-proof, trusted introductions, connecting entrepreneurs with business development, mentors, and funding sources.","title":"Rutu Shah"},{"location":"RWoT4/what-users-really-want/#the-magic-of-reframing","text":"Reframing is a powerful tool, based on the work of George Lakoff ^3 , linguist, former Distinguished Professor of Cognitive Science and Linguistics at the University of California at Berkeley, and current Director of the Center for the Neural Mind & Society ^4 . We use it here to avoid the common pitfall of point-by-point refutation of obstacles and misconceptions, to instead discover a transformative, indisputable win for individuals from self-sovereign identity systems. The process of reframing is based on finding blind spots. When you detect blind spots, either in the user or in the self, you unveil the potential for flipping the system. Useful techniques include ethnography, empathy, design-thinking and design sprints. Rather than fighting existing references, create a new one, framed indisputably in terms of something individuals clearly want. Eschew obscure technical jargon for emotionally impactful and concise trigger phrases that immediately resonate. When you illuminate a blind spot with emotionally desirable terminology, listeners naturally align their mental model and find it hard to challenge the excitement and possibility. By focusing on a single win for each individual, we further simplify the message and focus positive resonance. Reframing greatly increases the possibility for substantive engagement and adoption.","title":"The Magic of Reframing"},{"location":"RWoT4/what-users-really-want/#a-day-in-the-life-of-rutu-shah","text":"A fictional persona, Ms. Rutu Shah is the mayor of a modest city, Ahmedabad, India ^5 . Here we envision a day in her live, enhanced by self-sovereign identity.","title":"A Day in the Life of Rutu Shah"},{"location":"RWoT5/","text":"Rebooting the Web of Trust V: Boston (October 2017) This repository contains documents related to RWOT5, the fifth Rebooting the Web of Trust design workshop, which is to run in Boston, Massachusetts, on October 3rd-5th, 2017. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers. Completed Papers The design workshop exceeded its goal of three to five white papers with a total of eight publications: ActivityPub: From Decentralized to Distributed Social Networks (Text) by Christopher Lemmer Webber & Manu Sporny An overview of the W3C ActivityPub protocol. Amira 1.0.0 (Text) by Joe Andrieu & Team This paper reinterprets Christopher Allen\u2019s Rebooting the Web of Trust user story through the lens of the Information Lifecycle Engagement Model. It presents a human-centric illustration of an individual\u2019s experience in a self-sovereign, decentralized realization of the Web of Trust as originally conceived by Phil Zimmerman for PGP. The DCS Theorem by Greg Slepak & Anya Petrova A probability proof of the DCS Triangle. Why can't decentralized consensus systems have all three of decentralization, consensus, and scale? Plus, two methods for getting around these limitations. A Decentralized Approach to Blockcerts Credential Revocation (Text) by Jo\u00e3o Santos & Kim Hamilton Duffy Blockcerts are blockchain-anchored credentials with a verification process designed to be decentralized and trustless. This proposal describes an alternate method of issuing Blockcerts using Ethereum, which allows for a new form of revocation by either the issuer or the recipient. Engineering Privacy for Verified Credentials: In Which We Describe Data Minimization, Selective Disclosure, and Progressive Trust (Text) by Lionel Wolberger, Brent Zundel, Zachary Larson, Irene Hernandez & Katryna Dow We often share information on the World Wide Web, though some of it is private. The W3C Credentials Community Group focuses on how privacy can be enhanced when attributes are shared electronically. In the course of our work, we have identified three related but distinct privacy enhancing strategies: \"data minimization,\" \"selective disclosure,\" and \"progressive trust.\" These enhancements are enabled with cryptography. The goal of this paper is to enable decision makers, particularly non-technical ones, to gain a nuanced grasp of these enhancements along with some idea of how their enablers work. We describe them below in plain English, but with some rigor. This knowledge will enable readers of this paper to be better able to know when they need privacy enhancements, to select the type of enhancement needed, to assess techniques that enable those enhancements, and to adopt the correct enhancement for the correct use case. Identity Hubs Capabilities Perspective (Text) by Adrian Gropper, Drummond Reed & Mark S. Miller Identity Hubs as currently proposed in the Decentralized Identity Foundation (DIF) are a subset of a general Decentralized Identifier (DID) based user-controlled agent, based on ACLs rather than an object-capabilities (ocap) architecture. Transitioning the Hubs design to an ocap model can be achieved by introducing an UMA authorization server as the control endpoint. Linked Data Capabilities (Text) Christopher Lemmer Webber & Mark S. Miller Linked Data Signatures enable a method of asserting the integrity of linked data documents that are passed throughout the web. The object capability model is a powerful system for ensuring the security of computing systems. Veres One DID Method (Text) by Manu Sporny & Dave Longley The Veres One Ledger is a permissionless public ledger designed specifically for the creation and management of decentralized identifiers (DIDs). This specification defines how a developer may create and update DIDs in the Veres One Ledger. When GDPR becomes real, and Blockchain is no longer Fairy Dust (Text) by Marta Piekarska, Michael Lodder, Zachary Larson & Kaliya Young (Identity Woman) This document describes the GDPR requirements and the different approaches to digital identity solutions and finally explains why distributed ledger technology may offer an opportunity for enterprises to simplify data management solutions that are GDPR compliant. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Index"},{"location":"RWoT5/#rebooting-the-web-of-trust-v-boston-october-2017","text":"This repository contains documents related to RWOT5, the fifth Rebooting the Web of Trust design workshop, which is to run in Boston, Massachusetts, on October 3rd-5th, 2017. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community, including upcoming events.","title":"Rebooting the Web of Trust V: Boston (October 2017)"},{"location":"RWoT5/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Advance Readings README for a listing of all of the papers.","title":"Topics &amp; Advance Readings"},{"location":"RWoT5/#completed-papers","text":"The design workshop exceeded its goal of three to five white papers with a total of eight publications:","title":"Completed Papers"},{"location":"RWoT5/#activitypub-from-decentralized-to-distributed-social-networks-text","text":"","title":"ActivityPub: From Decentralized to Distributed Social Networks (Text)"},{"location":"RWoT5/#by-christopher-lemmer-webber-manu-sporny","text":"An overview of the W3C ActivityPub protocol.","title":"by Christopher Lemmer Webber &amp; Manu Sporny"},{"location":"RWoT5/#amira-100-text","text":"","title":"Amira 1.0.0 (Text)"},{"location":"RWoT5/#by-joe-andrieu-team","text":"This paper reinterprets Christopher Allen\u2019s Rebooting the Web of Trust user story through the lens of the Information Lifecycle Engagement Model. It presents a human-centric illustration of an individual\u2019s experience in a self-sovereign, decentralized realization of the Web of Trust as originally conceived by Phil Zimmerman for PGP.","title":"by Joe Andrieu &amp; Team"},{"location":"RWoT5/#the-dcs-theorem","text":"","title":"The DCS Theorem"},{"location":"RWoT5/#by-greg-slepak-anya-petrova","text":"A probability proof of the DCS Triangle. Why can't decentralized consensus systems have all three of decentralization, consensus, and scale? Plus, two methods for getting around these limitations.","title":"by Greg Slepak &amp; Anya Petrova"},{"location":"RWoT5/#a-decentralized-approach-to-blockcerts-credential-revocation-text","text":"","title":"A Decentralized Approach to Blockcerts Credential Revocation (Text)"},{"location":"RWoT5/#by-joao-santos-kim-hamilton-duffy","text":"Blockcerts are blockchain-anchored credentials with a verification process designed to be decentralized and trustless. This proposal describes an alternate method of issuing Blockcerts using Ethereum, which allows for a new form of revocation by either the issuer or the recipient.","title":"by Jo\u00e3o Santos &amp; Kim Hamilton Duffy"},{"location":"RWoT5/#engineering-privacy-for-verified-credentials-in-which-we-describe-data-minimization-selective-disclosure-and-progressive-trust-text","text":"","title":"Engineering Privacy for Verified Credentials: In Which We Describe Data Minimization, Selective Disclosure, and Progressive Trust (Text)"},{"location":"RWoT5/#by-lionel-wolberger-brent-zundel-zachary-larson-irene-hernandez-katryna-dow","text":"We often share information on the World Wide Web, though some of it is private. The W3C Credentials Community Group focuses on how privacy can be enhanced when attributes are shared electronically. In the course of our work, we have identified three related but distinct privacy enhancing strategies: \"data minimization,\" \"selective disclosure,\" and \"progressive trust.\" These enhancements are enabled with cryptography. The goal of this paper is to enable decision makers, particularly non-technical ones, to gain a nuanced grasp of these enhancements along with some idea of how their enablers work. We describe them below in plain English, but with some rigor. This knowledge will enable readers of this paper to be better able to know when they need privacy enhancements, to select the type of enhancement needed, to assess techniques that enable those enhancements, and to adopt the correct enhancement for the correct use case.","title":"by Lionel Wolberger, Brent Zundel, Zachary Larson, Irene Hernandez &amp; Katryna Dow"},{"location":"RWoT5/#identity-hubs-capabilities-perspective-text","text":"","title":"Identity Hubs Capabilities Perspective (Text)"},{"location":"RWoT5/#by-adrian-gropper-drummond-reed-mark-s-miller","text":"Identity Hubs as currently proposed in the Decentralized Identity Foundation (DIF) are a subset of a general Decentralized Identifier (DID) based user-controlled agent, based on ACLs rather than an object-capabilities (ocap) architecture. Transitioning the Hubs design to an ocap model can be achieved by introducing an UMA authorization server as the control endpoint.","title":"by Adrian Gropper, Drummond Reed &amp; Mark S. Miller"},{"location":"RWoT5/#linked-data-capabilities-text","text":"","title":"Linked Data Capabilities (Text)"},{"location":"RWoT5/#christopher-lemmer-webber-mark-s-miller","text":"Linked Data Signatures enable a method of asserting the integrity of linked data documents that are passed throughout the web. The object capability model is a powerful system for ensuring the security of computing systems.","title":"Christopher Lemmer Webber &amp; Mark S. Miller"},{"location":"RWoT5/#veres-one-did-method-text","text":"","title":"Veres One DID Method (Text)"},{"location":"RWoT5/#by-manu-sporny-dave-longley","text":"The Veres One Ledger is a permissionless public ledger designed specifically for the creation and management of decentralized identifiers (DIDs). This specification defines how a developer may create and update DIDs in the Veres One Ledger.","title":"by Manu Sporny &amp; Dave Longley"},{"location":"RWoT5/#when-gdpr-becomes-real-and-blockchain-is-no-longer-fairy-dust-text","text":"","title":"When GDPR becomes real, and Blockchain is no longer Fairy Dust (Text)"},{"location":"RWoT5/#by-marta-piekarska-michael-lodder-zachary-larson-kaliya-young-identity-woman","text":"This document describes the GDPR requirements and the different approaches to digital identity solutions and finally explains why distributed ledger technology may offer an opportunity for enterprises to simplify data management solutions that are GDPR compliant.","title":"by Marta Piekarska, Michael Lodder, Zachary Larson &amp; Kaliya Young (Identity Woman)"},{"location":"RWoT5/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"RWoT5/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"RWoT5/activitypub-decentralized-distributed/","text":"ActivityPub: from decentralized to distributed social networks This paper was written originally for the 2017 Rebooting Web of Trust summit. Contributors * Christopher Webber, Independent * Manu Sporny, Digital Bazaar Introduction ActivityPub is a protocol being developed at the W3C for the purpose of building federated social systems. Users can use implementations of ActivityPub like Mastodon and MediaGoblin as libre alternatives to large siloed social networking systems such as Facebook, Twitter, YouTube, and Instagram. 1 In general ActivityPub follows the client-server paradigm that has been popular on the World Wide Web, while restoring some level of decentralization. Current implementations of ActivityPub go as far as to bring a level of decentralization akin to email, 2 but there are many opportunities to go further. By attaching public keys to the profiles of actors (users) on the network and using Linked Data Signatures , we can add a web of trust to the federated social web and use it to enhance user privacy and to assert the integrity of messages sent over the network. By using a decentralized identifier system such as Decentralized Identifiers (DIDs) we can move fully from a decentralized to a distributed system, 3 by escaping the core centralization mechanisms of DNS and SSL certificate authorities. At this point, users could even optionally transition from a client-server model system to a fully peer-to-peer system. ActivityPub overview This section is borrowed from the ActivityPub standard's Overview section; if you are already familiar with ActivityPub then you may skip this section. ActivityPub provides two layers: - **A server-to-server federation protocol:** so decentralized websites can share information - **A client-to-server protocol:** so users can communicate with ActivityPub using servers, from a phone or desktop or web application or whatever ActivityPub implementations can implement just one of these things or both of them. However, once you've implemented one, it isn't too many steps to implement the other, and there are a lot of benefits to both (making your website part of the decentralized social web and able to use clients and client libraries that work across a wide variety of social websites). In ActivityPub, every actor (users are represented as \"actors\" here) has: - An **inbox**: How they get messages from the world - An **outbox**: How they send messages to others ![img](./images/activitypub/tutorial-1.png \"Actor with inbox and outbox\") These are endpoints, or really, just URLs which are listed in the ActivityPub actor's ActivityStreams description. (More on ActivityStreams later.) Here's an example of the record of our friend Alyssa P. Hacker: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Person\", \"id\": \"https://social.example/alyssa/\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"https://social.example/alyssa/inbox/\", \"outbox\": \"https://social.example/alyssa/outbox/\", \"followers\": \"https://social.example/alyssa/followers/\", \"following\": \"https://social.example/alyssa/following/\", \"liked\": \"https://social.example/alyssa/liked/\"} ActivityPub uses [ActivityStreams](https://www.w3.org/TR/activitystreams-core/) for its [vocabulary](https://www.w3.org/TR/activitystreams-vocabulary/). This is pretty great because ActivityStreams includes all the common terms you need to represent all the activities and content flowing around a social network. It's likely that ActivityStreams already includes all the vocabulary you need, but even if it doesn't, ActivityStreams can be extended via [JSON-LD](https://json-ld.org/). If you know what JSON-LD is, you can take advantage of the cool linked data approaches provided by JSON-LD. If you don't, don't worry, JSON-LD documents and ActivityStreams can be understood as plain old simple JSON. (If you're going to add extensions, that's the point at which JSON-LD really helps you out.) So, okay. Alyssa wants to talk to her friends, and her friends want to talk to her! Luckily these \"inbox\" and \"outbox\" things can help us out. They both behave differently for GET and POST. ![img](./images/activitypub/tutorial-2.png \"Actor with messages flowing from rest of world to inbox and from outbox to rest of world\") So the full workflow is: - You can POST to someone's inbox to send them a message (server-to-server / federation only\u2026 this *is* federation!) - You can GET from your inbox to read your latest messages (client-to-server; this is like reading your social network stream) - You can POST to your outbox to send messages to the world (client-to-server) - You can GET from someone's outbox to see what messages they've posted, or at least the ones you're authorized to see. (client-to-server and/or server-to-server) Of course, if that last one (GET'ing from someone's outbox) was the only way to see what people have sent, this wouldn't be a very efficient federation protocol! Indeed, federation happens usually by servers posting messages sent by actors to actors on other servers' inboxes. Let's see an example! Let's say Alyssa wants to catch up with her friend, Ben Bitdiddle. She lent him a book recently and she wants to make sure he returns it to her. Here's the message she composes, as an ActivityStreams object: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Note\", \"to\": [\"https://chatty.example/ben/\"], \"attributedTo\": \"https://social.example/alyssa/\", \"content\": \"Say, did you finish reading that book I lent you?\"} This is a note addressed to Ben. She POSTs it to her outbox. ![img](./images/activitypub/tutorial-3.png \"Actor posting message to outbox\") Since this is a non-activity object, the server recognizes that this is an object being newly created, and does the courtesy of wrapping it in a Create activity. (Activities sent around in ActivityPub generally follow the pattern of some activity by some actor being taken on some object. In this case the activity is a Create of a Note object, posted by a Person.) {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://social.example/alyssa/posts/a29a6843-9feb-4c74-...\", \"to\": [\"https://chatty.example/ben/\"], \"actor\": \"https://social.example/alyssa/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/49e2d03d-b53a-4c4c-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://chatty.example/ben/\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} Alyssa's server looks up Ben's ActivityStreams actor object, finds his inbox endpoint, and POSTs her object to his inbox. ![img](./images/activitypub/tutorial-4.png \"Server posting to remote actor's inbox\") Technically these are two separate steps\u2026 one is client-to-server communication, and one is server-to-server communication (federation). But, since we're using them both in this example, we can abstractly think of this as being a streamlined submission from outbox to inbox. ![img](./images/activitypub/tutorial-5.png \"Note flowing from one actor's outbox to other actor's inbox\") A while later, Alyssa checks what new messages she's gotten. Her phone polls her inbox via GET, and amongst a bunch of cat videos posted by friends and photos of her nephew posted by her sister, she sees [4](#fn.4) the following: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://chatty.example/ben/p/51086\", \"to\": [\"https://social.example/alyssa/\"], \"actor\": \"https://chatty.example/ben/\", \"object\": {\"type\": \"Note\", \"id\": \"https://chatty.example/ben/p/51085\", \"attributedTo\": \"https://chatty.example/ben/\", \"to\": [\"https://social.example/alyssa/\"], \"inReplyTo\": \"https://social.example/alyssa/posts/49e2d03d-b53a-...\", \"content\": \"Argh, yeah, sorry, I'll get it back to you tomorrow. I was reviewing the section on register machines, since it's been a while since I wrote one.\"}} Alyssa is relieved, and likes Ben's post: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Like\", \"id\": \"https://social.example/alyssa/posts/5312e10e-5110-42e5-...\", \"to\": [\"https://chatty.example/ben/\"], \"actor\": \"https://social.example/alyssa/\", \"object\": \"https://chatty.example/ben/p/51086\"} She POSTs this message to her outbox. (Since it's an activity, her server knows it doesn't need to wrap it in a Create object.) Feeling happy about things, she decides to post a public message to her followers. Soon the following message is blasted to all the members of her followers collection, and since it has the special Public group addressed, is generally readable by anyone. {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://social.example/alyssa/posts/9282e9cc-14d0-42b3-...\", \"to\": [\"https://social.example/alyssa/followers/\", \"https://www.w3.org/ns/activitystreams#Public\"], \"actor\": \"https://social.example/alyssa/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/d18c55d4-8a63-4181-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://social.example/alyssa/followers/\", \"https://www.w3.org/ns/activitystreams#Public\"], \"content\": \"Lending books to friends is nice. Getting them back is even nicer! :)\"}} ## Bringing public key cryptography to the federated social web We can dramatically improve the state of the federated social web by having each actor on the system hold a public and private keypair, and by having actors have their public key attached directly to their actor object: {\"@context\": [\"https://www.w3.org/ns/activitystreams\", \"https://w3id.org/security/v1\"], \"id\": \"https://schemers.example/u/alyssa\", \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"publicKey\": [{ \"id\": \"https://schemers.example/u/alyssa#main-key\", \"owner\": \"https://schemers.example/u/alyssa\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} This provides significant improvements to the system, which we explore below. ### Signing objects Sharing messages is common in social networks. But how can you verify that someone really said what they claimed? The user Mallet is trying to cause havoc in their social network. They pretend to \"share\" [5](#fn.5) the following post that they claim Alyssa sent to the pasta-enthusiasts group, which Ben is a member of. {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Announce\", \"id\": \"https://havoc.example/~mallet/p/90815\", \"to\": [\"https://pastalovers.example/groups/pasta-enthusiasts/\"], \"actor\": \"https://havoc.example/~mallet/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/63cc87ec-416e-437d-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://havoc.example~mallet/\"], \"content\": \"Tortellini is a poor and disgusting imitation of ravioli. Any chef serving tortellini should hang up their aprons in disgrace and never cook again.\", \"signature\": { \"type\": \"RsaSignature2017\", \"creator\": \"https://social.example/alyssa/\", \"created\": \"2017-09-23T20:21:34Z\", \"nonce\": \"e3689a56da9b4bc\", \"signatureValue\": \"mJfe5OCb7J3WwI...8t5/m=\"}}, \"signature\": { \"type\": \"RsaSignature2017\", \"creator\": \"https://social.example/alyssa/\", \"created\": \"2017-09-23T21:32:21Z\", \"nonce\": \"22e8e7683f56c08bb873\", \"signatureValue\": \"wTjLtnZVYF79pq9Ts...OU1jYPSjvcE2jNc=\"}} Ben's server, or even the server hosting `pastalovers.example`, can check the signature against the `publicKey` listed on Alyssa's actor object. This check fails, and so Mallet's attempt at slander of Alyssa amongst the pasta enthusiast community fails. While the above example looks at protecting against a malicious interaction, forwarding and sharing content is desirable for positive reasons. One common problem in federated social networks that support private interactions is that a conversation can become fragmented: if Ben is posting to private collection [6](#fn.5) she has curated containing both his friends and coworkers, and members of coworkers can't see who is in the private family collection, when they address to include the family in the conversation they can't traverse the collection of family actors to deliver to all relevant participants. (This \"ghost replies\" problem happens frequently on federated networks even when messages are being sent to the actor's own `followers`, where breaks tend to happen across server boundaries.) ActivityPub includes a solution to this via a [forwarding mechanism](https://www.w3.org/TR/activitypub/#inbox-delivery), but the solution does not really work without signatures, as the forwards are happening indirectly rather than from the \"same origin/domain\", so servers are unable to check/trust that the content is as claimed. Even if the receiving server tries to look up the object the receiving actor's credentials, access control may not have been enabled for the actor who was forwarded to, since the commenter had no way of knowing who was in the private collection to enable access for. This is a frequently requested feature in federated social networks, so we should ensure that the necessary public key infrastructure is provided. [7](#fn.7) ### An easier to use web of trust? The PGP-style \"web of trust\" has been around for some time now, but the term \"web of trust\" is somewhat mired by the historically most popular method by which the trust network has populated. Key signing parties, while effective, have never taken off beyond a very small set of the population. Such parties are rewarding but difficult for most of the population to attend and organize, and even more difficult still is learning the (generally) command line tooling necessary to participate in the system. While some work has been done in this area (for example with [Monkeysign](https://monkeysign.readthedocs.io/en/2.x/) and [Gibberbot](https://guardianproject.info/apps/gibber/)), it would be even better if building your trust network was incidental to participating in the network. [8](#fn.8) To a certain extent, this could come \"for free, with caveats\" in ActivityPub deployments that exist today, where subscriptions and object lookup are done over HTTPS. Merely by sending a follow request (or some other action connecting users on the social graph) a certain amount of trust between users can be expressed. Keys can be looked up and recorded at actor profile urls, and users can even observe and share information about whom else they know on the social network. There's a major caveat using HTTPS for these lookups requires trust in SSL certificate authorities. Better than nothing, but not great, and not the distributed systems we want. Furthermore, a malicious actor can still trick users; a user may believe they are subscribing to `https://social.example/alyssa/`, but perhaps Mallet tricked them into subscribing to `https://social.example/alyssaa/` instead. [9](#fn.9) Happily there are other ways to encourage stronger trust networks. Carl Ellison's paper *[Establishing Identity Without Certification Authorities](http://world.std.com/~cme/usenix.html)* describes several classes of relationships amongst users, and many Off The Record clients (such as available in Pidgin, etc) provide interfaces for verifying challenges between users. Users on a federated social network could be provided an opportunity to perform a textual challenge, perform a brief video call where they verify a shared code (as done in Jitsi), or scan a QR code (as in Monkeysign and Gibberbot) to establish stronger trust that an actor on the network is the entity they claim to be. The level of trust gained could be signed, recorded, and itself propagated as appropriate throughout the network. This kind of mechanism would work nicely even in a system like DIDs, where a human-readable identifier does not exist. ### End-to-end encryption A malicious server administrator may still snoop on all communication of participants on a system. Even a non-malicious administrator may be coerced into snooping on their users, or may have their entire system compromised without their knowledge. SSL Certificate Authorities may also be compromised into giving out fake certificates, allowing man in the middle attacks that neither the user nor server administrator may be aware of. End-to-end encryption can solve this (with some tradeoffs); in this case, rather than having the server manage the public and private keys of a user, a user may provide a public key on their actor object to which only their own computer(s) hold the corresponding private key. Other actors on the network may then send an object encrypted to the actor's inbox. For example, an actor may receive the following object [9](#fn.10) in their inbox: {\"@context\": [\"https://securityns.example/\", \"https://www.w3.org/ns/activitystreams\"], \"type\": \"EncryptedEnvelope\", \"encryptedMessage\": \"-----BEGIN PGP MESSAGE-----\\r\\n...\", \"mediaType\": \"application/ld+json; profile=\\\"https://www.w3.org/ns/activitystreams\\\"\"} The server would put this object in the user's inbox, but if only the user's own computers hold the key, even the server would be unable to read the contents held within the envelope. Upon retrieving the object from the server via the client-to-server protocol, the user's client can decrypt the message. In this case, the message went directly to Alyssa's inbox. Upon decrypting the component in encryptedMessage, another object is found: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Note\", \"id\": \"https://chatty.example/ben/p/86187\", \"to\": [\"https://social.example/alyssa/\"], \"attributedTo\": \"https://chatty.example/ben/\", \"content\": \"Up for some root beer floats at my friend's house? Here's the address: ...\"} Note that while this improves privacy, it does come with several tradeoffs: - ActivityPub contains an entire suite of [server side side-effects](https://www.w3.org/TR/activitypub/#server-to-server-interactions) for federating common activities on a social network. For example, a Like object will increment a counter of how often a post has been liked, and even add that liked object to both the user's collection of liked objects as well as a collection of all users who have liked this object. Since servers are unable to observe data being sent across the network, these kinds of side effects will break. The server will also be unable to provide additional features such as being able to have server-based indexing of messages for easy search. [11](#fn.11) In a \"more peer-to-peer\" system (as discussed in the Distributed identity section) this becomes less of an issue because the distinction between client server blurs. Nevertheless, for existing client-to-server implementations, this is a strong issue to consider. - User maintenance of keys in end-to-end encryption systems is known to be a difficult user experience problem. - Key recovery is even harder. [DIDs](https://w3c-ccg.github.io/did-spec/) explore a method for key recovery, but this will not help users read old messages encrypted with keys they no longer have and which the original senders cannot send (or do not know how to). ## Distributed identity ActivityPub implementations at the present moment rely on HTTPS as their transport, which in turn relies on two centralized systems: DNS and SSL certificate authorities. Is there any way to bring self-sovereignty to the federated social web? Thankfully there is; ActivityPub was written intentionally to be layerable on any protocol that can support HTTP GET and POST verbs. The [Decentralized Identifiers](https://w3c-ccg.github.io/did-spec/) specification looks to be a good fit for ActivityPub. The simplest version of this can be seen simply by replacing the actor ids with DIDs. To transform an example from the overview from: {\"type\": \"Note\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://chatty.example/ben/\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} to: {\"type\": \"Note\", \"attributedTo\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"to\": [\"did:example:nJx2fgreaSfCujA0kMsiEW8Oz\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} Gosh! That was simple-ish. All we did was replace the human-readable identifiers representing the users with DIDs. If we look up Alyssa's [DID](https://w3c-ccg.github.io/did-spec/#dids-(decentralized-identifiers)) based id, we can retrieve her actor object as a [DDO](https://w3c-ccg.github.io/did-spec/#ddos-(did-descriptor-objects)), but this time there is extra information: { \"@context\": [\"https://example.org/did/v1\", \"https://www.w3.org/ns/activitystreams\"], \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"activityPubService\": { \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#services/ActivityPub\", // ActivityPub actor information \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"https://9GaksjPhy0mWToTV.onion/alyssa/inbox/\", \"outbox\": \"https://9GaksjPhy0mWToTV.onion/alyssa/outbox/\", \"followers\": \"https://9GaksjPhy0mWToTV.onion/alyssa/followers/\", \"following\": \"https://9GaksjPhy0mWToTV.onion/alyssa/following/\", \"liked\": \"https://9GaksjPhy0mWToTV.onion/alyssa/liked/\"}, // DDO information \"owner\": [{ \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#key-1\", \"type\": [\"CryptographicKey\", \"EdDsaPublicKey\"], \"curve\": \"ed25519\", \"expires\": \"2017-02-08T16:02:20Z\", \"publicKeyBase64\": \"lji9qTtkCydxtez/bt1zdLxVMMbz4SzWvlqgOBmURoM=\" }, { \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#key-2\", \"type\": [\"CryptographicKey\", \"RsaPublicKey\"], \"expires\": \"2017-03-22T00:00:00Z\", \"publicKeyPem\": \"----BEGIN PUBLIC KEY-----\\r\\n..\" }], \"control\": [{ \"type\": \"OrControl\", \"signer\": [ \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"did:example:8uQhQMGzWxR8vw5P3UWH1j\" ] }], \"created\": \"2002-10-10T17:00:00Z\", \"updated\": \"2016-10-17T02:41:00Z\", \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:8uQhQMGzWxR8vw5P3UWH1j#key/1\", \"signatureValue\": \"IOmA4R7TfhkYTYW8...CBMq2/gi25s=\" } } Hoo! That's a lot of additions. Except here we see an example of Alyssa's profile that is entirely free of traditional centralized DNS authorities. We were able to look up Alyssa's object via her DID, but we still have access to all her endpoints, which in this case are pointing to [Tor Hidden Services](https://www.torproject.org/docs/hidden-services.html.en). No central DNS required! Maybe in the future there will even be a protocol \u2013 let's call it `httpeer` \u2013 which supports all the standard HTTP verbage, but over some other peer-to-peer network. The DID spec supports [service endpoints](https://w3c-ccg.github.io/did-spec/#service-endpoint-references-(optional)), and Alyssa could take advantage of these to use her DID as base of the `inbox`, `outbox`, etc URIs. Here's a cut down and modified version of the previous example: { \"@context\": [\"https://example.org/did/v1\", \"https://www.w3.org/ns/activitystreams\"], \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"activityPubService\": { \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#services/ActivityPub\", // ActivityPub actor information \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/inbox/\", \"outbox\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/outbox/\", \"followers\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/followers/\", \"following\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/following/\", \"liked\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/liked/\"}, // DDO information goes here \"httpeerService\": { \"nodeId\": \"dI0tuXjISZEadSH6QV9EhBEdccL4ouePdF8P57BJ\"}} Now that's an identity system! ## Append only systems and content addressed storage Finally, it's worth mentioning the idea of moving ActivityPub to an entirely append-only, content-addressed system for object storage, \"modification\", and retrieval. Much success has been seen in recent years with these systems; enacting this change would allow for many of the side effects in the federation system to be dropped entirely. We leave this as a topic for a future paper. ## Conclusions ActivityPub goes a long way towards providing a standardized way to move the social web from isolated, centralized silos towards the decentralized nature that the World Wide Web is meant to encompass. Still, there is much to be done and improved. Yet there are risks in trying to engineer the right system all at once, and great is well known to be the enemy of good. Thankfully we do not need to throw out what we have to make the improvements that are discussed in this paper. ActivityPub already exists and works, and we can incrementally improve the systems we have and blur the line between the federated social web that works and more peer-to-peer systems which are desirable. By adding public key infrastructure and distributed identifiers to ActivityPub we can move from a decentralized system to a distributed one and truly build a network that is both self-sovereign and social. ## Acknowledgments Thanks to Evan Prodromou and Owen Shepherd for working on initial revisions of the ActivityPub standard. Thanks also to Jessica Tallon for being co-editor on ActivityPub and [everyone who helped make ActivityPub happen](https://www.w3.org/TR/activitypub/#acknowledgements), which is no small list. (Some of those people caught typos in the original version of the overview, as included in ActivityPub.) Thanks to Manu Sporny and Stephen Webber for thoughts and feedback about how to make ActivityPub into a more robust distributed system. Thanks to Dave Longley for pointing to the vocabulary drift / ambiguity of the terms \"decentralized\" and \"distributed\", which lead to the addition of Paul Baran's diagrams and some terminology clarification. [3](#fn.3) Thanks to Morgan Lemmer-Webber for the patience and for careful proofreading of this document. Thanks to Spec-Ops and Digital Bazaar for supporting my work/time on Verifiable Claims/Credentials. Though separate from my work on ActivityPub, obviously I've been thinking a lot about how to combine them this whole time. Thanks also to mray for the gorgeous illustrations in the overview section. This document and its images (with the exception of Paul Baran's drawings), like ActivityPub itself, are licensed under [W3C's permissive document license](https://www.w3.org/Consortium/Legal/2015/copyright-software-and-document). ## Footnotes: 1 Of course, there is nothing stopping current-silos of social networking from adopting ActivityPub, would they be willing to un-silo their users. 2 Observant readers may note that email is no longer as decentralized of a system as it once was. Consider this a lesson that a protocol alone cannot build a distributed network; the community must build and maintain a healthy number of nodes and avoid the temptation to let a few large providers control the space of a federated network. 3 It is worth spending some time to discuss what is meant by \"centralized\" versus \"distributed\" versus \"decentralized\". ![img](./static/centralized_decentralized_distributed.png \"Centralized, Decentralized, and Distributed drawings, from 'On Distributed Communications, part 1' by Paul Baran, 1964\") In the figure above we see images from Paul Baran's 1964 paper on the subject, and from these shapes we can see the kinds of shapes we mean: social silos resemble the the spoke-like centralized model, client-server federated social networks resemble the tree-like decentralized model, and a peer-to-peer network resembles the mesh-like distributed model. Since the writing of that paper there has been significant vocabulary drift (perhaps because English is such a decentralized/distributed language) and [clarifying the meaning of these terms](https://medium.com/@VitalikButerin/the-meaning-of-decentralization-a0c92b76a274) can be difficult. (In [one popular post on the Ethereum Stack Exchange](https://ethereum.stackexchange.com/a/7813), a diagram that looks almost exactly like Baran's diagram appears, but with the Decentralized and Distributed labels reversed!) The goal of this paper is really to seek out the systems that promote the greatest amount of reliability, security, and user autonomy, and some of the methods discussed, such as public key cryptography, promote both. Nonetheless, when the terms \"decentralized\" and \"distributed\" are used and meaning is to be sought out, look to Baran, for sometimes pictures are more descriptive than words. 4 Alyssa probably would likely not see the JSON-LD objects directly as described here, but the author believes that some narrative context still assists in the explaination of a UI-agnostic protocol. 5 `Announce` is essentially `Share` in ActivityStreams. The author of this document is not responsible for that terminology decision. 6 In ActivityPub, `Collection` objects may be used to contain sets of objects. Users of the system can curate sets of actors in collections that are publicly or privately readable which may be used for the addressing of distributed objects (similar to Google+'s circles or Diaspora's aspects). Indeed, even the actor's `followers` is a `Collection` like this! 7 Several decisions need to be made when storing signatures on objects which themselves reference other signed objects that may mutate, and this is currently a topic of open discussion . This may motivate more work on append only systems and content addressed storage. Existing implementations which operate in a mutation-prone environment must decide between letting signatures referencing mutated objects fail, including such objects recursively on every parent object, or employ some sort of content addressing of objects stored by the revision seen. The latter two options may pose some challenge to highly relational systems which were not originally designed with signatures in mind. 8 GNU Ring is an interesting example of a peer-to-peer social network system where a user's identity is actually their fingerprint. While not the first system to have this concept, it's very pleasant to see in action (and the interface is itself aesthetically pleasing); to build up your buddy list is quite literally to build your web of trust. 9 There are an incredible number of unicode hacks , which can trick even the most careful of technical users as well. 10 https://securityns.example/ is an imaginary json-ld context which is used only as a placeholder for the terms of EncryptedEnvelope and encryptedMessage . Perhaps in the future terms along these lines (maybe with better names) would appear in one of the other contexts/namespaces that appear in this document. 11 This is not unlike how PGP-wrapped email works. Receiving PGP-encrypted email means that a webmail interface would be unable to search through your messages. However, that does not mean searching is impossible; some programs like mu / mu4e can index encrypted email locally and provide such a search interface, on a user's local machine.","title":"ActivityPub: from decentralized to distributed social networks"},{"location":"RWoT5/activitypub-decentralized-distributed/#activitypub-from-decentralized-to-distributed-social-networks","text":"This paper was written originally for the 2017 Rebooting Web of Trust summit. Contributors * Christopher Webber, Independent * Manu Sporny, Digital Bazaar","title":"ActivityPub: from decentralized to distributed social networks"},{"location":"RWoT5/activitypub-decentralized-distributed/#introduction","text":"ActivityPub is a protocol being developed at the W3C for the purpose of building federated social systems. Users can use implementations of ActivityPub like Mastodon and MediaGoblin as libre alternatives to large siloed social networking systems such as Facebook, Twitter, YouTube, and Instagram. 1 In general ActivityPub follows the client-server paradigm that has been popular on the World Wide Web, while restoring some level of decentralization. Current implementations of ActivityPub go as far as to bring a level of decentralization akin to email, 2 but there are many opportunities to go further. By attaching public keys to the profiles of actors (users) on the network and using Linked Data Signatures , we can add a web of trust to the federated social web and use it to enhance user privacy and to assert the integrity of messages sent over the network. By using a decentralized identifier system such as Decentralized Identifiers (DIDs) we can move fully from a decentralized to a distributed system, 3 by escaping the core centralization mechanisms of DNS and SSL certificate authorities. At this point, users could even optionally transition from a client-server model system to a fully peer-to-peer system.","title":"Introduction"},{"location":"RWoT5/activitypub-decentralized-distributed/#activitypub-overview","text":"This section is borrowed from the ActivityPub standard's Overview section; if you are already familiar with ActivityPub then you may skip this section. ActivityPub provides two layers: - **A server-to-server federation protocol:** so decentralized websites can share information - **A client-to-server protocol:** so users can communicate with ActivityPub using servers, from a phone or desktop or web application or whatever ActivityPub implementations can implement just one of these things or both of them. However, once you've implemented one, it isn't too many steps to implement the other, and there are a lot of benefits to both (making your website part of the decentralized social web and able to use clients and client libraries that work across a wide variety of social websites). In ActivityPub, every actor (users are represented as \"actors\" here) has: - An **inbox**: How they get messages from the world - An **outbox**: How they send messages to others ![img](./images/activitypub/tutorial-1.png \"Actor with inbox and outbox\") These are endpoints, or really, just URLs which are listed in the ActivityPub actor's ActivityStreams description. (More on ActivityStreams later.) Here's an example of the record of our friend Alyssa P. Hacker: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Person\", \"id\": \"https://social.example/alyssa/\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"https://social.example/alyssa/inbox/\", \"outbox\": \"https://social.example/alyssa/outbox/\", \"followers\": \"https://social.example/alyssa/followers/\", \"following\": \"https://social.example/alyssa/following/\", \"liked\": \"https://social.example/alyssa/liked/\"} ActivityPub uses [ActivityStreams](https://www.w3.org/TR/activitystreams-core/) for its [vocabulary](https://www.w3.org/TR/activitystreams-vocabulary/). This is pretty great because ActivityStreams includes all the common terms you need to represent all the activities and content flowing around a social network. It's likely that ActivityStreams already includes all the vocabulary you need, but even if it doesn't, ActivityStreams can be extended via [JSON-LD](https://json-ld.org/). If you know what JSON-LD is, you can take advantage of the cool linked data approaches provided by JSON-LD. If you don't, don't worry, JSON-LD documents and ActivityStreams can be understood as plain old simple JSON. (If you're going to add extensions, that's the point at which JSON-LD really helps you out.) So, okay. Alyssa wants to talk to her friends, and her friends want to talk to her! Luckily these \"inbox\" and \"outbox\" things can help us out. They both behave differently for GET and POST. ![img](./images/activitypub/tutorial-2.png \"Actor with messages flowing from rest of world to inbox and from outbox to rest of world\") So the full workflow is: - You can POST to someone's inbox to send them a message (server-to-server / federation only\u2026 this *is* federation!) - You can GET from your inbox to read your latest messages (client-to-server; this is like reading your social network stream) - You can POST to your outbox to send messages to the world (client-to-server) - You can GET from someone's outbox to see what messages they've posted, or at least the ones you're authorized to see. (client-to-server and/or server-to-server) Of course, if that last one (GET'ing from someone's outbox) was the only way to see what people have sent, this wouldn't be a very efficient federation protocol! Indeed, federation happens usually by servers posting messages sent by actors to actors on other servers' inboxes. Let's see an example! Let's say Alyssa wants to catch up with her friend, Ben Bitdiddle. She lent him a book recently and she wants to make sure he returns it to her. Here's the message she composes, as an ActivityStreams object: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Note\", \"to\": [\"https://chatty.example/ben/\"], \"attributedTo\": \"https://social.example/alyssa/\", \"content\": \"Say, did you finish reading that book I lent you?\"} This is a note addressed to Ben. She POSTs it to her outbox. ![img](./images/activitypub/tutorial-3.png \"Actor posting message to outbox\") Since this is a non-activity object, the server recognizes that this is an object being newly created, and does the courtesy of wrapping it in a Create activity. (Activities sent around in ActivityPub generally follow the pattern of some activity by some actor being taken on some object. In this case the activity is a Create of a Note object, posted by a Person.) {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://social.example/alyssa/posts/a29a6843-9feb-4c74-...\", \"to\": [\"https://chatty.example/ben/\"], \"actor\": \"https://social.example/alyssa/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/49e2d03d-b53a-4c4c-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://chatty.example/ben/\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} Alyssa's server looks up Ben's ActivityStreams actor object, finds his inbox endpoint, and POSTs her object to his inbox. ![img](./images/activitypub/tutorial-4.png \"Server posting to remote actor's inbox\") Technically these are two separate steps\u2026 one is client-to-server communication, and one is server-to-server communication (federation). But, since we're using them both in this example, we can abstractly think of this as being a streamlined submission from outbox to inbox. ![img](./images/activitypub/tutorial-5.png \"Note flowing from one actor's outbox to other actor's inbox\") A while later, Alyssa checks what new messages she's gotten. Her phone polls her inbox via GET, and amongst a bunch of cat videos posted by friends and photos of her nephew posted by her sister, she sees [4](#fn.4) the following: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://chatty.example/ben/p/51086\", \"to\": [\"https://social.example/alyssa/\"], \"actor\": \"https://chatty.example/ben/\", \"object\": {\"type\": \"Note\", \"id\": \"https://chatty.example/ben/p/51085\", \"attributedTo\": \"https://chatty.example/ben/\", \"to\": [\"https://social.example/alyssa/\"], \"inReplyTo\": \"https://social.example/alyssa/posts/49e2d03d-b53a-...\", \"content\": \"Argh, yeah, sorry, I'll get it back to you tomorrow. I was reviewing the section on register machines, since it's been a while since I wrote one.\"}} Alyssa is relieved, and likes Ben's post: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Like\", \"id\": \"https://social.example/alyssa/posts/5312e10e-5110-42e5-...\", \"to\": [\"https://chatty.example/ben/\"], \"actor\": \"https://social.example/alyssa/\", \"object\": \"https://chatty.example/ben/p/51086\"} She POSTs this message to her outbox. (Since it's an activity, her server knows it doesn't need to wrap it in a Create object.) Feeling happy about things, she decides to post a public message to her followers. Soon the following message is blasted to all the members of her followers collection, and since it has the special Public group addressed, is generally readable by anyone. {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://social.example/alyssa/posts/9282e9cc-14d0-42b3-...\", \"to\": [\"https://social.example/alyssa/followers/\", \"https://www.w3.org/ns/activitystreams#Public\"], \"actor\": \"https://social.example/alyssa/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/d18c55d4-8a63-4181-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://social.example/alyssa/followers/\", \"https://www.w3.org/ns/activitystreams#Public\"], \"content\": \"Lending books to friends is nice. Getting them back is even nicer! :)\"}} ## Bringing public key cryptography to the federated social web We can dramatically improve the state of the federated social web by having each actor on the system hold a public and private keypair, and by having actors have their public key attached directly to their actor object: {\"@context\": [\"https://www.w3.org/ns/activitystreams\", \"https://w3id.org/security/v1\"], \"id\": \"https://schemers.example/u/alyssa\", \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"publicKey\": [{ \"id\": \"https://schemers.example/u/alyssa#main-key\", \"owner\": \"https://schemers.example/u/alyssa\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} This provides significant improvements to the system, which we explore below. ### Signing objects Sharing messages is common in social networks. But how can you verify that someone really said what they claimed? The user Mallet is trying to cause havoc in their social network. They pretend to \"share\" [5](#fn.5) the following post that they claim Alyssa sent to the pasta-enthusiasts group, which Ben is a member of. {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Announce\", \"id\": \"https://havoc.example/~mallet/p/90815\", \"to\": [\"https://pastalovers.example/groups/pasta-enthusiasts/\"], \"actor\": \"https://havoc.example/~mallet/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/63cc87ec-416e-437d-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://havoc.example~mallet/\"], \"content\": \"Tortellini is a poor and disgusting imitation of ravioli. Any chef serving tortellini should hang up their aprons in disgrace and never cook again.\", \"signature\": { \"type\": \"RsaSignature2017\", \"creator\": \"https://social.example/alyssa/\", \"created\": \"2017-09-23T20:21:34Z\", \"nonce\": \"e3689a56da9b4bc\", \"signatureValue\": \"mJfe5OCb7J3WwI...8t5/m=\"}}, \"signature\": { \"type\": \"RsaSignature2017\", \"creator\": \"https://social.example/alyssa/\", \"created\": \"2017-09-23T21:32:21Z\", \"nonce\": \"22e8e7683f56c08bb873\", \"signatureValue\": \"wTjLtnZVYF79pq9Ts...OU1jYPSjvcE2jNc=\"}} Ben's server, or even the server hosting `pastalovers.example`, can check the signature against the `publicKey` listed on Alyssa's actor object. This check fails, and so Mallet's attempt at slander of Alyssa amongst the pasta enthusiast community fails. While the above example looks at protecting against a malicious interaction, forwarding and sharing content is desirable for positive reasons. One common problem in federated social networks that support private interactions is that a conversation can become fragmented: if Ben is posting to private collection [6](#fn.5) she has curated containing both his friends and coworkers, and members of coworkers can't see who is in the private family collection, when they address to include the family in the conversation they can't traverse the collection of family actors to deliver to all relevant participants. (This \"ghost replies\" problem happens frequently on federated networks even when messages are being sent to the actor's own `followers`, where breaks tend to happen across server boundaries.) ActivityPub includes a solution to this via a [forwarding mechanism](https://www.w3.org/TR/activitypub/#inbox-delivery), but the solution does not really work without signatures, as the forwards are happening indirectly rather than from the \"same origin/domain\", so servers are unable to check/trust that the content is as claimed. Even if the receiving server tries to look up the object the receiving actor's credentials, access control may not have been enabled for the actor who was forwarded to, since the commenter had no way of knowing who was in the private collection to enable access for. This is a frequently requested feature in federated social networks, so we should ensure that the necessary public key infrastructure is provided. [7](#fn.7) ### An easier to use web of trust? The PGP-style \"web of trust\" has been around for some time now, but the term \"web of trust\" is somewhat mired by the historically most popular method by which the trust network has populated. Key signing parties, while effective, have never taken off beyond a very small set of the population. Such parties are rewarding but difficult for most of the population to attend and organize, and even more difficult still is learning the (generally) command line tooling necessary to participate in the system. While some work has been done in this area (for example with [Monkeysign](https://monkeysign.readthedocs.io/en/2.x/) and [Gibberbot](https://guardianproject.info/apps/gibber/)), it would be even better if building your trust network was incidental to participating in the network. [8](#fn.8) To a certain extent, this could come \"for free, with caveats\" in ActivityPub deployments that exist today, where subscriptions and object lookup are done over HTTPS. Merely by sending a follow request (or some other action connecting users on the social graph) a certain amount of trust between users can be expressed. Keys can be looked up and recorded at actor profile urls, and users can even observe and share information about whom else they know on the social network. There's a major caveat using HTTPS for these lookups requires trust in SSL certificate authorities. Better than nothing, but not great, and not the distributed systems we want. Furthermore, a malicious actor can still trick users; a user may believe they are subscribing to `https://social.example/alyssa/`, but perhaps Mallet tricked them into subscribing to `https://social.example/alyssaa/` instead. [9](#fn.9) Happily there are other ways to encourage stronger trust networks. Carl Ellison's paper *[Establishing Identity Without Certification Authorities](http://world.std.com/~cme/usenix.html)* describes several classes of relationships amongst users, and many Off The Record clients (such as available in Pidgin, etc) provide interfaces for verifying challenges between users. Users on a federated social network could be provided an opportunity to perform a textual challenge, perform a brief video call where they verify a shared code (as done in Jitsi), or scan a QR code (as in Monkeysign and Gibberbot) to establish stronger trust that an actor on the network is the entity they claim to be. The level of trust gained could be signed, recorded, and itself propagated as appropriate throughout the network. This kind of mechanism would work nicely even in a system like DIDs, where a human-readable identifier does not exist. ### End-to-end encryption A malicious server administrator may still snoop on all communication of participants on a system. Even a non-malicious administrator may be coerced into snooping on their users, or may have their entire system compromised without their knowledge. SSL Certificate Authorities may also be compromised into giving out fake certificates, allowing man in the middle attacks that neither the user nor server administrator may be aware of. End-to-end encryption can solve this (with some tradeoffs); in this case, rather than having the server manage the public and private keys of a user, a user may provide a public key on their actor object to which only their own computer(s) hold the corresponding private key. Other actors on the network may then send an object encrypted to the actor's inbox. For example, an actor may receive the following object [9](#fn.10) in their inbox: {\"@context\": [\"https://securityns.example/\", \"https://www.w3.org/ns/activitystreams\"], \"type\": \"EncryptedEnvelope\", \"encryptedMessage\": \"-----BEGIN PGP MESSAGE-----\\r\\n...\", \"mediaType\": \"application/ld+json; profile=\\\"https://www.w3.org/ns/activitystreams\\\"\"} The server would put this object in the user's inbox, but if only the user's own computers hold the key, even the server would be unable to read the contents held within the envelope. Upon retrieving the object from the server via the client-to-server protocol, the user's client can decrypt the message. In this case, the message went directly to Alyssa's inbox. Upon decrypting the component in encryptedMessage, another object is found: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Note\", \"id\": \"https://chatty.example/ben/p/86187\", \"to\": [\"https://social.example/alyssa/\"], \"attributedTo\": \"https://chatty.example/ben/\", \"content\": \"Up for some root beer floats at my friend's house? Here's the address: ...\"} Note that while this improves privacy, it does come with several tradeoffs: - ActivityPub contains an entire suite of [server side side-effects](https://www.w3.org/TR/activitypub/#server-to-server-interactions) for federating common activities on a social network. For example, a Like object will increment a counter of how often a post has been liked, and even add that liked object to both the user's collection of liked objects as well as a collection of all users who have liked this object. Since servers are unable to observe data being sent across the network, these kinds of side effects will break. The server will also be unable to provide additional features such as being able to have server-based indexing of messages for easy search. [11](#fn.11) In a \"more peer-to-peer\" system (as discussed in the Distributed identity section) this becomes less of an issue because the distinction between client server blurs. Nevertheless, for existing client-to-server implementations, this is a strong issue to consider. - User maintenance of keys in end-to-end encryption systems is known to be a difficult user experience problem. - Key recovery is even harder. [DIDs](https://w3c-ccg.github.io/did-spec/) explore a method for key recovery, but this will not help users read old messages encrypted with keys they no longer have and which the original senders cannot send (or do not know how to). ## Distributed identity ActivityPub implementations at the present moment rely on HTTPS as their transport, which in turn relies on two centralized systems: DNS and SSL certificate authorities. Is there any way to bring self-sovereignty to the federated social web? Thankfully there is; ActivityPub was written intentionally to be layerable on any protocol that can support HTTP GET and POST verbs. The [Decentralized Identifiers](https://w3c-ccg.github.io/did-spec/) specification looks to be a good fit for ActivityPub. The simplest version of this can be seen simply by replacing the actor ids with DIDs. To transform an example from the overview from: {\"type\": \"Note\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://chatty.example/ben/\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} to: {\"type\": \"Note\", \"attributedTo\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"to\": [\"did:example:nJx2fgreaSfCujA0kMsiEW8Oz\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} Gosh! That was simple-ish. All we did was replace the human-readable identifiers representing the users with DIDs. If we look up Alyssa's [DID](https://w3c-ccg.github.io/did-spec/#dids-(decentralized-identifiers)) based id, we can retrieve her actor object as a [DDO](https://w3c-ccg.github.io/did-spec/#ddos-(did-descriptor-objects)), but this time there is extra information: { \"@context\": [\"https://example.org/did/v1\", \"https://www.w3.org/ns/activitystreams\"], \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"activityPubService\": { \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#services/ActivityPub\", // ActivityPub actor information \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"https://9GaksjPhy0mWToTV.onion/alyssa/inbox/\", \"outbox\": \"https://9GaksjPhy0mWToTV.onion/alyssa/outbox/\", \"followers\": \"https://9GaksjPhy0mWToTV.onion/alyssa/followers/\", \"following\": \"https://9GaksjPhy0mWToTV.onion/alyssa/following/\", \"liked\": \"https://9GaksjPhy0mWToTV.onion/alyssa/liked/\"}, // DDO information \"owner\": [{ \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#key-1\", \"type\": [\"CryptographicKey\", \"EdDsaPublicKey\"], \"curve\": \"ed25519\", \"expires\": \"2017-02-08T16:02:20Z\", \"publicKeyBase64\": \"lji9qTtkCydxtez/bt1zdLxVMMbz4SzWvlqgOBmURoM=\" }, { \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#key-2\", \"type\": [\"CryptographicKey\", \"RsaPublicKey\"], \"expires\": \"2017-03-22T00:00:00Z\", \"publicKeyPem\": \"----BEGIN PUBLIC KEY-----\\r\\n..\" }], \"control\": [{ \"type\": \"OrControl\", \"signer\": [ \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"did:example:8uQhQMGzWxR8vw5P3UWH1j\" ] }], \"created\": \"2002-10-10T17:00:00Z\", \"updated\": \"2016-10-17T02:41:00Z\", \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:8uQhQMGzWxR8vw5P3UWH1j#key/1\", \"signatureValue\": \"IOmA4R7TfhkYTYW8...CBMq2/gi25s=\" } } Hoo! That's a lot of additions. Except here we see an example of Alyssa's profile that is entirely free of traditional centralized DNS authorities. We were able to look up Alyssa's object via her DID, but we still have access to all her endpoints, which in this case are pointing to [Tor Hidden Services](https://www.torproject.org/docs/hidden-services.html.en). No central DNS required! Maybe in the future there will even be a protocol \u2013 let's call it `httpeer` \u2013 which supports all the standard HTTP verbage, but over some other peer-to-peer network. The DID spec supports [service endpoints](https://w3c-ccg.github.io/did-spec/#service-endpoint-references-(optional)), and Alyssa could take advantage of these to use her DID as base of the `inbox`, `outbox`, etc URIs. Here's a cut down and modified version of the previous example: { \"@context\": [\"https://example.org/did/v1\", \"https://www.w3.org/ns/activitystreams\"], \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"activityPubService\": { \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#services/ActivityPub\", // ActivityPub actor information \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/inbox/\", \"outbox\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/outbox/\", \"followers\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/followers/\", \"following\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/following/\", \"liked\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/liked/\"}, // DDO information goes here \"httpeerService\": { \"nodeId\": \"dI0tuXjISZEadSH6QV9EhBEdccL4ouePdF8P57BJ\"}} Now that's an identity system! ## Append only systems and content addressed storage Finally, it's worth mentioning the idea of moving ActivityPub to an entirely append-only, content-addressed system for object storage, \"modification\", and retrieval. Much success has been seen in recent years with these systems; enacting this change would allow for many of the side effects in the federation system to be dropped entirely. We leave this as a topic for a future paper. ## Conclusions ActivityPub goes a long way towards providing a standardized way to move the social web from isolated, centralized silos towards the decentralized nature that the World Wide Web is meant to encompass. Still, there is much to be done and improved. Yet there are risks in trying to engineer the right system all at once, and great is well known to be the enemy of good. Thankfully we do not need to throw out what we have to make the improvements that are discussed in this paper. ActivityPub already exists and works, and we can incrementally improve the systems we have and blur the line between the federated social web that works and more peer-to-peer systems which are desirable. By adding public key infrastructure and distributed identifiers to ActivityPub we can move from a decentralized system to a distributed one and truly build a network that is both self-sovereign and social. ## Acknowledgments Thanks to Evan Prodromou and Owen Shepherd for working on initial revisions of the ActivityPub standard. Thanks also to Jessica Tallon for being co-editor on ActivityPub and [everyone who helped make ActivityPub happen](https://www.w3.org/TR/activitypub/#acknowledgements), which is no small list. (Some of those people caught typos in the original version of the overview, as included in ActivityPub.) Thanks to Manu Sporny and Stephen Webber for thoughts and feedback about how to make ActivityPub into a more robust distributed system. Thanks to Dave Longley for pointing to the vocabulary drift / ambiguity of the terms \"decentralized\" and \"distributed\", which lead to the addition of Paul Baran's diagrams and some terminology clarification. [3](#fn.3) Thanks to Morgan Lemmer-Webber for the patience and for careful proofreading of this document. Thanks to Spec-Ops and Digital Bazaar for supporting my work/time on Verifiable Claims/Credentials. Though separate from my work on ActivityPub, obviously I've been thinking a lot about how to combine them this whole time. Thanks also to mray for the gorgeous illustrations in the overview section. This document and its images (with the exception of Paul Baran's drawings), like ActivityPub itself, are licensed under [W3C's permissive document license](https://www.w3.org/Consortium/Legal/2015/copyright-software-and-document). ## Footnotes: 1 Of course, there is nothing stopping current-silos of social networking from adopting ActivityPub, would they be willing to un-silo their users. 2 Observant readers may note that email is no longer as decentralized of a system as it once was. Consider this a lesson that a protocol alone cannot build a distributed network; the community must build and maintain a healthy number of nodes and avoid the temptation to let a few large providers control the space of a federated network. 3 It is worth spending some time to discuss what is meant by \"centralized\" versus \"distributed\" versus \"decentralized\". ![img](./static/centralized_decentralized_distributed.png \"Centralized, Decentralized, and Distributed drawings, from 'On Distributed Communications, part 1' by Paul Baran, 1964\") In the figure above we see images from Paul Baran's 1964 paper on the subject, and from these shapes we can see the kinds of shapes we mean: social silos resemble the the spoke-like centralized model, client-server federated social networks resemble the tree-like decentralized model, and a peer-to-peer network resembles the mesh-like distributed model. Since the writing of that paper there has been significant vocabulary drift (perhaps because English is such a decentralized/distributed language) and [clarifying the meaning of these terms](https://medium.com/@VitalikButerin/the-meaning-of-decentralization-a0c92b76a274) can be difficult. (In [one popular post on the Ethereum Stack Exchange](https://ethereum.stackexchange.com/a/7813), a diagram that looks almost exactly like Baran's diagram appears, but with the Decentralized and Distributed labels reversed!) The goal of this paper is really to seek out the systems that promote the greatest amount of reliability, security, and user autonomy, and some of the methods discussed, such as public key cryptography, promote both. Nonetheless, when the terms \"decentralized\" and \"distributed\" are used and meaning is to be sought out, look to Baran, for sometimes pictures are more descriptive than words. 4 Alyssa probably would likely not see the JSON-LD objects directly as described here, but the author believes that some narrative context still assists in the explaination of a UI-agnostic protocol. 5 `Announce` is essentially `Share` in ActivityStreams. The author of this document is not responsible for that terminology decision. 6 In ActivityPub, `Collection` objects may be used to contain sets of objects. Users of the system can curate sets of actors in collections that are publicly or privately readable which may be used for the addressing of distributed objects (similar to Google+'s circles or Diaspora's aspects). Indeed, even the actor's `followers` is a `Collection` like this! 7 Several decisions need to be made when storing signatures on objects which themselves reference other signed objects that may mutate, and this is currently a topic of open discussion . This may motivate more work on append only systems and content addressed storage. Existing implementations which operate in a mutation-prone environment must decide between letting signatures referencing mutated objects fail, including such objects recursively on every parent object, or employ some sort of content addressing of objects stored by the revision seen. The latter two options may pose some challenge to highly relational systems which were not originally designed with signatures in mind. 8 GNU Ring is an interesting example of a peer-to-peer social network system where a user's identity is actually their fingerprint. While not the first system to have this concept, it's very pleasant to see in action (and the interface is itself aesthetically pleasing); to build up your buddy list is quite literally to build your web of trust. 9 There are an incredible number of unicode hacks , which can trick even the most careful of technical users as well. 10 https://securityns.example/ is an imaginary json-ld context which is used only as a placeholder for the terms of EncryptedEnvelope and encryptedMessage . Perhaps in the future terms along these lines (maybe with better names) would appear in one of the other contexts/namespaces that appear in this document. 11 This is not unlike how PGP-wrapped email works. Receiving PGP-encrypted email means that a webmail interface would be unable to search through your messages. However, that does not mean searching is impossible; some programs like mu / mu4e can index encrypted email locally and provide such a search interface, on a user's local machine.","title":"ActivityPub overview"},{"location":"RWoT5/amira/","text":"Amira 1.0.0 A Self-Sovereign Web of Trust Engagement Model Joe Andrieu, Legendary Requirements, < joe@legreq.com > Christopher Allen < ChristopherA@LifeWithAlacrity.com > Shannon Appelcline, Skotos Tech, < shannona@skotos.net > Lyre Calliope, @CaptainCalliope , Securing Change, < lyre.calliope@gmail.com > Toni Lane Casserly, @tonilanec , Cultu.re / CoinTelegraph, < tlc@cultu.re > Tim Chen, Coding Dojo, < tchen@codingdojo.com > David Fields, PTB Ventures, < dave@ptbvc.com > Michael Haley, AlphaPoint, < michael@alphapoint.com > Shireen Mitchell, Stop Online Violence Against Women, < founder@stoponlinevaw.com > Heather Vescent, The Purple Tornado, < puissant@heathervescent.com > Kaliya \u201cIdentity Woman\u201d Young, @identitywoman, < kaliya@identitywoman.net > With special thanks to Pat McBennett and Raghav Chawla for their contributions. Introduction This paper began as a collaborative project at the fifth Rebooting the Web of Trust[ 1 ] workshop, held in Cambridge MA in October 2017. We reinterpret Christopher Allen\u2019s Rebooting the Web of Trust user story,[ 2 ] through the lens of the Information Lifecycle Engagement Model (described in Appendix A). We present a human-centric illustration of an individual\u2019s experience in a self-sovereign, decentralized realization of the Web of Trust as originally conceived by Phil Zimmerman for PGP.[ 3 ] In our scenario, Amira is a successful programmer working in Boston at a prestigious multi-national bank. Outside of working hours, Amira wants to give back to her community by writing software that matters. On the advice of her friend Charlene, Amira joins RISK, a self-sovereign reputation network that connects developers with projects while protecting participants\u2019 anonymity, building reputation, and sending & receiving secure payments. Information Lifecycle Engagement Model We describe Amira\u2019s interactions with RISK using the Information Lifecycle Engagement Model. This 15-stage model captures the experience of a single, fictitious individual as she interacts with a proposed system. The model intentionally limits our focus to the connected experience of a single human protagonist and the people they engage along the way, in one or more paragraphs about interactions in each of the 15 stages. The Information Lifecycle Engagement Model is based on the work of Joe Andrieu and Ian Henderson from The Customer-Supplier Engagement Model Quick Starter [ 4 ], which uses a 12-stage model to illustrate how personal data stores can interact with CRM systems throughout the customer-supplier lifecycle. The model doesn\u2019t attempt to define all the requirements or possibilities in a system, but rather serves as a lens to focus on a single person across the full set of fifteen categories of interactions. These fifteen stages ensure that we have an example of each of the different steps found in essentially all information systems. The engagement model presents the human context from \u201cboth sides\u201d of the system. On one side, Amira interacts with RISK. On the other side, a variety of counter-participants in different roles interact with Amira through the system. We adopt a \u201cstrawman\u201d architecture for exploring the human experience using one possible embodiment. This non-binding set of suggested functionality allows authors and readers to sanity-check the feasibility of both individual actions and system capability. The strawman intentionally defers deeper technical issues for future design and implementation. The goal is not to design any particular implementation, but instead to make sure the basics ideas are sound from start to finish, capturing the human requirements independent of any specific technology choice. The strawmen helps us understand the proposed system by providing a clear idea of one workable approach. Any actual implementation could vary greatly as subsequent constraints lead to different choices. This paper focuses on the human interactions and human relevance that drive the model, and not the specifics of any proposed implementation. Participants Throughout Amira\u2019s experience, different individuals create and query information in RISK. Amira ( BWHacker) \u2014 The main actor in our model, Amira has a politically tense background. She wants to use her programming skills to positively impact the lives of oppressed people around the world. Ben ( BigBen44) \u2014 A manager at a women\u2019s services non-profit. Ben is looking to connect with developers to build an app that helps women in difficult situations. Charlene ( CharlieOne) \u2014 Amira\u2019s friend and fellow developer. She knows Ben indirectly through the RISK Web of Trust. She helps Amira get started with RISK. Elias \u2014 A mercenary working for the Syrian Hacker Army. Firefly \u2014 A developer who becomes Amira\u2019s successor as an app maintainer. Services RISK depends on, and is partially constituted by a number of decentralized services. anonymous Internet \u2014 support for private communication over the Internet. identifiers \u2014 self-sovereign, pseudo-anonymous sources of cryptographic authority. DIDs, etc. name discovery \u2014ability to use an identifier to look up a unique name or address within a community. web of trust reputation database \u2014 non-hierarchical sources for reputation information (claims and evidence) about people within a community. status database \u2014 non-hierarchical sources of revocation and other status information for identifiers, names, and reputation. community \u2014 synchronous and asynchronous discussions and documents for a group of people. source code repository \u2014 location for developers to collaborate on software. contract \u2014 method for multiple parties to securely contract services with each other. payment \u2014 settlement between parties. app marketplace \u2014 repository for people to be download and install software created by developers. Assumptions All parties are users of computer and internet technology; most have above-average level of technical sophistication including development skills. The Syrian Army is actively monitoring \u201cenemy\u201d technology and technologists and engaging in attacks against enemy technology. Amira and Charlene have an existing relationship; Charlene is willing to vouch for Amira. Charlene has a mobile application for RISK installed and configured on her mobile phone. Decentralized developer systems (e.g., git) are easily available and accessible. Amira uses multiple security technologies and techniques to protect her privacy. Amira is not monitoring changes to her account while on vacation. Stage 1 \u2014 Pre-contact Parties engage in activities that lead to contact. Amira escaped life in Syria when she was 16. She resettled in Boston with an adoptive family and studied computer science in college, taking a job at a bank. The job pays her bills, but Amira wants to make a difference in the world. One night, Amira discovers her opportunity. She sees an interview with Ben, the executive director of an international non-profit, on Cambridge Community Television[ 5 ], sharing his organization\u2019s vision for supporting oppressed women around the world. Ben envisions smart phones as the key to creating anonymous communication channels that safely and reliably connect women with local community services, specifically supporting victims of domestic violence and sexual assault. Amira wants to contribute to Ben\u2019s cause, but worries that engaging in activism might affect her job. Fortunately, as the public face of his non-profit, Ben has established an identifier, BigBen44, on RISK and freely shares it with contacts, leaders, and developers who might help his cause. This gives Amira an opening to reach out to Ben. Stage 2 \u2014 Contact Initial contact is made between the individual and an initial steward. At a local professional event, Amira runs into her good friend Charlene, a fellow technologist with shared values. Amira discreetly asks Charlene if she knows anything about RISK and how Amira might use it to anonymously contribute to Ben\u2019s project. Using her phone, Charlene searches RISK\u2019s developer network to see if Ben is a connection. Charlene finds him, separated by just two degrees. The proximity of their connection on RISK allows Charlene to send Ben a priority introduction. But first, Charlene needs to understand if RISK is the right option for Amira. Charlene tells Amira she may be able to help and suggests they meet after the event, without the crowd of their professional peers nearby. Amira agrees, intrigued and hopeful. Stage 3 \u2014 Triage The initial steward investigates and inquires to decide the most appropriate direction for the individual. Negotiations may occur. To understand if RISK is a good fit for Amira, Charlene asks about her goals and concerns. The security features of the system require a non-trivial amount of effort and if Amira isn\u2019t committed to doing it properly, the consequences could be worse than if she found some other way to work with Ben. Amira explains that she wants to contribute to the project without fear of reprisal because of her background, ethnicity, or gender. She has strong technical skills and a proven ability to deliver useful, quality software. She wants work where she is valued for what she does, without risking reprisals for who she is. She wants to work in a purpose-driven community and to make a difference in the world, not just to make money. She\u2019d like to find a community of peers who share these ideas \u2014 people willing to help shift society\u2019s systemic imbalances anchored in race, gender, location, and nationality while protecting real-world identities. Amira\u2019s ambition perfectly matches the reasons RISK was created. RISK is a decentralized system that relies on transactional reputation to establish pseudonymous identities for people who want to keep their real identities secret and to change the world. RISK connects developers with stakeholders to build projects that improve people\u2019s lives, while simultaneously protecting the identity of those using the system. Charlene decides to act as an introducer for Amira. Stage 4 \u2014 Direction Based on triage and negotiations, both parties agree on a particular course of action. Charlene tells Amira about RISK, explaining the culture as well as the underlying cryptography and consensus mechanisms that make it all work. Amira immediately gets it and hopes she can find the relevant work missing in her life. Charlene writes down the URL for the RISK decentralized source code repository, along with her public-facing RISK handle: CharlieOne. In addition, Charlene verbally shares a memorable phrase that will identify her on the RISK network, instructing Amira to repeat it, remember it, and to NOT write it down. Charlene asks Amira to create her own phrase to uniquely identify herself to Charlene when they connect online. To help remember the phrases, they use them in conversation a few times. It\u2019s a bit silly, but the humorous reinforcement helps. Charlene recommends that Amira set up a bootable portable drive (e.g., a USB stick) to create a secure development environment while working with RISK. Fortunately, Amira isn\u2019t afraid to build a Linux machine from scratch; she\u2019s confident she can get the system up and running. Stage 5 \u2014 Consent Consent is asked and given for creating an identity record, which is then provisioned. During their conversation, Charlene explained that RISK is a system of mutual, earned trust, founded on a commitment to respect individuals\u2019 right to privacy. This commitment is captured in a mutual agreement between all parties using the network. RISK\u2019s Terms of Engagement include: A Code of Ethical Conduct. A confidentiality agreement to maintain anonymity of others and to keep the details of any project confidential \u2014 except and unless explicitly authorized through the reputation system. Consent to use a registered pseudonym for tracking activities and building reputation. Procedures for registering and challenging reputation records. Procedures for updating RISK, including the Terms of Engagement and an IP assignment. Limited liability to minimize frivolous lawsuits and exorbitant claims. A warranty of suitability that requires that participants act on their own free will as an individual and not an organization, entity, or company. Using a credential she generated, 1) Amira cryptographically signs a statement containing her new pseudonym, BWHacker , short for \u201cBetter World Hacker,\u201d 2) the name and a hash of the current RISK terms (v2.4.1), and 3) a statement that BWHacker agrees to these terms. This signed statement is a consent receipt. Amira stores it on her drive and on a backup service, then posts a hash of the receipt to RISK, recording the timestamp when BWHacker formally agreed to the terms. Stage 6 \u2014 Configure Information is shared and associated with the individual. Services are configured. Using a bootable portable drive, Amira sets up a development environment on her laptop. She then downloads RISK and begins working through the READMEs and installation directions. Amira creates a unique identifier in the form of a DID, a Decentralized Identifier,[ 6 ] on the RISK decentralized network. This binds a publicly-verifiable credential to her new pseudonym, BWHacker. This pseudonym and the related DID will allow Amira to securely interact through RISK without revealing her real-world identity. Using the RISK introduction system, Amira looks up CharlieOne and contacts her using her new handle, BWHacker. RISK establishes a secure channel and Amira and Charlene confirm each other\u2019s identity using their memorized phrases. Now both parties have each other\u2019s cryptographically secure identifiers for ongoing correspondence. Stage 7 \u2014 Services Services are provided. To bootstrap her reputation on RISK, Amira creates a Verifiable Credential, self-asserting her capabilities as a mobile software developer with her RISK pseudonym. Through RISK, she sends CharlieOne the credential and asks for an endorsement. Charlene is happy to help. She creates an endorsement, including the original claim, posts a hash of the endorsement to RISK, then sends both to BWHacker. This allows BWHacker to selectively enable anyone to verify the credential and CharlieOne\u2019s endorsement. In addition, Charlene has the ability to revoke this endorsement in the future, if the need arises (for example, the BWHacker account is transferred or penetrated.). Now that BWHacker is in RISK with a suitable endorsement, CharlieOne introduces BWHacker to BigBen44. As BWHacker has not performed work through RISK, BigBen44 relies on CharlieOne\u2019s endorsement giving her status as a subject matter expert. BigBen44 shares background materials with BWHacker outlining the project and specifying requirements, timeframe, and acceptance criteria. After reviewing the materials, BWHacker responds with some clarifying discussion points and a draft statement of work (\u201cSoW\u201d). The discussion reflects a depth of professional experience that confirms CharlieOne\u2019s endorsement to BigBen44. BigBen44 and BWHacker formalize the SoW into a contract to create a \u201cSisterSpaces\u201d app using a RISK contract template that includes pricing, technical milestones, evaluation criteria, and progress payments. RISK\u2019s standard contract templates have support for cryptographic smart contracts. Upon contract signing, BigBen44 deposits the full contract value in bitcoin into a smart escrow account. Each time BWHacker commits code achieving a technical milestone, BigBen44 is sent a randomized code sample to review as well as the ability to spin up a demo guaranteed to utilize the current codebase. BigBen44 can comment and suggest changes as desired. Upon milestone acceptance, the smart contract releases agreed-upon payment to BWHacker\u2019s wallet address. Finally, the smart contract independently records BWHacker\u2019s code delivery as well as BigBen44\u2019s reviews and final acceptance in the RISK reputation system. The completed version 1.0.0 of the SisterSpaces app acts as the final deliverable of the contract. Ben accepts the deliverable, and final payment is released. Amira and Ben then post reviews of the experience of working together; this is a reputational claim on each other\u2019s RISK identity, confirmed by the execution of their smart contract. Stage 8 \u2014 Enhancements Enhancements are offered and accepted. After successfully delivering BigBen44\u2019s project and receiving a glowing review, Amira decides to list her profile in the RISK developer directory. She reviews other entries in the directory to find examples of existing professional pseudo-anonymous profiles: RISK provides professional profile templates that show recent projects and endorsements. When creating her own profile, Amira opts out of displaying a picture of herself on the BWHacker profile. She is careful not to reveal any personal information. Her profile is stored on a decentralized file system and linked to her BWHacker RISK account. When people search RISK for developers, BWHacker\u2019s profile will be displayed with her endorsements. Stage 9 \u2014 Updates Anticipated changes in information lead to revised records. After the account compromise (discussed in Stage 10) and creation of the BWHacker2 pseudonym, Amira buys a hardware \u201cwallet\u201d to help isolate and secure the credentials she uses to access RISK. She posts a transaction to RISK using her initial BWHacker2 credential, replacing the public part with one generated by her new wallet. The old credentials are no longer valid and her identifier remains under her control through her new credentials. Stage 10 \u2014 Problems/Issues Issues beyond the normal support interfaces arise and are resolved. Elias is a mercenary hired by the Syrian Hacker Army to track down and hack the author of SisterSpaces \u2014 the app that Amira coded. Elias tracks BWHacker\u2019s interactions to a dev environment (the one on Amira\u2019s USB key) and captures the private keys associated with the BWHacker RISK DID. Elias changes the master key in the RISK DID, stealing Amira\u2019s ability to communicate as BWHacker, and locking Amira out of her RISK identity. Unfortunately for Amira, this hack happens while she is on vacation. She doesn\u2019t realize the account is compromised until she returns two weeks later. In this interim, Elias patches the SisterSpaces source code so that all traffic is sent to a Syrian surveillance system. Posting as BWHacker, Elias sends a message to BigBen44, telling him about \u201ca security bug fix\u201d for SisterSpaces and strongly recommends immediately releasing the \u201cnew and improved\u201d v1.0.1. Ben, not realizing BWHacker\u2019s account has been compromised, submits the v1.0.1 update to the app store. Returning from her vacation, Amira realizes she\u2019s lost control of BWHacker. Not only has a fake app been released, but she can\u2019t even check the git repo to see what has changed in the new version. Amira calls Charlene for help. Charlene immediately revokes her endorsement of BWHacker and sends a message to BigBen44, telling him that BWHacker has been compromised along with SisterSpace\u2019s code. Taking no chances, Ben immediately disables BWHacker\u2019s access to the git repository, reverts the recent commits, updates the version, and pushes v1.0.2 to the app store. Unfortunately, Amira has no way to regain control of the original BWHacker identity. Instead, She creates a new DID as BWHacker2, repeating the steps she followed when she initially set up her account. Amira generates her credentials directly, making a point to update them with a more secure set as soon as possible (described in Stage 9 Updates). Amira recovers her reputational claims in Stage 13 Recovery. Stage 11 \u2014 Maintenance Software, hardware, or operations are updated and upgraded. Meanwhile, members of RISK are considering an improvement proposal concerning reputation claims. Originally, RISK reputations were limited to one-on-one reveals. Under the proposed improvement, users can choose to aggregate reputation data for presentation on their RISK profiles. The majority of the RISK developers are in favor of the proposal, and it gets approval. Developers update the code base with the new feature and release the improvement. Users like Amira, Charlene, and Ben get a notification that there is a new reference implementation available. Separately, they download it and begin to use the new aggregated reputations. Stage 12 \u2014 Migration Records are moved, transformed to a new schema, input, or output. Seeing the power of displaying her RISK reputations on her profile, Amira decides to move her BWHacker2 persona off RISK to CommonX, a more widely-used system with more developers and projects. She cryptographically links BWHacker2 to CommonX so that users of both systems can verify that the two profiles reflect the same individual. BWHacker2 then reaches out to her existing reputation sources to ask for an endorsement of essentially the same reputation data in the new system. While this is largely automated, it does require the original author to countersign the reputational assertion in a format usable by CommonX. As the authors do so, the migrated reputations start to appear in BWHacker2\u2019s CommonX profile. Stage 13 \u2014 Recovery Lost credentials or identifiers are restored or reset. Unfortunately, Elias has taken complete control over the BWHacker profile. There is no way for Amira to recover it, but she can recreate the endorsements from her work as BWHacker. After setting up a new profile, BWHacker2, Amira recovers the rest of the reputation claims that were attached to BWHacker. She begins by contacting Charlene. They meet in person to confirm the change. Charlene creates and records an endorsement that BWHacker2 is the person formerly known as BWHacker, prior to the account compromise. CharlieOne then reaches out to BigBen44. After he verifies that CharlieOne\u2019s credential keys have not recently changed, Ben connects with BWHacker2 and asks some questions of his own to verify this is really the person he\u2019s been working with. Ben updates his account to set up a maintenance contract with BWHacker2 and double checks that the contract with BWHacker is terminated with prejudice. Stage 14 \u2014 Exit The individual concludes their relationship with stewards in this lifecycle. Amira had thought about leaving her job at the bank to work on developer projects full-time, but then the bank gives her an opportunity she can\u2019t refuse. The CIO asks Amira to oversee a comprehensive overhaul of its internal systems as its new Principal Architect. Amira accepts. However, in her new role, she realizes she no longer has the time to continue working on SisterSpaces. She tells BenBen44, and they agree to end the maintenance contract they had established after the initial version shipped. Ben hires a new developer, Firefly, gives them write access to GitHub, and revokes BWHacker2\u2019s access. To avoid future inquiries for development, Amira removes her BWHacker2 profile from the directory. Stage 15 \u2014 Re-engagement The individual is invited and accepts a new opportunity to engage the system. A year into the rebuild, the bank\u2019s CIO has been replaced and Amira is fed up. She decides to try Firefly\u2019s latest version of SisterSpaces. While the app has become popular, she notices it is straining under heavy technical debt. Amira reaches out to BigBen44 once again as BWHacker2. The two rekindle their connection, and Ben tells Amira that he wants her back full time. Amira gives notice to the bank, reactivates her BWHacker2 profile in the directory, and steps into her dream job of writing socially-responsible software. Appendix A Information Lifecycle Engagement Model The 15-stage model presented here is an instance of an Information Lifecycle Engagement Framework, as illustrated below. Each phase in the model presents a concise description of one or two interactions illustrating the phase. Rather than attempting to describe every interaction across multiple users, the engagement model captures illustrative interactions for each phase in the lifecycle of a particular individual\u2019s engagement with the system. The model should be readable as a sympathetic narrative, clarifying the motivations of all parties and the viability of the described use. We use it to tease out the human requirements independent of the underlying technology. This Information Lifecycle Engagement Model is based on the work of Joe Andrieu and Ian Henderson from The Customer-Supplier Engagement Model Quick Starter [ 7 ] . We have re-interpreted and expanded the 12 phases of the Customer-Supplier Engagement Model for suitability to identity information more generally, shifting away from the commercial nature of the relationship. We also added Maintenance, Migration, and Recovery phases to address the needs of operating an ongoing system. The premise remains: working through each phase of the model provides a comprehensive yet lightweight review of necessary transactions for a given system. This can be done before a detailed specification, giving a coherent view of functional requirements that supports subsequent innovation as design and implementation details emerge. Footnotes [1]: http://www.weboftrust.info/ [2]: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2017/blob/master/topics-and-advance-readings/RWOT-User-Story.md [3]: https://en.wikipedia.org/wiki/Web_of_trust [4]: http://kantarainitiative.org/confluence/display/infosharing/Customer-Supplier+Engagement+Model+Quick+Starter [5]: https://www.cctvcambridge.org/about [6]: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2017/blob/master/topics-and-advance-readings/did-primer.md [7]: http://kantarainitiative.org/confluence/display/infosharing/Customer-Supplier+Engagement+Model+Quick+Starter","title":"Amira"},{"location":"RWoT5/amira/#amira-100","text":"A Self-Sovereign Web of Trust Engagement Model Joe Andrieu, Legendary Requirements, < joe@legreq.com > Christopher Allen < ChristopherA@LifeWithAlacrity.com > Shannon Appelcline, Skotos Tech, < shannona@skotos.net > Lyre Calliope, @CaptainCalliope , Securing Change, < lyre.calliope@gmail.com > Toni Lane Casserly, @tonilanec , Cultu.re / CoinTelegraph, < tlc@cultu.re > Tim Chen, Coding Dojo, < tchen@codingdojo.com > David Fields, PTB Ventures, < dave@ptbvc.com > Michael Haley, AlphaPoint, < michael@alphapoint.com > Shireen Mitchell, Stop Online Violence Against Women, < founder@stoponlinevaw.com > Heather Vescent, The Purple Tornado, < puissant@heathervescent.com > Kaliya \u201cIdentity Woman\u201d Young, @identitywoman, < kaliya@identitywoman.net > With special thanks to Pat McBennett and Raghav Chawla for their contributions. Introduction This paper began as a collaborative project at the fifth Rebooting the Web of Trust[ 1 ] workshop, held in Cambridge MA in October 2017. We reinterpret Christopher Allen\u2019s Rebooting the Web of Trust user story,[ 2 ] through the lens of the Information Lifecycle Engagement Model (described in Appendix A). We present a human-centric illustration of an individual\u2019s experience in a self-sovereign, decentralized realization of the Web of Trust as originally conceived by Phil Zimmerman for PGP.[ 3 ] In our scenario, Amira is a successful programmer working in Boston at a prestigious multi-national bank. Outside of working hours, Amira wants to give back to her community by writing software that matters. On the advice of her friend Charlene, Amira joins RISK, a self-sovereign reputation network that connects developers with projects while protecting participants\u2019 anonymity, building reputation, and sending & receiving secure payments. Information Lifecycle Engagement Model We describe Amira\u2019s interactions with RISK using the Information Lifecycle Engagement Model. This 15-stage model captures the experience of a single, fictitious individual as she interacts with a proposed system. The model intentionally limits our focus to the connected experience of a single human protagonist and the people they engage along the way, in one or more paragraphs about interactions in each of the 15 stages. The Information Lifecycle Engagement Model is based on the work of Joe Andrieu and Ian Henderson from The Customer-Supplier Engagement Model Quick Starter [ 4 ], which uses a 12-stage model to illustrate how personal data stores can interact with CRM systems throughout the customer-supplier lifecycle. The model doesn\u2019t attempt to define all the requirements or possibilities in a system, but rather serves as a lens to focus on a single person across the full set of fifteen categories of interactions. These fifteen stages ensure that we have an example of each of the different steps found in essentially all information systems. The engagement model presents the human context from \u201cboth sides\u201d of the system. On one side, Amira interacts with RISK. On the other side, a variety of counter-participants in different roles interact with Amira through the system. We adopt a \u201cstrawman\u201d architecture for exploring the human experience using one possible embodiment. This non-binding set of suggested functionality allows authors and readers to sanity-check the feasibility of both individual actions and system capability. The strawman intentionally defers deeper technical issues for future design and implementation. The goal is not to design any particular implementation, but instead to make sure the basics ideas are sound from start to finish, capturing the human requirements independent of any specific technology choice. The strawmen helps us understand the proposed system by providing a clear idea of one workable approach. Any actual implementation could vary greatly as subsequent constraints lead to different choices. This paper focuses on the human interactions and human relevance that drive the model, and not the specifics of any proposed implementation. Participants Throughout Amira\u2019s experience, different individuals create and query information in RISK. Amira ( BWHacker) \u2014 The main actor in our model, Amira has a politically tense background. She wants to use her programming skills to positively impact the lives of oppressed people around the world. Ben ( BigBen44) \u2014 A manager at a women\u2019s services non-profit. Ben is looking to connect with developers to build an app that helps women in difficult situations. Charlene ( CharlieOne) \u2014 Amira\u2019s friend and fellow developer. She knows Ben indirectly through the RISK Web of Trust. She helps Amira get started with RISK. Elias \u2014 A mercenary working for the Syrian Hacker Army. Firefly \u2014 A developer who becomes Amira\u2019s successor as an app maintainer. Services RISK depends on, and is partially constituted by a number of decentralized services. anonymous Internet \u2014 support for private communication over the Internet. identifiers \u2014 self-sovereign, pseudo-anonymous sources of cryptographic authority. DIDs, etc. name discovery \u2014ability to use an identifier to look up a unique name or address within a community. web of trust reputation database \u2014 non-hierarchical sources for reputation information (claims and evidence) about people within a community. status database \u2014 non-hierarchical sources of revocation and other status information for identifiers, names, and reputation. community \u2014 synchronous and asynchronous discussions and documents for a group of people. source code repository \u2014 location for developers to collaborate on software. contract \u2014 method for multiple parties to securely contract services with each other. payment \u2014 settlement between parties. app marketplace \u2014 repository for people to be download and install software created by developers. Assumptions All parties are users of computer and internet technology; most have above-average level of technical sophistication including development skills. The Syrian Army is actively monitoring \u201cenemy\u201d technology and technologists and engaging in attacks against enemy technology. Amira and Charlene have an existing relationship; Charlene is willing to vouch for Amira. Charlene has a mobile application for RISK installed and configured on her mobile phone. Decentralized developer systems (e.g., git) are easily available and accessible. Amira uses multiple security technologies and techniques to protect her privacy. Amira is not monitoring changes to her account while on vacation. Stage 1 \u2014 Pre-contact Parties engage in activities that lead to contact. Amira escaped life in Syria when she was 16. She resettled in Boston with an adoptive family and studied computer science in college, taking a job at a bank. The job pays her bills, but Amira wants to make a difference in the world. One night, Amira discovers her opportunity. She sees an interview with Ben, the executive director of an international non-profit, on Cambridge Community Television[ 5 ], sharing his organization\u2019s vision for supporting oppressed women around the world. Ben envisions smart phones as the key to creating anonymous communication channels that safely and reliably connect women with local community services, specifically supporting victims of domestic violence and sexual assault. Amira wants to contribute to Ben\u2019s cause, but worries that engaging in activism might affect her job. Fortunately, as the public face of his non-profit, Ben has established an identifier, BigBen44, on RISK and freely shares it with contacts, leaders, and developers who might help his cause. This gives Amira an opening to reach out to Ben. Stage 2 \u2014 Contact Initial contact is made between the individual and an initial steward. At a local professional event, Amira runs into her good friend Charlene, a fellow technologist with shared values. Amira discreetly asks Charlene if she knows anything about RISK and how Amira might use it to anonymously contribute to Ben\u2019s project. Using her phone, Charlene searches RISK\u2019s developer network to see if Ben is a connection. Charlene finds him, separated by just two degrees. The proximity of their connection on RISK allows Charlene to send Ben a priority introduction. But first, Charlene needs to understand if RISK is the right option for Amira. Charlene tells Amira she may be able to help and suggests they meet after the event, without the crowd of their professional peers nearby. Amira agrees, intrigued and hopeful. Stage 3 \u2014 Triage The initial steward investigates and inquires to decide the most appropriate direction for the individual. Negotiations may occur. To understand if RISK is a good fit for Amira, Charlene asks about her goals and concerns. The security features of the system require a non-trivial amount of effort and if Amira isn\u2019t committed to doing it properly, the consequences could be worse than if she found some other way to work with Ben. Amira explains that she wants to contribute to the project without fear of reprisal because of her background, ethnicity, or gender. She has strong technical skills and a proven ability to deliver useful, quality software. She wants work where she is valued for what she does, without risking reprisals for who she is. She wants to work in a purpose-driven community and to make a difference in the world, not just to make money. She\u2019d like to find a community of peers who share these ideas \u2014 people willing to help shift society\u2019s systemic imbalances anchored in race, gender, location, and nationality while protecting real-world identities. Amira\u2019s ambition perfectly matches the reasons RISK was created. RISK is a decentralized system that relies on transactional reputation to establish pseudonymous identities for people who want to keep their real identities secret and to change the world. RISK connects developers with stakeholders to build projects that improve people\u2019s lives, while simultaneously protecting the identity of those using the system. Charlene decides to act as an introducer for Amira. Stage 4 \u2014 Direction Based on triage and negotiations, both parties agree on a particular course of action. Charlene tells Amira about RISK, explaining the culture as well as the underlying cryptography and consensus mechanisms that make it all work. Amira immediately gets it and hopes she can find the relevant work missing in her life. Charlene writes down the URL for the RISK decentralized source code repository, along with her public-facing RISK handle: CharlieOne. In addition, Charlene verbally shares a memorable phrase that will identify her on the RISK network, instructing Amira to repeat it, remember it, and to NOT write it down. Charlene asks Amira to create her own phrase to uniquely identify herself to Charlene when they connect online. To help remember the phrases, they use them in conversation a few times. It\u2019s a bit silly, but the humorous reinforcement helps. Charlene recommends that Amira set up a bootable portable drive (e.g., a USB stick) to create a secure development environment while working with RISK. Fortunately, Amira isn\u2019t afraid to build a Linux machine from scratch; she\u2019s confident she can get the system up and running. Stage 5 \u2014 Consent Consent is asked and given for creating an identity record, which is then provisioned. During their conversation, Charlene explained that RISK is a system of mutual, earned trust, founded on a commitment to respect individuals\u2019 right to privacy. This commitment is captured in a mutual agreement between all parties using the network. RISK\u2019s Terms of Engagement include: A Code of Ethical Conduct. A confidentiality agreement to maintain anonymity of others and to keep the details of any project confidential \u2014 except and unless explicitly authorized through the reputation system. Consent to use a registered pseudonym for tracking activities and building reputation. Procedures for registering and challenging reputation records. Procedures for updating RISK, including the Terms of Engagement and an IP assignment. Limited liability to minimize frivolous lawsuits and exorbitant claims. A warranty of suitability that requires that participants act on their own free will as an individual and not an organization, entity, or company. Using a credential she generated, 1) Amira cryptographically signs a statement containing her new pseudonym, BWHacker , short for \u201cBetter World Hacker,\u201d 2) the name and a hash of the current RISK terms (v2.4.1), and 3) a statement that BWHacker agrees to these terms. This signed statement is a consent receipt. Amira stores it on her drive and on a backup service, then posts a hash of the receipt to RISK, recording the timestamp when BWHacker formally agreed to the terms. Stage 6 \u2014 Configure Information is shared and associated with the individual. Services are configured. Using a bootable portable drive, Amira sets up a development environment on her laptop. She then downloads RISK and begins working through the READMEs and installation directions. Amira creates a unique identifier in the form of a DID, a Decentralized Identifier,[ 6 ] on the RISK decentralized network. This binds a publicly-verifiable credential to her new pseudonym, BWHacker. This pseudonym and the related DID will allow Amira to securely interact through RISK without revealing her real-world identity. Using the RISK introduction system, Amira looks up CharlieOne and contacts her using her new handle, BWHacker. RISK establishes a secure channel and Amira and Charlene confirm each other\u2019s identity using their memorized phrases. Now both parties have each other\u2019s cryptographically secure identifiers for ongoing correspondence. Stage 7 \u2014 Services Services are provided. To bootstrap her reputation on RISK, Amira creates a Verifiable Credential, self-asserting her capabilities as a mobile software developer with her RISK pseudonym. Through RISK, she sends CharlieOne the credential and asks for an endorsement. Charlene is happy to help. She creates an endorsement, including the original claim, posts a hash of the endorsement to RISK, then sends both to BWHacker. This allows BWHacker to selectively enable anyone to verify the credential and CharlieOne\u2019s endorsement. In addition, Charlene has the ability to revoke this endorsement in the future, if the need arises (for example, the BWHacker account is transferred or penetrated.). Now that BWHacker is in RISK with a suitable endorsement, CharlieOne introduces BWHacker to BigBen44. As BWHacker has not performed work through RISK, BigBen44 relies on CharlieOne\u2019s endorsement giving her status as a subject matter expert. BigBen44 shares background materials with BWHacker outlining the project and specifying requirements, timeframe, and acceptance criteria. After reviewing the materials, BWHacker responds with some clarifying discussion points and a draft statement of work (\u201cSoW\u201d). The discussion reflects a depth of professional experience that confirms CharlieOne\u2019s endorsement to BigBen44. BigBen44 and BWHacker formalize the SoW into a contract to create a \u201cSisterSpaces\u201d app using a RISK contract template that includes pricing, technical milestones, evaluation criteria, and progress payments. RISK\u2019s standard contract templates have support for cryptographic smart contracts. Upon contract signing, BigBen44 deposits the full contract value in bitcoin into a smart escrow account. Each time BWHacker commits code achieving a technical milestone, BigBen44 is sent a randomized code sample to review as well as the ability to spin up a demo guaranteed to utilize the current codebase. BigBen44 can comment and suggest changes as desired. Upon milestone acceptance, the smart contract releases agreed-upon payment to BWHacker\u2019s wallet address. Finally, the smart contract independently records BWHacker\u2019s code delivery as well as BigBen44\u2019s reviews and final acceptance in the RISK reputation system. The completed version 1.0.0 of the SisterSpaces app acts as the final deliverable of the contract. Ben accepts the deliverable, and final payment is released. Amira and Ben then post reviews of the experience of working together; this is a reputational claim on each other\u2019s RISK identity, confirmed by the execution of their smart contract. Stage 8 \u2014 Enhancements Enhancements are offered and accepted. After successfully delivering BigBen44\u2019s project and receiving a glowing review, Amira decides to list her profile in the RISK developer directory. She reviews other entries in the directory to find examples of existing professional pseudo-anonymous profiles: RISK provides professional profile templates that show recent projects and endorsements. When creating her own profile, Amira opts out of displaying a picture of herself on the BWHacker profile. She is careful not to reveal any personal information. Her profile is stored on a decentralized file system and linked to her BWHacker RISK account. When people search RISK for developers, BWHacker\u2019s profile will be displayed with her endorsements. Stage 9 \u2014 Updates Anticipated changes in information lead to revised records. After the account compromise (discussed in Stage 10) and creation of the BWHacker2 pseudonym, Amira buys a hardware \u201cwallet\u201d to help isolate and secure the credentials she uses to access RISK. She posts a transaction to RISK using her initial BWHacker2 credential, replacing the public part with one generated by her new wallet. The old credentials are no longer valid and her identifier remains under her control through her new credentials. Stage 10 \u2014 Problems/Issues Issues beyond the normal support interfaces arise and are resolved. Elias is a mercenary hired by the Syrian Hacker Army to track down and hack the author of SisterSpaces \u2014 the app that Amira coded. Elias tracks BWHacker\u2019s interactions to a dev environment (the one on Amira\u2019s USB key) and captures the private keys associated with the BWHacker RISK DID. Elias changes the master key in the RISK DID, stealing Amira\u2019s ability to communicate as BWHacker, and locking Amira out of her RISK identity. Unfortunately for Amira, this hack happens while she is on vacation. She doesn\u2019t realize the account is compromised until she returns two weeks later. In this interim, Elias patches the SisterSpaces source code so that all traffic is sent to a Syrian surveillance system. Posting as BWHacker, Elias sends a message to BigBen44, telling him about \u201ca security bug fix\u201d for SisterSpaces and strongly recommends immediately releasing the \u201cnew and improved\u201d v1.0.1. Ben, not realizing BWHacker\u2019s account has been compromised, submits the v1.0.1 update to the app store. Returning from her vacation, Amira realizes she\u2019s lost control of BWHacker. Not only has a fake app been released, but she can\u2019t even check the git repo to see what has changed in the new version. Amira calls Charlene for help. Charlene immediately revokes her endorsement of BWHacker and sends a message to BigBen44, telling him that BWHacker has been compromised along with SisterSpace\u2019s code. Taking no chances, Ben immediately disables BWHacker\u2019s access to the git repository, reverts the recent commits, updates the version, and pushes v1.0.2 to the app store. Unfortunately, Amira has no way to regain control of the original BWHacker identity. Instead, She creates a new DID as BWHacker2, repeating the steps she followed when she initially set up her account. Amira generates her credentials directly, making a point to update them with a more secure set as soon as possible (described in Stage 9 Updates). Amira recovers her reputational claims in Stage 13 Recovery. Stage 11 \u2014 Maintenance Software, hardware, or operations are updated and upgraded. Meanwhile, members of RISK are considering an improvement proposal concerning reputation claims. Originally, RISK reputations were limited to one-on-one reveals. Under the proposed improvement, users can choose to aggregate reputation data for presentation on their RISK profiles. The majority of the RISK developers are in favor of the proposal, and it gets approval. Developers update the code base with the new feature and release the improvement. Users like Amira, Charlene, and Ben get a notification that there is a new reference implementation available. Separately, they download it and begin to use the new aggregated reputations. Stage 12 \u2014 Migration Records are moved, transformed to a new schema, input, or output. Seeing the power of displaying her RISK reputations on her profile, Amira decides to move her BWHacker2 persona off RISK to CommonX, a more widely-used system with more developers and projects. She cryptographically links BWHacker2 to CommonX so that users of both systems can verify that the two profiles reflect the same individual. BWHacker2 then reaches out to her existing reputation sources to ask for an endorsement of essentially the same reputation data in the new system. While this is largely automated, it does require the original author to countersign the reputational assertion in a format usable by CommonX. As the authors do so, the migrated reputations start to appear in BWHacker2\u2019s CommonX profile. Stage 13 \u2014 Recovery Lost credentials or identifiers are restored or reset. Unfortunately, Elias has taken complete control over the BWHacker profile. There is no way for Amira to recover it, but she can recreate the endorsements from her work as BWHacker. After setting up a new profile, BWHacker2, Amira recovers the rest of the reputation claims that were attached to BWHacker. She begins by contacting Charlene. They meet in person to confirm the change. Charlene creates and records an endorsement that BWHacker2 is the person formerly known as BWHacker, prior to the account compromise. CharlieOne then reaches out to BigBen44. After he verifies that CharlieOne\u2019s credential keys have not recently changed, Ben connects with BWHacker2 and asks some questions of his own to verify this is really the person he\u2019s been working with. Ben updates his account to set up a maintenance contract with BWHacker2 and double checks that the contract with BWHacker is terminated with prejudice. Stage 14 \u2014 Exit The individual concludes their relationship with stewards in this lifecycle. Amira had thought about leaving her job at the bank to work on developer projects full-time, but then the bank gives her an opportunity she can\u2019t refuse. The CIO asks Amira to oversee a comprehensive overhaul of its internal systems as its new Principal Architect. Amira accepts. However, in her new role, she realizes she no longer has the time to continue working on SisterSpaces. She tells BenBen44, and they agree to end the maintenance contract they had established after the initial version shipped. Ben hires a new developer, Firefly, gives them write access to GitHub, and revokes BWHacker2\u2019s access. To avoid future inquiries for development, Amira removes her BWHacker2 profile from the directory. Stage 15 \u2014 Re-engagement The individual is invited and accepts a new opportunity to engage the system. A year into the rebuild, the bank\u2019s CIO has been replaced and Amira is fed up. She decides to try Firefly\u2019s latest version of SisterSpaces. While the app has become popular, she notices it is straining under heavy technical debt. Amira reaches out to BigBen44 once again as BWHacker2. The two rekindle their connection, and Ben tells Amira that he wants her back full time. Amira gives notice to the bank, reactivates her BWHacker2 profile in the directory, and steps into her dream job of writing socially-responsible software. Appendix A Information Lifecycle Engagement Model The 15-stage model presented here is an instance of an Information Lifecycle Engagement Framework, as illustrated below. Each phase in the model presents a concise description of one or two interactions illustrating the phase. Rather than attempting to describe every interaction across multiple users, the engagement model captures illustrative interactions for each phase in the lifecycle of a particular individual\u2019s engagement with the system. The model should be readable as a sympathetic narrative, clarifying the motivations of all parties and the viability of the described use. We use it to tease out the human requirements independent of the underlying technology. This Information Lifecycle Engagement Model is based on the work of Joe Andrieu and Ian Henderson from The Customer-Supplier Engagement Model Quick Starter [ 7 ] . We have re-interpreted and expanded the 12 phases of the Customer-Supplier Engagement Model for suitability to identity information more generally, shifting away from the commercial nature of the relationship. We also added Maintenance, Migration, and Recovery phases to address the needs of operating an ongoing system. The premise remains: working through each phase of the model provides a comprehensive yet lightweight review of necessary transactions for a given system. This can be done before a detailed specification, giving a coherent view of functional requirements that supports subsequent innovation as design and implementation details emerge.","title":"Amira 1.0.0"},{"location":"RWoT5/amira/#footnotes","text":"[1]: http://www.weboftrust.info/ [2]: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2017/blob/master/topics-and-advance-readings/RWOT-User-Story.md [3]: https://en.wikipedia.org/wiki/Web_of_trust [4]: http://kantarainitiative.org/confluence/display/infosharing/Customer-Supplier+Engagement+Model+Quick+Starter [5]: https://www.cctvcambridge.org/about [6]: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2017/blob/master/topics-and-advance-readings/did-primer.md [7]: http://kantarainitiative.org/confluence/display/infosharing/Customer-Supplier+Engagement+Model+Quick+Starter","title":"Footnotes"},{"location":"RWoT5/blockcerts-revocation/","text":"A Decentralized Approach to Blockcerts Credential Revocation By Jo\u00e3o Santos (Instituto Superior T\u00e9cnico) and Kim Hamilton Duffy (Learning Machine) Abstract Blockcerts are blockchain-anchored credentials with a verification process designed to be decentralized and trustless. While the Blockcerts standard itself is extensible, the revocation method used in the reference implementation is an issuer-hosted revocation list, which is a known centralization point. This proposal describes an alternate method of issuing Blockcerts using Ethereum, which allows for a new form of revocation by either the issuer or the recipient. Introduction & Motivation The Blockcerts standard specifies a record for accomplishments compliant with the Open Badges v2.0 specification -- and soon, Verifiable Credentials . A distinguishing part of the Blockcerts standard is the verification process, which checks the integrity and authenticity of the credential via its presence in a (timestamped) blockchain transaction. The initial release of the Blockcerts standard and reference implementation described only one revocation mechanism, the issuer-hosted revocation list approach also used by Open Badges. This has known limitations, including: centralization, single point of failure, and inability for a recipient to revoke. Other approaches to revocation were considered, but none were technically or economically feasible at the time given the project goals, including Bitcoin blockchain anchoring, low overhead, and minimal cost. For example, one approach included spending a transaction output. This had the advantage that revocations were on-chain, and that either issuer or recipient could revoke. But the approach caused transaction cost to scale with the number of recipients for a batch of certificates, which became too expensive. Revocation is one of the most difficult and incomplete aspects of any verification process, and therefore -- as outlined in Goals and Non-Goals -- a general solution is outside the scope of this paper. In this paper we revisit the revocation aspect of Blockcerts and consider other, decentralized approaches to revocation using smart contracts. Terminology Actors in this scenario: Bob - Issuer Alice - Recipient Carol - Verifier An issuer issues a record of recipient 's accomplishment ( credential ) on a blockchain and shares the resulting blockchain-anchored credential (which we also call a Blockcert ) with the recipient . The recipient can share their Blockcert and an indepedendent verifier can establish the authenticity and integrity of the record. Bob issues a Blockcert to Alice. Alice then gives the Blockcert to Carol, who is able to verify that Bob actually issued that Blockcert, that it hasn't been tampered with, and that it hasn't been revoked. Why Revocation is Important There are several reasons for a credential to be revoked. Let us look at reasons why Alice and Bob might want to revoke Alice's credential. 1. Let us assume that some time after issuing the credential Bob notices an inaccuracy in Alice's achievement. At this point he may want to revoke the credential he issued Alice. 2. Similar to the example above, let us assume that some time after the issuance of the credential, Alice learns new information about Bob that makes her no longer want to be associated with him. She may wish to revoke the credential she received. Goals and Non-goals The goal of this proposal is to outline an approach to revocation that has better properties than the current method, including: Granting recipient ability to revoke Reducing the centralization point caused by issuer revocation lists Improving the auditability of revocations (e.g. on-chain approaches have the advantage that the issuer cannot lie/rewrite history/etc.) Preserving privacy as least as well as the current method used in Blockcerts (more details in Privacy ) Scaling cost with number of revocations, not number of recipients The approach described here is not intended to address all revocation scenarios. The intent is to allow issuer and recipient revocation in order to increase recipient control and improve auditability of revocation events. Longer term solutions could have improved privacy characteristics, described in Context and Future Directions . Issuing, Revoking, and Verifying This section describes a way of issuing, revoking, and verifying Blockcerts by leveraging Ethereum smart-contracts. This extends the Blockcerts reference implementations described in Blockcerts issuing Blockcerts verification Issuing We assume that the Issuer knows each receiver's Ethereum address to be included in the credential. Creating a Credential Batch First the issuer instantiates an Blockcerts issuance smart contract, which we'll refer to as the \"issuance contract\". This will eventially include the batch's Merkle root and list of revoked credentials. For the moment, we only need the issuance contract address and its Application Binary Interface (ABI), which will be embedded in each recipient's credential. To embed the contract address and ABI in each credential, we extend the Blockcerts cert-tools utility, which generates Blockcerts-formatted credentials ready for blockchain issuance. The following abbreviated excerpt shows the important changes in the credentials after cert-tools is finished: New verification type, for now called BlockcertsVerification2018 Addition of the contractAddress Addition of the contract abi { \"type\": \"Assertion\", ... \"badge\": { ... \"issuer\": { \"id\": \"https://www.blockcerts.org/samples/2.0/issuer-testnet.json\", \"type\": \"Profile\", ... \"revocationList\": null }, }, \"verification\": { \"type\": \"BlockcertsVerification2018\", \"contractAddress\": \"0x8efce4923b3238a747e3ee0f725da50bc245142d\", \"abi\": [ ... ] } } It's important that the issuance contract's address and ABI are included in the credential to be part of the hashed input of the batch's Merkle Tree, because otherwise the proper revocation contract could be spoofed. In our prototypes, we included the full ABI for convenience. Eventually the contracts can be standardized; for example, other variants could support only-issuer revocation, per-issuer revocation rules, etc. Doing so would mean only the contract address and reference to the contract ABI would need to be included in the credential. Issuing a Credential Batch The issuer now issues the credential batch on the blockchain using cert-issuer . However, after forming the Merkle Tree of the credential hashes, the issuer updates the issuance contract as follows. Set merkleRoot to the batch's Merkle root Set issuerId to the Issuer's Ethereum address { merkleRoot = \"0x0043...\", issuerId = \"0x12345...\" batchRevocationStatus = false, individualRevokedList = [] } ( batchRevocationStatus and individualRevokedList fields are explained in the Revoking section.) This update records the issuance of the credential batch (via its Merkle root) on the Ethereum blockchain. Note that the issuer does not need to issue to the Bitcoin blockchain unless Bitcoin is specifically desired. We omit that here After blockchain issuance, cert-issuer embeds the signature (\"receipt\") into each Blockcert, so that each recipient can prove their credential is part of the batch. As usual, the receipt contains the current Blockcert's expected hash and Merkle proof (the path from the credential hash to the value on the blockchain). However, instead of the Bitcoin transaction id (as used in the current reference implementation) cert-issuer records the transaction id of the above issuance contract update. Aditionally a link to an Ethereum blockchain explorer can be added, which would allow for the Blockcert's issuance/revocation status to be checked in real time by querying the contract, without the need to run an Ethereum node. Revoking a Credential batchRevocationStatus keeps track of the batch's revocation status and can only by changed by the Issuer. The individualRevokedList is what allows for individual credentials to be revoked. Anyone can append an item to this list, which can be seen as a claim. Extending the example above, let's assume user Alice , whose Ethereum address is 0xew3428376... makes a revocation statement about the Blockcert with certificateId = \"0x353456354...\" . It would be up to the verifying party (who is verifying Blockcert 0x353456354... ) to check whether Alice's claim is valid; that is, to check whether Alice is authorized to revoke the Blockcert in question. This can be done by checking the Blockcert's authorizedRevokingParties for Alice's Ethereum Wallet address. { merkleRoot = \"0x0043...\", issuerId = \"0x12345...\" batchRevocationStatus = false, individualRevokedList = [ { userId = \"0xew3428376...\", // this comes from msg.sender, so can not be spoofed certificateId = \"0x353456354...\" // this should be a part of the merkle tree }] } Verification of Credentials Blockcerts verification performs the usual steps: - Ensure the local Blockcert hash (H_local, computed) matches the expected hash (H_target, from the receipt) - Ensure the Merkle proof from the receipt is valid - Lookup the Blockcert's batch contract address (embedded in the hashed input, therefore tamper resistant) - Ensure the Merkle root (M_receipt) matches the value in the contract (M_target) - Ensure the batch is not revoked - Ensure the Blockcert is not revoked - A Blockcert is revoked if the certificateId appears in the individualRevokedList AND userId is a member of authorizedRevokingParties for this Blockcert. Design and Implementation Choices Who Can Revoke This approach describes a means of allowing either recipient or issuer to revoke a credential. However, in general an issuing contract can support a variety of revocation rules. For example, some credentials such as driver's licenses may not need recipient revocation. Perhaps the issuer may want different revocation rules per-recipient in batch or the ability for parties other than the issuer and recipient to revoke. How Revocation Is Enforced We originally considered a permissioned approach to revocation. For example, since we know the issuer and recipient Ethereum addresses, we could enforce in the contract that the caller is in a valid list. However, this would allow anyone inspecting the contract to see all recipient Ethereum addresses -- not just the revoked ones. this violates the goal of being at least as privacy preserving as the current Blockcerts revocation method (more in Privacy ). Instead, we opted to allow any contract caller to submit a revocation claim, which may or may not be ignored by a verifier. The verifier must ignore any revocation claim from a message sender that is not in the credential's authorizedRevokingParties list. Spam no-op revocation claims are discouraged because parties must spend money to revoke. Credential States We kept a simple model of a binary revoked status. In general, a contract would support multiple states, including a \"suspended\" state, which could be used, for example, if the credential were under review. Omitted Bitcoin Blockchain Anchoring If the use case does not require issuance to the Bitcoin blockchain, this approach results in a verifiable Ethereum-anchored Blockcert. For simplicity, we skipped the Bitcoin issuance step. This can be done if the issuer desires extra assurance. Storing individual Blockcerts in IPFS After the Blockcert is issued (i.e. Issue Credential Batch is finished), individual Blockcerts may be stored in IPFS by either the issuer or recipient. This allows the recipient to retain their credential in a distributed store, accessible and shareable with a permanent and immutable link (which is also tamper-evident by construction). Context and Future Directions Privacy The Verifiable Credentials data model's privacy considerations section provides a more complete framework of privacy concerns. We'll focus on a few aspects: - Tracking: ability for the issuer or anyone other than the recipient to track verification of a credential - Discoverability of credential content - Discoverability of other credential recipient addresses Because the contract does not list any recipient addresses -- it only lists revoked credential UIDs, which are intended to be unique and non-correlatable -- the only time an address is revealed is if the recipient revokes their credential; it won't be revealed if the issuer revokes it. Some third party scanning the contract could obtain credential IDs from the contract, but there is in general no way to look up credential contents from an ID, unless the issuer and recipient mutually agree. There is the concern that correlation could be performed on an address. Because of this, recipients are encouraged to provide new addresses for each credential. Note that the recipient may want to \"advertise\" a certain credential or curate groups of credentials, but there are better ways to achieve this than reuse of addresses for credentials. Data Minimization and Selective Disclosure This paper doesn't touch on other efforts we are pursuing for high-stakes credentials, including the ability to selectively disclose contents of a credential, requiring the recipient's (or a guardian's) involvement in a verifying transaction, or zero-knowledge proofs for verification/revocation steps that reveal addresses. Identity This paper continues use of public keys for identification of both issuer and recipient. The Blockcerts roadmap requires replacing these with Decentralized Identifiers (DIDs) , which are better suited to long-lived recipient-owned credentials. In the Blockcerts schema, this means that publicKey fields will be deprecated in favor of @id fields with DID values, per the Verifiable Credentials data model. This also could integrate with DID authentication methods for interacting with the Blockcerts issuing contract.","title":"A Decentralized Approach to Blockcerts Credential Revocation"},{"location":"RWoT5/blockcerts-revocation/#a-decentralized-approach-to-blockcerts-credential-revocation","text":"By Jo\u00e3o Santos (Instituto Superior T\u00e9cnico) and Kim Hamilton Duffy (Learning Machine)","title":"A Decentralized Approach to Blockcerts Credential Revocation"},{"location":"RWoT5/blockcerts-revocation/#abstract","text":"Blockcerts are blockchain-anchored credentials with a verification process designed to be decentralized and trustless. While the Blockcerts standard itself is extensible, the revocation method used in the reference implementation is an issuer-hosted revocation list, which is a known centralization point. This proposal describes an alternate method of issuing Blockcerts using Ethereum, which allows for a new form of revocation by either the issuer or the recipient.","title":"Abstract"},{"location":"RWoT5/blockcerts-revocation/#introduction-motivation","text":"The Blockcerts standard specifies a record for accomplishments compliant with the Open Badges v2.0 specification -- and soon, Verifiable Credentials . A distinguishing part of the Blockcerts standard is the verification process, which checks the integrity and authenticity of the credential via its presence in a (timestamped) blockchain transaction. The initial release of the Blockcerts standard and reference implementation described only one revocation mechanism, the issuer-hosted revocation list approach also used by Open Badges. This has known limitations, including: centralization, single point of failure, and inability for a recipient to revoke. Other approaches to revocation were considered, but none were technically or economically feasible at the time given the project goals, including Bitcoin blockchain anchoring, low overhead, and minimal cost. For example, one approach included spending a transaction output. This had the advantage that revocations were on-chain, and that either issuer or recipient could revoke. But the approach caused transaction cost to scale with the number of recipients for a batch of certificates, which became too expensive. Revocation is one of the most difficult and incomplete aspects of any verification process, and therefore -- as outlined in Goals and Non-Goals -- a general solution is outside the scope of this paper. In this paper we revisit the revocation aspect of Blockcerts and consider other, decentralized approaches to revocation using smart contracts.","title":"Introduction &amp; Motivation"},{"location":"RWoT5/blockcerts-revocation/#terminology","text":"Actors in this scenario: Bob - Issuer Alice - Recipient Carol - Verifier An issuer issues a record of recipient 's accomplishment ( credential ) on a blockchain and shares the resulting blockchain-anchored credential (which we also call a Blockcert ) with the recipient . The recipient can share their Blockcert and an indepedendent verifier can establish the authenticity and integrity of the record. Bob issues a Blockcert to Alice. Alice then gives the Blockcert to Carol, who is able to verify that Bob actually issued that Blockcert, that it hasn't been tampered with, and that it hasn't been revoked.","title":"Terminology"},{"location":"RWoT5/blockcerts-revocation/#why-revocation-is-important","text":"There are several reasons for a credential to be revoked. Let us look at reasons why Alice and Bob might want to revoke Alice's credential. 1. Let us assume that some time after issuing the credential Bob notices an inaccuracy in Alice's achievement. At this point he may want to revoke the credential he issued Alice. 2. Similar to the example above, let us assume that some time after the issuance of the credential, Alice learns new information about Bob that makes her no longer want to be associated with him. She may wish to revoke the credential she received.","title":"Why Revocation is Important"},{"location":"RWoT5/blockcerts-revocation/#goals-and-non-goals","text":"The goal of this proposal is to outline an approach to revocation that has better properties than the current method, including: Granting recipient ability to revoke Reducing the centralization point caused by issuer revocation lists Improving the auditability of revocations (e.g. on-chain approaches have the advantage that the issuer cannot lie/rewrite history/etc.) Preserving privacy as least as well as the current method used in Blockcerts (more details in Privacy ) Scaling cost with number of revocations, not number of recipients The approach described here is not intended to address all revocation scenarios. The intent is to allow issuer and recipient revocation in order to increase recipient control and improve auditability of revocation events. Longer term solutions could have improved privacy characteristics, described in Context and Future Directions .","title":"Goals and Non-goals"},{"location":"RWoT5/blockcerts-revocation/#issuing-revoking-and-verifying","text":"This section describes a way of issuing, revoking, and verifying Blockcerts by leveraging Ethereum smart-contracts. This extends the Blockcerts reference implementations described in Blockcerts issuing Blockcerts verification","title":"Issuing, Revoking, and Verifying"},{"location":"RWoT5/blockcerts-revocation/#issuing","text":"We assume that the Issuer knows each receiver's Ethereum address to be included in the credential.","title":"Issuing"},{"location":"RWoT5/blockcerts-revocation/#creating-a-credential-batch","text":"First the issuer instantiates an Blockcerts issuance smart contract, which we'll refer to as the \"issuance contract\". This will eventially include the batch's Merkle root and list of revoked credentials. For the moment, we only need the issuance contract address and its Application Binary Interface (ABI), which will be embedded in each recipient's credential. To embed the contract address and ABI in each credential, we extend the Blockcerts cert-tools utility, which generates Blockcerts-formatted credentials ready for blockchain issuance. The following abbreviated excerpt shows the important changes in the credentials after cert-tools is finished: New verification type, for now called BlockcertsVerification2018 Addition of the contractAddress Addition of the contract abi { \"type\": \"Assertion\", ... \"badge\": { ... \"issuer\": { \"id\": \"https://www.blockcerts.org/samples/2.0/issuer-testnet.json\", \"type\": \"Profile\", ... \"revocationList\": null }, }, \"verification\": { \"type\": \"BlockcertsVerification2018\", \"contractAddress\": \"0x8efce4923b3238a747e3ee0f725da50bc245142d\", \"abi\": [ ... ] } } It's important that the issuance contract's address and ABI are included in the credential to be part of the hashed input of the batch's Merkle Tree, because otherwise the proper revocation contract could be spoofed. In our prototypes, we included the full ABI for convenience. Eventually the contracts can be standardized; for example, other variants could support only-issuer revocation, per-issuer revocation rules, etc. Doing so would mean only the contract address and reference to the contract ABI would need to be included in the credential.","title":"Creating a Credential Batch"},{"location":"RWoT5/blockcerts-revocation/#issuing-a-credential-batch","text":"The issuer now issues the credential batch on the blockchain using cert-issuer . However, after forming the Merkle Tree of the credential hashes, the issuer updates the issuance contract as follows. Set merkleRoot to the batch's Merkle root Set issuerId to the Issuer's Ethereum address { merkleRoot = \"0x0043...\", issuerId = \"0x12345...\" batchRevocationStatus = false, individualRevokedList = [] } ( batchRevocationStatus and individualRevokedList fields are explained in the Revoking section.) This update records the issuance of the credential batch (via its Merkle root) on the Ethereum blockchain. Note that the issuer does not need to issue to the Bitcoin blockchain unless Bitcoin is specifically desired. We omit that here After blockchain issuance, cert-issuer embeds the signature (\"receipt\") into each Blockcert, so that each recipient can prove their credential is part of the batch. As usual, the receipt contains the current Blockcert's expected hash and Merkle proof (the path from the credential hash to the value on the blockchain). However, instead of the Bitcoin transaction id (as used in the current reference implementation) cert-issuer records the transaction id of the above issuance contract update. Aditionally a link to an Ethereum blockchain explorer can be added, which would allow for the Blockcert's issuance/revocation status to be checked in real time by querying the contract, without the need to run an Ethereum node.","title":"Issuing a Credential Batch"},{"location":"RWoT5/blockcerts-revocation/#revoking-a-credential","text":"batchRevocationStatus keeps track of the batch's revocation status and can only by changed by the Issuer. The individualRevokedList is what allows for individual credentials to be revoked. Anyone can append an item to this list, which can be seen as a claim. Extending the example above, let's assume user Alice , whose Ethereum address is 0xew3428376... makes a revocation statement about the Blockcert with certificateId = \"0x353456354...\" . It would be up to the verifying party (who is verifying Blockcert 0x353456354... ) to check whether Alice's claim is valid; that is, to check whether Alice is authorized to revoke the Blockcert in question. This can be done by checking the Blockcert's authorizedRevokingParties for Alice's Ethereum Wallet address. { merkleRoot = \"0x0043...\", issuerId = \"0x12345...\" batchRevocationStatus = false, individualRevokedList = [ { userId = \"0xew3428376...\", // this comes from msg.sender, so can not be spoofed certificateId = \"0x353456354...\" // this should be a part of the merkle tree }] }","title":"Revoking a Credential"},{"location":"RWoT5/blockcerts-revocation/#verification-of-credentials","text":"Blockcerts verification performs the usual steps: - Ensure the local Blockcert hash (H_local, computed) matches the expected hash (H_target, from the receipt) - Ensure the Merkle proof from the receipt is valid - Lookup the Blockcert's batch contract address (embedded in the hashed input, therefore tamper resistant) - Ensure the Merkle root (M_receipt) matches the value in the contract (M_target) - Ensure the batch is not revoked - Ensure the Blockcert is not revoked - A Blockcert is revoked if the certificateId appears in the individualRevokedList AND userId is a member of authorizedRevokingParties for this Blockcert.","title":"Verification of Credentials"},{"location":"RWoT5/blockcerts-revocation/#design-and-implementation-choices","text":"","title":"Design and Implementation Choices"},{"location":"RWoT5/blockcerts-revocation/#who-can-revoke","text":"This approach describes a means of allowing either recipient or issuer to revoke a credential. However, in general an issuing contract can support a variety of revocation rules. For example, some credentials such as driver's licenses may not need recipient revocation. Perhaps the issuer may want different revocation rules per-recipient in batch or the ability for parties other than the issuer and recipient to revoke.","title":"Who Can Revoke"},{"location":"RWoT5/blockcerts-revocation/#how-revocation-is-enforced","text":"We originally considered a permissioned approach to revocation. For example, since we know the issuer and recipient Ethereum addresses, we could enforce in the contract that the caller is in a valid list. However, this would allow anyone inspecting the contract to see all recipient Ethereum addresses -- not just the revoked ones. this violates the goal of being at least as privacy preserving as the current Blockcerts revocation method (more in Privacy ). Instead, we opted to allow any contract caller to submit a revocation claim, which may or may not be ignored by a verifier. The verifier must ignore any revocation claim from a message sender that is not in the credential's authorizedRevokingParties list. Spam no-op revocation claims are discouraged because parties must spend money to revoke.","title":"How Revocation Is Enforced"},{"location":"RWoT5/blockcerts-revocation/#credential-states","text":"We kept a simple model of a binary revoked status. In general, a contract would support multiple states, including a \"suspended\" state, which could be used, for example, if the credential were under review.","title":"Credential States"},{"location":"RWoT5/blockcerts-revocation/#omitted-bitcoin-blockchain-anchoring","text":"If the use case does not require issuance to the Bitcoin blockchain, this approach results in a verifiable Ethereum-anchored Blockcert. For simplicity, we skipped the Bitcoin issuance step. This can be done if the issuer desires extra assurance.","title":"Omitted Bitcoin Blockchain Anchoring"},{"location":"RWoT5/blockcerts-revocation/#storing-individual-blockcerts-in-ipfs","text":"After the Blockcert is issued (i.e. Issue Credential Batch is finished), individual Blockcerts may be stored in IPFS by either the issuer or recipient. This allows the recipient to retain their credential in a distributed store, accessible and shareable with a permanent and immutable link (which is also tamper-evident by construction).","title":"Storing individual Blockcerts in IPFS"},{"location":"RWoT5/blockcerts-revocation/#context-and-future-directions","text":"","title":"Context and Future Directions"},{"location":"RWoT5/blockcerts-revocation/#privacy","text":"The Verifiable Credentials data model's privacy considerations section provides a more complete framework of privacy concerns. We'll focus on a few aspects: - Tracking: ability for the issuer or anyone other than the recipient to track verification of a credential - Discoverability of credential content - Discoverability of other credential recipient addresses Because the contract does not list any recipient addresses -- it only lists revoked credential UIDs, which are intended to be unique and non-correlatable -- the only time an address is revealed is if the recipient revokes their credential; it won't be revealed if the issuer revokes it. Some third party scanning the contract could obtain credential IDs from the contract, but there is in general no way to look up credential contents from an ID, unless the issuer and recipient mutually agree. There is the concern that correlation could be performed on an address. Because of this, recipients are encouraged to provide new addresses for each credential. Note that the recipient may want to \"advertise\" a certain credential or curate groups of credentials, but there are better ways to achieve this than reuse of addresses for credentials.","title":"Privacy"},{"location":"RWoT5/blockcerts-revocation/#data-minimization-and-selective-disclosure","text":"This paper doesn't touch on other efforts we are pursuing for high-stakes credentials, including the ability to selectively disclose contents of a credential, requiring the recipient's (or a guardian's) involvement in a verifying transaction, or zero-knowledge proofs for verification/revocation steps that reveal addresses.","title":"Data Minimization and Selective Disclosure"},{"location":"RWoT5/blockcerts-revocation/#identity","text":"This paper continues use of public keys for identification of both issuer and recipient. The Blockcerts roadmap requires replacing these with Decentralized Identifiers (DIDs) , which are better suited to long-lived recipient-owned credentials. In the Blockcerts schema, this means that publicKey fields will be deprecated in favor of @id fields with DID values, per the Verifiable Credentials data model. This also could integrate with DID authentication methods for interacting with the Blockcerts issuing contract.","title":"Identity"},{"location":"RWoT5/data-minimization-sd/","text":"Engineering Privacy for Verified Credentials: In Which We Describe Data Minimization, Selective Disclosure, and Progressive Trust ============================== Contributors: Lionel Wolberger, Platin.io Brent Zundel, Evernym/Sovrin Zachary Larson, Independent Irene Hernandez, Independent Katryna Dow, Meeco Introduction We often share information on the World Wide Web, though some of it is private. The W3C Credentials Community Group focuses on how privacy can be enhanced when attributes are shared electronically. In the course of our work, we have identified three related but distinct privacy enhancing strategies: \"data minimization,\" \"selective disclosure,\" and \"progressive trust.\" These enhancements are enabled with cryptography. The goal of this paper is to enable decision makers, particularly non-technical ones, to gain a nuanced grasp of these enhancements along with some idea of how their enablers work. We describe them below in plain English, but with some rigor. This knowledge will enable readers of this paper to be better able to know when they need privacy enhancements, to select the type of enhancement needed, to assess techniques that enable those enhancements, and to adopt the correct enhancement for the correct use case. Three Examples Three examples of how people would like their privacy preserved in the process of sharing credentials help to illuminate these three techniques. Diego attempts to use an online service and is asked to share his location in order to prove his geolocation. Diego hesitates, since the service doesn't need his location everyday, everywhere. He knows that the service may share this information with other parties without meaningful consent on his part. Thoughts pass through his mind: What location data does the service actually need? What will it read in future? Is there a way for him to share his location just this once, or to only share an approximate location? Selena hands her driver's license to a bouncer to prove she is of drinking age. As he looks it over, she sees him inspecting her date of birth and home address. He only needs to know that she is over 21. Is there a way to disclose that she is indeed old enough without revealing her actual age, along with her home address and city of residence as well? Proctor , negotiating with a real estate agent to purchase a home, reveals a letter from his bank stating his credit limit. He wanted to reveal its approximate amount only, but the agent insisted on verifying that the letter was authentic. Proctor feels the agent now has the upper hand in the negotiation, as the letter reveals more than just its authenticity. Could he have revealed only an approximate amount and reveal more details as the negotiations progress? Each story features information that is verifiable: a home address, age, or credit limit. We call such information a credential, and a detail of a credential we call an attribute. We have three strategies for enhancing the privacy of digitally shared credential attributes, and each story highlights one. Diego's story highlights the need for \"data minimization,\" Selena's for \"selective disclosure,\" and Proctor's for \"progressive trust.\" Let's examine each one in detail before discussing enablers. Privacy Enhancements We propose the following three privacy enhancements. (Sources used to curate these definitions are listed in Appendix A.) Data Minimization Data minimization is the act of limiting the amount of shared data strictly to the minimum necessary in order to successfully accomplish a task or goal. There are three types of minimization: * Content minimization \u2013 the amount of data should be strictly the minimum necessary. * Temporal minimization \u2013 the data should be stored by the receiver strictly for the minimum amount of time necessary to execute the task. * Scope minimization \u2013 the data should only be used for the strict purpose of the active task. Data minimization is enacted primarily by policy decisions made by stakeholders in the credentials ecosystem: * Credential issuers ensure that credentials may be presented in such a way as to enable data minimization. This may require issuing multiple, related, granular sub-credentials. * Credential inspectors establish in advance policies regarding the data they will examine: * what is the minimum data necessary to accomplish the task or goal? * what is the minimum time the data can be stored to execute the task? * what processes ensure that the data is applied only to the task at hand and does not, by a process of scope creep, become applied to other tasks or goals? Data minimization policies impact selective disclosure, the next privacy enhancement. Selective disclosure Selective disclosure is the ability of an individual to granularly decide what information to share. Stakeholders in the credentials ecosystem enable selective disclosure capabilities in the following ways: * Credential issuers format the credential and its attributes in such a way as to enable selective disclosure. As with the strategy of data minimization, they may issue multiple, related, granular sub-credentials. Each attribute and the overall credential may be formatted to support cryptography, a capability described in more detail below. * Credential inspectors ensure the request is framed in such a way as to enable selective disclosure, using the cryptographic tools required. Once data minimization policies and selective disclosure are in place, the third and last enhancement can be applied. Progressive Trust Progressive trust is the ability of an individual to gradually increase the amount of relevant data revealed as trust is built or value generated. To enable progressive trust capabilities, stakeholders in the credentials ecosystem act in the following ways: * Issuers format the credential(s) in such a way as to enable progressive trust. This may require issuing multiple, related, atomic sub-credentials. It also may require formatting the credential to support mathematical queries and cryptographic proofs. Finally, the issuer's data model may express how the various sub-credentials are related in a scenario involving progressive trust. * Inspectors ensure that requests are framed in such a way as to enable progressive trust. They structure the communication in order to to gradually escalate credential requests in order to enable a subject to progressively trust the inspector more and more, revealing the minimum data necessary to accomplish each step of the task or goal and revealing more and more as the mutual communication progresses. Crypto Enablers Implementing privacy enhancements depends on organizational decisions. Determination of the data needed, with an eye towards data minimization, along with a clear model of how data is used over the lifecycle of engagement, goes a long way towards enabling progressive trust. However, policies are not enough. When enhancing privacy online, some data parts must be revealed while others remain concealed. Concealment is achieved mostly by the art of cryptography, from the greek word \"kryptos,\" meaning hidden, like in a crypt. Crypto (a short word we will use for cryptography) enables us to achieve our goal by means of three primary enablers: having a secret, having a difficult mathematical task, and having zero-knowledge enablers. The children's \"Where's Waldo?\" illustrated book series helps us to understand these three enablers. In these books a distinctively dressed man appears only once on each page, wearing a striped hat. Readers are asked to scour the page and locate him. We can understand the three enablers by examining Where's Waldo one step at a time. A Secret : For the new reader, Waldo's location is a secret. The illustrator knows it, and the reader doesn't. The reader is encouraged to search the page and find Waldo, but that is a difficult task. Some readers give up and ask someone who has already found Waldo to show them his location. In essence, they are asking another reader to reveal the secret. Once found, a reader could keep the information secret by circling Waldo in red and storing the book in a safe. This amounts to storing the secret for future use. Secrets are essential to crypto. They are usually called keys, and they must be managed carefully. A Difficult Task : Waldo is difficult to find on the page. The reader has to search everywhere and mistakenly identify many Waldo look-alike characters before reaching a satisfactory conclusion and finding him. Yet when he is finally discovered, or someone points Waldo out, it's easy to see where he is. That's why it's a fun task. This difference between the difficulty of conducting the task and the ease of verifying the task lies at the heart of cryptographic enablers. A Zero Knowledge Enabler : Can you prove you found Waldo without revealing the secret of his actual location on the page? There is a simple way to do so. Take a rectangular piece of white cardboard that is much larger than the book. Cut a hole exactly fitting Waldo to reveal his silhouette only, nothing else. You can now show Waldo to anyone, peeking out of the cardboard. Yet the cardboard is wide and opaque, hiding the book thoroughly, so a verifier has no idea where Waldo is on the page. The puzzle was solved and someone verified the achievement, without revealing any knowledge of how to solve the puzzle. The secret is still safe, the task still just as difficult as before. Where's Waldo books are drawings, while crypto is built from mathematical equations, basically puzzles based on numbers. We provide the interested reader with a layman's overview in Appendix B. Three Solutions We now return to our opening examples, apply the privacy preserving strategies and enablers described, and describe the improved outcomes. The online service that Diego uses does an internal policy review and realizes (a) it only needs a location when a user signs up for an account, and (b) it does not need an exact address, only the county district. It changes its interface to request a Verifiable Credential for Diego's location. Diego's system creates this credential for him, which can be inspected to reveal the county district. The crypto to enable this would be similar to that described in Appendix C. With this data minimization, the online service has less risk of violating data protection rules, is less a target for hacking, and has lower overall costs, while at the same time preserving Diego's privacy. The bar seeking to verify Selena 's age uses selective disclosure as built into the Verifiable Claims system. Selena will no longer share her date of birth. Instead, Selena creates a secret that we harness to craft a crypto-formatted credential. This crypto makes it easy to verify her age, but difficult to determine her exact date of birth. The bouncer's system can perform a zero-knowledge proof to determine the credential is valid and that Selena is older than twenty-one, without revealing her birthday or her secret. The bouncer sees she is over twenty-one without seeing her date of birth, residence address, or any other unnecessary information. In Appendix C we show the process step-by-step. The real estate agency working with Proctor implements a data model specifying what is required at each step of the real estate negotiation. The first step requires only proof of being an account holder in good standing at a known bank, so Proctor does not have to reveal the detailed letter at this point. As their negotiation continues, Proctor reveals more and more information as required. Some steps of the process may share Verifiable Claims encoded with crypto. Summary The World Wide Web accelerates the sharing of credentials and other digital interactions, and many regulations have been passed and strategies proposed to protect privacy, some of which require cryptography. To align terminology, the World Wide Web Credentials Community Group has found three related but distinct privacy enhancing strategies that create a useful rubric for discussing the challenges and arriving at solutions. We share the examples of Diego, Selena, and Proctor and propose \"data minimization,\" \"selective disclosure,\" and \"progressive trust,\" with accompanying crypto protocols as useful semantics for accelerating the adoption of digital interaction while protecting privacy. Appendix A: Definition Sources This section contains definitions we curated, based on research and oral interviews, to create the definitions of data minimization, selective disclosure and progressive trust. Data Minimization Definitions of data minimization that we considered in the formation of our definition above. GDPR Rec.39; Art.5(1)(c) definition: \u201cThe personal data should be adequate, relevant and limited to what is necessary for the purposes for which they are processed. This requires, in particular, ensuring that the period for which the personal data are stored is limited to a strict minimum. Personal data should be processed only if the purpose of the processing could not reasonably be fulfilled by other means.\u201d Reducing your overall footprint of data outside of your control. Can be accomplished by using selective disclosure. Adequate, relevant and non-excessive. Reducing the amount of data you are sending in a payload to only the one ... needed. That prevents leakage of confidential information. Providing people with the information they need without revealing non necessary info. If I need to prove if I am old enough without revealing an actual birthdate. Best practices \u2013 your should deeply inspect your use case and come to a conclusion as to what is the minimum data you need to accomplish your goal. Don\u2019t be greedy. Data has proven to be toxic. Mathematical \u2013 finding a way to express the data that you wish as an equation related to the data you have. Minimizing the amount of data to achieve your goal or communicate what you need to. Designing systems to operate efficiently in order to maximize privacy. Choosing to only share the minimal amount of data about yourself or something during an interaction. Trying to keep the amount of info that is being disclosed as limited as possible to the requirements of the vulnerability. The minimum is what ... will lead them to move to action. Always happens in a context: a relationship where the two parties are considering interacting in some way. Sending only the signals that I want to send and that are needed by the other party, hem to interact with me in a particular The least amount of data needed for a system to function Collecting the least amount of date for the highest outcome. Also known as minimal disclosure, data minimization is the principle of using the least amount of data to accomplish a transaction. This is incumbent on all three parties in an exchange. The holder should attempt to share the minimum. The issuer needs to create attributes designed for composition and minimal use, as opposed to monolithic credentials with all the data. The verifier needs to ask only for what they need. The motivation to minimize data is that unneeded data is potentially \u201ctoxic.\u201d Selective Disclosure Definitions of selective disclosure that we considered in the formation of our definition above. Ability to decide what info you give and how it can be used. Smart disclosure, allows [selecting] what information to give based on logic. Blind search. You can decide who gets to see what. Means by which we achieve data minimization. Form of policies. Ability to mask attributes that you do not have to share. Relates to mathematical definition \u2013 the computational ability to reveal only parts of your data profile. Act of communicating or revealing only what you intend to, and not any peripheric data Having granular control over the ways in which data is shared Is a pattern for user interfaces allowing people to choose what to share about them during an interaction Method for achieving data minimization where only certain signals are being shared and there is control of who it is being shared with. That control is never perfect. The communication channel matters. An entity having granular control on what\u2019s revealed. The individual having the freedom to decide what to share, or the acquirer using data minimization approach requesting the minimum amount amount of data for the maximum impact Progressive Trust Definitions of progressive trust that we considered in the formation of our definition above. Note that we included definitions of progressive trust and progressive disclosure as well. Procedure for increasing revelation of relevant data as the communication proceeds. As we continue to communicate we decide to reveal more information. It becomes more generous as trust builds. Being able to reveal more data as you need to given certain conditions Information is disclosed as needed when needed. You can choose to increase the amount of data you disclose over time as needed. Taking as little vulnerability as possible at the beginning, then gaining information and becoming willing to take on additional vulnerability by revealing more information. Trust is built through step by step interactions where we start making ourselves vulnerable in a very small way and we observe how this works out. Based on results we consider making ourselves further vulnerable or not. It is about increasing levels of familiarity and prediction making (I am better able to predict your behavior). Releasing information as needed Escalation of the previous steps (data minimization, selective disclosure) in line with the value increasing. Purpose binding is the auditable use of data, so I can audit the use of my data and determine that it was used for the purposes declared. Progressive trust is the feeling of assurance and safety that develops over time, based on a history of data used only for its bound purposes, and so based on this feeling a data holder will be ready to share more data or other data, if at some point in the relationship this other data is requested. Trust is required when you depend on the actions of someone who you can't control. Appendix B: Basic Crypto Concepts This appendix describes basic cryptographic concepts critical to the privacy preserving engineering of credential attributes. For readability, we use the short word, \"crypto.\" Overview Crypto is a huge field with highly specialized jargon, too much to cover here. But non-specialists would benefit from some understanding of relevant crypto in order to make informed decisions. We begin with a brief overview of several concepts from number theory that serve as a foundation for all crypto used in this process. This is a curated list of topics progressing from the simple to the more complex. Notice how ideas are re-used and layered as you read on. Number Theory Number theory refers to the study of the behavior of integer numbers such as one, three, or two hundred. The following are behaviors of these numbers that make them useful for crypto: Prime or not : Some numbers are only divisible by themselves and one. These are called 'prime.' One-way function : A numerical function that takes a publicly known number, and without any secret information, computes a value. Like a one-way street, the computation goes only one direction. Given the computed value, it is hard to find the publicly known number. Clock arithmetic , aka modular arithmetic: a system of arithmetic where numbers \"wrap around\" upon reaching a certain value. A familiar use is in the ordinary clock, in which our day is divided into twelve-hour periods. If the time is seven o'clock now, then ten hours later it will be five o'clock, though a military man might say seventeen hundred hours. Even he will say that ten hours later it is three o'clock, and not twenty-seven hundred hour, because his clock time \"wraps around\" every twenty-four hours (as opposed to twelve). Groups and Finite Fields : Some subsets of all integers form a \"group\" that behaves in ways very useful to the performance of cryptography. In extremely simplified terms, a group is a self-sufficient set of integers, where any possible manipulation returns an answer from within the group. Finite fields are types of groups that satisfy certain demanding properties. Notice how this resembles clock arithmetic, where the same numbers are used over and over again. Discrete Logarithms : A discrete logarithm is a property for numbers in a group. Since there is no efficient method for computing discrete logarithms, they form a \"difficult problem\" and so are very useful in cryptography. (The logarithm log(b) of a is an exponent x such that b^x ( b raised to the x exponent) = a .) Quadratic Residues : Quadratic refers to 'squared' numbers, a number raised to the second exponential power. Quadratic residues are a useful property of squared numbers as they behave in modular arithmetic. Primary Objectives The curious behavior of numbers is exploited to achieve four primary crypto objectives. Confidentiality : a hidden part of a credential cannot be understood by anyone for whom it is unintended. Often called \"privacy,\" we avoid that word here since it can mean many things in addition to confidentiality. Authentication : the identity of information shared can be validated as authentic. Integrity : the revealed part of the credential cannot be altered without such alteration being detected. Also known as validity, fidelity or verifiability. Non-repudiation : aka non-deniability, a credential's creator cannot deny at a later stage his or her involvement. Ten Crypto Concepts Over the decades hundreds if not thousands of crypto protocols, processes, algorithms and protocols have been innovated to achieve these objectives, by cobbling together the above six behaviors in different ways. We present here a brief tour of the ten most significant ones in our field of verifiable credentials: PKI or \"public and private keys\": a system that lies at the heart of most relevant crypto since a publicly shared digital asset can be locked to, or encumbered by, a private key that is kept secret. Only the person with knoweldge of that private key can, with the right software, unlock that asset. This enables a broad range of activities such as \"signature,\" \"authentication,\" and \"certificate validation.\" In general we call these activities PKI, a public key infrastructure. Signature : a signature in this context is a use of PKI. A valid signature gives a recipient \"authentication\", confidence that the message was created by a known sender; \"non-repudiation,\" that the sender cannot deny having sent the message; and \"integrity,\" that the message was not altered in transit. Signatures enable every cryptographic objective except for confidentiality. There are many types of signature schemes in use including Digital Signature Algorithm (DSA), Camenisch-Lysyanskaya (CL) signatures, and Boneh\u2013Lynn\u2013Shacham (BLS) signatures. Key exchange : exchange methods enable two parties that have no prior knowledge of each other to jointly establish a shared secret key over an insecure channel. This key can then be used to encrypt subsequent communications using a symmetric key cipher. Diffie\u2013Hellman is a common example. Elliptic-curve cryptography (ECC): an approach to PKI based on the numeric structure of elliptic curves over finite fields. ECC is useful as it requires smaller keys compared to non-ECC cryptography. There are many variants of ECC used including Edwards-curve Digital Signature Algorithm (EdDSA). Hash or message digest : one-way functions, such as SHA-256. A set of many one-way functions may be applied to a tree of data to form a Merkle Tree (or trie). Zero-Knowledge (ZK): zero knowledge is defined above loosely as a set of practices where some data is revealed while other parts are kept secret. Many ZK methods are used in cryptography incuding Fiat Shamir, Proof of knowledge of discrete logarithms, ZK Snarks, and ZK Starks. Accumulators : a form of ZK, a cryptographic accumulator is a one-way membership function that answers a query as to whether a potential candidate is a member of a set without revealing the individual members of the set. Similar to a one-way hash function, cryptographic accumulators generate a fixed-size digest representing an arbitrarily large set of values. Some further provide a fixed-size witness for any value of the set, which can be used together with the accumulated digest to verify its membership in the set. Commitment : a cryptographic commitment, which allows one to commit to a chosen value (or chosen statement) while keeping it hidden to others, with the ability to reveal the committed value later. Witness : a term that has different applications in cryptography. In this paper, a witness is a value used in a cryptographic accumulator. In Bitcoin the unlocking signature is called the \"witness data.\" Quantum Computing and Cryptography: as quantum computing is developed, it poses a threat to the difficulty of puzzles. For example, they are likely to be much faster at determining if a number is truly prime or not. Appendix C: Drinking Age Credential Implementation The birthday of an individual is formatted into a verifiable credential, which can be inspected to reveal the age of the credential holder without revealing their birthdate. The flow described here is based on the developing Verifiable Claims standard of the W3C Credentials Community Group. It uses cryptography developed by Jan Camenisch, as implemented by Sovrin. This is a work in progress. Note that other types of crypto could be applied to achieve the same privacy preserving goals. Communication Flow The flow below may be copies and pasted into the Web Sequence Diagram webpage to generate a flow diagram. title Verifiable credential using Selective Disclosure participant Valid Time Oracle participant Janet participant ID Provider participant Ledger participant Bar note over Janet:Prover note over Bar:Validator note over Janet,Bar: Preparation and Setup note right of ID Provider:Infrastructure ID Provider->Ledger: Define Schema (Name, Birthdate, Address) ID Provider->Ledger: credential Definition (Pub Key, etc.) ID Provider->ID Provider: Generate Prv Key for this credential ID Provider->Ledger:Revocation Registry note left of Bar: Prepare to accept credentials Bar->Bar:Install Agent Bar->Ledger: Check schema note over Janet,Bar: Begin Use Case Janet->ID Provider: Request ID ID Provider-->Janet: ID will be issued as a digital credential note right of Janet: Prepare to receive credentials Janet->Janet: Install Agent Janet->Janet: Prv Key Generate, Store Janet->Ledger:Check Schema Ledger->Janet:credential Definition Janet-->ID Provider:Proof of Name, Birthdate, Address Janet->ID Provider: Blinded secret ID Provider->Janet: credential Janet->Janet: Validate credential against credential Def note over Janet,Bar: Janet goes to the bar note left of Bar: Can Janet Enter? Bar->Janet: Request Proof of Age Janet->Valid Time Oracle: Get time Valid Time Oracle->Janet: Time credential Janet->Janet:Generate Proof (This person is over 21) Janet->Bar: Provide Proof Bar->Bar: Evaluate proof Bar->Ledger: Verify on Ledger Ledger->Bar: Verification Bar->Janet: Come in note left of Bar: Invite to club Bar->Janet: Join loyalty club? (requires valid postal code) Janet->Janet:Generate Proof (postal code) Janet->Bar: Provide Proof Bar->Bar: Evaluate proof Bar->Ledger: Verify on Ledger Ledger->Bar: Verification Bar->Janet: Have Loyalty Card Crypto Details Below are some of the detailed mathematics involved in issuing a verifiable credential as implemented by Sovrin, a non-profit organization dedicated to managing a decentralized, public network for the purposes of self-sovereign identity. Issuer Setup The following setup is a necessary precursor to issuing a privacy-preserving credential. Compute Perform the mathematical calculations required to curate the essential ingredients of the operations we are about to perform. Some of these results, like the private keys, are very sensitive and must be kept secret by the credential holder; others are to be shared. Random \ud835\udcf9', \ud835\udcfa', 1024-bit prime numbers, such that \ud835\udcf9 = 2\ud835\udcf9' + 1 and \ud835\udcfa = 2\ud835\udcfa' + 1 are both 1024-bit prime numbers. \ud835\udcf7 = \ud835\udcf9\ud835\udcfa. Random quadratic residue: \ud835\udce2 mod \ud835\udcf7 Random \ud835\udce7 \ud835\udce9 , \ud835\udce7 \ud835\udce11 , . . . , \ud835\udce7 \ud835\udce1\ud835\udcf5 \u2208 [2: \ud835\udcf9'\ud835\udcfa' - 1], where \ud835\udcf5 is the number of attributes in the credential. \ud835\udce9 = \ud835\udce2 \ud835\udce7\ud835\udce9 mod \ud835\udcf7 \ud835\udce1 \ud835\udcf2 = \ud835\udce2 \ud835\udce7\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 Issuer private key \ud835\udcfc\ud835\udcf4 \ud835\udcec = \ud835\udcf9'\ud835\udcfa' Issuer public key \ud835\udcf9\ud835\udcf4 \ud835\udcec = {\ud835\udcf7, \ud835\udce2, \ud835\udce9, \ud835\udce1 1 , . . . , \ud835\udce1 \ud835\udcf5 } Proof of Correctness As a result of the above computations, we then curate the following. This proof, along with the public keys, is the computational algorithm that will be used to validate the credential. Random \ud835\udce7' \ud835\udce9 , \ud835\udce7' \ud835\udce11 , . . . , \ud835\udce7' \ud835\udce1\ud835\udcf5 \u2208 [2: \ud835\udcf9'\ud835\udcfa' - 1] \ud835\udce9' = \ud835\udce2 \ud835\udce7'\ud835\udce9 mod \ud835\udcf7 \ud835\udce1' \ud835\udcf2 = \ud835\udce2 \ud835\udce7'\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 \ud835\udcec = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce9 || \ud835\udce1 1 || . . . || \ud835\udce1 \ud835\udcf5 || \ud835\udce9' || \ud835\udce1' 1 || . . . || \ud835\udce1' \ud835\udcf5 ) \ud835\udce7'' \ud835\udce9 = \ud835\udce7' \ud835\udce9 + \ud835\udcec \ud835\udce7 \ud835\udce9 \ud835\udce7'' \ud835\udce1\ud835\udcf2 = \ud835\udce7' \ud835\udce1\ud835\udcf2 + \ud835\udcec \ud835\udce7 \ud835\udce1\ud835\udcf2 , 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 The Cred Def is comprised of the public key and the proof of correctness; this is published to the distributed ledger. Issuing a Credential With setup complete, we can now issue the credential in a privacy-preserving manner. For Each Credential For each credential issued, perform the following operations. Issuer Computes A cryptographic accumulator is constructed in order to enable zero-knowledge queries further on. It is a one-way membership function, including the claim in the membership set. The operation can then answers a query as to whether a potential candidate is a member of a set without revealing the individual members of the set. \ud835\udcd0 \ud835\udcf2 = accumulator index \ud835\udce4 \ud835\udcf2 = user index \ud835\udcf6 2 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udcd0 \ud835\udcf2 || \ud835\udce4 \ud835\udcf2 ) 256-bit integer representations of each of the attributes: \ud835\udcf6 3 , . . . , \ud835\udcf6 \ud835\udcf5 \ud835\udcf7 0 = nonce Issuer Sends \ud835\udcf7 0 to Prover This nonce is provided to the Prover for calculation of the Prover's proof of correctness. Prover Receives \ud835\udcf7 0 and Computes the Following The prover aggregates and prepares public keys for use in validating the signatures. The prover also commits to a chosen value while keeping it temporarily hidden, making the calculation binding. Retrieves Issuer\u2019s public key \ud835\udcf9\ud835\udcf4 \ud835\udcec Retrieves Issuer\u2019s proof of correctness Generates: \ud835\udcf6 1 = pedersen commitment of claim link secret Random \ud835\udcff', \ud835\udcff'', \ud835\udcf6' 1 \ud835\udcf7 1 = nonce Prover Verifies the Issuer\u2019s Proof of Correctness \ud835\udce9^ = \ud835\udce9 \ud835\udcec \ud835\udce2 \ud835\udce7''\ud835\udce9 mod \ud835\udcf7 \ud835\udce1^ \ud835\udcf2 = \ud835\udce1 \ud835\udcf2 \ud835\udcec \ud835\udce2 \ud835\udce7''\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 Verifies \ud835\udcec = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce9 || \ud835\udce1 1 || . . . || \ud835\udce1 \ud835\udcf5 || \ud835\udce9^ || \ud835\udce1^ 1 || . . . || \ud835\udce1^ \ud835\udcf5 ) Prover Computes \ud835\udce4 = \ud835\udce2 \ud835\udcff\u2019 \ud835\udce1 1 \ud835\udcf61 mod \ud835\udcf7 \ud835\udce4\u2019 = \ud835\udce2 \ud835\udcff\u2019\u2019 \ud835\udce1 1 \ud835\udcf6\u20191 mod \ud835\udcf7 \ud835\udcec\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce4 || \ud835\udce4\u2019 || \ud835\udcf7 0 ) \ud835\udcff^ = \ud835\udcff\u2019\u2019 + \ud835\udcec\u2019\ud835\udcff\u2019 \ud835\udcf6^ 1 = \ud835\udcf6\u2019 1 + \ud835\udcec\u2019\ud835\udcf6 1 Prover Sends \ud835\udcdf = { \ud835\udce4, \ud835\udcec\u2019, \ud835\udcff^, \ud835\udcf6^ 1 , \ud835\udcf7 1 } to the Issuer Issuer Verifies Prover Setup Computes \ud835\udce4^ = \ud835\udce4 -\ud835\udcec \ud835\udce2 \ud835\udcff^ \ud835\udce1 1 \ud835\udcf6^ 1 mod \ud835\udcf7 Verifies \ud835\udcec\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce4 || \ud835\udce4^ || \ud835\udcf7 0 ) Issuer Signs the Credential by Computing the Following \ud835\udce0 = \ud835\udce9 / (\ud835\udce4\ud835\udce2 \ud835\udcff* \ud835\udce1 2 \ud835\udcf62 \ud835\udce1 3 \ud835\udcf63 \u00b7\u00b7\u00b7 \ud835\udce1 \ud835\udcf5 \ud835\udcf6\ud835\udcf5 ) mod \ud835\udcf7 \ud835\udced = \ud835\udcee -1 mod \ud835\udcf9\u2019\ud835\udcfa\u2019 \ud835\udcd0 = \ud835\udce0 \ud835\udced mod \ud835\udcf7 \ud835\udcd0\u2019 = \ud835\udce0 \ud835\udcfb mod \ud835\udcf7 \ud835\udcec\u2019\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 (\ud835\udce0 || \ud835\udcd0 || \ud835\udcd0\u2019|| \ud835\udcf7 1 ) \ud835\udcfc \ud835\udcee = (\ud835\udcfb - \ud835\udcec\u2019\u2019\ud835\udcee -1 ) mod \ud835\udcf9\u2019\ud835\udcfa\u2019 Issuer Sends \ud835\udcde = {\ud835\udcd0, \ud835\udcee, \ud835\udcff*, \ud835\udcfc \ud835\udcee , \ud835\udcec\u2019\u2019, \ud835\udcf6 2 , . . . , \ud835\udcf6 \ud835\udcf5 } to the Prover Prover Receives \ud835\udcde and Does the Following Prover Computes \ud835\udcff = \ud835\udcff\u2019 + \ud835\udcff* \ud835\udce0\u2019 = \ud835\udce9 / (\ud835\udce2 \ud835\udcff \ud835\udce1 2 \ud835\udcf62 \ud835\udce1 3 \ud835\udcf63 \u00b7\u00b7\u00b7 \ud835\udce1 \ud835\udcf5 \ud835\udcf6\ud835\udcf5 ) mod \ud835\udcf7 \ud835\udced\u2019 = \ud835\udcec\u2019\u2019 + \ud835\udcfc \ud835\udcee \ud835\udcee \ud835\udcd0^ = \ud835\udcd0 \ud835\udced\u2019 \ud835\udce2 \ud835\udcff\u2019\ud835\udcfc\ud835\udcee mod \ud835\udcf7 Prover Verifies \ud835\udcee is prime and 2 596 \u2264 \ud835\udcee \u2264 2 596 + 2 119 \ud835\udce0\u2019 = \ud835\udcd0 \ud835\udcee mod \ud835\udcf7 \ud835\udcec\u2019\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 (\ud835\udce0\u2019 || \ud835\udcd0 || \ud835\udcd0^ || \ud835\udcf7 1 ) Prover Stores Primary Claim ({\ud835\udcf6 1 , . . . , \ud835\udcf6 \ud835\udcf5 }, \ud835\udcd0, \ud835\udcee, \ud835\udcff) For Additional Information The crypto used here is originally from the Identity Mixer. Link: (https://www.zurich.ibm.com/identity_mixer/) The Sovrin team shares additional information and working code at the following links. * Verifiable Credentials Code * Verifiable Credentials Example Usage in Python References: W3C Credentials Community Group Home Page Data Minimization and Selective Disclosure Repo Camenisch, Lysyanskaya. An Efficient System for Non-transferable Anonymous Credentials with Optional Anonymity Revocation Pfitzmann, Hansen. 2010. A terminology for talking about privacy by data minimization: Anonymity, Unlinkability, Undetectability, Unobservability, Pseudonymity, and Identity Management Cooper, Tschofenig, Aboba, Peterson, Morris, Hansen, Smith, Janet. 2013. RFC6973 . The draft can also be helpful, \"This document focuses on introducing terms used to describe privacy properties that support data minimization.\" Hansen, Tschofenig, Smith, Cooper. 2012 Privacy Terminology and Concepts. Network Working Group Internet-Draft Expires: September 13, 2012 Longley, Sporny. Redaction Signature Suite 2016. 26 June 2017. Draft Community Group Report \"This specification describes the Redaction Signature Suite created in 2016 for the Linked Data Signatures specification. It enables a sender to redact information in a message without invalidating the digital signature.\"","title":"Data minimization sd"},{"location":"RWoT5/data-minimization-sd/#introduction","text":"We often share information on the World Wide Web, though some of it is private. The W3C Credentials Community Group focuses on how privacy can be enhanced when attributes are shared electronically. In the course of our work, we have identified three related but distinct privacy enhancing strategies: \"data minimization,\" \"selective disclosure,\" and \"progressive trust.\" These enhancements are enabled with cryptography. The goal of this paper is to enable decision makers, particularly non-technical ones, to gain a nuanced grasp of these enhancements along with some idea of how their enablers work. We describe them below in plain English, but with some rigor. This knowledge will enable readers of this paper to be better able to know when they need privacy enhancements, to select the type of enhancement needed, to assess techniques that enable those enhancements, and to adopt the correct enhancement for the correct use case.","title":"Introduction"},{"location":"RWoT5/data-minimization-sd/#three-examples","text":"Three examples of how people would like their privacy preserved in the process of sharing credentials help to illuminate these three techniques. Diego attempts to use an online service and is asked to share his location in order to prove his geolocation. Diego hesitates, since the service doesn't need his location everyday, everywhere. He knows that the service may share this information with other parties without meaningful consent on his part. Thoughts pass through his mind: What location data does the service actually need? What will it read in future? Is there a way for him to share his location just this once, or to only share an approximate location? Selena hands her driver's license to a bouncer to prove she is of drinking age. As he looks it over, she sees him inspecting her date of birth and home address. He only needs to know that she is over 21. Is there a way to disclose that she is indeed old enough without revealing her actual age, along with her home address and city of residence as well? Proctor , negotiating with a real estate agent to purchase a home, reveals a letter from his bank stating his credit limit. He wanted to reveal its approximate amount only, but the agent insisted on verifying that the letter was authentic. Proctor feels the agent now has the upper hand in the negotiation, as the letter reveals more than just its authenticity. Could he have revealed only an approximate amount and reveal more details as the negotiations progress? Each story features information that is verifiable: a home address, age, or credit limit. We call such information a credential, and a detail of a credential we call an attribute. We have three strategies for enhancing the privacy of digitally shared credential attributes, and each story highlights one. Diego's story highlights the need for \"data minimization,\" Selena's for \"selective disclosure,\" and Proctor's for \"progressive trust.\" Let's examine each one in detail before discussing enablers.","title":"Three Examples"},{"location":"RWoT5/data-minimization-sd/#privacy-enhancements","text":"We propose the following three privacy enhancements. (Sources used to curate these definitions are listed in Appendix A.)","title":"Privacy Enhancements"},{"location":"RWoT5/data-minimization-sd/#data-minimization","text":"Data minimization is the act of limiting the amount of shared data strictly to the minimum necessary in order to successfully accomplish a task or goal. There are three types of minimization: * Content minimization \u2013 the amount of data should be strictly the minimum necessary. * Temporal minimization \u2013 the data should be stored by the receiver strictly for the minimum amount of time necessary to execute the task. * Scope minimization \u2013 the data should only be used for the strict purpose of the active task. Data minimization is enacted primarily by policy decisions made by stakeholders in the credentials ecosystem: * Credential issuers ensure that credentials may be presented in such a way as to enable data minimization. This may require issuing multiple, related, granular sub-credentials. * Credential inspectors establish in advance policies regarding the data they will examine: * what is the minimum data necessary to accomplish the task or goal? * what is the minimum time the data can be stored to execute the task? * what processes ensure that the data is applied only to the task at hand and does not, by a process of scope creep, become applied to other tasks or goals? Data minimization policies impact selective disclosure, the next privacy enhancement.","title":"Data Minimization"},{"location":"RWoT5/data-minimization-sd/#selective-disclosure","text":"Selective disclosure is the ability of an individual to granularly decide what information to share. Stakeholders in the credentials ecosystem enable selective disclosure capabilities in the following ways: * Credential issuers format the credential and its attributes in such a way as to enable selective disclosure. As with the strategy of data minimization, they may issue multiple, related, granular sub-credentials. Each attribute and the overall credential may be formatted to support cryptography, a capability described in more detail below. * Credential inspectors ensure the request is framed in such a way as to enable selective disclosure, using the cryptographic tools required. Once data minimization policies and selective disclosure are in place, the third and last enhancement can be applied.","title":"Selective disclosure"},{"location":"RWoT5/data-minimization-sd/#progressive-trust","text":"Progressive trust is the ability of an individual to gradually increase the amount of relevant data revealed as trust is built or value generated. To enable progressive trust capabilities, stakeholders in the credentials ecosystem act in the following ways: * Issuers format the credential(s) in such a way as to enable progressive trust. This may require issuing multiple, related, atomic sub-credentials. It also may require formatting the credential to support mathematical queries and cryptographic proofs. Finally, the issuer's data model may express how the various sub-credentials are related in a scenario involving progressive trust. * Inspectors ensure that requests are framed in such a way as to enable progressive trust. They structure the communication in order to to gradually escalate credential requests in order to enable a subject to progressively trust the inspector more and more, revealing the minimum data necessary to accomplish each step of the task or goal and revealing more and more as the mutual communication progresses.","title":"Progressive Trust"},{"location":"RWoT5/data-minimization-sd/#crypto-enablers","text":"Implementing privacy enhancements depends on organizational decisions. Determination of the data needed, with an eye towards data minimization, along with a clear model of how data is used over the lifecycle of engagement, goes a long way towards enabling progressive trust. However, policies are not enough. When enhancing privacy online, some data parts must be revealed while others remain concealed. Concealment is achieved mostly by the art of cryptography, from the greek word \"kryptos,\" meaning hidden, like in a crypt. Crypto (a short word we will use for cryptography) enables us to achieve our goal by means of three primary enablers: having a secret, having a difficult mathematical task, and having zero-knowledge enablers. The children's \"Where's Waldo?\" illustrated book series helps us to understand these three enablers. In these books a distinctively dressed man appears only once on each page, wearing a striped hat. Readers are asked to scour the page and locate him. We can understand the three enablers by examining Where's Waldo one step at a time. A Secret : For the new reader, Waldo's location is a secret. The illustrator knows it, and the reader doesn't. The reader is encouraged to search the page and find Waldo, but that is a difficult task. Some readers give up and ask someone who has already found Waldo to show them his location. In essence, they are asking another reader to reveal the secret. Once found, a reader could keep the information secret by circling Waldo in red and storing the book in a safe. This amounts to storing the secret for future use. Secrets are essential to crypto. They are usually called keys, and they must be managed carefully. A Difficult Task : Waldo is difficult to find on the page. The reader has to search everywhere and mistakenly identify many Waldo look-alike characters before reaching a satisfactory conclusion and finding him. Yet when he is finally discovered, or someone points Waldo out, it's easy to see where he is. That's why it's a fun task. This difference between the difficulty of conducting the task and the ease of verifying the task lies at the heart of cryptographic enablers. A Zero Knowledge Enabler : Can you prove you found Waldo without revealing the secret of his actual location on the page? There is a simple way to do so. Take a rectangular piece of white cardboard that is much larger than the book. Cut a hole exactly fitting Waldo to reveal his silhouette only, nothing else. You can now show Waldo to anyone, peeking out of the cardboard. Yet the cardboard is wide and opaque, hiding the book thoroughly, so a verifier has no idea where Waldo is on the page. The puzzle was solved and someone verified the achievement, without revealing any knowledge of how to solve the puzzle. The secret is still safe, the task still just as difficult as before. Where's Waldo books are drawings, while crypto is built from mathematical equations, basically puzzles based on numbers. We provide the interested reader with a layman's overview in Appendix B.","title":"Crypto Enablers"},{"location":"RWoT5/data-minimization-sd/#three-solutions","text":"We now return to our opening examples, apply the privacy preserving strategies and enablers described, and describe the improved outcomes. The online service that Diego uses does an internal policy review and realizes (a) it only needs a location when a user signs up for an account, and (b) it does not need an exact address, only the county district. It changes its interface to request a Verifiable Credential for Diego's location. Diego's system creates this credential for him, which can be inspected to reveal the county district. The crypto to enable this would be similar to that described in Appendix C. With this data minimization, the online service has less risk of violating data protection rules, is less a target for hacking, and has lower overall costs, while at the same time preserving Diego's privacy. The bar seeking to verify Selena 's age uses selective disclosure as built into the Verifiable Claims system. Selena will no longer share her date of birth. Instead, Selena creates a secret that we harness to craft a crypto-formatted credential. This crypto makes it easy to verify her age, but difficult to determine her exact date of birth. The bouncer's system can perform a zero-knowledge proof to determine the credential is valid and that Selena is older than twenty-one, without revealing her birthday or her secret. The bouncer sees she is over twenty-one without seeing her date of birth, residence address, or any other unnecessary information. In Appendix C we show the process step-by-step. The real estate agency working with Proctor implements a data model specifying what is required at each step of the real estate negotiation. The first step requires only proof of being an account holder in good standing at a known bank, so Proctor does not have to reveal the detailed letter at this point. As their negotiation continues, Proctor reveals more and more information as required. Some steps of the process may share Verifiable Claims encoded with crypto.","title":"Three Solutions"},{"location":"RWoT5/data-minimization-sd/#summary","text":"The World Wide Web accelerates the sharing of credentials and other digital interactions, and many regulations have been passed and strategies proposed to protect privacy, some of which require cryptography. To align terminology, the World Wide Web Credentials Community Group has found three related but distinct privacy enhancing strategies that create a useful rubric for discussing the challenges and arriving at solutions. We share the examples of Diego, Selena, and Proctor and propose \"data minimization,\" \"selective disclosure,\" and \"progressive trust,\" with accompanying crypto protocols as useful semantics for accelerating the adoption of digital interaction while protecting privacy.","title":"Summary"},{"location":"RWoT5/data-minimization-sd/#appendix-a-definition-sources","text":"This section contains definitions we curated, based on research and oral interviews, to create the definitions of data minimization, selective disclosure and progressive trust.","title":"Appendix A: Definition Sources"},{"location":"RWoT5/data-minimization-sd/#data-minimization_1","text":"Definitions of data minimization that we considered in the formation of our definition above. GDPR Rec.39; Art.5(1)(c) definition: \u201cThe personal data should be adequate, relevant and limited to what is necessary for the purposes for which they are processed. This requires, in particular, ensuring that the period for which the personal data are stored is limited to a strict minimum. Personal data should be processed only if the purpose of the processing could not reasonably be fulfilled by other means.\u201d Reducing your overall footprint of data outside of your control. Can be accomplished by using selective disclosure. Adequate, relevant and non-excessive. Reducing the amount of data you are sending in a payload to only the one ... needed. That prevents leakage of confidential information. Providing people with the information they need without revealing non necessary info. If I need to prove if I am old enough without revealing an actual birthdate. Best practices \u2013 your should deeply inspect your use case and come to a conclusion as to what is the minimum data you need to accomplish your goal. Don\u2019t be greedy. Data has proven to be toxic. Mathematical \u2013 finding a way to express the data that you wish as an equation related to the data you have. Minimizing the amount of data to achieve your goal or communicate what you need to. Designing systems to operate efficiently in order to maximize privacy. Choosing to only share the minimal amount of data about yourself or something during an interaction. Trying to keep the amount of info that is being disclosed as limited as possible to the requirements of the vulnerability. The minimum is what ... will lead them to move to action. Always happens in a context: a relationship where the two parties are considering interacting in some way. Sending only the signals that I want to send and that are needed by the other party, hem to interact with me in a particular The least amount of data needed for a system to function Collecting the least amount of date for the highest outcome. Also known as minimal disclosure, data minimization is the principle of using the least amount of data to accomplish a transaction. This is incumbent on all three parties in an exchange. The holder should attempt to share the minimum. The issuer needs to create attributes designed for composition and minimal use, as opposed to monolithic credentials with all the data. The verifier needs to ask only for what they need. The motivation to minimize data is that unneeded data is potentially \u201ctoxic.\u201d","title":"Data Minimization"},{"location":"RWoT5/data-minimization-sd/#selective-disclosure_1","text":"Definitions of selective disclosure that we considered in the formation of our definition above. Ability to decide what info you give and how it can be used. Smart disclosure, allows [selecting] what information to give based on logic. Blind search. You can decide who gets to see what. Means by which we achieve data minimization. Form of policies. Ability to mask attributes that you do not have to share. Relates to mathematical definition \u2013 the computational ability to reveal only parts of your data profile. Act of communicating or revealing only what you intend to, and not any peripheric data Having granular control over the ways in which data is shared Is a pattern for user interfaces allowing people to choose what to share about them during an interaction Method for achieving data minimization where only certain signals are being shared and there is control of who it is being shared with. That control is never perfect. The communication channel matters. An entity having granular control on what\u2019s revealed. The individual having the freedom to decide what to share, or the acquirer using data minimization approach requesting the minimum amount amount of data for the maximum impact","title":"Selective Disclosure"},{"location":"RWoT5/data-minimization-sd/#progressive-trust_1","text":"Definitions of progressive trust that we considered in the formation of our definition above. Note that we included definitions of progressive trust and progressive disclosure as well. Procedure for increasing revelation of relevant data as the communication proceeds. As we continue to communicate we decide to reveal more information. It becomes more generous as trust builds. Being able to reveal more data as you need to given certain conditions Information is disclosed as needed when needed. You can choose to increase the amount of data you disclose over time as needed. Taking as little vulnerability as possible at the beginning, then gaining information and becoming willing to take on additional vulnerability by revealing more information. Trust is built through step by step interactions where we start making ourselves vulnerable in a very small way and we observe how this works out. Based on results we consider making ourselves further vulnerable or not. It is about increasing levels of familiarity and prediction making (I am better able to predict your behavior). Releasing information as needed Escalation of the previous steps (data minimization, selective disclosure) in line with the value increasing. Purpose binding is the auditable use of data, so I can audit the use of my data and determine that it was used for the purposes declared. Progressive trust is the feeling of assurance and safety that develops over time, based on a history of data used only for its bound purposes, and so based on this feeling a data holder will be ready to share more data or other data, if at some point in the relationship this other data is requested. Trust is required when you depend on the actions of someone who you can't control.","title":"Progressive Trust"},{"location":"RWoT5/data-minimization-sd/#appendix-b-basic-crypto-concepts","text":"This appendix describes basic cryptographic concepts critical to the privacy preserving engineering of credential attributes. For readability, we use the short word, \"crypto.\"","title":"Appendix B: Basic Crypto Concepts"},{"location":"RWoT5/data-minimization-sd/#overview","text":"Crypto is a huge field with highly specialized jargon, too much to cover here. But non-specialists would benefit from some understanding of relevant crypto in order to make informed decisions. We begin with a brief overview of several concepts from number theory that serve as a foundation for all crypto used in this process. This is a curated list of topics progressing from the simple to the more complex. Notice how ideas are re-used and layered as you read on.","title":"Overview"},{"location":"RWoT5/data-minimization-sd/#number-theory","text":"Number theory refers to the study of the behavior of integer numbers such as one, three, or two hundred. The following are behaviors of these numbers that make them useful for crypto: Prime or not : Some numbers are only divisible by themselves and one. These are called 'prime.' One-way function : A numerical function that takes a publicly known number, and without any secret information, computes a value. Like a one-way street, the computation goes only one direction. Given the computed value, it is hard to find the publicly known number. Clock arithmetic , aka modular arithmetic: a system of arithmetic where numbers \"wrap around\" upon reaching a certain value. A familiar use is in the ordinary clock, in which our day is divided into twelve-hour periods. If the time is seven o'clock now, then ten hours later it will be five o'clock, though a military man might say seventeen hundred hours. Even he will say that ten hours later it is three o'clock, and not twenty-seven hundred hour, because his clock time \"wraps around\" every twenty-four hours (as opposed to twelve). Groups and Finite Fields : Some subsets of all integers form a \"group\" that behaves in ways very useful to the performance of cryptography. In extremely simplified terms, a group is a self-sufficient set of integers, where any possible manipulation returns an answer from within the group. Finite fields are types of groups that satisfy certain demanding properties. Notice how this resembles clock arithmetic, where the same numbers are used over and over again. Discrete Logarithms : A discrete logarithm is a property for numbers in a group. Since there is no efficient method for computing discrete logarithms, they form a \"difficult problem\" and so are very useful in cryptography. (The logarithm log(b) of a is an exponent x such that b^x ( b raised to the x exponent) = a .) Quadratic Residues : Quadratic refers to 'squared' numbers, a number raised to the second exponential power. Quadratic residues are a useful property of squared numbers as they behave in modular arithmetic.","title":"Number Theory"},{"location":"RWoT5/data-minimization-sd/#primary-objectives","text":"The curious behavior of numbers is exploited to achieve four primary crypto objectives. Confidentiality : a hidden part of a credential cannot be understood by anyone for whom it is unintended. Often called \"privacy,\" we avoid that word here since it can mean many things in addition to confidentiality. Authentication : the identity of information shared can be validated as authentic. Integrity : the revealed part of the credential cannot be altered without such alteration being detected. Also known as validity, fidelity or verifiability. Non-repudiation : aka non-deniability, a credential's creator cannot deny at a later stage his or her involvement.","title":"Primary Objectives"},{"location":"RWoT5/data-minimization-sd/#ten-crypto-concepts","text":"Over the decades hundreds if not thousands of crypto protocols, processes, algorithms and protocols have been innovated to achieve these objectives, by cobbling together the above six behaviors in different ways. We present here a brief tour of the ten most significant ones in our field of verifiable credentials: PKI or \"public and private keys\": a system that lies at the heart of most relevant crypto since a publicly shared digital asset can be locked to, or encumbered by, a private key that is kept secret. Only the person with knoweldge of that private key can, with the right software, unlock that asset. This enables a broad range of activities such as \"signature,\" \"authentication,\" and \"certificate validation.\" In general we call these activities PKI, a public key infrastructure. Signature : a signature in this context is a use of PKI. A valid signature gives a recipient \"authentication\", confidence that the message was created by a known sender; \"non-repudiation,\" that the sender cannot deny having sent the message; and \"integrity,\" that the message was not altered in transit. Signatures enable every cryptographic objective except for confidentiality. There are many types of signature schemes in use including Digital Signature Algorithm (DSA), Camenisch-Lysyanskaya (CL) signatures, and Boneh\u2013Lynn\u2013Shacham (BLS) signatures. Key exchange : exchange methods enable two parties that have no prior knowledge of each other to jointly establish a shared secret key over an insecure channel. This key can then be used to encrypt subsequent communications using a symmetric key cipher. Diffie\u2013Hellman is a common example. Elliptic-curve cryptography (ECC): an approach to PKI based on the numeric structure of elliptic curves over finite fields. ECC is useful as it requires smaller keys compared to non-ECC cryptography. There are many variants of ECC used including Edwards-curve Digital Signature Algorithm (EdDSA). Hash or message digest : one-way functions, such as SHA-256. A set of many one-way functions may be applied to a tree of data to form a Merkle Tree (or trie). Zero-Knowledge (ZK): zero knowledge is defined above loosely as a set of practices where some data is revealed while other parts are kept secret. Many ZK methods are used in cryptography incuding Fiat Shamir, Proof of knowledge of discrete logarithms, ZK Snarks, and ZK Starks. Accumulators : a form of ZK, a cryptographic accumulator is a one-way membership function that answers a query as to whether a potential candidate is a member of a set without revealing the individual members of the set. Similar to a one-way hash function, cryptographic accumulators generate a fixed-size digest representing an arbitrarily large set of values. Some further provide a fixed-size witness for any value of the set, which can be used together with the accumulated digest to verify its membership in the set. Commitment : a cryptographic commitment, which allows one to commit to a chosen value (or chosen statement) while keeping it hidden to others, with the ability to reveal the committed value later. Witness : a term that has different applications in cryptography. In this paper, a witness is a value used in a cryptographic accumulator. In Bitcoin the unlocking signature is called the \"witness data.\" Quantum Computing and Cryptography: as quantum computing is developed, it poses a threat to the difficulty of puzzles. For example, they are likely to be much faster at determining if a number is truly prime or not.","title":"Ten Crypto Concepts"},{"location":"RWoT5/data-minimization-sd/#appendix-c-drinking-age-credential-implementation","text":"The birthday of an individual is formatted into a verifiable credential, which can be inspected to reveal the age of the credential holder without revealing their birthdate. The flow described here is based on the developing Verifiable Claims standard of the W3C Credentials Community Group. It uses cryptography developed by Jan Camenisch, as implemented by Sovrin. This is a work in progress. Note that other types of crypto could be applied to achieve the same privacy preserving goals.","title":"Appendix C: Drinking Age Credential Implementation"},{"location":"RWoT5/data-minimization-sd/#communication-flow","text":"The flow below may be copies and pasted into the Web Sequence Diagram webpage to generate a flow diagram. title Verifiable credential using Selective Disclosure participant Valid Time Oracle participant Janet participant ID Provider participant Ledger participant Bar note over Janet:Prover note over Bar:Validator note over Janet,Bar: Preparation and Setup note right of ID Provider:Infrastructure ID Provider->Ledger: Define Schema (Name, Birthdate, Address) ID Provider->Ledger: credential Definition (Pub Key, etc.) ID Provider->ID Provider: Generate Prv Key for this credential ID Provider->Ledger:Revocation Registry note left of Bar: Prepare to accept credentials Bar->Bar:Install Agent Bar->Ledger: Check schema note over Janet,Bar: Begin Use Case Janet->ID Provider: Request ID ID Provider-->Janet: ID will be issued as a digital credential note right of Janet: Prepare to receive credentials Janet->Janet: Install Agent Janet->Janet: Prv Key Generate, Store Janet->Ledger:Check Schema Ledger->Janet:credential Definition Janet-->ID Provider:Proof of Name, Birthdate, Address Janet->ID Provider: Blinded secret ID Provider->Janet: credential Janet->Janet: Validate credential against credential Def note over Janet,Bar: Janet goes to the bar note left of Bar: Can Janet Enter? Bar->Janet: Request Proof of Age Janet->Valid Time Oracle: Get time Valid Time Oracle->Janet: Time credential Janet->Janet:Generate Proof (This person is over 21) Janet->Bar: Provide Proof Bar->Bar: Evaluate proof Bar->Ledger: Verify on Ledger Ledger->Bar: Verification Bar->Janet: Come in note left of Bar: Invite to club Bar->Janet: Join loyalty club? (requires valid postal code) Janet->Janet:Generate Proof (postal code) Janet->Bar: Provide Proof Bar->Bar: Evaluate proof Bar->Ledger: Verify on Ledger Ledger->Bar: Verification Bar->Janet: Have Loyalty Card","title":"Communication Flow"},{"location":"RWoT5/data-minimization-sd/#crypto-details","text":"Below are some of the detailed mathematics involved in issuing a verifiable credential as implemented by Sovrin, a non-profit organization dedicated to managing a decentralized, public network for the purposes of self-sovereign identity.","title":"Crypto Details"},{"location":"RWoT5/data-minimization-sd/#issuer-setup","text":"The following setup is a necessary precursor to issuing a privacy-preserving credential.","title":"Issuer Setup"},{"location":"RWoT5/data-minimization-sd/#compute","text":"Perform the mathematical calculations required to curate the essential ingredients of the operations we are about to perform. Some of these results, like the private keys, are very sensitive and must be kept secret by the credential holder; others are to be shared. Random \ud835\udcf9', \ud835\udcfa', 1024-bit prime numbers, such that \ud835\udcf9 = 2\ud835\udcf9' + 1 and \ud835\udcfa = 2\ud835\udcfa' + 1 are both 1024-bit prime numbers. \ud835\udcf7 = \ud835\udcf9\ud835\udcfa. Random quadratic residue: \ud835\udce2 mod \ud835\udcf7 Random \ud835\udce7 \ud835\udce9 , \ud835\udce7 \ud835\udce11 , . . . , \ud835\udce7 \ud835\udce1\ud835\udcf5 \u2208 [2: \ud835\udcf9'\ud835\udcfa' - 1], where \ud835\udcf5 is the number of attributes in the credential. \ud835\udce9 = \ud835\udce2 \ud835\udce7\ud835\udce9 mod \ud835\udcf7 \ud835\udce1 \ud835\udcf2 = \ud835\udce2 \ud835\udce7\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 Issuer private key \ud835\udcfc\ud835\udcf4 \ud835\udcec = \ud835\udcf9'\ud835\udcfa' Issuer public key \ud835\udcf9\ud835\udcf4 \ud835\udcec = {\ud835\udcf7, \ud835\udce2, \ud835\udce9, \ud835\udce1 1 , . . . , \ud835\udce1 \ud835\udcf5 }","title":"Compute"},{"location":"RWoT5/data-minimization-sd/#proof-of-correctness","text":"As a result of the above computations, we then curate the following. This proof, along with the public keys, is the computational algorithm that will be used to validate the credential. Random \ud835\udce7' \ud835\udce9 , \ud835\udce7' \ud835\udce11 , . . . , \ud835\udce7' \ud835\udce1\ud835\udcf5 \u2208 [2: \ud835\udcf9'\ud835\udcfa' - 1] \ud835\udce9' = \ud835\udce2 \ud835\udce7'\ud835\udce9 mod \ud835\udcf7 \ud835\udce1' \ud835\udcf2 = \ud835\udce2 \ud835\udce7'\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 \ud835\udcec = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce9 || \ud835\udce1 1 || . . . || \ud835\udce1 \ud835\udcf5 || \ud835\udce9' || \ud835\udce1' 1 || . . . || \ud835\udce1' \ud835\udcf5 ) \ud835\udce7'' \ud835\udce9 = \ud835\udce7' \ud835\udce9 + \ud835\udcec \ud835\udce7 \ud835\udce9 \ud835\udce7'' \ud835\udce1\ud835\udcf2 = \ud835\udce7' \ud835\udce1\ud835\udcf2 + \ud835\udcec \ud835\udce7 \ud835\udce1\ud835\udcf2 , 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 The Cred Def is comprised of the public key and the proof of correctness; this is published to the distributed ledger.","title":"Proof of Correctness"},{"location":"RWoT5/data-minimization-sd/#issuing-a-credential","text":"With setup complete, we can now issue the credential in a privacy-preserving manner.","title":"Issuing a Credential"},{"location":"RWoT5/data-minimization-sd/#for-each-credential","text":"For each credential issued, perform the following operations.","title":"For Each Credential"},{"location":"RWoT5/data-minimization-sd/#issuer-computes","text":"A cryptographic accumulator is constructed in order to enable zero-knowledge queries further on. It is a one-way membership function, including the claim in the membership set. The operation can then answers a query as to whether a potential candidate is a member of a set without revealing the individual members of the set. \ud835\udcd0 \ud835\udcf2 = accumulator index \ud835\udce4 \ud835\udcf2 = user index \ud835\udcf6 2 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udcd0 \ud835\udcf2 || \ud835\udce4 \ud835\udcf2 ) 256-bit integer representations of each of the attributes: \ud835\udcf6 3 , . . . , \ud835\udcf6 \ud835\udcf5 \ud835\udcf7 0 = nonce","title":"Issuer Computes"},{"location":"RWoT5/data-minimization-sd/#issuer-sends-n0-to-prover","text":"This nonce is provided to the Prover for calculation of the Prover's proof of correctness.","title":"Issuer Sends \ud835\udcf70 to Prover"},{"location":"RWoT5/data-minimization-sd/#prover-receives-n0-and-computes-the-following","text":"The prover aggregates and prepares public keys for use in validating the signatures. The prover also commits to a chosen value while keeping it temporarily hidden, making the calculation binding. Retrieves Issuer\u2019s public key \ud835\udcf9\ud835\udcf4 \ud835\udcec Retrieves Issuer\u2019s proof of correctness Generates: \ud835\udcf6 1 = pedersen commitment of claim link secret Random \ud835\udcff', \ud835\udcff'', \ud835\udcf6' 1 \ud835\udcf7 1 = nonce","title":"Prover Receives \ud835\udcf70 and Computes the Following"},{"location":"RWoT5/data-minimization-sd/#prover-verifies-the-issuers-proof-of-correctness","text":"\ud835\udce9^ = \ud835\udce9 \ud835\udcec \ud835\udce2 \ud835\udce7''\ud835\udce9 mod \ud835\udcf7 \ud835\udce1^ \ud835\udcf2 = \ud835\udce1 \ud835\udcf2 \ud835\udcec \ud835\udce2 \ud835\udce7''\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 Verifies \ud835\udcec = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce9 || \ud835\udce1 1 || . . . || \ud835\udce1 \ud835\udcf5 || \ud835\udce9^ || \ud835\udce1^ 1 || . . . || \ud835\udce1^ \ud835\udcf5 )","title":"Prover Verifies the Issuer\u2019s Proof of Correctness"},{"location":"RWoT5/data-minimization-sd/#prover-computes","text":"\ud835\udce4 = \ud835\udce2 \ud835\udcff\u2019 \ud835\udce1 1 \ud835\udcf61 mod \ud835\udcf7 \ud835\udce4\u2019 = \ud835\udce2 \ud835\udcff\u2019\u2019 \ud835\udce1 1 \ud835\udcf6\u20191 mod \ud835\udcf7 \ud835\udcec\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce4 || \ud835\udce4\u2019 || \ud835\udcf7 0 ) \ud835\udcff^ = \ud835\udcff\u2019\u2019 + \ud835\udcec\u2019\ud835\udcff\u2019 \ud835\udcf6^ 1 = \ud835\udcf6\u2019 1 + \ud835\udcec\u2019\ud835\udcf6 1","title":"Prover Computes"},{"location":"RWoT5/data-minimization-sd/#prover-sends-p-u-c-v-m1-n1-to-the-issuer","text":"","title":"Prover Sends \ud835\udcdf = { \ud835\udce4, \ud835\udcec\u2019, \ud835\udcff^, \ud835\udcf6^1, \ud835\udcf71 } to the Issuer"},{"location":"RWoT5/data-minimization-sd/#issuer-verifies-prover-setup","text":"Computes \ud835\udce4^ = \ud835\udce4 -\ud835\udcec \ud835\udce2 \ud835\udcff^ \ud835\udce1 1 \ud835\udcf6^ 1 mod \ud835\udcf7 Verifies \ud835\udcec\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce4 || \ud835\udce4^ || \ud835\udcf7 0 )","title":"Issuer Verifies Prover Setup"},{"location":"RWoT5/data-minimization-sd/#issuer-signs-the-credential-by-computing-the-following","text":"\ud835\udce0 = \ud835\udce9 / (\ud835\udce4\ud835\udce2 \ud835\udcff* \ud835\udce1 2 \ud835\udcf62 \ud835\udce1 3 \ud835\udcf63 \u00b7\u00b7\u00b7 \ud835\udce1 \ud835\udcf5 \ud835\udcf6\ud835\udcf5 ) mod \ud835\udcf7 \ud835\udced = \ud835\udcee -1 mod \ud835\udcf9\u2019\ud835\udcfa\u2019 \ud835\udcd0 = \ud835\udce0 \ud835\udced mod \ud835\udcf7 \ud835\udcd0\u2019 = \ud835\udce0 \ud835\udcfb mod \ud835\udcf7 \ud835\udcec\u2019\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 (\ud835\udce0 || \ud835\udcd0 || \ud835\udcd0\u2019|| \ud835\udcf7 1 ) \ud835\udcfc \ud835\udcee = (\ud835\udcfb - \ud835\udcec\u2019\u2019\ud835\udcee -1 ) mod \ud835\udcf9\u2019\ud835\udcfa\u2019","title":"Issuer Signs the Credential by Computing the Following"},{"location":"RWoT5/data-minimization-sd/#issuer-sends-o-a-e-v-se-c-m2-ml-to-the-prover","text":"","title":"Issuer Sends \ud835\udcde = {\ud835\udcd0, \ud835\udcee, \ud835\udcff*, \ud835\udcfc\ud835\udcee, \ud835\udcec\u2019\u2019, \ud835\udcf62, . . . , \ud835\udcf6\ud835\udcf5 } to the Prover"},{"location":"RWoT5/data-minimization-sd/#prover-receives-o-and-does-the-following","text":"","title":"Prover Receives \ud835\udcde and Does the Following"},{"location":"RWoT5/data-minimization-sd/#prover-computes_1","text":"\ud835\udcff = \ud835\udcff\u2019 + \ud835\udcff* \ud835\udce0\u2019 = \ud835\udce9 / (\ud835\udce2 \ud835\udcff \ud835\udce1 2 \ud835\udcf62 \ud835\udce1 3 \ud835\udcf63 \u00b7\u00b7\u00b7 \ud835\udce1 \ud835\udcf5 \ud835\udcf6\ud835\udcf5 ) mod \ud835\udcf7 \ud835\udced\u2019 = \ud835\udcec\u2019\u2019 + \ud835\udcfc \ud835\udcee \ud835\udcee \ud835\udcd0^ = \ud835\udcd0 \ud835\udced\u2019 \ud835\udce2 \ud835\udcff\u2019\ud835\udcfc\ud835\udcee mod \ud835\udcf7","title":"Prover Computes"},{"location":"RWoT5/data-minimization-sd/#prover-verifies","text":"\ud835\udcee is prime and 2 596 \u2264 \ud835\udcee \u2264 2 596 + 2 119 \ud835\udce0\u2019 = \ud835\udcd0 \ud835\udcee mod \ud835\udcf7 \ud835\udcec\u2019\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 (\ud835\udce0\u2019 || \ud835\udcd0 || \ud835\udcd0^ || \ud835\udcf7 1 )","title":"Prover Verifies"},{"location":"RWoT5/data-minimization-sd/#prover-stores-primary-claim-m1-ml-a-e-v","text":"","title":"Prover Stores Primary Claim ({\ud835\udcf61, . . . , \ud835\udcf6\ud835\udcf5}, \ud835\udcd0, \ud835\udcee, \ud835\udcff)"},{"location":"RWoT5/data-minimization-sd/#for-additional-information","text":"The crypto used here is originally from the Identity Mixer. Link: (https://www.zurich.ibm.com/identity_mixer/) The Sovrin team shares additional information and working code at the following links. * Verifiable Credentials Code * Verifiable Credentials Example Usage in Python","title":"For Additional Information"},{"location":"RWoT5/data-minimization-sd/#references","text":"W3C Credentials Community Group Home Page Data Minimization and Selective Disclosure Repo Camenisch, Lysyanskaya. An Efficient System for Non-transferable Anonymous Credentials with Optional Anonymity Revocation Pfitzmann, Hansen. 2010. A terminology for talking about privacy by data minimization: Anonymity, Unlinkability, Undetectability, Unobservability, Pseudonymity, and Identity Management Cooper, Tschofenig, Aboba, Peterson, Morris, Hansen, Smith, Janet. 2013. RFC6973 . The draft can also be helpful, \"This document focuses on introducing terms used to describe privacy properties that support data minimization.\" Hansen, Tschofenig, Smith, Cooper. 2012 Privacy Terminology and Concepts. Network Working Group Internet-Draft Expires: September 13, 2012 Longley, Sporny. Redaction Signature Suite 2016. 26 June 2017. Draft Community Group Report \"This specification describes the Redaction Signature Suite created in 2016 for the Linked Data Signatures specification. It enables a sender to redact information in a message without invalidating the digital signature.\"","title":"References:"},{"location":"RWoT5/did-method-veres-one/","text":"[![W3C](https://www.w3.org/StyleSheets/TR/2016/logos/W3C)](https://www.w3.org/) Veres One DID Method 1.0 ======================== A decentralized identifier method for the Veres One Ledger ---------------------------------------------------------- Draft Community Group Report 01 October 2017 -------------------------------------------- Latest editor's draft: : Editors: : [Manu Sporny](http://manu.sporny.org/), [Digital Bazaar](https://digitalbazaar.com/) : [Dave Longley](https://github.com/dlongley), [Digital Bazaar](https://digitalbazaar.com/) Authors: : [Manu Sporny](http://manu.sporny.org/), [Digital Bazaar](https://digitalbazaar.com/) : [Dave Longley](https://github.com/dlongley), [Digital Bazaar](https://digitalbazaar.com/) Participate: : [GitHub w3c-ccg/didm-veres-one](https://github.com/w3c-ccg/didm-veres-one) : [File a bug](https://github.com/w3c-ccg/didm-veres-one/issues/) : [Commit history](https://github.com/w3c-ccg/didm-veres-one/commits/gh-pages) [Copyright](https://www.w3.org/Consortium/Legal/ipr-notice#Copyright) \u00a9 2017 the Contributors to the Veres One DID Method 1.0 Specification, published by the [Credentials Community Group](https://www.w3.org/community/credentials/) under the [W3C Community Contributor License Agreement (CLA)](https://www.w3.org/community/about/agreements/cla/). A human-readable [summary](https://www.w3.org/community/about/agreements/cla-deed/) is available. ------------------------------------------------------------------------ Abstract -------- The Veres One Ledger is a permissionless public ledger designed specifically for the creation and management of [decentralized identifiers](https://w3c-ccg.github.io/did-spec/) (DIDs). Veres One DIDs are [self-sovereign identifiers](https://www.coindesk.com/path-self-sovereign-identity/) that may be used by people, organizations, and digital devices to establish an identifier that is under their control. Veres One DIDs are useful in ecosystems where one needs to issue, store, and use [Verifiable Claims](https://w3c.github.io/vc-data-model/). This specification defines how a developer may create and update DIDs in the Veres One Ledger. Status of This Document ----------------------- This specification was published by the [Credentials Community Group](https://www.w3.org/community/credentials/). It is not a W3C Standard nor is it on the W3C Standards Track. Please note that under the [W3C Community Contributor License Agreement (CLA)](https://www.w3.org/community/about/agreements/cla/) there is a limited opt-out and other conditions apply. Learn more about [W3C Community and Business Groups](https://www.w3.org/community/). Comments regarding this document are welcome. Please file issues directly on [GitHub](https://github.com/w3c-ccg/didm-veres-one/issues/), or send them to ([subscribe](mailto:public-credentials-request@w3.org?subject=subscribe), [archives](https://lists.w3.org/Archives/Public/public-credentials/)). Work on this specification has been funded in part by the United States Department of Homeland Security's Science and Technology Directorate under contract HSHQDC-17-C-00019. The content of this specification does not necessarily reflect the position or the policy of the U.S. Government and no official endorsement should be inferred. Work on this specification has also been supported by the Rebooting the Web of Trust group facilitated by Christopher Allen, Shannon Appelcline, Kiara Robles, Kaliya Young, Brian Weller, and Betty Dhamers. If you wish to make comments regarding this document, please send them to ([subscribe](mailto:public-credentials-request@w3.org?subject=subscribe), [archives](https://lists.w3.org/Archives/Public/public-credentials/)). Table of Contents 1. Introduction 2. Core Data Model 3. Basic Concepts 3.1 Authentication 3.2 Authorization 3.3 Service Descriptions 4. Operations 4.1 Discovering Service Endpoints 4.2 Creating a DID 4.3 Updating a DID Document 4.4 Delegating Control 4.5 Key Rotation and Transferring Control 4.6 Recovering a DID 5. Appendix A: Examples 5.1 Typical DID Document 5.2 Legacy DID Document 1. Introduction ------------------------------------------ Issue 1 TBD: This section will provide a gentle introduction to the purpose of the Veres One Ledger, expanding upon the abstract of the document. 2. Core Data Model --------------------------------------------- Issue 2 TBD: This section will describe the use of the Web Ledger, JSON-LD, and the DID spec to build the Veres One Ledger. 3. Basic Concepts -------------------------------------------- ### 3.1 Authentication Authentication is the process the ledger uses to determine if an entity is associated with a DID. Example 1 : Expressing authentication credentials { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"authenticationCredential\": [... array of acceptable authentication credentials ...] } A detailed example of a valid set of authentication credentials follows: Example 2 : Detailed example of authentication credentials entry { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"authenticationCredential\": [{ \"type\": \"RsaSignature2017\", \"publicKey\": { \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938/keys/2\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\", } }] } ### 3.2 Authorization Authorization is the process the ledger uses to determine what an entity may to do the DID Document. Example 3 ``` {.nohighlight title=\"\"} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"authorizationCapability\": [... array of capability descriptions ...] } </div> A detailed example of a valid set of authorization capability descriptions follows: <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 4</span> </div> ``` {.nohighlight title=\"\"} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", // proof of update authorization may be provided by digital wallet + friend OR // by mobile phone \"authorizationCapability\": [{ // this entity may update any field in this DID Document using any // authentication mechanism understood by the ledger \"permission\": \"UpdateDidDocument\", \"entity\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\" }, { // this entity may update the authenticationCredential field in this // DID Document as long as they authenticate with RsaSignature2017 \"entity\": \"did:v1:b5f8c320-f7ca-4869-85e6-a1bcbf825b2a\", \"permission\": \"UpdateDidDocument\", \"field\": [\"authenticationCredential\"], \"permittedProofType\": [{ \"proofType\": \"RsaSignature2017\" }] }, { // anyone may update the authenticationCredential and writeAuthorization // fields as long as they provide a specific multi-signature proof \"permission\": \"UpdateDidDocument\", \"field\": [\"authenticationCredential\", \"authorizationCapability\"], \"permittedProofType\": [{ \"proofType\": \"RsaSignature2017\", \"minimumSignatures\": 3, \"authenticationCredential\": [{ \"id\": \"did:v1:304ebc3e-7997-4bf4-a915-dd87e8455941/keys/123\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:v1:304ebc3e-7997-4bf4-a915-dd87e8455941\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:v1:0f22346a-a360-4f3e-9b42-3366e348e941/keys/foo\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:v1:0f22346a-a360-4f3e-9b42-3366e348e941\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:v1:a8d00377-e9f1-44df-a1b9-55072e13262a/keys/abc\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:v1:a8d00377-e9f1-44df-a1b9-55072e13262a\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }] }] } ### 3.3 Service Descriptions Services may be listed by including them at the top-level of the DID Document. Example 5 : Simple example of a service description { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"credentialRepositoryService\": \"https://wallet.veres.io/\" } A detailed example of the expression of a service description follows: Example 6 ``` {.nohighlight title=\"\"} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"credentialRepositoryService\": [{ // the verifiable credential repository service \"id\": \"did:v1:5d6c3b20-56a9-42e1-bfc8-ed7e685c9039\", \"type\": \"VerifiableCredentialRepository\", \"url\": \"https://wallet.veres.io/\", \"description\": \"Pat Doe's Digital Wallet\" }] } </div> </div> </div> <div id=\"operations\" class=\"section\"> <span class=\"secno\">4. </span>Operations ---------------------------------------- Every conforming Veres Ledger node *MUST* expose at least the following HTTP endpoints: Service Example URL Description ----------------------- ------------------ ---------------------------------- veresOneCreateService POST /dids Create a new DID. veresOneReadService GET /dids/{did} Gets an existing DID Document. veresOneUpdateService POST /dids/{did} Update an existing DID Document. <div id=\"discovering-service-endpoints\" class=\"section\"> ### <span class=\"secno\">4.1 </span>Discovering Service Endpoints {#x4.1-discovering-service-endpoints} A website may provide service endpoint discovery by embedding JSON-LD in their top-most HTML web page (e.g. at `https://example.com/`): <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 7</span><span style=\"text-transform: none\">: Example of HTML-based service description</span> </div> ``` {.highlight .hljs .xml aria-busy=\"false\" aria-live=\"polite\"} <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <title>Example Website</title> <link rel=\"stylesheet\" href=\"style.css\"> <script src=\"script.js\"></script> <script type=\"application/ld+json\"> { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"https://example.com/\", \"name\": \"Example Website\", \"veresOneCreateService\": \"https://example.com/veres-one/dids\", \"veresOneReadService\": \"https://example.com/veres-one/dids/\", \"veresOneUpdateService\": \"https://example.com/veres-one/dids/\" } </script> </head> <body> <!-- page content --> </body> </html> Service descriptions may also be requested via content negotiation. In the following example a JSON-compatible service description is provided (e.g. `curl -H \"Accept: application/json\" https://example.com/`): Example 8 : Example of a JSON-based service description ``` {.highlight .hljs .json aria-busy=\"false\" aria-live=\"polite\"} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"https://example.com/\", \"name\": \"Example Website\", \"veresOneCreateService\": \"https://example.com/veres-one/dids\", \"veresOneReadService\": \"https://example.com/veres-one/dids/\", \"veresOneUpdateService\": \"https://example.com/veres-one/dids/\" } </div> </div> <div id=\"creating-a-did\" class=\"section\"> ### <span class=\"secno\">4.2 </span>Creating a DID A DID is created by performing an HTTP POST of a signed DID Document to the `veresOneCreateService`. The following HTTP status codes are defined for this service: HTTP Status Description ------------- -------------------------------------------------------------------------------------------------------------------------- 201 DID creation request was successful. The HTTP `Location` header will contain the URL for the newly created DID Document. 400 DID creation request failed. 409 A duplicate DID exists. An example exchange of DID creation request is shown below: <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 9</span><span style=\"text-transform: none\">: DID creation request</span> </div> ``` {.highlight .hljs .http aria-busy=\"false\" aria-live=\"polite\"} POST /dids HTTP/1.1 Host: example.com Content-Type: application/ld+json Content-Length: 1062 Accept: application/ld+json, application/json, text/plain, */* Accept-Encoding: gzip, deflate { \"@context\": \"https://w3id.org/webledger/v1\", \"type\": \"WebLedgerEvent\", \"operation\": \"Create\", \"input\": [{ \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"authorizationCapability\": [{ \"permission\": \"UpdateDidDocument\", \"entity\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"permittedProofType\": [{ \"proofType\": \"LinkedDataSignature2015\" }, { \"proofType\": \"EquihashProof2017\", \"equihashParameterAlgorithm\": \"VeresOne2017\" }] }], \"authenticationCredential\": [{ \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmbDqPu6IKHiiIQ4d0AQ6\\r\\n PBduDhUUVqyQirvxqsdcNdKgZ2L8whBml/nTyuB4cd+hHrsfMDiHiT5kX2pbZ7Yy\\r\\n 2ctWkGw8e0J94CbwVh2H15gBQBUCjLiGvVIHO2pni693qmre+3Ya2NJ8gGwPLJ7h\\r\\n TLca2b2dX0y16qu0MT0osUGGEoPsdg6ibD2pxnADS3GNPObHT12GrAuxjYFMHecF\\r\\n A4hLZ8U+MIcVmHZuokqqbcyJyjOV+kmhFNeTKFP5P5U8HA3Y42/rE1UJp/wyy7Lc\\r\\n ZAvq0t75ddXKyvYh5dkzxxeeELNKNWVxJ2yvgAr0SatLEPzxJoeYdCyU5N5E22Fj\\r\\n jQIDAQAB\\r\\n-----END PUBLIC KEY-----\\r\\n\" }] }], \"signature\": [{ \"type\": \"EquihashProof2017\", \"equihashParameterN\": 64, \"equihashParameterK\": 3, \"nonce\": \"AQAAAA==\", \"proofValue\": \"AAAaPwABxrIAAFOKAAGo4QAAVW0AAN7cAACXcgABjEI=\" }, { \"type\": \"LinkedDataSignature2015\", \"created\": \"2017-09-30T02:54:31Z\", \"creator\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"signatureValue\": \"SNMbsPqLnB+hJFhXzS6hcpZnm9cGvSZZg7o26UYnyGYTvKder/S+Xk hNhXisS5385Ljlf5CXTQT5j6qYZtP8ut1Benaae8TMH17txP0CfzHbUMJFnHA1+Nru+e/Pw yPwuQ+VZYlXOB7g/tKVVZsxAYTKCAOJvJMIE+nlHjpB+RsKs9z4ZzVtddntqqAcvbZxV/o7 azBFDizeJu/gHVVMncCJ00SRoOzCOZUABRJV/k68bNSAfpELkrdWx8/xvMIF8r+LWhwdKCS iOw4DjSwIK40yD5rOvQn/GlC+unyB8zFe60jCToz/UOJNZBiIYwo+Pwwx28Wqd4Jkb3IeDr /L2Q==\" }] } If the creation of the DID was successful, an HTTP 201 status code is expected in return: Example 10 : Successful DID creation response ``` {.highlight .hljs .http aria-busy=\"false\" aria-live=\"polite\"} HTTP/1.1 201 Created Location: https://ledger.example.com/dids/did:v1:215cb1dc-1f44-4695-a07f-97649cad9938 Cache-Control: no-cache, no-store, must-revalidate Pragma: no-cache Expires: 0 Date: Fri, 14 Oct 2016 18:35:33 GMT Connection: keep-alive Transfer-Encoding: chunked </div> </div> <div id=\"updating-a-did-document\" class=\"section\"> ### <span class=\"secno\">4.3 </span>Updating a DID Document A DID is updated by performing an HTTP POST of a signed DID Document to the `veresOneUpdateService`. The following HTTP status codes are defined for this service: HTTP Status Description ------------- ------------------------------------ 200 DID update request was successful. 400 DID update request failed. An example exchange for a DID update request is shown below: <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 11</span><span style=\"text-transform: none\">: DID Document update request</span> </div> ``` {.highlight .hljs .http aria-busy=\"false\" aria-live=\"polite\"} POST /dids/did:v1:215cb1dc-1f44-4695-a07f-97649cad9938 HTTP/1.1 Host: example.com Content-Type: application/ld+json Content-Length: 1062 Accept: application/ld+json, application/json, text/plain, */* Accept-Encoding: gzip, deflate { \"@context\": \"https://w3id.org/webledger/v1\", \"type\": \"WebLedgerEvent\", \"operation\": \"Update\", \"input\": [{ \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"authorizationCapability\": [{ \"permission\": \"UpdateDidDocument\", \"entity\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"permittedProofType\": [{ \"proofType\": \"LinkedDataSignature2015\" }, { \"proofType\": \"EquihashProof2017\", \"equihashParameterAlgorithm\": \"VeresOne2017\" }] }], \"authenticationCredential\": [{ \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmbDqPu6IKHiiIQ4d0AQ6\\r\\n PBduDhUUVqyQirvxqsdcNdKgZ2L8whBml/nTyuB4cd+hHrsfMDiHiT5kX2pbZ7Yy\\r\\n 2ctWkGw8e0J94CbwVh2H15gBQBUCjLiGvVIHO2pni693qmre+3Ya2NJ8gGwPLJ7h\\r\\n TLca2b2dX0y16qu0MT0osUGGEoPsdg6ibD2pxnADS3GNPObHT12GrAuxjYFMHecF\\r\\n A4hLZ8U+MIcVmHZuokqqbcyJyjOV+kmhFNeTKFP5P5U8HA3Y42/rE1UJp/wyy7Lc\\r\\n ZAvq0t75ddXKyvYh5dkzxxeeELNKNWVxJ2yvgAr0SatLEPzxJoeYdCyU5N5E22Fj\\r\\n jQIDAQAB\\r\\n-----END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/2\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIj0BAQEFAKHiiIQ4d0AQ6ANBgkqhkiG9wAOCAQ8AMIIBCgKCAQEAmbDqPu6I\\r\\n xqsdcNdKgZ2L8whBml/nTyuBiHiPBduDhUUVqyQirvT5kX2pbZ7Yy4cd+hHrsfMD\\r\\n VhVIHO2pni693qmre+2ctWkGw8e0J94Cbw3Ya2NJ8gGwPLJ7hH15gBQBUCjLiGv\\r\\n 0osUNPObHT12GrAuxjYFMHecFGTLca2b2dX0y16qu0MTGEoPsdg6ibD2pxnADS3G\\r\\n kqqbcyJyjOV+kmh8HA3Y42/rE1UJpA4hLZ8U+MIcVmHZuo/wyy7LcFNeTKFP5P5U\\r\\n 5dkzxxezxJoeYdCyU5N5E2ZAvq0t75ddXKyvYh2FjeELNKNWVxJ2yvgAr0SatLEP\\r\\n AQABjQID\\r\\n-----END PUBLIC KEY-----\\r\\n\" }] }], \"signature\": [{ \"type\": \"EquihashProof2017\", \"equihashParameterN\": 64, \"equihashParameterK\": 3, \"nonce\": \"AQAAAA==\", \"proofValue\": \"AAAaPwABxrIAAFOKAAGo4QAAVW0AAN7cAACXcgABjEI=\" }, { \"type\": \"LinkedDataSignature2015\", \"created\": \"2017-09-30T02:54:31Z\", \"creator\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/2\", \"signatureValue\": \"Zg7o26UYnyGYTvKdSN6hcpZnm9cGvSZB+hJFhXzSer/S+XkMbsPqLn VMncCJ00SRoOzCOZUABRJV/azBFDizeJu/gHVk68bNSAfpELkrdWx8/xvMIF8r+LWhwdKCS XTQT5j6qYZtP8ut1BenahNhXisS5385Ljlf5Cae8TMH17txP0CfzHbUMJFnHA1+Nru+e/Pw K40yD5rOvQn/GlC+unyB8ziOw4DjSwIFe60jCToz/UOJNZBiIYwo+Pwwx28Wqd4Jkb3IeDr VVZsxAYTKCAOJvJMIE+nlHjpB+RyPwuQ+VZYlXOB7g/tKsKs9z4ZzVtddntqqAcvbZxV/o7 /45H==\" }] } If the update request for the DID was successful, an HTTP 200 status code is expected in return: Example 12 : Successful ledger creation response ``` {.highlight .hljs .http aria-busy=\"false\" aria-live=\"polite\"} HTTP/1.1 200 Success Cache-Control: no-cache, no-store, must-revalidate Pragma: no-cache Expires: 0 Date: Fri, 14 Oct 2016 18:35:33 GMT Connection: keep-alive Transfer-Encoding: chunked </div> </div> <div id=\"delegating-control\" class=\"section\"> ### <span class=\"secno\">4.4 </span>Delegating Control <div id=\"issue-3\" class=\"issue\"> <div id=\"h-issue3\" class=\"issue-title marker\" aria-level=\"4\" role=\"heading\"> <span>Issue 3</span> </div> TBD: Explain that delegation of control is merely placing a digital wallet provider in the proofOfControl field. </div> </div> <div id=\"key-rotation-and-transferring-control\" class=\"section\"> ### <span class=\"secno\">4.5 </span>Key Rotation and Transferring Control <div id=\"issue-4\" class=\"issue\"> <div id=\"h-issue4\" class=\"issue-title marker\" aria-level=\"4\" role=\"heading\"> <span>Issue 4</span> </div> TBD: Explain that transferring control and rotating keys is a matter of adding and removing the appropriate keys from proofofControl and proofOfUpdateAuthorization. </div> </div> <div id=\"recovering-a-did\" class=\"section\"> ### <span class=\"secno\">4.6 </span>Recovering a DID <div id=\"issue-5\" class=\"issue\"> <div id=\"h-issue5\" class=\"issue-title marker\" aria-level=\"4\" role=\"heading\"> <span>Issue 5</span> </div> TBD: Explain that recoverying a DID is a matter of meeting the requirements under proofOfUpdateAuthorization. </div> </div> </div> <div id=\"appendix-a:-examples\" class=\"section\"> <span class=\"secno\">5. </span>Appendix A: Examples -------------------------------------------------- <div id=\"typical-did-document\" class=\"section\"> ### <span class=\"secno\">5.1 </span>Typical DID Document The following is a complete example of a typical Veres One DID Document: <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 13</span> </div> ``` {.nohighlight} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:eaaf4df5-471d-404e-b143-652fe38cd2c7\", \"authorizationCapability\": [{ \"permission\": \"UpdateDidDocument\", \"entity\": \"did:v1:eaaf4df5-471d-404e-b143-652fe38cd2c7\", \"permittedProofType\": [{ \"proofType\": \"LinkedDataSignature2015\" }, { \"proofType\": \"EquihashProof2017\", \"equihashParameterAlgorithm\": \"VeresOne2017\" }] }], \"authenticationCredential\": [{ \"id\": \"did:v1:eaaf4df5-471d-404e-b143-652fe38cd2c7/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:eaaf4df5-471d-404e-b143-652fe38cd2c7\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvZXq8jX38lwndvzadCsT\\r\\n Xa2Zafdrg9I69gzfccH6XWY3Ddi/JoMuTSB1GwxKBfXpo9gjaPYsm6wCLv9Kku4x\\r\\n HL4LA1kmIalVTVDYgSO4sGK9k9oQNTY+hgUoTtdMxMShWrVy6+DIS/ZzIPyQBtbm\\r\\n 9D7RojrvESmjq/OuMs6sTlC0JjEE1ijuuHY+iY7gDYcR7RGFAsi4WGbCVy6c8VqL\\r\\n 29h8yGps2U+AxKr9f783VGMCk469ESHVwVyw6Jbxihn/h4TH3ZH8WTQW9rpS9GhO\\r\\n euSAA6iSH5UcmAzJZKSzaC+oghEJwMtOcgvr1F9iSn9tuHebgy9R6tHvEChhvdgz\\r\\n 2wIDAQAB\\r\\n -----END PUBLIC KEY-----\\r\\n\", }] } ### 5.2 Legacy DID Document The Veres One ledger was launched in 2015, predated this specification, and as a result has a number of legacy objects that developers should be aware of. The typical format for these objects are shown below: Example 14 { \"@context\": \"https://w3id.org/identity/v1\", \"id\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460\", \"idp\": \"did:d1d1d1d1-d1d1-d1d1-d1d1-d1d1d1d1d1d1\", \"accessControl\": { \"writePermission\": [ { \"id\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460/keys/1\", \"type\": \"CryptographicKey\" }, { \"id\": \"did:d1d1d1d1-d1d1-d1d1-d1d1-d1d1d1d1d1d1\", \"type\": \"Identity\" } ] }, \"publicKey\": [ { \"id\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\nMIIBIjA...DAQAB\\r\\n-----END PUBLIC KEY-----\\r\\n\", \"@context\": \"https://w3id.org/identity/v1\" } ], \"signature\": { \"type\": \"LinkedDataSignature2015\", \"created\": \"2017-07-25T17:29:49Z\", \"creator\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460/keys/1\", \"signatureValue\": \"LJoxpV...daOLHbA==\" } } \u2191","title":"Did method veres one"},{"location":"RWoT5/did-method-veres-one/#table-of-contents","text":"1. Introduction 2. Core Data Model 3. Basic Concepts 3.1 Authentication 3.2 Authorization 3.3 Service Descriptions 4. Operations 4.1 Discovering Service Endpoints 4.2 Creating a DID 4.3 Updating a DID Document 4.4 Delegating Control 4.5 Key Rotation and Transferring Control 4.6 Recovering a DID 5. Appendix A: Examples 5.1 Typical DID Document 5.2 Legacy DID Document 1. Introduction ------------------------------------------ Issue 1 TBD: This section will provide a gentle introduction to the purpose of the Veres One Ledger, expanding upon the abstract of the document. 2. Core Data Model --------------------------------------------- Issue 2 TBD: This section will describe the use of the Web Ledger, JSON-LD, and the DID spec to build the Veres One Ledger. 3. Basic Concepts -------------------------------------------- ### 3.1 Authentication Authentication is the process the ledger uses to determine if an entity is associated with a DID. Example 1 : Expressing authentication credentials { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"authenticationCredential\": [... array of acceptable authentication credentials ...] } A detailed example of a valid set of authentication credentials follows: Example 2 : Detailed example of authentication credentials entry { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"authenticationCredential\": [{ \"type\": \"RsaSignature2017\", \"publicKey\": { \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938/keys/2\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\", } }] } ### 3.2 Authorization Authorization is the process the ledger uses to determine what an entity may to do the DID Document. Example 3 ``` {.nohighlight title=\"\"} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"authorizationCapability\": [... array of capability descriptions ...] } </div> A detailed example of a valid set of authorization capability descriptions follows: <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 4</span> </div> ``` {.nohighlight title=\"\"} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", // proof of update authorization may be provided by digital wallet + friend OR // by mobile phone \"authorizationCapability\": [{ // this entity may update any field in this DID Document using any // authentication mechanism understood by the ledger \"permission\": \"UpdateDidDocument\", \"entity\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\" }, { // this entity may update the authenticationCredential field in this // DID Document as long as they authenticate with RsaSignature2017 \"entity\": \"did:v1:b5f8c320-f7ca-4869-85e6-a1bcbf825b2a\", \"permission\": \"UpdateDidDocument\", \"field\": [\"authenticationCredential\"], \"permittedProofType\": [{ \"proofType\": \"RsaSignature2017\" }] }, { // anyone may update the authenticationCredential and writeAuthorization // fields as long as they provide a specific multi-signature proof \"permission\": \"UpdateDidDocument\", \"field\": [\"authenticationCredential\", \"authorizationCapability\"], \"permittedProofType\": [{ \"proofType\": \"RsaSignature2017\", \"minimumSignatures\": 3, \"authenticationCredential\": [{ \"id\": \"did:v1:304ebc3e-7997-4bf4-a915-dd87e8455941/keys/123\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:v1:304ebc3e-7997-4bf4-a915-dd87e8455941\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:v1:0f22346a-a360-4f3e-9b42-3366e348e941/keys/foo\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:v1:0f22346a-a360-4f3e-9b42-3366e348e941\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:v1:a8d00377-e9f1-44df-a1b9-55072e13262a/keys/abc\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:v1:a8d00377-e9f1-44df-a1b9-55072e13262a\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }] }] } ### 3.3 Service Descriptions Services may be listed by including them at the top-level of the DID Document. Example 5 : Simple example of a service description { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"credentialRepositoryService\": \"https://wallet.veres.io/\" } A detailed example of the expression of a service description follows: Example 6 ``` {.nohighlight title=\"\"} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:215cb1dc-1f44-4695-a07f-97649cad9938\", \"credentialRepositoryService\": [{ // the verifiable credential repository service \"id\": \"did:v1:5d6c3b20-56a9-42e1-bfc8-ed7e685c9039\", \"type\": \"VerifiableCredentialRepository\", \"url\": \"https://wallet.veres.io/\", \"description\": \"Pat Doe's Digital Wallet\" }] } </div> </div> </div> <div id=\"operations\" class=\"section\"> <span class=\"secno\">4. </span>Operations ---------------------------------------- Every conforming Veres Ledger node *MUST* expose at least the following HTTP endpoints: Service Example URL Description ----------------------- ------------------ ---------------------------------- veresOneCreateService POST /dids Create a new DID. veresOneReadService GET /dids/{did} Gets an existing DID Document. veresOneUpdateService POST /dids/{did} Update an existing DID Document. <div id=\"discovering-service-endpoints\" class=\"section\"> ### <span class=\"secno\">4.1 </span>Discovering Service Endpoints {#x4.1-discovering-service-endpoints} A website may provide service endpoint discovery by embedding JSON-LD in their top-most HTML web page (e.g. at `https://example.com/`): <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 7</span><span style=\"text-transform: none\">: Example of HTML-based service description</span> </div> ``` {.highlight .hljs .xml aria-busy=\"false\" aria-live=\"polite\"} <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"utf-8\"> <title>Example Website</title> <link rel=\"stylesheet\" href=\"style.css\"> <script src=\"script.js\"></script> <script type=\"application/ld+json\"> { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"https://example.com/\", \"name\": \"Example Website\", \"veresOneCreateService\": \"https://example.com/veres-one/dids\", \"veresOneReadService\": \"https://example.com/veres-one/dids/\", \"veresOneUpdateService\": \"https://example.com/veres-one/dids/\" } </script> </head> <body> <!-- page content --> </body> </html> Service descriptions may also be requested via content negotiation. In the following example a JSON-compatible service description is provided (e.g. `curl -H \"Accept: application/json\" https://example.com/`): Example 8 : Example of a JSON-based service description ``` {.highlight .hljs .json aria-busy=\"false\" aria-live=\"polite\"} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"https://example.com/\", \"name\": \"Example Website\", \"veresOneCreateService\": \"https://example.com/veres-one/dids\", \"veresOneReadService\": \"https://example.com/veres-one/dids/\", \"veresOneUpdateService\": \"https://example.com/veres-one/dids/\" } </div> </div> <div id=\"creating-a-did\" class=\"section\"> ### <span class=\"secno\">4.2 </span>Creating a DID A DID is created by performing an HTTP POST of a signed DID Document to the `veresOneCreateService`. The following HTTP status codes are defined for this service: HTTP Status Description ------------- -------------------------------------------------------------------------------------------------------------------------- 201 DID creation request was successful. The HTTP `Location` header will contain the URL for the newly created DID Document. 400 DID creation request failed. 409 A duplicate DID exists. An example exchange of DID creation request is shown below: <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 9</span><span style=\"text-transform: none\">: DID creation request</span> </div> ``` {.highlight .hljs .http aria-busy=\"false\" aria-live=\"polite\"} POST /dids HTTP/1.1 Host: example.com Content-Type: application/ld+json Content-Length: 1062 Accept: application/ld+json, application/json, text/plain, */* Accept-Encoding: gzip, deflate { \"@context\": \"https://w3id.org/webledger/v1\", \"type\": \"WebLedgerEvent\", \"operation\": \"Create\", \"input\": [{ \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"authorizationCapability\": [{ \"permission\": \"UpdateDidDocument\", \"entity\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"permittedProofType\": [{ \"proofType\": \"LinkedDataSignature2015\" }, { \"proofType\": \"EquihashProof2017\", \"equihashParameterAlgorithm\": \"VeresOne2017\" }] }], \"authenticationCredential\": [{ \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmbDqPu6IKHiiIQ4d0AQ6\\r\\n PBduDhUUVqyQirvxqsdcNdKgZ2L8whBml/nTyuB4cd+hHrsfMDiHiT5kX2pbZ7Yy\\r\\n 2ctWkGw8e0J94CbwVh2H15gBQBUCjLiGvVIHO2pni693qmre+3Ya2NJ8gGwPLJ7h\\r\\n TLca2b2dX0y16qu0MT0osUGGEoPsdg6ibD2pxnADS3GNPObHT12GrAuxjYFMHecF\\r\\n A4hLZ8U+MIcVmHZuokqqbcyJyjOV+kmhFNeTKFP5P5U8HA3Y42/rE1UJp/wyy7Lc\\r\\n ZAvq0t75ddXKyvYh5dkzxxeeELNKNWVxJ2yvgAr0SatLEPzxJoeYdCyU5N5E22Fj\\r\\n jQIDAQAB\\r\\n-----END PUBLIC KEY-----\\r\\n\" }] }], \"signature\": [{ \"type\": \"EquihashProof2017\", \"equihashParameterN\": 64, \"equihashParameterK\": 3, \"nonce\": \"AQAAAA==\", \"proofValue\": \"AAAaPwABxrIAAFOKAAGo4QAAVW0AAN7cAACXcgABjEI=\" }, { \"type\": \"LinkedDataSignature2015\", \"created\": \"2017-09-30T02:54:31Z\", \"creator\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"signatureValue\": \"SNMbsPqLnB+hJFhXzS6hcpZnm9cGvSZZg7o26UYnyGYTvKder/S+Xk hNhXisS5385Ljlf5CXTQT5j6qYZtP8ut1Benaae8TMH17txP0CfzHbUMJFnHA1+Nru+e/Pw yPwuQ+VZYlXOB7g/tKVVZsxAYTKCAOJvJMIE+nlHjpB+RsKs9z4ZzVtddntqqAcvbZxV/o7 azBFDizeJu/gHVVMncCJ00SRoOzCOZUABRJV/k68bNSAfpELkrdWx8/xvMIF8r+LWhwdKCS iOw4DjSwIK40yD5rOvQn/GlC+unyB8zFe60jCToz/UOJNZBiIYwo+Pwwx28Wqd4Jkb3IeDr /L2Q==\" }] } If the creation of the DID was successful, an HTTP 201 status code is expected in return: Example 10 : Successful DID creation response ``` {.highlight .hljs .http aria-busy=\"false\" aria-live=\"polite\"} HTTP/1.1 201 Created Location: https://ledger.example.com/dids/did:v1:215cb1dc-1f44-4695-a07f-97649cad9938 Cache-Control: no-cache, no-store, must-revalidate Pragma: no-cache Expires: 0 Date: Fri, 14 Oct 2016 18:35:33 GMT Connection: keep-alive Transfer-Encoding: chunked </div> </div> <div id=\"updating-a-did-document\" class=\"section\"> ### <span class=\"secno\">4.3 </span>Updating a DID Document A DID is updated by performing an HTTP POST of a signed DID Document to the `veresOneUpdateService`. The following HTTP status codes are defined for this service: HTTP Status Description ------------- ------------------------------------ 200 DID update request was successful. 400 DID update request failed. An example exchange for a DID update request is shown below: <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 11</span><span style=\"text-transform: none\">: DID Document update request</span> </div> ``` {.highlight .hljs .http aria-busy=\"false\" aria-live=\"polite\"} POST /dids/did:v1:215cb1dc-1f44-4695-a07f-97649cad9938 HTTP/1.1 Host: example.com Content-Type: application/ld+json Content-Length: 1062 Accept: application/ld+json, application/json, text/plain, */* Accept-Encoding: gzip, deflate { \"@context\": \"https://w3id.org/webledger/v1\", \"type\": \"WebLedgerEvent\", \"operation\": \"Update\", \"input\": [{ \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"authorizationCapability\": [{ \"permission\": \"UpdateDidDocument\", \"entity\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"permittedProofType\": [{ \"proofType\": \"LinkedDataSignature2015\" }, { \"proofType\": \"EquihashProof2017\", \"equihashParameterAlgorithm\": \"VeresOne2017\" }] }], \"authenticationCredential\": [{ \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmbDqPu6IKHiiIQ4d0AQ6\\r\\n PBduDhUUVqyQirvxqsdcNdKgZ2L8whBml/nTyuB4cd+hHrsfMDiHiT5kX2pbZ7Yy\\r\\n 2ctWkGw8e0J94CbwVh2H15gBQBUCjLiGvVIHO2pni693qmre+3Ya2NJ8gGwPLJ7h\\r\\n TLca2b2dX0y16qu0MT0osUGGEoPsdg6ibD2pxnADS3GNPObHT12GrAuxjYFMHecF\\r\\n A4hLZ8U+MIcVmHZuokqqbcyJyjOV+kmhFNeTKFP5P5U8HA3Y42/rE1UJp/wyy7Lc\\r\\n ZAvq0t75ddXKyvYh5dkzxxeeELNKNWVxJ2yvgAr0SatLEPzxJoeYdCyU5N5E22Fj\\r\\n jQIDAQAB\\r\\n-----END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/2\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIj0BAQEFAKHiiIQ4d0AQ6ANBgkqhkiG9wAOCAQ8AMIIBCgKCAQEAmbDqPu6I\\r\\n xqsdcNdKgZ2L8whBml/nTyuBiHiPBduDhUUVqyQirvT5kX2pbZ7Yy4cd+hHrsfMD\\r\\n VhVIHO2pni693qmre+2ctWkGw8e0J94Cbw3Ya2NJ8gGwPLJ7hH15gBQBUCjLiGv\\r\\n 0osUNPObHT12GrAuxjYFMHecFGTLca2b2dX0y16qu0MTGEoPsdg6ibD2pxnADS3G\\r\\n kqqbcyJyjOV+kmh8HA3Y42/rE1UJpA4hLZ8U+MIcVmHZuo/wyy7LcFNeTKFP5P5U\\r\\n 5dkzxxezxJoeYdCyU5N5E2ZAvq0t75ddXKyvYh2FjeELNKNWVxJ2yvgAr0SatLEP\\r\\n AQABjQID\\r\\n-----END PUBLIC KEY-----\\r\\n\" }] }], \"signature\": [{ \"type\": \"EquihashProof2017\", \"equihashParameterN\": 64, \"equihashParameterK\": 3, \"nonce\": \"AQAAAA==\", \"proofValue\": \"AAAaPwABxrIAAFOKAAGo4QAAVW0AAN7cAACXcgABjEI=\" }, { \"type\": \"LinkedDataSignature2015\", \"created\": \"2017-09-30T02:54:31Z\", \"creator\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/2\", \"signatureValue\": \"Zg7o26UYnyGYTvKdSN6hcpZnm9cGvSZB+hJFhXzSer/S+XkMbsPqLn VMncCJ00SRoOzCOZUABRJV/azBFDizeJu/gHVk68bNSAfpELkrdWx8/xvMIF8r+LWhwdKCS XTQT5j6qYZtP8ut1BenahNhXisS5385Ljlf5Cae8TMH17txP0CfzHbUMJFnHA1+Nru+e/Pw K40yD5rOvQn/GlC+unyB8ziOw4DjSwIFe60jCToz/UOJNZBiIYwo+Pwwx28Wqd4Jkb3IeDr VVZsxAYTKCAOJvJMIE+nlHjpB+RyPwuQ+VZYlXOB7g/tKsKs9z4ZzVtddntqqAcvbZxV/o7 /45H==\" }] } If the update request for the DID was successful, an HTTP 200 status code is expected in return: Example 12 : Successful ledger creation response ``` {.highlight .hljs .http aria-busy=\"false\" aria-live=\"polite\"} HTTP/1.1 200 Success Cache-Control: no-cache, no-store, must-revalidate Pragma: no-cache Expires: 0 Date: Fri, 14 Oct 2016 18:35:33 GMT Connection: keep-alive Transfer-Encoding: chunked </div> </div> <div id=\"delegating-control\" class=\"section\"> ### <span class=\"secno\">4.4 </span>Delegating Control <div id=\"issue-3\" class=\"issue\"> <div id=\"h-issue3\" class=\"issue-title marker\" aria-level=\"4\" role=\"heading\"> <span>Issue 3</span> </div> TBD: Explain that delegation of control is merely placing a digital wallet provider in the proofOfControl field. </div> </div> <div id=\"key-rotation-and-transferring-control\" class=\"section\"> ### <span class=\"secno\">4.5 </span>Key Rotation and Transferring Control <div id=\"issue-4\" class=\"issue\"> <div id=\"h-issue4\" class=\"issue-title marker\" aria-level=\"4\" role=\"heading\"> <span>Issue 4</span> </div> TBD: Explain that transferring control and rotating keys is a matter of adding and removing the appropriate keys from proofofControl and proofOfUpdateAuthorization. </div> </div> <div id=\"recovering-a-did\" class=\"section\"> ### <span class=\"secno\">4.6 </span>Recovering a DID <div id=\"issue-5\" class=\"issue\"> <div id=\"h-issue5\" class=\"issue-title marker\" aria-level=\"4\" role=\"heading\"> <span>Issue 5</span> </div> TBD: Explain that recoverying a DID is a matter of meeting the requirements under proofOfUpdateAuthorization. </div> </div> </div> <div id=\"appendix-a:-examples\" class=\"section\"> <span class=\"secno\">5. </span>Appendix A: Examples -------------------------------------------------- <div id=\"typical-did-document\" class=\"section\"> ### <span class=\"secno\">5.1 </span>Typical DID Document The following is a complete example of a typical Veres One DID Document: <div class=\"example\"> <div class=\"example-title marker\"> <span>Example 13</span> </div> ``` {.nohighlight} { \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:eaaf4df5-471d-404e-b143-652fe38cd2c7\", \"authorizationCapability\": [{ \"permission\": \"UpdateDidDocument\", \"entity\": \"did:v1:eaaf4df5-471d-404e-b143-652fe38cd2c7\", \"permittedProofType\": [{ \"proofType\": \"LinkedDataSignature2015\" }, { \"proofType\": \"EquihashProof2017\", \"equihashParameterAlgorithm\": \"VeresOne2017\" }] }], \"authenticationCredential\": [{ \"id\": \"did:v1:eaaf4df5-471d-404e-b143-652fe38cd2c7/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:eaaf4df5-471d-404e-b143-652fe38cd2c7\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvZXq8jX38lwndvzadCsT\\r\\n Xa2Zafdrg9I69gzfccH6XWY3Ddi/JoMuTSB1GwxKBfXpo9gjaPYsm6wCLv9Kku4x\\r\\n HL4LA1kmIalVTVDYgSO4sGK9k9oQNTY+hgUoTtdMxMShWrVy6+DIS/ZzIPyQBtbm\\r\\n 9D7RojrvESmjq/OuMs6sTlC0JjEE1ijuuHY+iY7gDYcR7RGFAsi4WGbCVy6c8VqL\\r\\n 29h8yGps2U+AxKr9f783VGMCk469ESHVwVyw6Jbxihn/h4TH3ZH8WTQW9rpS9GhO\\r\\n euSAA6iSH5UcmAzJZKSzaC+oghEJwMtOcgvr1F9iSn9tuHebgy9R6tHvEChhvdgz\\r\\n 2wIDAQAB\\r\\n -----END PUBLIC KEY-----\\r\\n\", }] } ### 5.2 Legacy DID Document The Veres One ledger was launched in 2015, predated this specification, and as a result has a number of legacy objects that developers should be aware of. The typical format for these objects are shown below: Example 14 { \"@context\": \"https://w3id.org/identity/v1\", \"id\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460\", \"idp\": \"did:d1d1d1d1-d1d1-d1d1-d1d1-d1d1d1d1d1d1\", \"accessControl\": { \"writePermission\": [ { \"id\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460/keys/1\", \"type\": \"CryptographicKey\" }, { \"id\": \"did:d1d1d1d1-d1d1-d1d1-d1d1-d1d1d1d1d1d1\", \"type\": \"Identity\" } ] }, \"publicKey\": [ { \"id\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\nMIIBIjA...DAQAB\\r\\n-----END PUBLIC KEY-----\\r\\n\", \"@context\": \"https://w3id.org/identity/v1\" } ], \"signature\": { \"type\": \"LinkedDataSignature2015\", \"created\": \"2017-07-25T17:29:49Z\", \"creator\": \"did:8743453f-e45e-4ac6-b85f-4513ba4c1460/keys/1\", \"signatureValue\": \"LJoxpV...daOLHbA==\" } } \u2191","title":"Table of Contents"},{"location":"RWoT5/gdpr/","text":"When GDPR becomes real, and Blockchain is no longer Fairy Dust by Marta Piekarska (Linux Foundation), Michael Lodder (Evernym), Zachary Larson (Economic Space Agency), Kaliya Young (Identity Woman) Abstract The General Data Protection Regulation (GDPR), enacted by the European Parliament in 2016, was designed to give users more control and rights over their personal data. Companies and governments will find it increasingly difficult to be GDPR compliant with current industry practices. Following the implementation date of May 25, 2018, managing data will be both toxic and expensive. Many precious resources will be required for improving and maintaining the security, privacy, and governance of personal data. Methods for storing less personal data will ease the burden of GDPR compliance. This document describes the GDPR requirements and the different approaches to digital identity solutions and finally explains why distributed ledger technology may offer an opportunity for enterprises to simplify data management solutions that are GDPR compliant. GDPR Requirements Existing infrastructure has data processors housing personal data and continually collecting more. A person may not know what is known about them until they petition for it. To do so is often difficult, and entities are not required to respond in a timely manner or at all. Entities often share the data with others for marketing purposes. This can cause people to receive unwanted, automated marketing without understanding why they are being targeted. A person can call a number to be put on a do-not-contact list, but companies frequently do not honor those requests. Sometimes data processors have incorrect data, and it can be difficult for individuals to correct this data and those changes may not be propagated to other entities. A person must try to fix the problem with every data processor they know about that has knowledge of said data. GDPR was created to help address these issues. Unfortunately the GDPR brings with it liabilities forth for enterprises storing personal data. When personal data becomes compromised or known, the companies become targets of government investigations and lawsuits. Smaller entities can go bankrupt when this happens. Compliance with data laws can require costly and time intensive audits, investing in technical and physical security measures, and hiring trained security personnel in order to limit their liability to financial lawsuits and fines and susceptibility to attacks. Many companies do not want to store personal data, but nonetheless have a real, operational need for the data. The current infrastructure doesn\u2019t enable just-in-time access that would enable them to get access to what they need, when they need it, without storing the data themselves. GDPR[1] is composed of articles that outline the rights of individuals and requirements of data processors. The following is a brief summary of rights granted to individuals: Article 6 : Lawfulness of processing. Processing personal data is generally forbidden if it is not expressly allowed by law or if the impacted persons have not consented to processing these data. Article 7: The right of consent. The individual must consent to personal data being collected and can rescind that consent at any time. Article 12 : The right to ask questions about use of personal data and to seek redress if questions are not answered in a clear, concise, timely manner. Articles 13 & 14 : The right to know how personal data is used at the time of collection and the length of time for which it will be stored and contact information for the collecting party. Article 15 : The right to access the personal data that is being processed. Article 16 : The right to have incorrect personal data rectified. Article 17 : The right to have personal data erased when it is no longer necessary for the purposes for which they were collected and there is no legal ground for their maintenance. Article 18 : The right to restrict data processing where the data is inaccurate, its collection unlawful, or its processing no longer required. Article 19 : The data collecting party must inform all additional data processors with whom it shares personal data to cease processing data that has been rectified or erased. Article 20 : The right to receive their personal data in a structured, commonly-used, machine-readable format which they can freely share with other data processors. Article 21 : The right to object to personal data being used to profile or market to them. Article 22 : The right to not be subject to legal outcomes that rely solely on automated data processing. Article 25: The right to have the minimal amount of data stored as necessary for data processors to do their work. Article 77: The right to file a complaint against non-compliant data processors. Article 80: The right to have a legal representative for actions against data processors. The following is a brief summary of obligations for data processors: Article 24: Must be able to demonstrate all processing and handling is in compliance. Article 28: Must notify data owners when data will be shared with other processors. Article 29: Must only process authorized data for authorized purposes. Article 30: Must maintain a record of all data processing activity. The record must include who processed it, what was processed, where it was processed or transferred, when it will be erased, and the security measures in place when it was done. Article 32: Must protect the data using pseudonymization and encryption. They must ensure those measures are tested regularly and they can recover in the event of failures. Article 33: Must notify data owners and other data processors in the event of a breach within 72 hours of first having become aware of the breach. Article 37: Must appoint a data protection officer. Article 50: The transfer of data must only happen to countries deemed as having adequate data protection laws. The legal language must be translated into the technical requirements. Without a solid solution, organizations in breach of GDPR can be fined up to 4% of annual global turnover or \u20ac20 Million (whichever is greater). This is the maximum fine that can be imposed for the most serious infringements e.g. not having sufficient customer consent to process data or violating the core of Privacy by Design concepts. There is a tiered approach to fines, e.g. a company can be fined 2% for not having their records in order (article 28), not notifying the supervising authority and data subject about a breach or not conducting impact assessment. It is important to note that these rules apply to both controllers and processors \u2014 'clouds' will not be exempt from GDPR enforcement. Technical Requirements GDPR mandates in order to ensure a complete solution that protects the user, or data subject, and restricting enterprises can be summarized in the following points. Availability : The user should always have access to their data, no matter if it is stored locally or remotely. The data should be protected from leakages or attacks because it affects availability. Completeness: Data and any event regarding its collection and processing should be recorded. Confidentiality : Only parties involved in the exchange of data should be able to see details of that transaction. Correctness: The accuracy of data recorded should be assured. Immutability: There should be no possibility of changing historical logs. Integrity: The content of the data store should be protected from malicious or unintentional changes. Interoperability : Users should be able to combine data coming from various sources. Non-repudiation: Interaction with any data should not be deniable at later points in time. Rectification & Erasure: Users must be able to change or erase their personal data. They must also be able to make corrections of erroneous data. Traceability: Any occurrence of processing data must be traceable and linkable to previous occurrences of processing of that data. GDPR is the standard proposed by European Parliament to protect European citizens and return control over private data to the users. However, the legislators in other parts of the world recognize that same need for data control. For instance in China the Standing Committee of the National People\u2019s Congress is already proposing changes similar to the GDPR[2]. Similarly there are laws like COPPA in the U.S. and PIPA in Japan. Self-Sovereign Identity as a Model for GDPR The requirements of the GDPR call for user control of their private information. One model for that is self-sovereign identity, a movement that started before the GDPR was introduced. It grew out of the user-centric identity efforts to support the individual being at the core of their own digital identity. The community is focused on how various sophisticated cryptography and shared distributed ledgers allow for this to actually happen. The community focused on user-centric identity and personal data control has several times produced guiding rules or principles that also align with the vision of GDPR and actually influenced its authors. In 2005 Kim Cameron defined Laws of Identity, In 2010 during the same week while Phil Windley[3] and Kaliya Young[4] each posted created Personal Data Ecosystem principles. Several years ago new term gained traction\u201cself-sovereign identity\u201d. Devon Lefretto first used the term Self-Sovereign Authority within the VRM community in 2011 and in that context worked to articulate its meaning. References to Self-Sovereign Identity appear in 2015 by John Edge founder of ID2020. Devon Lofretto published a post defining the term in February 2016[5]. Christopher Allen circulated ideas about a set of principles and published them in the spring of 2016.[6]. The W3C standardization body has a Verifiable Claims Working Group, which focuses on the API definitions. While Self-Sovereign identity is not necessarily the ideal or full answer to GDPR requirements, it is interesting to analyze it, as one of the possible solutions. Christopher Allen describes 10 principles for a self-sovereign identity system. The person behind the identity needs to exist . Nobody can be fully digital, we need to have a \u201csum\u201d ( cogito ergo sum ) that is the core of who one is. That sum can later be described in a digital form. Every entity should be in control of their identity. One who is described by the identity is the one who is the central authority on that identity. While others can make claims to one\u2019s identity, they should not have control over it. The owner should always be able to access their own data. This is crucial, as without access to the identity, a user has no way to prove or control who they are. While the owner may not be able to modify all the information about themselves (if there is a court ruling attached to a given identity the owner should not be able to change that ruling) but they have to be made aware of all that is created about them. The self-sovereign system requires full transparency on all the algorithms, designs, management and updates. In order to ensure that transparency, the algorithms used should be free, open-source and well described in public. Such systems must be designed to exist forever, or be persistent as long as the owner wants them to be. The protection mechanisms and the implementation details may change over time, but the core identity should last forever. All of these should not prevent the rule of right to be forgotten, that allows users to request removal of any of the claims made to their identity. Thus, the identity must be separate from claims made to it. Following the understanding that the system needs to be available and persistent, the data should be portable . The identity cannot be held by a single entity, in case this body disappears. A transferable architecture allows for a simple method of moving data while keeping it available. Connected to this is the rule of interoperability . All systems should communicate and be available anywhere in the world, not dependent on borders or governments. As users have full control over the system, they should also be able to give their consent to the entities that want to use their identity. The design should be such, that while allowing for full information sharing, it is always done with the agreement of the user. It should also be strongly minimized . When requested to disclose information, the system should give away as little as possible. Finally, the user should always be the center of interest, and his rights should be protected , independently from any third party. While the rules describing self-sovereign identity are theoretical and not necessarily all applicable, most modern systems that aim to define who we are and how we work in the digital world, closely follow the above principles. Let us now look at the technical implementations proposed by various entities. Technical Approaches to Digital Identity A few different technical proposals for digital identity have been incubated in conjunctions with the discussions of self-sovereign identity. These may offer strong foundations for any attempts to implement GDPR-compliant data. Some follow the self-sovereign rules defined above, while some only take what is needed. This section describes the two most mature developments. However, it is important to recognize that each suits the use case it is trying to solve. We have yet to see a universal approach acceptable on a global scale. Decentralized Identity Foundation \u2013 DIF Self-Sovereign Focus: Control In May 2017, a new movement was started: creating a space for organizations to develop fundamental primitives, protocols and tools for an interoperable ecosystem under the umbrella of Decentralized Identity Foundation[7]. We no longer live in a world which accepts Big Brother approach of storing all information about everyone in one place. However, a huge paradigm shift needs to happen when moving from a centralized registry of objects \u2014 people, devices and entities \u2014 to a decentralized system. This requires development of new specifications, protocols, formats, and implementations for cross-chain rooting, indexing, and resolution of decentralized identifiers and names. Decentralized Identifiers (DIDs) are used as bases for a verifiable, fully self-controlled digital identity. In ideal circumstances, DIDs adhere to the rules of self-sovereignty, not being dependent on any centralized registry, identity provider or certificate authority. To interact with the world, DIDs need to be resolved to DID documents that contain authentication mechanisms, authorization information and service endpoints. With that, a trusted interaction can be started with an entity. Unlike in a traditional model, where identity management system relies on a centralized authority that provides cryptographic trust verification, in a DID world we need a federated identity management[8]. Following the dictums of Privacy by Design[9], each entity may have as many DIDs as necessary, to respect the entity\u2019s desired separation of identities, personas and contexts. Such a decentralized design eliminates the need for registries that collect all the identifiers as well as centralized certificate authorities for key management. Thus, a traditional Public Key Infrastructure is no longer valid. However, it needs to be replaced with a new, DLT-compliant architecture, where each entity can have its own root authority. Such system is called a Decentralized PKI. As interoperability is one of the key focuses of DIF, DID methods that allow for collaboration with centralized or federated systems can be developed and are encouraged as a bridging mechanism. Until now there have been several DID methods proposed by various entities ranging from the Sovrin Foundation, Bitcoin, Ethereum and Verifiable Claims worlds. However, DID is only the way to address the objects; the question is how to store and compute the data. The control and ownership need to be in the hands of the object, and such a system is also part of what DIF is focusing on, through their work on Hub. A Hub is a data store containing semantic data objects at well-known locations. Objects in a Hub are signed by an identity and mapped to semantic data objects through an API. Anyone can address them through a global namespace. The final part of DIF\u2019s work is Chainpoint, which focuses on a scalable protocol to anchor data in the Blockchain and generate receipts. One of the key factors of the future interactions of entities on a Blockchain will be trust and reputation. These can only be built through attestations between groups of individuals and businesses, which can be used make important decisions in personal and business life. Verifiable Claims Self-Sovereign Focus: Minimization Today we have to reveal the maximum amount of information even when interacting with people who do not need that information. Moreover, it is hard to prove your qualification over the internet: driver\u2019s license, proof of age, education qualification is not easily verifiable. The purpose of verifiable claims is to resolve these twin issues. This is what the W3C Verifiable Claims Working Group[10] is focusing on. 150+ individuals and organizations are now planning how to create, store, transmit and verify digital credentials via the Internet. A Verifiable Claim is defined as an identifier that describes four roles within a single capsule. An Issuer issues verifiable credentials about a specific Subject. The Holder stores credentials on behalf of a Subject. Holders are typically also the Subject of a credential. The Verifier requests a profile of the Subject. A profile, which contains a specific set of credentials. The verifier verifies that the credentials provided in the profile are fit-for-purpose. The Identifier Registry is a mechanism that is used to issue identifiers for Subjects. Why do we need Verifiable Claims? Because one does not need to show their precise age, just that they are above a certain age. That they are capable of driving. That they are eligible to practice as a medical doctor and so on. This minimizes data disclosure, fulfilling one of the precepts of self-sovereign identity while simultaneously reducing liability for a data holder, especially under laws like GDPR. Blockchain Solutions to GDPR Blockchain is a up-and-coming technology for digital identities, and is already in extensive use for DIF and other self-sovereign-focused technologies. It needs to be part of any discussion about GDPR compliance. However, some fear that the Blockchain is the exact opposite of what GDPR requires of enterprises. If we take public, permissionless Blockchain, then indeed everything that happens within such a system is visible and available to anyone. However, the domain of Blockchain technology expands beyond permissionless, public versions. If we take the example of medical records, nobody would find it advisable or feasible to store their data in a public ledger. For such a use case we need a private, permissioned ledger, where only a certain group of people may access the ledger for read and write purposes. Then, there is a middle ground \u2013 between the crypto currency and medical records \u2013 for instance certification. Issuance of such should be limited to those, who are eligible to certify that a given person can hold a given certificate. However, anyone should be able to inquire about the validity of the claim. So, there is space for both permissionless-private, and permissioned-public blockchains. It is our strong belief that there will not be one Blockchain to rule them all, but a spectrum of solutions to choose from, which we summarize in the Figure below. Given such a model, we can compare how GDPR requirements can map into what Blockchain-based solution may offer. First rule is to ensure availability . The user should always have access to their data, no matter if it is stored locally or remotely. The data should be protected from leakages or attacks as that stands against availability. Availability is ensured through the distribution of nodes that hold the same copy of the ledger and create a peer-to-peer network. Even if a certain number of these nodes becomes unavailable, the others still have an identical copy of the same data, meaning that participants can always access the information. Such data may not be leaked to participants outside of the permissioned structure of the Blockchain, which relies on a good governance model and a solution that is built with security in mind. This assumption, however, holds true for most technologies no matter whether they include Blockchain or not. Distributed Ledger Technology (DLT) is a way of ensuring availability, immutability, transparency and lack of reliance on trusted third party. As any innovation, it can only serve as part of a bigger solution that has to be properly architected and designed with all the right principles in mind. Next GDPR requirement is completeness : every event and data have to be recorded. This is exactly what Blockchain and DLT have been designed for. The structure of a chain of blocks connected to one another through cryptographic hashes of previous blocks ensures that any changes in the history will be immediately recognized and flagged as an error. Any event or data is announced to the participants of the network and once they agree on the content of it, it is published on the Blockchain and stored forever. Similarly, the rule of correctness , meaning that there is an assurance of accuracy of data, is guaranteed with DLT. Of course, data always needs to be verified before it is amended to the Blockchain. The sheer fact it exists on the Blockchain, does not mean it is true. However, the participants of the network achieve a consensus over the correctness of the (hopefully) audited data any changes cannot be done to it. This also means that integrity of data is protected. As the GDPR states, the content of the data store should be protected from malicious or unintentional changes. This happens by default in the Blockchain as changes can only be introduced as new inputs to the blocks. If University X publishes incorrectly that Alice has passed an exam with a grade B on a Blockchain, the only way to change that is to introduce a new transaction announcing that the previous entry was incorrect, and that Alice passed the exam with a grade A. Immutability of Blockchain technology has been mentioned several times here and is also a requirement of the GDPR. There are several ways how to achieve confidentiality , meaning that only parties involved in the exchange of data can be able to see the details of the transaction, while still ensuring full transparency, where any occurrence of processing data must be traceable and linkable to previous occurrences of processing of that data. In a setting where we can create permissioned Blockchain and confidential transactions, one can also create channels where only parties involved in the exchange of the information know the content of it, while the outside world acknowledges that such a transaction occurred. This is done in case of Hyperledger Fabric Channels[11] or Hyperledger Sawtooth\u2019s Private UTXO Transaction Family[12]. There are also many more ways to do this, depending on the technology chosen. Transparency , on the other hand, can be implemented by tracking the transactions by their precise hashes and referring to their exact position in a Blockchain, so that any new exchange can clearly build on top of it. Creating an Off-Chain Wallet One of the big challenges facing enterprises when it comes to complying with GDPR is the user-centric approach to data handling. It is no longer the case that a company can store and be responsible for all the information. Quite the opposite \u2013 it is the users themselves that now need to decide if and how would they like to share their data. Thus, the requirements for: consensual data sharing, where permission can be withdrawn any time; data minimization , where only the minimum amount of data should be requested by a company; and interoperability and portability, where user can combine data from various sources. In the world of traditional databases this may be impossible to solve. However, with decentralized DLT systems, where one does not rely on a trusted third party such a solution no longer creates a problem. Using DIDs and Verifiable Claims, we can create a solution that puts the users in charge and control of their identity, a wallet of attestations and information about them that only they are responsible for. This is what Hyperledger Indy[13] is focusing on: creating a Blockchain so that the identities can be kept in a secure storage and the Blockchain can serve as a common reference to the records, rather than storage for the data itself. By having a single wallet of identity, the data is portable and interoperable. we can ensure revealing minimum data disclosure. As for consent, simple interaction with an entity and recording that transaction counts as a timestamped consent form; no further action is needed. The only way to stay GDPR compliant with immutable records is to avoid uploading person-related data at all. A ledger can only be used for verifiers of person related data. Not even hashing can be used as a method of anonymization, but rather pseudonymization. In fact, logical deletion by this definition can only be achieved by verifiers in the form of ZKPs. It is had to recognize what information will be considered sensitive in the future, and thus, ZKPs seem like the right answer for handling personal data. With the design of off-blockchain wallets that are recorded on a Blockchain, we achieve rectification . If a user changes their data, the anchor to their wallet changes, meaning that they have to update their entry on the Blockchain. They cannot simply rewrite the history: they must make amendments, and inform everyone about the fact that the changes have been made (not necessarily revealing what these changes were). Blockchain and Erasure Finally let us move to the most disputed part of the GDPR. Right to erasure , mandates that anyone may request that their data should be deleted from the company's\u2019 servers. In today\u2019s settings the best Alice can do is send an email to company X to delete her data. If an auditor comes in, such email may, or may not disappear and the data may or may not be deleted. If Alice makes that same exchange through a Blockchain, the request is timestamped and must be acknowledged by company X. Then the erasure is also confirmed on a Blockchain or simply executed as a smart contract in the first place when Alice gave access to the data only for limited time. Now if data is found by an auditor at any later point, the company can be held accountable for it in a much stronger way. The same holds for rectification . In case Alice wants to change her personal data, she makes a request which should be recorded on a Blockchain, acknowledged by the company and executed. This of course means non-repudiation . Interaction with any data should not be deniable at a later point in time; the system should ensure that every request for data is recorded on a Blockchain. This however boils down to the architecture of the system, not to the technology itself. Cryptography and data pseudonymization section - an analysis of available cryptographic techniques and way they reflect data de-pseudonymization. Should pseudonymous data encrypted with ZKP for example, be treated as pseudonymous? As long as ZKP are non-interactive and the process of decrypting the data is challenging. Private key to decrypt data and limited MITM interventions with PKI, don\u2019t they make encrypted data almost anonymous? How we should treat such encrypted data? The same with other advanced computational techniques, like Secure Multiparty Computation, is the data truly pseudonymous? Or closer to anonymous. I assume it would add great value to the paper. Summary Today there are many third-party entities who collect and sell user data without permission. However, violation of privacy seems to be most rampant within the digital advertising industry. Some American companies think GDPR won\u2019t apply to them, but this European legislation will have truly global impact. The Drum^[14]^ describes how GDPR applies to digital marketers in the United States. The most relevant requirements include requiring consent for data processing, anonymizing collected data to protect privacy, which can be easily done with DIDs, providing data breach notifications, safely handling the transfer of data across borders and finally requiring certain companies to have a data protection officer to oversee GDPR compliance. The movement towards proactive introduction of GDPR-compliant standards has already started. A good example are media buying agencies and their demand-side platforms like mParticle.^[15]^ A PwC Pulse Survey^[16]^ that studied how much US Companies are spending on GDPR compliance showed that over half of US multinationals say GDPR is their top data-protection priority, and information security enhancement is a top GDPR initiative. As binding corporate rules are gaining popularity, 77% plan to spend \\$1 million or more on GDPR. On a final note, in the world of Blockchain the responsibilities of companies for the data they store, and process are much lower. It can be a fear, that the data herding that is happening now, will no longer be possible. On the other hand users will be fully responsible to handle their data and incentivized to work with companies to share it with them. Why? By giving access to their data they will still want to get better recommendations and better services. Only now, with well-designed Blockchain based solutions, these transactions can happen in a way that protects both parties and is accessible. Some would argue that audience data belongs to publishers and their supply-side platforms, but projects like Tor, Brave, Ghostery, Evidon and many others give users control. Perhaps viewers will gain the power to monetize their own identities. For example, anonymously sharing a device\u2019s location drastically improves relevance of recommended content and/or services. Users might opt in for discounts and other sales promotions from local businesses. Endnotes [1] http://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679&from=EN#d1e3265-1-1 [2] https://www.chinalawinsight.com/2018/02/articles/corporate/antitrust-competition/the-wise-and-informed-adapts-to-the-changing-time-and-circumstances-discussing-the-issues-on-information-technology-personal-information-security-specification-from-a-pract/ [3] http://www.windley.com/archives/2010/09/pdx_principles.shtml [4] https://identitywoman.net/vision-principles-for-the-personal-data-ecosystem/ [5] https://www.moxytongue.com/2016/02/self-sovereign-identity.html [6] http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html [7] https://medium.com/decentralized-identity/the-rising-tide-of-decentralized-identity-2e163e4ec663 [8] https://en.wikipedia.org/wiki/Federated_identity [9] https://en.wikipedia.org/wiki/Privacy_by_design [10] https://www.w3.org/2017/vc/WG/ [11] http://hyperledger-fabric.readthedocs.io/en/release/channels.html [12] https://sawtooth.hyperledger.org/docs/core/nightly/0-8/examples/private_utxo/private_utxo_transaction_family.html [13] https://www.hyperledger.org/projects/hyperledger-indy [14] http://www.thedrum.com/news/2017/10/05/what-does-the-eu-s-privacy-reform-mean-us-marketers-and-what-should-you-do-now [15] https://www.mparticle.com/blog/what-gdpr-means-for-data-driven-marketing [16] http://www.pwc.com/us/en/increasing-it-effectiveness/publications/gdpr-readiness.html","title":"When GDPR becomes real, and Blockchain is no longer Fairy Dust"},{"location":"RWoT5/gdpr/#when-gdpr-becomes-real-and-blockchain-is-no-longer-fairy-dust","text":"","title":"When GDPR becomes real, and Blockchain is no longer Fairy Dust"},{"location":"RWoT5/gdpr/#by-marta-piekarska-linux-foundation-michael-lodder-evernym-zachary-larson-economic-space-agency-kaliya-young-identity-woman","text":"","title":"by Marta Piekarska (Linux Foundation), Michael Lodder (Evernym), Zachary Larson (Economic Space Agency), Kaliya Young (Identity Woman)"},{"location":"RWoT5/gdpr/#abstract","text":"The General Data Protection Regulation (GDPR), enacted by the European Parliament in 2016, was designed to give users more control and rights over their personal data. Companies and governments will find it increasingly difficult to be GDPR compliant with current industry practices. Following the implementation date of May 25, 2018, managing data will be both toxic and expensive. Many precious resources will be required for improving and maintaining the security, privacy, and governance of personal data. Methods for storing less personal data will ease the burden of GDPR compliance. This document describes the GDPR requirements and the different approaches to digital identity solutions and finally explains why distributed ledger technology may offer an opportunity for enterprises to simplify data management solutions that are GDPR compliant.","title":"Abstract"},{"location":"RWoT5/gdpr/#gdpr-requirements","text":"Existing infrastructure has data processors housing personal data and continually collecting more. A person may not know what is known about them until they petition for it. To do so is often difficult, and entities are not required to respond in a timely manner or at all. Entities often share the data with others for marketing purposes. This can cause people to receive unwanted, automated marketing without understanding why they are being targeted. A person can call a number to be put on a do-not-contact list, but companies frequently do not honor those requests. Sometimes data processors have incorrect data, and it can be difficult for individuals to correct this data and those changes may not be propagated to other entities. A person must try to fix the problem with every data processor they know about that has knowledge of said data. GDPR was created to help address these issues. Unfortunately the GDPR brings with it liabilities forth for enterprises storing personal data. When personal data becomes compromised or known, the companies become targets of government investigations and lawsuits. Smaller entities can go bankrupt when this happens. Compliance with data laws can require costly and time intensive audits, investing in technical and physical security measures, and hiring trained security personnel in order to limit their liability to financial lawsuits and fines and susceptibility to attacks. Many companies do not want to store personal data, but nonetheless have a real, operational need for the data. The current infrastructure doesn\u2019t enable just-in-time access that would enable them to get access to what they need, when they need it, without storing the data themselves. GDPR[1] is composed of articles that outline the rights of individuals and requirements of data processors. The following is a brief summary of rights granted to individuals: Article 6 : Lawfulness of processing. Processing personal data is generally forbidden if it is not expressly allowed by law or if the impacted persons have not consented to processing these data. Article 7: The right of consent. The individual must consent to personal data being collected and can rescind that consent at any time. Article 12 : The right to ask questions about use of personal data and to seek redress if questions are not answered in a clear, concise, timely manner. Articles 13 & 14 : The right to know how personal data is used at the time of collection and the length of time for which it will be stored and contact information for the collecting party. Article 15 : The right to access the personal data that is being processed. Article 16 : The right to have incorrect personal data rectified. Article 17 : The right to have personal data erased when it is no longer necessary for the purposes for which they were collected and there is no legal ground for their maintenance. Article 18 : The right to restrict data processing where the data is inaccurate, its collection unlawful, or its processing no longer required. Article 19 : The data collecting party must inform all additional data processors with whom it shares personal data to cease processing data that has been rectified or erased. Article 20 : The right to receive their personal data in a structured, commonly-used, machine-readable format which they can freely share with other data processors. Article 21 : The right to object to personal data being used to profile or market to them. Article 22 : The right to not be subject to legal outcomes that rely solely on automated data processing. Article 25: The right to have the minimal amount of data stored as necessary for data processors to do their work. Article 77: The right to file a complaint against non-compliant data processors. Article 80: The right to have a legal representative for actions against data processors. The following is a brief summary of obligations for data processors: Article 24: Must be able to demonstrate all processing and handling is in compliance. Article 28: Must notify data owners when data will be shared with other processors. Article 29: Must only process authorized data for authorized purposes. Article 30: Must maintain a record of all data processing activity. The record must include who processed it, what was processed, where it was processed or transferred, when it will be erased, and the security measures in place when it was done. Article 32: Must protect the data using pseudonymization and encryption. They must ensure those measures are tested regularly and they can recover in the event of failures. Article 33: Must notify data owners and other data processors in the event of a breach within 72 hours of first having become aware of the breach. Article 37: Must appoint a data protection officer. Article 50: The transfer of data must only happen to countries deemed as having adequate data protection laws. The legal language must be translated into the technical requirements. Without a solid solution, organizations in breach of GDPR can be fined up to 4% of annual global turnover or \u20ac20 Million (whichever is greater). This is the maximum fine that can be imposed for the most serious infringements e.g. not having sufficient customer consent to process data or violating the core of Privacy by Design concepts. There is a tiered approach to fines, e.g. a company can be fined 2% for not having their records in order (article 28), not notifying the supervising authority and data subject about a breach or not conducting impact assessment. It is important to note that these rules apply to both controllers and processors \u2014 'clouds' will not be exempt from GDPR enforcement.","title":"GDPR Requirements"},{"location":"RWoT5/gdpr/#technical-requirements","text":"GDPR mandates in order to ensure a complete solution that protects the user, or data subject, and restricting enterprises can be summarized in the following points. Availability : The user should always have access to their data, no matter if it is stored locally or remotely. The data should be protected from leakages or attacks because it affects availability. Completeness: Data and any event regarding its collection and processing should be recorded. Confidentiality : Only parties involved in the exchange of data should be able to see details of that transaction. Correctness: The accuracy of data recorded should be assured. Immutability: There should be no possibility of changing historical logs. Integrity: The content of the data store should be protected from malicious or unintentional changes. Interoperability : Users should be able to combine data coming from various sources. Non-repudiation: Interaction with any data should not be deniable at later points in time. Rectification & Erasure: Users must be able to change or erase their personal data. They must also be able to make corrections of erroneous data. Traceability: Any occurrence of processing data must be traceable and linkable to previous occurrences of processing of that data. GDPR is the standard proposed by European Parliament to protect European citizens and return control over private data to the users. However, the legislators in other parts of the world recognize that same need for data control. For instance in China the Standing Committee of the National People\u2019s Congress is already proposing changes similar to the GDPR[2]. Similarly there are laws like COPPA in the U.S. and PIPA in Japan.","title":"Technical Requirements"},{"location":"RWoT5/gdpr/#self-sovereign-identity-as-a-model-for-gdpr","text":"The requirements of the GDPR call for user control of their private information. One model for that is self-sovereign identity, a movement that started before the GDPR was introduced. It grew out of the user-centric identity efforts to support the individual being at the core of their own digital identity. The community is focused on how various sophisticated cryptography and shared distributed ledgers allow for this to actually happen. The community focused on user-centric identity and personal data control has several times produced guiding rules or principles that also align with the vision of GDPR and actually influenced its authors. In 2005 Kim Cameron defined Laws of Identity, In 2010 during the same week while Phil Windley[3] and Kaliya Young[4] each posted created Personal Data Ecosystem principles. Several years ago new term gained traction\u201cself-sovereign identity\u201d. Devon Lefretto first used the term Self-Sovereign Authority within the VRM community in 2011 and in that context worked to articulate its meaning. References to Self-Sovereign Identity appear in 2015 by John Edge founder of ID2020. Devon Lofretto published a post defining the term in February 2016[5]. Christopher Allen circulated ideas about a set of principles and published them in the spring of 2016.[6]. The W3C standardization body has a Verifiable Claims Working Group, which focuses on the API definitions. While Self-Sovereign identity is not necessarily the ideal or full answer to GDPR requirements, it is interesting to analyze it, as one of the possible solutions. Christopher Allen describes 10 principles for a self-sovereign identity system. The person behind the identity needs to exist . Nobody can be fully digital, we need to have a \u201csum\u201d ( cogito ergo sum ) that is the core of who one is. That sum can later be described in a digital form. Every entity should be in control of their identity. One who is described by the identity is the one who is the central authority on that identity. While others can make claims to one\u2019s identity, they should not have control over it. The owner should always be able to access their own data. This is crucial, as without access to the identity, a user has no way to prove or control who they are. While the owner may not be able to modify all the information about themselves (if there is a court ruling attached to a given identity the owner should not be able to change that ruling) but they have to be made aware of all that is created about them. The self-sovereign system requires full transparency on all the algorithms, designs, management and updates. In order to ensure that transparency, the algorithms used should be free, open-source and well described in public. Such systems must be designed to exist forever, or be persistent as long as the owner wants them to be. The protection mechanisms and the implementation details may change over time, but the core identity should last forever. All of these should not prevent the rule of right to be forgotten, that allows users to request removal of any of the claims made to their identity. Thus, the identity must be separate from claims made to it. Following the understanding that the system needs to be available and persistent, the data should be portable . The identity cannot be held by a single entity, in case this body disappears. A transferable architecture allows for a simple method of moving data while keeping it available. Connected to this is the rule of interoperability . All systems should communicate and be available anywhere in the world, not dependent on borders or governments. As users have full control over the system, they should also be able to give their consent to the entities that want to use their identity. The design should be such, that while allowing for full information sharing, it is always done with the agreement of the user. It should also be strongly minimized . When requested to disclose information, the system should give away as little as possible. Finally, the user should always be the center of interest, and his rights should be protected , independently from any third party. While the rules describing self-sovereign identity are theoretical and not necessarily all applicable, most modern systems that aim to define who we are and how we work in the digital world, closely follow the above principles. Let us now look at the technical implementations proposed by various entities.","title":"Self-Sovereign Identity as a Model for GDPR"},{"location":"RWoT5/gdpr/#technical-approaches-to-digital-identity","text":"A few different technical proposals for digital identity have been incubated in conjunctions with the discussions of self-sovereign identity. These may offer strong foundations for any attempts to implement GDPR-compliant data. Some follow the self-sovereign rules defined above, while some only take what is needed. This section describes the two most mature developments. However, it is important to recognize that each suits the use case it is trying to solve. We have yet to see a universal approach acceptable on a global scale.","title":"Technical Approaches to Digital Identity"},{"location":"RWoT5/gdpr/#decentralized-identity-foundation-dif","text":"Self-Sovereign Focus: Control In May 2017, a new movement was started: creating a space for organizations to develop fundamental primitives, protocols and tools for an interoperable ecosystem under the umbrella of Decentralized Identity Foundation[7]. We no longer live in a world which accepts Big Brother approach of storing all information about everyone in one place. However, a huge paradigm shift needs to happen when moving from a centralized registry of objects \u2014 people, devices and entities \u2014 to a decentralized system. This requires development of new specifications, protocols, formats, and implementations for cross-chain rooting, indexing, and resolution of decentralized identifiers and names. Decentralized Identifiers (DIDs) are used as bases for a verifiable, fully self-controlled digital identity. In ideal circumstances, DIDs adhere to the rules of self-sovereignty, not being dependent on any centralized registry, identity provider or certificate authority. To interact with the world, DIDs need to be resolved to DID documents that contain authentication mechanisms, authorization information and service endpoints. With that, a trusted interaction can be started with an entity. Unlike in a traditional model, where identity management system relies on a centralized authority that provides cryptographic trust verification, in a DID world we need a federated identity management[8]. Following the dictums of Privacy by Design[9], each entity may have as many DIDs as necessary, to respect the entity\u2019s desired separation of identities, personas and contexts. Such a decentralized design eliminates the need for registries that collect all the identifiers as well as centralized certificate authorities for key management. Thus, a traditional Public Key Infrastructure is no longer valid. However, it needs to be replaced with a new, DLT-compliant architecture, where each entity can have its own root authority. Such system is called a Decentralized PKI. As interoperability is one of the key focuses of DIF, DID methods that allow for collaboration with centralized or federated systems can be developed and are encouraged as a bridging mechanism. Until now there have been several DID methods proposed by various entities ranging from the Sovrin Foundation, Bitcoin, Ethereum and Verifiable Claims worlds. However, DID is only the way to address the objects; the question is how to store and compute the data. The control and ownership need to be in the hands of the object, and such a system is also part of what DIF is focusing on, through their work on Hub. A Hub is a data store containing semantic data objects at well-known locations. Objects in a Hub are signed by an identity and mapped to semantic data objects through an API. Anyone can address them through a global namespace. The final part of DIF\u2019s work is Chainpoint, which focuses on a scalable protocol to anchor data in the Blockchain and generate receipts. One of the key factors of the future interactions of entities on a Blockchain will be trust and reputation. These can only be built through attestations between groups of individuals and businesses, which can be used make important decisions in personal and business life.","title":"Decentralized Identity Foundation \u2013 DIF"},{"location":"RWoT5/gdpr/#verifiable-claims","text":"Self-Sovereign Focus: Minimization Today we have to reveal the maximum amount of information even when interacting with people who do not need that information. Moreover, it is hard to prove your qualification over the internet: driver\u2019s license, proof of age, education qualification is not easily verifiable. The purpose of verifiable claims is to resolve these twin issues. This is what the W3C Verifiable Claims Working Group[10] is focusing on. 150+ individuals and organizations are now planning how to create, store, transmit and verify digital credentials via the Internet. A Verifiable Claim is defined as an identifier that describes four roles within a single capsule. An Issuer issues verifiable credentials about a specific Subject. The Holder stores credentials on behalf of a Subject. Holders are typically also the Subject of a credential. The Verifier requests a profile of the Subject. A profile, which contains a specific set of credentials. The verifier verifies that the credentials provided in the profile are fit-for-purpose. The Identifier Registry is a mechanism that is used to issue identifiers for Subjects. Why do we need Verifiable Claims? Because one does not need to show their precise age, just that they are above a certain age. That they are capable of driving. That they are eligible to practice as a medical doctor and so on. This minimizes data disclosure, fulfilling one of the precepts of self-sovereign identity while simultaneously reducing liability for a data holder, especially under laws like GDPR.","title":"Verifiable Claims"},{"location":"RWoT5/gdpr/#blockchain-solutions-to-gdpr","text":"Blockchain is a up-and-coming technology for digital identities, and is already in extensive use for DIF and other self-sovereign-focused technologies. It needs to be part of any discussion about GDPR compliance. However, some fear that the Blockchain is the exact opposite of what GDPR requires of enterprises. If we take public, permissionless Blockchain, then indeed everything that happens within such a system is visible and available to anyone. However, the domain of Blockchain technology expands beyond permissionless, public versions. If we take the example of medical records, nobody would find it advisable or feasible to store their data in a public ledger. For such a use case we need a private, permissioned ledger, where only a certain group of people may access the ledger for read and write purposes. Then, there is a middle ground \u2013 between the crypto currency and medical records \u2013 for instance certification. Issuance of such should be limited to those, who are eligible to certify that a given person can hold a given certificate. However, anyone should be able to inquire about the validity of the claim. So, there is space for both permissionless-private, and permissioned-public blockchains. It is our strong belief that there will not be one Blockchain to rule them all, but a spectrum of solutions to choose from, which we summarize in the Figure below. Given such a model, we can compare how GDPR requirements can map into what Blockchain-based solution may offer. First rule is to ensure availability . The user should always have access to their data, no matter if it is stored locally or remotely. The data should be protected from leakages or attacks as that stands against availability. Availability is ensured through the distribution of nodes that hold the same copy of the ledger and create a peer-to-peer network. Even if a certain number of these nodes becomes unavailable, the others still have an identical copy of the same data, meaning that participants can always access the information. Such data may not be leaked to participants outside of the permissioned structure of the Blockchain, which relies on a good governance model and a solution that is built with security in mind. This assumption, however, holds true for most technologies no matter whether they include Blockchain or not. Distributed Ledger Technology (DLT) is a way of ensuring availability, immutability, transparency and lack of reliance on trusted third party. As any innovation, it can only serve as part of a bigger solution that has to be properly architected and designed with all the right principles in mind. Next GDPR requirement is completeness : every event and data have to be recorded. This is exactly what Blockchain and DLT have been designed for. The structure of a chain of blocks connected to one another through cryptographic hashes of previous blocks ensures that any changes in the history will be immediately recognized and flagged as an error. Any event or data is announced to the participants of the network and once they agree on the content of it, it is published on the Blockchain and stored forever. Similarly, the rule of correctness , meaning that there is an assurance of accuracy of data, is guaranteed with DLT. Of course, data always needs to be verified before it is amended to the Blockchain. The sheer fact it exists on the Blockchain, does not mean it is true. However, the participants of the network achieve a consensus over the correctness of the (hopefully) audited data any changes cannot be done to it. This also means that integrity of data is protected. As the GDPR states, the content of the data store should be protected from malicious or unintentional changes. This happens by default in the Blockchain as changes can only be introduced as new inputs to the blocks. If University X publishes incorrectly that Alice has passed an exam with a grade B on a Blockchain, the only way to change that is to introduce a new transaction announcing that the previous entry was incorrect, and that Alice passed the exam with a grade A. Immutability of Blockchain technology has been mentioned several times here and is also a requirement of the GDPR. There are several ways how to achieve confidentiality , meaning that only parties involved in the exchange of data can be able to see the details of the transaction, while still ensuring full transparency, where any occurrence of processing data must be traceable and linkable to previous occurrences of processing of that data. In a setting where we can create permissioned Blockchain and confidential transactions, one can also create channels where only parties involved in the exchange of the information know the content of it, while the outside world acknowledges that such a transaction occurred. This is done in case of Hyperledger Fabric Channels[11] or Hyperledger Sawtooth\u2019s Private UTXO Transaction Family[12]. There are also many more ways to do this, depending on the technology chosen. Transparency , on the other hand, can be implemented by tracking the transactions by their precise hashes and referring to their exact position in a Blockchain, so that any new exchange can clearly build on top of it.","title":"Blockchain Solutions to GDPR"},{"location":"RWoT5/gdpr/#creating-an-off-chain-wallet","text":"One of the big challenges facing enterprises when it comes to complying with GDPR is the user-centric approach to data handling. It is no longer the case that a company can store and be responsible for all the information. Quite the opposite \u2013 it is the users themselves that now need to decide if and how would they like to share their data. Thus, the requirements for: consensual data sharing, where permission can be withdrawn any time; data minimization , where only the minimum amount of data should be requested by a company; and interoperability and portability, where user can combine data from various sources. In the world of traditional databases this may be impossible to solve. However, with decentralized DLT systems, where one does not rely on a trusted third party such a solution no longer creates a problem. Using DIDs and Verifiable Claims, we can create a solution that puts the users in charge and control of their identity, a wallet of attestations and information about them that only they are responsible for. This is what Hyperledger Indy[13] is focusing on: creating a Blockchain so that the identities can be kept in a secure storage and the Blockchain can serve as a common reference to the records, rather than storage for the data itself. By having a single wallet of identity, the data is portable and interoperable. we can ensure revealing minimum data disclosure. As for consent, simple interaction with an entity and recording that transaction counts as a timestamped consent form; no further action is needed. The only way to stay GDPR compliant with immutable records is to avoid uploading person-related data at all. A ledger can only be used for verifiers of person related data. Not even hashing can be used as a method of anonymization, but rather pseudonymization. In fact, logical deletion by this definition can only be achieved by verifiers in the form of ZKPs. It is had to recognize what information will be considered sensitive in the future, and thus, ZKPs seem like the right answer for handling personal data. With the design of off-blockchain wallets that are recorded on a Blockchain, we achieve rectification . If a user changes their data, the anchor to their wallet changes, meaning that they have to update their entry on the Blockchain. They cannot simply rewrite the history: they must make amendments, and inform everyone about the fact that the changes have been made (not necessarily revealing what these changes were).","title":"Creating an Off-Chain Wallet"},{"location":"RWoT5/gdpr/#blockchain-and-erasure","text":"Finally let us move to the most disputed part of the GDPR. Right to erasure , mandates that anyone may request that their data should be deleted from the company's\u2019 servers. In today\u2019s settings the best Alice can do is send an email to company X to delete her data. If an auditor comes in, such email may, or may not disappear and the data may or may not be deleted. If Alice makes that same exchange through a Blockchain, the request is timestamped and must be acknowledged by company X. Then the erasure is also confirmed on a Blockchain or simply executed as a smart contract in the first place when Alice gave access to the data only for limited time. Now if data is found by an auditor at any later point, the company can be held accountable for it in a much stronger way. The same holds for rectification . In case Alice wants to change her personal data, she makes a request which should be recorded on a Blockchain, acknowledged by the company and executed. This of course means non-repudiation . Interaction with any data should not be deniable at a later point in time; the system should ensure that every request for data is recorded on a Blockchain. This however boils down to the architecture of the system, not to the technology itself. Cryptography and data pseudonymization section - an analysis of available cryptographic techniques and way they reflect data de-pseudonymization. Should pseudonymous data encrypted with ZKP for example, be treated as pseudonymous? As long as ZKP are non-interactive and the process of decrypting the data is challenging. Private key to decrypt data and limited MITM interventions with PKI, don\u2019t they make encrypted data almost anonymous? How we should treat such encrypted data? The same with other advanced computational techniques, like Secure Multiparty Computation, is the data truly pseudonymous? Or closer to anonymous. I assume it would add great value to the paper.","title":"Blockchain and Erasure"},{"location":"RWoT5/gdpr/#summary","text":"Today there are many third-party entities who collect and sell user data without permission. However, violation of privacy seems to be most rampant within the digital advertising industry. Some American companies think GDPR won\u2019t apply to them, but this European legislation will have truly global impact. The Drum^[14]^ describes how GDPR applies to digital marketers in the United States. The most relevant requirements include requiring consent for data processing, anonymizing collected data to protect privacy, which can be easily done with DIDs, providing data breach notifications, safely handling the transfer of data across borders and finally requiring certain companies to have a data protection officer to oversee GDPR compliance. The movement towards proactive introduction of GDPR-compliant standards has already started. A good example are media buying agencies and their demand-side platforms like mParticle.^[15]^ A PwC Pulse Survey^[16]^ that studied how much US Companies are spending on GDPR compliance showed that over half of US multinationals say GDPR is their top data-protection priority, and information security enhancement is a top GDPR initiative. As binding corporate rules are gaining popularity, 77% plan to spend \\$1 million or more on GDPR. On a final note, in the world of Blockchain the responsibilities of companies for the data they store, and process are much lower. It can be a fear, that the data herding that is happening now, will no longer be possible. On the other hand users will be fully responsible to handle their data and incentivized to work with companies to share it with them. Why? By giving access to their data they will still want to get better recommendations and better services. Only now, with well-designed Blockchain based solutions, these transactions can happen in a way that protects both parties and is accessible. Some would argue that audience data belongs to publishers and their supply-side platforms, but projects like Tor, Brave, Ghostery, Evidon and many others give users control. Perhaps viewers will gain the power to monetize their own identities. For example, anonymously sharing a device\u2019s location drastically improves relevance of recommended content and/or services. Users might opt in for discounts and other sales promotions from local businesses.","title":"Summary"},{"location":"RWoT5/gdpr/#endnotes","text":"[1] http://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32016R0679&from=EN#d1e3265-1-1 [2] https://www.chinalawinsight.com/2018/02/articles/corporate/antitrust-competition/the-wise-and-informed-adapts-to-the-changing-time-and-circumstances-discussing-the-issues-on-information-technology-personal-information-security-specification-from-a-pract/ [3] http://www.windley.com/archives/2010/09/pdx_principles.shtml [4] https://identitywoman.net/vision-principles-for-the-personal-data-ecosystem/ [5] https://www.moxytongue.com/2016/02/self-sovereign-identity.html [6] http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html [7] https://medium.com/decentralized-identity/the-rising-tide-of-decentralized-identity-2e163e4ec663 [8] https://en.wikipedia.org/wiki/Federated_identity [9] https://en.wikipedia.org/wiki/Privacy_by_design [10] https://www.w3.org/2017/vc/WG/ [11] http://hyperledger-fabric.readthedocs.io/en/release/channels.html [12] https://sawtooth.hyperledger.org/docs/core/nightly/0-8/examples/private_utxo/private_utxo_transaction_family.html [13] https://www.hyperledger.org/projects/hyperledger-indy [14] http://www.thedrum.com/news/2017/10/05/what-does-the-eu-s-privacy-reform-mean-us-marketers-and-what-should-you-do-now [15] https://www.mparticle.com/blog/what-gdpr-means-for-data-driven-marketing [16] http://www.pwc.com/us/en/increasing-it-effectiveness/publications/gdpr-readiness.html","title":"Endnotes"},{"location":"RWoT5/identity-hubs-capabilities-perspective/","text":"Identity Hubs Capabilities Perspective by Adrian Gropper, Drummond Reed, Mark S. Miller Abstract Identity Hubs as currently proposed in the Decentralized Identity Foundation (DIF) are a subset of a general Decentralized Identifier (DID) based user-controlled agent, based on ACLs rather than an object-capabilities (ocap) architecture. The current approach has both security and scalability issues. Transitioning the Hubs design to an ocap model can be achieved by introducing an UMA authorization server as the control endpoint. This avoids creating confused-deputy security issues and expands scale by enabling the hub to delegate access to resources not stored in the hub itself. Security and Privacy Impact Assessment Control over personal data is becoming more important as networking and storage costs become negligible compared to the value of our data itself. A prominent example of this is the EU General Data Protection Regulations (GDPR) and Payment Services Directive (PSD2), which mandate sweeping new responsibilities for transparency and control to all enterprises that hold personal data. In general, personal data fits into three distinct categories: Data generated by a service or device (e.g., a lab test of my anonymous blood sample or location streamed by my mobile device) Data aggregated by a service (e.g., a credit reporting enterprise) Data aggregated by a fiduciary or self-sovereign agent (e.g., a personal server that I own or a secure mobile wallet) Privacy threads through all aspects of access to personal data for: the subject (we ignore guardians in this short paper) of the personal data; and the parties seeking access to the personal data about the subject. A scalable personal data architecture will guide the implementation of secure and privacy-preserving services without the costs and risks of unwanted aggregation. Self-Sovereign Identity The scale of a personal data architecture is limited by federation as a root of trust. Federations work best when applied to entities of similar size and scope. Banks can form successful federations of ATMs. Professional or semi-professional sports teams can federate, separately, to form leagues. But a federation related to identity could span the full range of human, machine, and institutional entity activity across countries, cultures, professions, and risks. A standardized self-sovereign identifier (e.g., a DID) uses distributed public ledgers as a source of trust while retaining the option, on a person-by-person basis, to leverage federated identities or not. Self-sovereign identifiers and verifiable credentials are useful for both the subject and the requesting party. Both can benefit from self-sovereign mobile wallets with local biometric protections and from the ability to sign documents in a legally non-repudiable manner. Self-sovereign identifiers serve as the root of trust for the subject\u2019s agent and for claims presented by a requesting party. Self-Sovereign Agent The essential component of the agent is the ability to authorize access to the subject\u2019s resources by requesting parties. As such, it secures the subject\u2019s policies without actually sharing the policies. By way of analogy, cryptographers design secure elements to protect private keys from having to be shared while still being effective in the actions they control. The execution of arbitrary code as part of a secure element is core to the general capability to issue authorization decisions while also keeping secret the policies that led to the decision. To the extent that an agent also hosts a database holding data about the subject that can be shared with requesting parties, access to that data is also under the control of the authorization server. Implementation Example Preamble: Subject Alice goes to have four blood tests at a resource service Lab; they will be made available, in the future, under her control to various requesting parties. One of these requesting parties is Bob. Bob has verifiable credentials as a licensed MD and a member of his temple. Alice controls an authorization server (AS). Alice presets a policy in her AS that any MD can access anything other than \u201cSensitive\u201d tests. This happens only once and applies to many transactions with many service providers. This can be inherited along with the code for the AS from a source that Alice trusts. Alice registers in-person at Lab and provides a blood sample and a pointer to her AS endpoint. The Lab registers four tests as four different resources with the AS. One of the resources has metadata STD. This happens only once regardless of how many times Alice gets lab tests at Lab. Time passes... Bob, using out-of-band directory interactions or a referral from someone, is given a pointer to Alice@Lab resources. Bob may or may not know that there are four of them at Lab. Bob goes to Lab to request the resources. He does not identify himself using any traditional forms of identification. Lab looks up Alice\u2019s AS endpoint and gives Bob, whoever he is, a pointer to the AS along with a ticket that captures the context of the transaction for later reference. Bob authenticates to Alice\u2019s AS and presents the MD credentials but not his temple membership. The context ticket says he\u2019s asking for three of the four results. Alice\u2019s AS consults the policies and the metadata associated with the context ticket (as registered, or dynamically at the time of the request) and determines that one of the three tests is Sensitive. In some cases, metadata shows up that the AS does not understand and Alice herself may need to be consulted; otherwise the process is automatic once Bob has submitted the ticket and the claims. The AS issues to Bob\u2019s software client a token granting access to two of the three tests he requested. The token can be associated with OAuth refresh so that Bob will not need to re-authenticate for a year. Bob\u2019s client presents the token to the Lab. The token can encode the permission for the two tests or the Lab can consult the AS to determine the scope of access associated with the token. The Lab exposes two of the four tests to Bob\u2019s client. Note that Alice can choose to move some or all of the Lab data to an aggregator because she considers the Lab to be unreliable beyond a certain time. If she does that, Alice will need to ensure that the lab test retains provenance to Bob\u2019s satisfaction and that represents a significant added cost. Although delegating access control to an external AS is an added cost to the Lab, it is also a feature that attracts Alice as a customer and it reduces their legal liability under GDPR. Note that Alice and Bob both have the ability to attenuate the scope of disclosure about them. The only single point of failure is Alice\u2019s AS, which must be sufficiently reliable. Alice has a single point of control for all of her participating services, which is a major convenience. Alice can have as many AS endpoints as she wants separate personas. To avoid unwarranted correlation, the AS endpoint can go through a mixer or hub. Token-based access control to RESTful APIs has been standardized as OAuth2. The UMA standard builds on top of OAuth2 to enable the separation of the authorization server from the resource server. The subject authenticates into the authorization server of the agent in order to create or modify policies and to manage the code that uses policies to respond to requesting parties. The requesting parties bring tickets to the authorization server that represent the context of a resource access request along with whatever credentials the requesting party chooses to expose to the authorization server. The requesting party then receives a limited scope access token to the resource regardless of whether the resource is coresident in the agent or hosted by an independent enterprise. Proposal to DIF We propose an update of the DIF Identity Hub to specify OAuth2 and UMA as the protection mechanisms of the hub. That would be followed by a privacy impact assessment of all aspects of the Identity Hub specification including specific reference to GDPR and PSD2 compliance. References ACLs Don\u2019t http://www.hpl.hp.com/techreports/2009/HPL-2009-20.pdf UMA https://kantarainitiative.org/confluence/display/uma/Home DIF Hubs https://github.com/decentralized-identity/hubs/blob/master/explainer.md","title":"Identity Hubs Capabilities Perspective"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#identity-hubs-capabilities-perspective","text":"","title":"Identity Hubs Capabilities Perspective"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#by-adrian-gropper-drummond-reed-mark-s-miller","text":"","title":"by Adrian Gropper, Drummond Reed, Mark S. Miller"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#abstract","text":"Identity Hubs as currently proposed in the Decentralized Identity Foundation (DIF) are a subset of a general Decentralized Identifier (DID) based user-controlled agent, based on ACLs rather than an object-capabilities (ocap) architecture. The current approach has both security and scalability issues. Transitioning the Hubs design to an ocap model can be achieved by introducing an UMA authorization server as the control endpoint. This avoids creating confused-deputy security issues and expands scale by enabling the hub to delegate access to resources not stored in the hub itself.","title":"Abstract"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#security-and-privacy-impact-assessment","text":"Control over personal data is becoming more important as networking and storage costs become negligible compared to the value of our data itself. A prominent example of this is the EU General Data Protection Regulations (GDPR) and Payment Services Directive (PSD2), which mandate sweeping new responsibilities for transparency and control to all enterprises that hold personal data. In general, personal data fits into three distinct categories: Data generated by a service or device (e.g., a lab test of my anonymous blood sample or location streamed by my mobile device) Data aggregated by a service (e.g., a credit reporting enterprise) Data aggregated by a fiduciary or self-sovereign agent (e.g., a personal server that I own or a secure mobile wallet) Privacy threads through all aspects of access to personal data for: the subject (we ignore guardians in this short paper) of the personal data; and the parties seeking access to the personal data about the subject. A scalable personal data architecture will guide the implementation of secure and privacy-preserving services without the costs and risks of unwanted aggregation.","title":"Security and Privacy Impact Assessment"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#self-sovereign-identity","text":"The scale of a personal data architecture is limited by federation as a root of trust. Federations work best when applied to entities of similar size and scope. Banks can form successful federations of ATMs. Professional or semi-professional sports teams can federate, separately, to form leagues. But a federation related to identity could span the full range of human, machine, and institutional entity activity across countries, cultures, professions, and risks. A standardized self-sovereign identifier (e.g., a DID) uses distributed public ledgers as a source of trust while retaining the option, on a person-by-person basis, to leverage federated identities or not. Self-sovereign identifiers and verifiable credentials are useful for both the subject and the requesting party. Both can benefit from self-sovereign mobile wallets with local biometric protections and from the ability to sign documents in a legally non-repudiable manner. Self-sovereign identifiers serve as the root of trust for the subject\u2019s agent and for claims presented by a requesting party.","title":"Self-Sovereign Identity"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#self-sovereign-agent","text":"The essential component of the agent is the ability to authorize access to the subject\u2019s resources by requesting parties. As such, it secures the subject\u2019s policies without actually sharing the policies. By way of analogy, cryptographers design secure elements to protect private keys from having to be shared while still being effective in the actions they control. The execution of arbitrary code as part of a secure element is core to the general capability to issue authorization decisions while also keeping secret the policies that led to the decision. To the extent that an agent also hosts a database holding data about the subject that can be shared with requesting parties, access to that data is also under the control of the authorization server.","title":"Self-Sovereign Agent"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#implementation-example","text":"Preamble: Subject Alice goes to have four blood tests at a resource service Lab; they will be made available, in the future, under her control to various requesting parties. One of these requesting parties is Bob. Bob has verifiable credentials as a licensed MD and a member of his temple. Alice controls an authorization server (AS). Alice presets a policy in her AS that any MD can access anything other than \u201cSensitive\u201d tests. This happens only once and applies to many transactions with many service providers. This can be inherited along with the code for the AS from a source that Alice trusts. Alice registers in-person at Lab and provides a blood sample and a pointer to her AS endpoint. The Lab registers four tests as four different resources with the AS. One of the resources has metadata STD. This happens only once regardless of how many times Alice gets lab tests at Lab. Time passes... Bob, using out-of-band directory interactions or a referral from someone, is given a pointer to Alice@Lab resources. Bob may or may not know that there are four of them at Lab. Bob goes to Lab to request the resources. He does not identify himself using any traditional forms of identification. Lab looks up Alice\u2019s AS endpoint and gives Bob, whoever he is, a pointer to the AS along with a ticket that captures the context of the transaction for later reference. Bob authenticates to Alice\u2019s AS and presents the MD credentials but not his temple membership. The context ticket says he\u2019s asking for three of the four results. Alice\u2019s AS consults the policies and the metadata associated with the context ticket (as registered, or dynamically at the time of the request) and determines that one of the three tests is Sensitive. In some cases, metadata shows up that the AS does not understand and Alice herself may need to be consulted; otherwise the process is automatic once Bob has submitted the ticket and the claims. The AS issues to Bob\u2019s software client a token granting access to two of the three tests he requested. The token can be associated with OAuth refresh so that Bob will not need to re-authenticate for a year. Bob\u2019s client presents the token to the Lab. The token can encode the permission for the two tests or the Lab can consult the AS to determine the scope of access associated with the token. The Lab exposes two of the four tests to Bob\u2019s client. Note that Alice can choose to move some or all of the Lab data to an aggregator because she considers the Lab to be unreliable beyond a certain time. If she does that, Alice will need to ensure that the lab test retains provenance to Bob\u2019s satisfaction and that represents a significant added cost. Although delegating access control to an external AS is an added cost to the Lab, it is also a feature that attracts Alice as a customer and it reduces their legal liability under GDPR. Note that Alice and Bob both have the ability to attenuate the scope of disclosure about them. The only single point of failure is Alice\u2019s AS, which must be sufficiently reliable. Alice has a single point of control for all of her participating services, which is a major convenience. Alice can have as many AS endpoints as she wants separate personas. To avoid unwarranted correlation, the AS endpoint can go through a mixer or hub. Token-based access control to RESTful APIs has been standardized as OAuth2. The UMA standard builds on top of OAuth2 to enable the separation of the authorization server from the resource server. The subject authenticates into the authorization server of the agent in order to create or modify policies and to manage the code that uses policies to respond to requesting parties. The requesting parties bring tickets to the authorization server that represent the context of a resource access request along with whatever credentials the requesting party chooses to expose to the authorization server. The requesting party then receives a limited scope access token to the resource regardless of whether the resource is coresident in the agent or hosted by an independent enterprise.","title":"Implementation Example"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#proposal-to-dif","text":"We propose an update of the DIF Identity Hub to specify OAuth2 and UMA as the protection mechanisms of the hub. That would be followed by a privacy impact assessment of all aspects of the Identity Hub specification including specific reference to GDPR and PSD2 compliance.","title":"Proposal to DIF"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#references","text":"","title":"References"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#acls-dont","text":"http://www.hpl.hp.com/techreports/2009/HPL-2009-20.pdf","title":"ACLs Don\u2019t"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#uma","text":"https://kantarainitiative.org/confluence/display/uma/Home","title":"UMA"},{"location":"RWoT5/identity-hubs-capabilities-perspective/#dif-hubs","text":"https://github.com/decentralized-identity/hubs/blob/master/explainer.md","title":"DIF Hubs"},{"location":"RWoT5/lds-ocap/","text":"Linked Data Capabilities By Christopher Lemmer Webber and Mark S. Miller Overview Linked Data Signatures enable a method of asserting the integrity of linked data documents that are passed throughout the web. The object capability model is a powerful system for ensuring the security of computing systems. In this paper, we explore layering an object capability model on top of Linked Data Signatures via chains of signed proclamations. fn:1 We call this system \"Linked Data Capabilities\", or \"ld-ocap\" for short. The system we propose can work regardless of whether https identifiers or DIDs are being used. Since DIDs work nicely with this system and add an additional layer of decentralization we use them for the URIs of this system. Example Scenario Alice (A) has a direct capability to store files in a \"Cloud Storage\" system (C). She would like to share this capability with Bob (B), but she is wary of Bob's fondness of storing high-resolution video, so she would like to add a constraint that he may only upload files that are no larger than 50 megabytes at a time. Bob is excited to take advantage of this service because he has recently been playing with Dummy Bot (D), which automatically uploads some photos now and then. But Bob has heard mixed reviews of Dummy Bot and is worried that maybe Dummy Bot will malfunction. He has decided that a 30-day window is a sufficient trial period for permitting Dummy Bot to upload to the storage system, so that he can determine whether to renew at some future date. The initial condition looks like this: .-. .-. .-. ( A )---->( B )---->( D ) '-' '-' '-' \\ \\ \\ \\ .-. '->( C ) '-' (A)lice has a capability to the (C)loud storage system through which she can upload files. (A)lice also has a capability to send a message to (B)ob, and (B)ob has a capability to send a message to (D)ummy Bot. Each of these characters has an associated linked data document that represents them within the system, making use of JSON-LD and Linked Data Signatures . Here is Alice: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], // This is a DID, but it could as well be an https: uri \"id\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab\", // This object is a person named Alice \"type\": \"Person\", \"name\": \"Alice\", // Finally, a signature verification key Alice will be using // for her upload capability to the Cloud Storage system \"publicKey\": [{ // This has its own separate id because it is technically // a separate document \"id\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab#key-1\", \"owner\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} Here is Bob: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66\", \"type\": \"Person\", \"name\": \"Bob\", \"publicKey\": [{ \"id\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66#key-1\", \"owner\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} Here is Dummy Bot: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4\", \"type\": \"Service\", \"name\": \"Dummy Bot\", \"publicKey\": [{ \"id\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4#key-1\", \"owner\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} Finally, here is the Cloud Storage service: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741\", \"type\": \"Service\", \"name\": \"Cloud Storage Pro\", \"publicKey\": [{ \"id\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741#key-1\", \"owner\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} Alice's capability to store an object in the Cloud Store is encoded in a proclamation, which looks like this: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:0b36c7844941b61b-c763-4617-94de-cf5c539041f1\", \"type\": \"Proclamation\", // The subject is who the capability operates on (in this case, // the Cloud Store object) \"subject\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741\", // We are granting access specifically to one of Alice's keys \"grantedKey\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab#key-1\", // No caveats on this capability... Alice has full access \"caveat\": [], // Finally we sign this object with one of the CloudStorage's keys \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741#key-1\", \"signatureValue\": \"IOmA4R7TfhkYTYW8...CBMq2/gi25s=\"}} Now Alice wants to share this capability to Bob, but with a couple of caveats (also known as an \"attenuation\"): Bob can only invoke the upload method, and can only upload 50 Megabyte files at a time. {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:f7412b9a-854b-47ab-806b-3ac736cc7cda\", \"type\": \"Proclamation\", // This new attenuated proclamation points to the previous one \"parent\": \"did:example:0b36c7844941b61b-c763-4617-94de-cf5c539041f1\", // Now we grant access to one of Bob's keys \"grantedKey\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66#key-1\", // This proclamation *does* have caveats: \"caveat\": [ // Only the UploadFile method is allowed... {\"id\": \"did:example:f7412b9a-854b-47ab-806b-3ac736cc7cda#caveats/upload-only\", \"type\": \"RestrictToMethod\", \"method\": \"UploadFile\"}, // ...and each upload can only be 50 Megabytes large. {\"id\": \"did:example:f7412b9a-854b-47ab-806b-3ac736cc7cda#caveats/50-megs-only\", \"type\": \"RestrictUploadSize\", // file limit here is in bytes, so 50 MB \"limit\": 52428800}], // Finally we sign this object with Alice's key \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab#key-1\", \"signatureValue\": \"...\"}} As this diagram demonstrates, Alice has created, and has access to, this attenuated capability. .-. .-. .-. ( A )------>( B )----->( D ) '-'\\ '-' '-' \\ \\ \\ \\ \\ '--->(R1) \\ \\ \\ .-. '->( C ) '-' Bob cannot use this capability until he receives it. Alice invokes her message sending capability between herself and Bob. .-. __ .-. .-. ( A )-[##\\->( B )----->( D ) '-'\\ '--/ '-' '-' \\ \\ | \\ \\ V \\ '--->(R1) \\ | \\ V \\ .-. '->( C ) '-' Now Bob has access to upload files sized 50MB or less to the Cloud Store. But he would prefer that Dummy Bot do uploads for him... well, for a month. He'll see how it goes. Luckily these capabilities are composable, and so Bob can create an attenuated capability out of the attenuated capability he already has! {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:d2c83c43-878a-4c01-984f-b2f57932ce5f\", \"type\": \"Proclamation\", // Yet again, point up the chain... \"parent\": \"did:example:f7412b9a-854b-47ab-806b-3ac736cc7cda\", // Now we grant access to one of Dummy Bot's keys \"grantedKey\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4#key-1\", // We add a new caveat/attenuation: this one will expire 30 days // in the future \"caveat\": [ {\"id\": \"did:example:d2c83c43-878a-4c01-984f-b2f57932ce5f#caveats/expire-time\", \"type\": \"ExpireTime\", \"date\": \"2017-09-23T20:21:34Z\"}], // Finally we sign this object with Bob's key \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T17:12:28Z\", \"creator\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66#key-1\", \"signatureValue\": \"...\"}} The capability graph now looks like this: .-. .-. .-. ( A )------->( B )------>( D ) '-'\\ '-' '-' \\ \\ | \\ \\ \\ V '--v \\ '---->(R1)<--(R2) \\ | \\ V \\ .-. '-->( C ) '-' Bob invokes his message sending capability to send the new attenuated capability to Dummy Bot: .-. .-. __ .-. ( A )------->( B )--[##\\-->( D ) '-'\\ '-' '--/ '-' \\ \\ | \\ | \\ \\ V '---v | \\ '---->(R1)<---(R2)<---' \\ | \\ V \\ .-. '-->( C ) '-' Now Dummy Bot has a capability to upload files to Cloud Store, but only files that are sized 50 megabytes or less, and only for the next month. These multiple caveats are possible because Dummy Bot is authorized on the final proclamation, and the proclamation \"chains upward\", including both the immediate restriction/caveat within R2 on time and also the restriction/caveat in R1 on space! .---------. .---------. V | V | __________ | __________ | __________ ( (_) | ( (_) | ( (_) ' \\ '- ' \\ '- ' \\ ) CRT1 ) ) CRT2 ) ) CRT3 ) ' ; ' ; ' ; (________(_) (________(_) (________(_) Soon Dummy Bot takes a picture and uploads it: .-. .-. .-. ( A )------->( B )-------------->( D ) '-'\\ '-' '-' \\ \\ | \\_______ | \\ \\ V __ V __ | \\ '---->(R1)<-/##]-(R2)<-/##]-' \\ |_ \\--' \\--' \\ |##| \\ \\/ \\ | \\ V \\ .-. >( C ) '-' This is done through an Invocation on the proclamation, containing additional parameters in the body: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:2bdf6273-a52e-4cdf-991f-b5f000008829\", \"type\": \"Invocation\", // Dummy Bot is invoking the proclamation it has, // but the whole chain will be checked for attenuation and // verification of access \"proclamation\": \"did:example:d2c83c43-878a-4c01-984f-b2f57932ce5f\", // The method being used \"method\": \"UploadFile\", // The key Dummy Bot is using in this invocation \"usingKey\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4#key-1\", // Here's the base64 encoded file as part of the payload \"file\": \"nEOSQ7jbzBNg0Glup/FfeGDDzvLDvgEL36wcNpmbvKDgPy6+...\", // Finally we sign this object with Dummy Bot's key \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T17:13:48Z\", \"creator\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4#key-1\", \"signatureValue\": \"...\"}} Related work SPKI/SDSI SPKI (and previously SDSI) is a key-management project that aimed to resolve many of the issues (including those surrounding centralization) that the X.509 infrastructure introduced and developed into over time. SPKI is almost but not quite an object capability system . (See From Capabilities To Financial Instruments and Capability Myths Demolished for more information.) SPKI uses \"certificates\" (akin to \"proclamations\" fn:1 here) to express authority, similar to what we are doing in this document, but did not exist in a linked data system as this proposal does. Importantly, SPKI's authority is a broader form of access control and for that reason carries some of the traditional problems of ACLs. Macaroons Macaroons are a credentials system that uphold most of the properties of capabilities. They also support delegation and attenuation (with some constraints as to who can attenuate) via a chain of signed messages, but there are some key differences. The biggest advantage of Macaroons over our design is that messages are smaller (a desirable property!) because a simple HMAC is used for signing rather than public key cryptography. Macaroons are thus passed around as bearer instruments over secure channels. This leads to a tradeoff: macaroons are smaller in size than Linked Data Capabilities, but unlike Linked Data Capabilities, cannot be sent or invoked over an insecure channel. Unlike Linked Data Capabilities, macaroons cannot be stored on a blockchain or be publicly retrievable from the web. One further difference is that while any entity that holds on to a macaroon may delegate that macaroon to any other entity, not all entities can attenuate macaroons. To see why, let us look at our final configuration between Alice, Bob, Dummy Bot, and Cloud Store: .-. .-. .-. ( A )------->( B )-------->( D ) '-'\\ '-' '-' \\ \\ | \\ | \\ \\ V '---v | \\ '---->(R1)<---(R2)<---' \\ | \\ V \\ .-. '-->( C ) '-' In this configuration, Alice was able to attenuate her capability to Cloud Store before delegating to Bob without any specific permission to do so; Bob was likewise able to attenuate the attenuated capability he held before passing to Dummy Bot without any specific permission. In Macaroons, Cloud Store and Alice must pre-arrange the shared key that Alice will use to attenuate the macaroon she holds before she can do so and successfully delegate to Bob (likewise for Bob to Dummy Bot). The reason for this is that in verifying HMAC signatures Cloud Store must check the macaroon's signatures against a key that Alice and Cloud Store must both have... Alice to sign it and Cloud Store to verify it. Even if Alice and Cloud Store had prearranged a shared key to be used for attenuating macaroons, if Bob and Cloud Store had not done so there would be no way for Bob to further attenuate the capability before passing to Dummy Bot. Bob may not prefer this to be the case since Bob wanted to only give Dummy Bot access for thirty days. (Notably, the Macaroons paper contains a short but underspecified section that outlines how Macaroons could be used with public keys instead of HMAC-signed bearer instruments; the design described, while scantly detailed, sounds very similar to how Linked Data Capabilities work.) Overall Macaroons and lds-ocaps are both reasonable systems with different tradeoffs. Implementers should be informed of these tradeoffs and make decisions accordingly. Object Capability Programming Languages Up until this point this paper has focused on different substrates on which to implement capabilities, which have all relied on some sort of shared vocabulary between entities in the system. Another way to build capabilities is to build them at the layer of a programming language. In addition to not requiring coordination on vocabulary from all entities in the system, this provides powerful compositional abilities which, as we will see, turn out to be highly desirable. In the W7 Security Kernel , Jonathan Rees introduces an implementation of object capabilities on nothing other than a strict lexically scoped environment, enforced by the runtime of the system. The example language uses a cut-down variant of Scheme, though it could be implemented in any language that provides the same strict lexical scoping properties in a carefully bounded initial environment. (This is the general mechanism for implementing capabilities at a programming language level.) The paper demonstrates all the same properties of capabilities demonstrated here: delegation, attenuation, and so on. However, there is one thing that is possible in W7 (and other similar systems) that is not possible in any of the other systems discussed in this paper, including the lds-ocaps system herein proposed. This is attenuation by composition in an enclosed environment. To see what this means and why it is desirable, consider this example. We have the following initial state: A: Alice C: Cloud Store H: Home Directory T: Timer Service .-. ( H ) '-' ^ / .-./ .-. ( A )------->( T ) '-'\\ '-' \\ v .-. ( C ) '-' (A)lice keeps her data in (H)ome Directory. She would like to back it up to (C)loud Service, but she is afraid she will forget to back up regularly, so she would like to grant a capability to (T)imer Service to run the backup for her. However, she would prefer that Timer Service not have access to actually read any of the contents of her data on Home Directory, and she does not want Timer Service to be able to write just anything to Cloud Store, only backups. Effectively she would like to send Timer Service a new capability that composes together reading from Home Directory and writing to Cloud Store without giving access to either independently. Here R represents the restricted-through-composition capability: .-. ( H ) '-'<. ^ | / _->(R)<-. .-./.' | .-. ( A )------->( T ) '-'\\ ; '-' \\ . v v .-. ( C ) '-' In Rees' W7 / lambda-calculus-ocap system, this could be represented as: ;; Run in A's environment (timer-run-every ; T (lambda () ; R (write-cloud-image ; C (get-homedir-image))) ; H (* 60 60 24 7)) ; run every 604800 seconds, or once a week The advantage here is that the runtime is able to enclose the capabilities and handle the composition of passing the returned value of one of the enclosed capabilities to the other, without exposing either individually, outside of the enclosure. It does not appear that lds-ocaps can do the same thing. Here is a highly cut down invocation that attempts to embed the capabilities, for the sake of demonstration: {\"type\": \"Invocation\", \"usingKey\": <alice-key-1>, \"method\": \"RunEvery\", \"proclamation\": <cert-id>, \"secs\": 30, /* But we would also need to clearly express how to combine these */ \"runTheseCombinedSomehow\": [ <alice-grants-timer-capability-to-homedir>, <alice-grants-timer-capability-to-cloudstore>], \"signature\": <signed-by-alice-key-1>} There are two troubles here: making it clear how Timer Service is supposed to compose these capabilities in runTheseCombinedSomehow is not obvious, and even worse, there is nothing preventing Timer Service from running these individually since they are not properly enclosed, unlike in the W7/Scheme example. The majority of needs for a capability system are likely served by attenuation and delegation on their own. Nonetheless, full composability within a capability's enclosure, as explored above, is still a desirable property for the systems that can provide it. CapCert CapCert is a (currently unimplemented) plan for a proclamation/certificate chain-based structure that looks a lot like the system discussed in this paper with one interesting change: real programs may be embedded in the proclamations. This approach bridges the gap between the proclamation-chain approach described in this paper and the object-capability programming languages described in the previous section; proclamations can be shared over insecure channels while also removing some need for shared vocabulary on both ends. Even more excitingly, the kinds of composition we do not have but would like to have would be possible, such as the example given above of Alice allowing a Timer Service to back up her Home Directory to Cloud Store, without giving Timer Service access to either independently. It would be possible to build such a system with Linked Data Capabilities by embedding an object capability programming language (with proper constraints on space and time for safety as well). This is a significant topic worth its own future paper. Capabilities on Blockchains Finally, one piece of related work that we have not addressed but would like to address in a future paper is enabling capabilities on blockchains. Motivating examples include attacks against Etherium smart contracts that would not have occurred in an object capabilities environment. The examples provided here demonstrate capabilities that may exist in environments where the only secrets that must be kept are the private keys of entities participating in the system. We would like objects committed to a blockchain to be able to express capabilities despite not being able to hold secrets on the blockchain itself. This is also a significant topic worth its own future paper. Conclusions Linked Data Systems are powerful ways to build collaborative, expressive systems. Today we are seeing Linked Data Systems crossing not only the traditional web but even into systems like distributed ledger technologies and so on. Unfortunately, security is frequently difficult on Linked Data Systems. For example, SoLiD directly uses and ActivityPub indirectly implies Access Control Lists. Unfortunately these are are known to create problems in systems , particularly: excess authority leading to needless vulnerability ambient authority leading to confused deputy problems lack of composability (including attenuation) We can avoid these risks by using an object capability system such as the one described above. Even more exciting, by combining this system with DIDs we can build a fully decentralized object capability system for the web that is safe to use. Note: what we are calling \"proclamations\" have also been called \"certificates\" in previous work such as SPKI and CapCert, we have chosen the name \"proclamation chain\" to make clear that the structure we are proposing holds none of the centralization traditionally associated with \"certificate authorities\".","title":"Linked Data Capabilities"},{"location":"RWoT5/lds-ocap/#linked-data-capabilities","text":"By Christopher Lemmer Webber and Mark S. Miller","title":"Linked Data Capabilities"},{"location":"RWoT5/lds-ocap/#overview","text":"Linked Data Signatures enable a method of asserting the integrity of linked data documents that are passed throughout the web. The object capability model is a powerful system for ensuring the security of computing systems. In this paper, we explore layering an object capability model on top of Linked Data Signatures via chains of signed proclamations. fn:1 We call this system \"Linked Data Capabilities\", or \"ld-ocap\" for short. The system we propose can work regardless of whether https identifiers or DIDs are being used. Since DIDs work nicely with this system and add an additional layer of decentralization we use them for the URIs of this system.","title":"Overview"},{"location":"RWoT5/lds-ocap/#example-scenario","text":"Alice (A) has a direct capability to store files in a \"Cloud Storage\" system (C). She would like to share this capability with Bob (B), but she is wary of Bob's fondness of storing high-resolution video, so she would like to add a constraint that he may only upload files that are no larger than 50 megabytes at a time. Bob is excited to take advantage of this service because he has recently been playing with Dummy Bot (D), which automatically uploads some photos now and then. But Bob has heard mixed reviews of Dummy Bot and is worried that maybe Dummy Bot will malfunction. He has decided that a 30-day window is a sufficient trial period for permitting Dummy Bot to upload to the storage system, so that he can determine whether to renew at some future date. The initial condition looks like this: .-. .-. .-. ( A )---->( B )---->( D ) '-' '-' '-' \\ \\ \\ \\ .-. '->( C ) '-' (A)lice has a capability to the (C)loud storage system through which she can upload files. (A)lice also has a capability to send a message to (B)ob, and (B)ob has a capability to send a message to (D)ummy Bot. Each of these characters has an associated linked data document that represents them within the system, making use of JSON-LD and Linked Data Signatures . Here is Alice: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], // This is a DID, but it could as well be an https: uri \"id\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab\", // This object is a person named Alice \"type\": \"Person\", \"name\": \"Alice\", // Finally, a signature verification key Alice will be using // for her upload capability to the Cloud Storage system \"publicKey\": [{ // This has its own separate id because it is technically // a separate document \"id\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab#key-1\", \"owner\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} Here is Bob: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66\", \"type\": \"Person\", \"name\": \"Bob\", \"publicKey\": [{ \"id\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66#key-1\", \"owner\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} Here is Dummy Bot: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4\", \"type\": \"Service\", \"name\": \"Dummy Bot\", \"publicKey\": [{ \"id\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4#key-1\", \"owner\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} Finally, here is the Cloud Storage service: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741\", \"type\": \"Service\", \"name\": \"Cloud Storage Pro\", \"publicKey\": [{ \"id\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741#key-1\", \"owner\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} Alice's capability to store an object in the Cloud Store is encoded in a proclamation, which looks like this: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:0b36c7844941b61b-c763-4617-94de-cf5c539041f1\", \"type\": \"Proclamation\", // The subject is who the capability operates on (in this case, // the Cloud Store object) \"subject\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741\", // We are granting access specifically to one of Alice's keys \"grantedKey\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab#key-1\", // No caveats on this capability... Alice has full access \"caveat\": [], // Finally we sign this object with one of the CloudStorage's keys \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:0b36c784-f9f4-4c1e-b76c-d821a4b32741#key-1\", \"signatureValue\": \"IOmA4R7TfhkYTYW8...CBMq2/gi25s=\"}} Now Alice wants to share this capability to Bob, but with a couple of caveats (also known as an \"attenuation\"): Bob can only invoke the upload method, and can only upload 50 Megabyte files at a time. {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:f7412b9a-854b-47ab-806b-3ac736cc7cda\", \"type\": \"Proclamation\", // This new attenuated proclamation points to the previous one \"parent\": \"did:example:0b36c7844941b61b-c763-4617-94de-cf5c539041f1\", // Now we grant access to one of Bob's keys \"grantedKey\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66#key-1\", // This proclamation *does* have caveats: \"caveat\": [ // Only the UploadFile method is allowed... {\"id\": \"did:example:f7412b9a-854b-47ab-806b-3ac736cc7cda#caveats/upload-only\", \"type\": \"RestrictToMethod\", \"method\": \"UploadFile\"}, // ...and each upload can only be 50 Megabytes large. {\"id\": \"did:example:f7412b9a-854b-47ab-806b-3ac736cc7cda#caveats/50-megs-only\", \"type\": \"RestrictUploadSize\", // file limit here is in bytes, so 50 MB \"limit\": 52428800}], // Finally we sign this object with Alice's key \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:83f75926-51ba-4472-84ff-51f5e39ab9ab#key-1\", \"signatureValue\": \"...\"}} As this diagram demonstrates, Alice has created, and has access to, this attenuated capability. .-. .-. .-. ( A )------>( B )----->( D ) '-'\\ '-' '-' \\ \\ \\ \\ \\ '--->(R1) \\ \\ \\ .-. '->( C ) '-' Bob cannot use this capability until he receives it. Alice invokes her message sending capability between herself and Bob. .-. __ .-. .-. ( A )-[##\\->( B )----->( D ) '-'\\ '--/ '-' '-' \\ \\ | \\ \\ V \\ '--->(R1) \\ | \\ V \\ .-. '->( C ) '-' Now Bob has access to upload files sized 50MB or less to the Cloud Store. But he would prefer that Dummy Bot do uploads for him... well, for a month. He'll see how it goes. Luckily these capabilities are composable, and so Bob can create an attenuated capability out of the attenuated capability he already has! {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:d2c83c43-878a-4c01-984f-b2f57932ce5f\", \"type\": \"Proclamation\", // Yet again, point up the chain... \"parent\": \"did:example:f7412b9a-854b-47ab-806b-3ac736cc7cda\", // Now we grant access to one of Dummy Bot's keys \"grantedKey\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4#key-1\", // We add a new caveat/attenuation: this one will expire 30 days // in the future \"caveat\": [ {\"id\": \"did:example:d2c83c43-878a-4c01-984f-b2f57932ce5f#caveats/expire-time\", \"type\": \"ExpireTime\", \"date\": \"2017-09-23T20:21:34Z\"}], // Finally we sign this object with Bob's key \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T17:12:28Z\", \"creator\": \"did:example:ee568de7-2970-4925-ad09-c685ab367b66#key-1\", \"signatureValue\": \"...\"}} The capability graph now looks like this: .-. .-. .-. ( A )------->( B )------>( D ) '-'\\ '-' '-' \\ \\ | \\ \\ \\ V '--v \\ '---->(R1)<--(R2) \\ | \\ V \\ .-. '-->( C ) '-' Bob invokes his message sending capability to send the new attenuated capability to Dummy Bot: .-. .-. __ .-. ( A )------->( B )--[##\\-->( D ) '-'\\ '-' '--/ '-' \\ \\ | \\ | \\ \\ V '---v | \\ '---->(R1)<---(R2)<---' \\ | \\ V \\ .-. '-->( C ) '-' Now Dummy Bot has a capability to upload files to Cloud Store, but only files that are sized 50 megabytes or less, and only for the next month. These multiple caveats are possible because Dummy Bot is authorized on the final proclamation, and the proclamation \"chains upward\", including both the immediate restriction/caveat within R2 on time and also the restriction/caveat in R1 on space! .---------. .---------. V | V | __________ | __________ | __________ ( (_) | ( (_) | ( (_) ' \\ '- ' \\ '- ' \\ ) CRT1 ) ) CRT2 ) ) CRT3 ) ' ; ' ; ' ; (________(_) (________(_) (________(_) Soon Dummy Bot takes a picture and uploads it: .-. .-. .-. ( A )------->( B )-------------->( D ) '-'\\ '-' '-' \\ \\ | \\_______ | \\ \\ V __ V __ | \\ '---->(R1)<-/##]-(R2)<-/##]-' \\ |_ \\--' \\--' \\ |##| \\ \\/ \\ | \\ V \\ .-. >( C ) '-' This is done through an Invocation on the proclamation, containing additional parameters in the body: {\"@context\": [\"https://example.org/did/v1\", \"https://example.org/ocap/v1\", \"http://schema.org\"], \"id\": \"did:example:2bdf6273-a52e-4cdf-991f-b5f000008829\", \"type\": \"Invocation\", // Dummy Bot is invoking the proclamation it has, // but the whole chain will be checked for attenuation and // verification of access \"proclamation\": \"did:example:d2c83c43-878a-4c01-984f-b2f57932ce5f\", // The method being used \"method\": \"UploadFile\", // The key Dummy Bot is using in this invocation \"usingKey\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4#key-1\", // Here's the base64 encoded file as part of the payload \"file\": \"nEOSQ7jbzBNg0Glup/FfeGDDzvLDvgEL36wcNpmbvKDgPy6+...\", // Finally we sign this object with Dummy Bot's key \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T17:13:48Z\", \"creator\": \"did:example:5e0fe086-3dd7-4b9b-a25f-023a567951a4#key-1\", \"signatureValue\": \"...\"}}","title":"Example Scenario"},{"location":"RWoT5/lds-ocap/#related-work","text":"","title":"Related work"},{"location":"RWoT5/lds-ocap/#spkisdsi","text":"SPKI (and previously SDSI) is a key-management project that aimed to resolve many of the issues (including those surrounding centralization) that the X.509 infrastructure introduced and developed into over time. SPKI is almost but not quite an object capability system . (See From Capabilities To Financial Instruments and Capability Myths Demolished for more information.) SPKI uses \"certificates\" (akin to \"proclamations\" fn:1 here) to express authority, similar to what we are doing in this document, but did not exist in a linked data system as this proposal does. Importantly, SPKI's authority is a broader form of access control and for that reason carries some of the traditional problems of ACLs.","title":"SPKI/SDSI"},{"location":"RWoT5/lds-ocap/#macaroons","text":"Macaroons are a credentials system that uphold most of the properties of capabilities. They also support delegation and attenuation (with some constraints as to who can attenuate) via a chain of signed messages, but there are some key differences. The biggest advantage of Macaroons over our design is that messages are smaller (a desirable property!) because a simple HMAC is used for signing rather than public key cryptography. Macaroons are thus passed around as bearer instruments over secure channels. This leads to a tradeoff: macaroons are smaller in size than Linked Data Capabilities, but unlike Linked Data Capabilities, cannot be sent or invoked over an insecure channel. Unlike Linked Data Capabilities, macaroons cannot be stored on a blockchain or be publicly retrievable from the web. One further difference is that while any entity that holds on to a macaroon may delegate that macaroon to any other entity, not all entities can attenuate macaroons. To see why, let us look at our final configuration between Alice, Bob, Dummy Bot, and Cloud Store: .-. .-. .-. ( A )------->( B )-------->( D ) '-'\\ '-' '-' \\ \\ | \\ | \\ \\ V '---v | \\ '---->(R1)<---(R2)<---' \\ | \\ V \\ .-. '-->( C ) '-' In this configuration, Alice was able to attenuate her capability to Cloud Store before delegating to Bob without any specific permission to do so; Bob was likewise able to attenuate the attenuated capability he held before passing to Dummy Bot without any specific permission. In Macaroons, Cloud Store and Alice must pre-arrange the shared key that Alice will use to attenuate the macaroon she holds before she can do so and successfully delegate to Bob (likewise for Bob to Dummy Bot). The reason for this is that in verifying HMAC signatures Cloud Store must check the macaroon's signatures against a key that Alice and Cloud Store must both have... Alice to sign it and Cloud Store to verify it. Even if Alice and Cloud Store had prearranged a shared key to be used for attenuating macaroons, if Bob and Cloud Store had not done so there would be no way for Bob to further attenuate the capability before passing to Dummy Bot. Bob may not prefer this to be the case since Bob wanted to only give Dummy Bot access for thirty days. (Notably, the Macaroons paper contains a short but underspecified section that outlines how Macaroons could be used with public keys instead of HMAC-signed bearer instruments; the design described, while scantly detailed, sounds very similar to how Linked Data Capabilities work.) Overall Macaroons and lds-ocaps are both reasonable systems with different tradeoffs. Implementers should be informed of these tradeoffs and make decisions accordingly.","title":"Macaroons"},{"location":"RWoT5/lds-ocap/#object-capability-programming-languages","text":"Up until this point this paper has focused on different substrates on which to implement capabilities, which have all relied on some sort of shared vocabulary between entities in the system. Another way to build capabilities is to build them at the layer of a programming language. In addition to not requiring coordination on vocabulary from all entities in the system, this provides powerful compositional abilities which, as we will see, turn out to be highly desirable. In the W7 Security Kernel , Jonathan Rees introduces an implementation of object capabilities on nothing other than a strict lexically scoped environment, enforced by the runtime of the system. The example language uses a cut-down variant of Scheme, though it could be implemented in any language that provides the same strict lexical scoping properties in a carefully bounded initial environment. (This is the general mechanism for implementing capabilities at a programming language level.) The paper demonstrates all the same properties of capabilities demonstrated here: delegation, attenuation, and so on. However, there is one thing that is possible in W7 (and other similar systems) that is not possible in any of the other systems discussed in this paper, including the lds-ocaps system herein proposed. This is attenuation by composition in an enclosed environment. To see what this means and why it is desirable, consider this example. We have the following initial state: A: Alice C: Cloud Store H: Home Directory T: Timer Service .-. ( H ) '-' ^ / .-./ .-. ( A )------->( T ) '-'\\ '-' \\ v .-. ( C ) '-' (A)lice keeps her data in (H)ome Directory. She would like to back it up to (C)loud Service, but she is afraid she will forget to back up regularly, so she would like to grant a capability to (T)imer Service to run the backup for her. However, she would prefer that Timer Service not have access to actually read any of the contents of her data on Home Directory, and she does not want Timer Service to be able to write just anything to Cloud Store, only backups. Effectively she would like to send Timer Service a new capability that composes together reading from Home Directory and writing to Cloud Store without giving access to either independently. Here R represents the restricted-through-composition capability: .-. ( H ) '-'<. ^ | / _->(R)<-. .-./.' | .-. ( A )------->( T ) '-'\\ ; '-' \\ . v v .-. ( C ) '-' In Rees' W7 / lambda-calculus-ocap system, this could be represented as: ;; Run in A's environment (timer-run-every ; T (lambda () ; R (write-cloud-image ; C (get-homedir-image))) ; H (* 60 60 24 7)) ; run every 604800 seconds, or once a week The advantage here is that the runtime is able to enclose the capabilities and handle the composition of passing the returned value of one of the enclosed capabilities to the other, without exposing either individually, outside of the enclosure. It does not appear that lds-ocaps can do the same thing. Here is a highly cut down invocation that attempts to embed the capabilities, for the sake of demonstration: {\"type\": \"Invocation\", \"usingKey\": <alice-key-1>, \"method\": \"RunEvery\", \"proclamation\": <cert-id>, \"secs\": 30, /* But we would also need to clearly express how to combine these */ \"runTheseCombinedSomehow\": [ <alice-grants-timer-capability-to-homedir>, <alice-grants-timer-capability-to-cloudstore>], \"signature\": <signed-by-alice-key-1>} There are two troubles here: making it clear how Timer Service is supposed to compose these capabilities in runTheseCombinedSomehow is not obvious, and even worse, there is nothing preventing Timer Service from running these individually since they are not properly enclosed, unlike in the W7/Scheme example. The majority of needs for a capability system are likely served by attenuation and delegation on their own. Nonetheless, full composability within a capability's enclosure, as explored above, is still a desirable property for the systems that can provide it.","title":"Object Capability Programming Languages"},{"location":"RWoT5/lds-ocap/#capcert","text":"CapCert is a (currently unimplemented) plan for a proclamation/certificate chain-based structure that looks a lot like the system discussed in this paper with one interesting change: real programs may be embedded in the proclamations. This approach bridges the gap between the proclamation-chain approach described in this paper and the object-capability programming languages described in the previous section; proclamations can be shared over insecure channels while also removing some need for shared vocabulary on both ends. Even more excitingly, the kinds of composition we do not have but would like to have would be possible, such as the example given above of Alice allowing a Timer Service to back up her Home Directory to Cloud Store, without giving Timer Service access to either independently. It would be possible to build such a system with Linked Data Capabilities by embedding an object capability programming language (with proper constraints on space and time for safety as well). This is a significant topic worth its own future paper.","title":"CapCert"},{"location":"RWoT5/lds-ocap/#capabilities-on-blockchains","text":"Finally, one piece of related work that we have not addressed but would like to address in a future paper is enabling capabilities on blockchains. Motivating examples include attacks against Etherium smart contracts that would not have occurred in an object capabilities environment. The examples provided here demonstrate capabilities that may exist in environments where the only secrets that must be kept are the private keys of entities participating in the system. We would like objects committed to a blockchain to be able to express capabilities despite not being able to hold secrets on the blockchain itself. This is also a significant topic worth its own future paper.","title":"Capabilities on Blockchains"},{"location":"RWoT5/lds-ocap/#conclusions","text":"Linked Data Systems are powerful ways to build collaborative, expressive systems. Today we are seeing Linked Data Systems crossing not only the traditional web but even into systems like distributed ledger technologies and so on. Unfortunately, security is frequently difficult on Linked Data Systems. For example, SoLiD directly uses and ActivityPub indirectly implies Access Control Lists. Unfortunately these are are known to create problems in systems , particularly: excess authority leading to needless vulnerability ambient authority leading to confused deputy problems lack of composability (including attenuation) We can avoid these risks by using an object capability system such as the one described above. Even more exciting, by combining this system with DIDs we can build a fully decentralized object capability system for the web that is safe to use. Note: what we are calling \"proclamations\" have also been called \"certificates\" in previous work such as SPKI and CapCert, we have chosen the name \"proclamation chain\" to make clear that the structure we are proposing holds none of the centralization traditionally associated with \"certificate authorities\".","title":"Conclusions"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/","text":"Definitions \\label{sec:defs} A system is defined as any set of components (see \\hyperref[sec:scope]{Decentralization Scope}) following precise rules in order to provide service(s) to the users of the system. These services constitute the system's intended behavior . In other words, a system $S$ consists of a set of components, called its scope ${S}$, and a program (\"state transition function\", $f_S$), that together define the system's intended behavior , which means: upon receipt of message $m$, $S$ uses $f_S$ to update the internal state from $s$ to $s^\\prime$ and send back reply $y$ within a time interval $S_\\tau$. $$ S(t) = \\left{ \\begin{array}{ll} {S} &= { component_1 , component_2, \\cdots } \\ f_S(m, s) &= { s^\\prime, y } \\end{array} \\right. $$ We note, additionally: The scope ${S}$ may change over time, but there are always several components of a vital type (i.e. \"all systems always have at least one CPU , one developer , and one user \"). The system's state $s$ includes all data necessary for the system to compute $f_S$ given a message $m$. The system may limit messages to those that are authorized in some way (in order to prevent denial-of-service).^[For decentralized systems, this is okay as long as there is no central authority determining who is or isn't authorized.] $S$ is considered compromised if it fails to perform its intended behavior within the interval $S_\\tau$. We will proceed to prove that any single such system may possess, at most, two of three properties: \\begin{figure} \\centering \\vspace{0.2cm} \\begin{tikzpicture} \\draw (0,0) node[anchor=north east]{Consensus} -- (1,2) node[anchor=south]{Scale} -- (2,0) node[anchor=north west]{Decentralized} -- (0,0); \\end{tikzpicture} % \\caption{Slepak's Triangle} \\vspace{0.2cm} \\end{figure} \\label{sec:triangle} Consensus means the system uses a collective decision-making process (\"consensus algorithm\") to update the system's state, $s$, which is shared by all \\hyperref[sec:consensus]{consensus participants} . The result of the consensus algorithm determines the network's accepted output of $f_S$, and whether or not $f_S$ completes within $S_\\tau$. Scale means the system is capable of handling the transactional demands of any competing system providing the same service to the same arbitrary set of users across the globe ( \"at scale\" ).^[Examples of \"services\" include: streaming video, sending messages, maintaining balances on a ledger, etc.] Decentralized means the system has no single point of failure or control (SPoF). Another way to state this is: if any single element is removed from ${S}$, the system continues to perform its intended behavior, and no single component in ${S}$ has the power to redefine $f_S$ on its own. Consensus participants and \"full\" consensus \\label{sec:consensus} The concept of a \"consensus participant\" is sometimes confused with the concept of a \"validator\", and in order to understand what the DCS Triangle is saying it's necessary to understand the difference between the two. Every consensus process has three ingredients: voters (consensus participants), voting rules, and the votes themselves. In distributed systems, the job of a validator is to verify that the voting rules were followed, accepting the outcome of the vote if that is so, and rejecting the outcome otherwise. For example, in the physical world a validator might be responsible for verifying ballot forms were filled out correctly and were cast by registered voters only, but beyond that they do not (generally speaking) have the ability to influence the outcome of the vote. Consensus participants, on the other hand, are the voters themselves, and their job is to not only ensure that voting rules are followed, but to cast a vote on some decision. In Bitcoin, for example, \"miners\" are consensus participants whose job is to vote on which transactions are accepted into the blockchain, whereas non-mining \"full nodes\" are validators only, and their job is to ensure that miners do not produce invalid blocks. \\begin{defn} $Consensus\\ participants$ are independent entities who each maintain a complete copy of a system's state, and together vote on updates to this shared state. \\end{defn} The notion of a \"complete copy of a system's state\" is of utmost importance for our proof. In other words, our proof focuses specifically on the strongest notion of \"consensus\", where each consensus participant has full knowledge of the entire system state, and therefore is able to cast a vote without needing to trust any other participant. To emphasize this notion of consensus over weaker forms, we'll refer to it as full consensus in our theorem. In \\hyperref[AppendixA]{\u00a73 - Getting around the DCS Triangle} , we'll explore how, by loosening this requirement and treating \"consensus\" as a spectrum of trust assumptions, it may be possible to design decentralized consensus systems that scale with \"good-enough-consensus\". Decentralization scope & relativity \\label{sec:scope} Implicit to our definition of a decentralized system is the idea that the system is not compromised. A non-functioning system does not fulfill its intended behavior, and therefore, by our definition, is not decentralized. Imagine a decentralized system $S$, whose intended behavior (its purpose) is to maintain the integrity of a database while being responsive to queries. It does so by attempting to eliminate all single points of failure within a given scope. \\begin{defn} The $scope$ of a system refers to all subcomponents and all entities reasonably relevant to a system's functioning. \\end{defn} If we consider the scope of our \"decentralized\" database to be a computer with two CPUs and two hard disks (one primary, another backup), then we can say $S$ is \"decentralized\" at $t=0$ (has no single point of failure). However, if at $t=1$ one of the hard disk fails, it is no longer decentralized since now there does exist a single component capable of compromising the entire system. This means: Whether or not a system is decentralized can change over time. Any system can be called \"decentralized\" if we define the scope narrowly enough. All decentralized systems can be called \"centralized\" if we define their scope broadly enough.^[The entire Internet could be considered centralized if we include the entire solar system as part of the scope. The \"single point of failure\" could be the Earth itself, its atmosphere, the Sun, etc. Or, perhaps in the not distant future, a single ISP.] The narrowing and enlarging of the scope is called the relativity of decentralization , and it is why first agreeing on a reasonable definition for a system's scope is vital before deciding whether or not it is \"decentralized\". Computational throughput of consensus systems \\begin{defn} The $computational\\ throughput$ of a consensus system refers to the rate at which the system updates its state by processing all input messages. \\end{defn} We'll use the shorthand $T(S)$ to represent this concept and note three factors that determine its value: The computational power ^[This refers to all computational requirements relevant for consensus participation, such as bandwidth, data storage, and processing speed.] of each consensus participant. The amount of time after which the consensus algorithm considers messages to be lost (the timeout period). The consensus threshold that decides when consensus has been reached (i.e. \"how big of a quorum is required\"). Note that if the computational power of a consensus participant is significantly less than that of the other participants, they are more likely to be excluded from the deciding quorum for several reasons: If there are no network partitions to determine otherwise, fast consensus participants will process messages more quickly and therefore will be first to create a quorum. If there are enough fast consensus participants to create a large enough quorum to exceed the system's consensus threshold, then there is no need to wait for the remaining votes of the slow participants. Slow consensus participants are more likely than fast consensus participants to hit the system's timeout period for processing and responding to messages, and therefore are more at risk of being excluded from the consensus process entirely. Therefore, $T(S)$ is a function that is limited by the slowest consensus participants not excluded in the deciding quorum. Coordination costs Relevant for our proof is the notion of coordination costs , or the difficulty for one entity to engage another and work toward a common goal, because that can result in the formation of a cartel, which in turn violates the requirement that consensus participants be independent . For example, when Bitcoin was first launched, it would be difficult for any miner to find enough collaborating miners to create a cartel with >50% of the hash power, simply because there were many \"relevant miners\" (consensus participants) distributed all over the world. Today, however, there are significantly fewer consensus participants in Bitcoin, and it is much easier to (1) identify them, and (2) bring them together to coordinate around some goal. Therefore, we say the coordination costs are lower today than before. We can approximate the coordination costs $C(S)$ of any consensus system simply as the number of consensus participants: $$C(S) = \\mathtt{num_consensus_participants}({S})$$ \\begin{figure} \\centering \\begin{tikzpicture} \\begin{axis}[ domain=0:5, xmin=-0.1, xmax=5.1, ymin=-0.1, ymax=5.1, % axis equal image, set layers, xlabel=Population of potential users, % xlabel style={scale=0.7}, xticklabels={}, xtick=\\empty, ytick=\\empty, % axis line style={opacity=0.3}, ] \\addplot [gray, only marks, mark=* , samples=500, mark size=0.75, on layer=axis background] {5*abs(rand)}; \\begin{pgfonlayer}{axis foreground} \\draw (3.5,3.5) node [ ellipse, minimum width=3.7cm, minimum height=2.5cm, fill=pink, opacity=0.6, label={[scale=0.8,fill=white,draw,ultra thin]below:Users of $S_1$} ] {}; \\draw (3.7,3.8) node [ ellipse, minimum width=2cm, minimum height=0.9cm, fill=green, opacity=0.5, label={[scale=0.7,fill=white]below:Consensus participants} ] {}; \\end{pgfonlayer} \\end{axis} \\end{tikzpicture} \\caption{If $S_1$ is a decentralized consensus system, the DCS Theorem states that as the number of users increases (red circle), the number of consensus participants decreases (green circle).} \\end{figure} Proof \\begin{theorem} Decentralized consensus systems centralize at scale when consensus participants maintain full consensus over the entire state of the system. \\end{theorem} We begin with the following axioms accepted as true: \\begin{axiom} \\label{Ax1} In any sufficiently large population (at scale), individual access to computational power is distributed unequally. Most have access to average computational power, and a few have access to large amounts. \\end{axiom} Justification: empirically true. \\begin{axiom} \\label{Ax2} For any two systems offering the same service to the same large population, the transactional demands of the average user converge at scale. \\end{axiom} Justification: follows from central limit theorem and the law of large numbers. \\begin{axiom} \\label{Ax3} Most users of a system do not have the computational power required to store and process all of the messages generated by all of the users of that system at scale. \\end{axiom} Justification: empirically true.^[And perhaps provably true, though such a proof is beyond the scope of this paper.] From those axioms, we derive the following lemmas: \\begin{lemma} \\label{Lem1} Let $S$ be a decentralized consensus system whose consensus participants maintain full consensus over the system's state. Let $T(S)$ refer to its computational throughput and $c$ refer to the average computational power of all historical consensus participants at any relevant instant in time. At scale, $T(S)$ exceeds $c$, and the more users $S$ obtains, the more $T(S)$ exceeds $c$. \\end{lemma} \\begin{figure} \\centering \\begin{tikzpicture} \\begin{axis}[domain = 1:2, samples = 100, clip = false, xmin = 1, xmax = 2, ymin = 0, ymax = 1, ytick = \\empty, xtick = \\empty, xlabel = {Throughput capability}, ylabel = {\\# users with capability}, xlabel shift = {12pt}, % xlabel near ticks, ylabel near ticks, set layers, ] \\addplot[thick, samples=400] {1/x^5}; % this requires clip=false \\node [anchor=near xticklabel] at (xticklabel cs:0.05) {slow}; \\node [anchor=near xticklabel] at (xticklabel cs:0.95) {fast}; \\end{axis} % we can optionally do it this other way if clip=false isn't set. % \\node at (rel axis cs:0.03,-.04) {slow}; % \\node at (rel axis cs:0.91,-.04) {fast}; \\end{tikzpicture} \\caption{Visualization of (Axiom~\\ref{Ax1}).} \\label{fig:CP} \\end{figure} \\begin{proof} This follows directly from Axiom \\ref{Ax1}, \\ref{Ax3}, and our definition of a decentralized system, which includes the \\hyperref[sec:scope]{understanding} that for a system to be considered decentralized, it must be uncompromised, and that in turn means it successfully processes all authorized\\footnote{See footnote 1 on page 1.} messages from new users within some interval $S_t$. For it to do this, $T(S)$ must exceed $c$, per (Axiom~\\ref{Ax1}) and (Axiom~\\ref{Ax3}). \\end{proof} \\begin{lemma} \\label{Lem2} Let $S$ be a consensus system as in (Lemma~\\ref{Lem1}). The coordination costs for $S$, $C(S)$, decrease at scale. \\end{lemma} \\begin{proof} This follows directly from our proof for (Lemma~\\ref{Lem1}) and our definition of $C(S)$. The more $S$ scales, the more $T(s)$ exceeds $c$, and the fewer potential consensus participants are able to participate in consensus. This, in turn, makes it easier for the remaining consensus participants to identify and coordinate with each other. \\end{proof} \\begin{lemma} \\label{Lem3} Let $S$ be a consensus system as in (Lemma~\\ref{Lem1}). The probability that ${S}$ contains a colluding group capable of censoring transactions increases at scale, and therefore $S$ tends toward centralization at scale. \\end{lemma} \\begin{proof}[Proof of the Main Theorem] The final lemma restates our original theorem. As coordination costs decrease (Lemma~\\ref{Lem2}), the probability of a colluding group (a cartel) increases. The presence of a cartel capable of controlling consensus represents a single point of failure \\emph{capable} of preventing the system from fulfilling its intended purpose. The definition of a centralized system is one that has a single point of failure. Therefore, we've shown that the probability of the initially decentralized system becoming centralized increases at scale. It is also worth considering our definition of \\hyperref[sec:triangle]{scale} and the implications of (Axiom~\\ref{Ax2}). Per (Axiom~\\ref{Ax2}), when a decentralized consensus system $S_1$ scales to the size of a similar centralized consensus system $S_2$, it will experience the same transactional demands as $S_2$. However, $S_2$ may scale to a size that would guarantee cartel formation in $S_1$ if it were to scale to the same size. Therefore, $S_1$ cannot scale to such a size while remaining decentralized, and therefore $S_1$ cannot satisfy our definition of scale. \\end{proof} Getting around the DCS Triangle \\label{AppendixA} As mentioned, the DCS triangle applies to systems employing \"full consensus\", or in other words, when all consensus participants are required to fully and independently verify the entire state of the system. It may be possible to \"get around\" the DCS Triangle by relaxing our definition of consensus. In this section we'll consider two such approaches. Combining DC and DS systems Let us suppose we have a DC-system that we wish to scale while preserving its decentralization. An example of such a system is Bitcoin.[@Bitcoin2008] Per the triangle, we know that increasing the system's throughput, $T(S)$, via any mechanism that requires all consensus participants to process the additional data, will result in a reduction in the number of independent consensus participants. And so, instead, we may choose to pair our DC-system with a DS-system in some clever way. \\begin{figure} \\centering \\begin{tikzpicture} \\draw (0,0) node[anchor=north east]{\\textbf D} -- (1,2) node[anchor=south]{S} -- (2,0) node[anchor=north west]{\\textbf C} -- (0,0); \\draw [line width=3pt,cap=round,xshift=0.2cm] (0,0) -- (1.6,0); \\draw (3.5,1) node {\\textbf +}; \\draw [dashed,yshift=-0.25cm] (2.6,0) -- (4.4,0); \\draw [xshift=5cm] (0,0) node[anchor=north east]{C} -- (1,2) node[anchor=south]{\\textbf S} -- (2,0) node[anchor=north west]{\\textbf D} -- (0,0); \\draw [xshift=5cm,line width=3pt,cap=round,rotate around={-63.435:(2,0)}] (0,0) -- (1.8,0); \\end{tikzpicture} \\end{figure} Our DS-system will give us the scale we're looking for, while our DC-system provides a stable and secure source of \"ultimate truth\" on an as-needed basis. We can connect the two systems in such a way that our DS-system only requires consensus in rare moments, and when it does it may communicate with our DC-system. The Lightning Network[@Poon2016] is a real-world example of such a pairing. Combining multiple DC systems Yet another possibility is to combine multiple DC systems to create a super-system of DC groups . This approach explores a middle-ground within the DCS triangle, and is the approach taken by systems like OmniLedger.[@Eleftherios2017] \\begin{figure} \\centering \\begin{tikzpicture} % See \"tikz pgf manual.pdf\" for info on options % \\draw is an abbreviation for \\path[draw] % \\shade is an abbreviation for \\path[shade] % \\shadedraw is an abbreviation for \\path[shade,draw] \\shadedraw[top color=gray!30, middle color=white, shading angle=180] (0,0) node[anchor=north east]{Consensus} -- (1,2) node[anchor=south]{Scale} -- (2,0) node[anchor=north west]{Decentralized} -- (0,0); \\draw[->,thick] (1,0.3) -- (1,1.6); \\end{tikzpicture} \\end{figure} Also known as sharding, each group (or shard ) of consensus participants no longer has complete knowledge of the entire system state, and therefore must (at least partially) trust the other consensus groups. Transparency techniques, such as merkle tree logs, make it possible to minimize the amount of \"faith\" groups must place in each other. Overall system consensus is progressively \"sacrificed\" as the system scales, but only in small, manageable increments. If the system does not need much inter-group consensus, it can scale significantly without issue. If necessary, a DS-system can be added for additional scale. Acknowledgements Thanks to Trent McConaghy and Andrea Devers for their feedback. References","title":"\\textbf{The DCS Theorem}"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#definitions","text":"\\label{sec:defs} A system is defined as any set of components (see \\hyperref[sec:scope]{Decentralization Scope}) following precise rules in order to provide service(s) to the users of the system. These services constitute the system's intended behavior . In other words, a system $S$ consists of a set of components, called its scope ${S}$, and a program (\"state transition function\", $f_S$), that together define the system's intended behavior , which means: upon receipt of message $m$, $S$ uses $f_S$ to update the internal state from $s$ to $s^\\prime$ and send back reply $y$ within a time interval $S_\\tau$. $$ S(t) = \\left{ \\begin{array}{ll} {S} &= { component_1 , component_2, \\cdots } \\ f_S(m, s) &= { s^\\prime, y } \\end{array} \\right. $$ We note, additionally: The scope ${S}$ may change over time, but there are always several components of a vital type (i.e. \"all systems always have at least one CPU , one developer , and one user \"). The system's state $s$ includes all data necessary for the system to compute $f_S$ given a message $m$. The system may limit messages to those that are authorized in some way (in order to prevent denial-of-service).^[For decentralized systems, this is okay as long as there is no central authority determining who is or isn't authorized.] $S$ is considered compromised if it fails to perform its intended behavior within the interval $S_\\tau$. We will proceed to prove that any single such system may possess, at most, two of three properties: \\begin{figure} \\centering \\vspace{0.2cm} \\begin{tikzpicture} \\draw (0,0) node[anchor=north east]{Consensus} -- (1,2) node[anchor=south]{Scale} -- (2,0) node[anchor=north west]{Decentralized} -- (0,0); \\end{tikzpicture} % \\caption{Slepak's Triangle} \\vspace{0.2cm} \\end{figure} \\label{sec:triangle} Consensus means the system uses a collective decision-making process (\"consensus algorithm\") to update the system's state, $s$, which is shared by all \\hyperref[sec:consensus]{consensus participants} . The result of the consensus algorithm determines the network's accepted output of $f_S$, and whether or not $f_S$ completes within $S_\\tau$. Scale means the system is capable of handling the transactional demands of any competing system providing the same service to the same arbitrary set of users across the globe ( \"at scale\" ).^[Examples of \"services\" include: streaming video, sending messages, maintaining balances on a ledger, etc.] Decentralized means the system has no single point of failure or control (SPoF). Another way to state this is: if any single element is removed from ${S}$, the system continues to perform its intended behavior, and no single component in ${S}$ has the power to redefine $f_S$ on its own.","title":"Definitions"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#consensus-participants-and-full-consensus","text":"\\label{sec:consensus} The concept of a \"consensus participant\" is sometimes confused with the concept of a \"validator\", and in order to understand what the DCS Triangle is saying it's necessary to understand the difference between the two. Every consensus process has three ingredients: voters (consensus participants), voting rules, and the votes themselves. In distributed systems, the job of a validator is to verify that the voting rules were followed, accepting the outcome of the vote if that is so, and rejecting the outcome otherwise. For example, in the physical world a validator might be responsible for verifying ballot forms were filled out correctly and were cast by registered voters only, but beyond that they do not (generally speaking) have the ability to influence the outcome of the vote. Consensus participants, on the other hand, are the voters themselves, and their job is to not only ensure that voting rules are followed, but to cast a vote on some decision. In Bitcoin, for example, \"miners\" are consensus participants whose job is to vote on which transactions are accepted into the blockchain, whereas non-mining \"full nodes\" are validators only, and their job is to ensure that miners do not produce invalid blocks. \\begin{defn} $Consensus\\ participants$ are independent entities who each maintain a complete copy of a system's state, and together vote on updates to this shared state. \\end{defn} The notion of a \"complete copy of a system's state\" is of utmost importance for our proof. In other words, our proof focuses specifically on the strongest notion of \"consensus\", where each consensus participant has full knowledge of the entire system state, and therefore is able to cast a vote without needing to trust any other participant. To emphasize this notion of consensus over weaker forms, we'll refer to it as full consensus in our theorem. In \\hyperref[AppendixA]{\u00a73 - Getting around the DCS Triangle} , we'll explore how, by loosening this requirement and treating \"consensus\" as a spectrum of trust assumptions, it may be possible to design decentralized consensus systems that scale with \"good-enough-consensus\".","title":"Consensus participants and \"full\" consensus"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#decentralization-scope-relativity","text":"\\label{sec:scope} Implicit to our definition of a decentralized system is the idea that the system is not compromised. A non-functioning system does not fulfill its intended behavior, and therefore, by our definition, is not decentralized. Imagine a decentralized system $S$, whose intended behavior (its purpose) is to maintain the integrity of a database while being responsive to queries. It does so by attempting to eliminate all single points of failure within a given scope. \\begin{defn} The $scope$ of a system refers to all subcomponents and all entities reasonably relevant to a system's functioning. \\end{defn} If we consider the scope of our \"decentralized\" database to be a computer with two CPUs and two hard disks (one primary, another backup), then we can say $S$ is \"decentralized\" at $t=0$ (has no single point of failure). However, if at $t=1$ one of the hard disk fails, it is no longer decentralized since now there does exist a single component capable of compromising the entire system. This means: Whether or not a system is decentralized can change over time. Any system can be called \"decentralized\" if we define the scope narrowly enough. All decentralized systems can be called \"centralized\" if we define their scope broadly enough.^[The entire Internet could be considered centralized if we include the entire solar system as part of the scope. The \"single point of failure\" could be the Earth itself, its atmosphere, the Sun, etc. Or, perhaps in the not distant future, a single ISP.] The narrowing and enlarging of the scope is called the relativity of decentralization , and it is why first agreeing on a reasonable definition for a system's scope is vital before deciding whether or not it is \"decentralized\".","title":"Decentralization scope &amp; relativity"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#computational-throughput-of-consensus-systems","text":"\\begin{defn} The $computational\\ throughput$ of a consensus system refers to the rate at which the system updates its state by processing all input messages. \\end{defn} We'll use the shorthand $T(S)$ to represent this concept and note three factors that determine its value: The computational power ^[This refers to all computational requirements relevant for consensus participation, such as bandwidth, data storage, and processing speed.] of each consensus participant. The amount of time after which the consensus algorithm considers messages to be lost (the timeout period). The consensus threshold that decides when consensus has been reached (i.e. \"how big of a quorum is required\"). Note that if the computational power of a consensus participant is significantly less than that of the other participants, they are more likely to be excluded from the deciding quorum for several reasons: If there are no network partitions to determine otherwise, fast consensus participants will process messages more quickly and therefore will be first to create a quorum. If there are enough fast consensus participants to create a large enough quorum to exceed the system's consensus threshold, then there is no need to wait for the remaining votes of the slow participants. Slow consensus participants are more likely than fast consensus participants to hit the system's timeout period for processing and responding to messages, and therefore are more at risk of being excluded from the consensus process entirely. Therefore, $T(S)$ is a function that is limited by the slowest consensus participants not excluded in the deciding quorum.","title":"Computational throughput of consensus systems"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#coordination-costs","text":"Relevant for our proof is the notion of coordination costs , or the difficulty for one entity to engage another and work toward a common goal, because that can result in the formation of a cartel, which in turn violates the requirement that consensus participants be independent . For example, when Bitcoin was first launched, it would be difficult for any miner to find enough collaborating miners to create a cartel with >50% of the hash power, simply because there were many \"relevant miners\" (consensus participants) distributed all over the world. Today, however, there are significantly fewer consensus participants in Bitcoin, and it is much easier to (1) identify them, and (2) bring them together to coordinate around some goal. Therefore, we say the coordination costs are lower today than before. We can approximate the coordination costs $C(S)$ of any consensus system simply as the number of consensus participants: $$C(S) = \\mathtt{num_consensus_participants}({S})$$ \\begin{figure} \\centering \\begin{tikzpicture} \\begin{axis}[ domain=0:5, xmin=-0.1, xmax=5.1, ymin=-0.1, ymax=5.1, % axis equal image, set layers, xlabel=Population of potential users, % xlabel style={scale=0.7}, xticklabels={}, xtick=\\empty, ytick=\\empty, % axis line style={opacity=0.3}, ] \\addplot [gray, only marks, mark=* , samples=500, mark size=0.75, on layer=axis background] {5*abs(rand)}; \\begin{pgfonlayer}{axis foreground} \\draw (3.5,3.5) node [ ellipse, minimum width=3.7cm, minimum height=2.5cm, fill=pink, opacity=0.6, label={[scale=0.8,fill=white,draw,ultra thin]below:Users of $S_1$} ] {}; \\draw (3.7,3.8) node [ ellipse, minimum width=2cm, minimum height=0.9cm, fill=green, opacity=0.5, label={[scale=0.7,fill=white]below:Consensus participants} ] {}; \\end{pgfonlayer} \\end{axis} \\end{tikzpicture} \\caption{If $S_1$ is a decentralized consensus system, the DCS Theorem states that as the number of users increases (red circle), the number of consensus participants decreases (green circle).} \\end{figure}","title":"Coordination costs"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#proof","text":"\\begin{theorem} Decentralized consensus systems centralize at scale when consensus participants maintain full consensus over the entire state of the system. \\end{theorem} We begin with the following axioms accepted as true: \\begin{axiom} \\label{Ax1} In any sufficiently large population (at scale), individual access to computational power is distributed unequally. Most have access to average computational power, and a few have access to large amounts. \\end{axiom} Justification: empirically true. \\begin{axiom} \\label{Ax2} For any two systems offering the same service to the same large population, the transactional demands of the average user converge at scale. \\end{axiom} Justification: follows from central limit theorem and the law of large numbers. \\begin{axiom} \\label{Ax3} Most users of a system do not have the computational power required to store and process all of the messages generated by all of the users of that system at scale. \\end{axiom} Justification: empirically true.^[And perhaps provably true, though such a proof is beyond the scope of this paper.] From those axioms, we derive the following lemmas: \\begin{lemma} \\label{Lem1} Let $S$ be a decentralized consensus system whose consensus participants maintain full consensus over the system's state. Let $T(S)$ refer to its computational throughput and $c$ refer to the average computational power of all historical consensus participants at any relevant instant in time. At scale, $T(S)$ exceeds $c$, and the more users $S$ obtains, the more $T(S)$ exceeds $c$. \\end{lemma} \\begin{figure} \\centering \\begin{tikzpicture} \\begin{axis}[domain = 1:2, samples = 100, clip = false, xmin = 1, xmax = 2, ymin = 0, ymax = 1, ytick = \\empty, xtick = \\empty, xlabel = {Throughput capability}, ylabel = {\\# users with capability}, xlabel shift = {12pt}, % xlabel near ticks, ylabel near ticks, set layers, ] \\addplot[thick, samples=400] {1/x^5}; % this requires clip=false \\node [anchor=near xticklabel] at (xticklabel cs:0.05) {slow}; \\node [anchor=near xticklabel] at (xticklabel cs:0.95) {fast}; \\end{axis} % we can optionally do it this other way if clip=false isn't set. % \\node at (rel axis cs:0.03,-.04) {slow}; % \\node at (rel axis cs:0.91,-.04) {fast}; \\end{tikzpicture} \\caption{Visualization of (Axiom~\\ref{Ax1}).} \\label{fig:CP} \\end{figure} \\begin{proof} This follows directly from Axiom \\ref{Ax1}, \\ref{Ax3}, and our definition of a decentralized system, which includes the \\hyperref[sec:scope]{understanding} that for a system to be considered decentralized, it must be uncompromised, and that in turn means it successfully processes all authorized\\footnote{See footnote 1 on page 1.} messages from new users within some interval $S_t$. For it to do this, $T(S)$ must exceed $c$, per (Axiom~\\ref{Ax1}) and (Axiom~\\ref{Ax3}). \\end{proof} \\begin{lemma} \\label{Lem2} Let $S$ be a consensus system as in (Lemma~\\ref{Lem1}). The coordination costs for $S$, $C(S)$, decrease at scale. \\end{lemma} \\begin{proof} This follows directly from our proof for (Lemma~\\ref{Lem1}) and our definition of $C(S)$. The more $S$ scales, the more $T(s)$ exceeds $c$, and the fewer potential consensus participants are able to participate in consensus. This, in turn, makes it easier for the remaining consensus participants to identify and coordinate with each other. \\end{proof} \\begin{lemma} \\label{Lem3} Let $S$ be a consensus system as in (Lemma~\\ref{Lem1}). The probability that ${S}$ contains a colluding group capable of censoring transactions increases at scale, and therefore $S$ tends toward centralization at scale. \\end{lemma} \\begin{proof}[Proof of the Main Theorem] The final lemma restates our original theorem. As coordination costs decrease (Lemma~\\ref{Lem2}), the probability of a colluding group (a cartel) increases. The presence of a cartel capable of controlling consensus represents a single point of failure \\emph{capable} of preventing the system from fulfilling its intended purpose. The definition of a centralized system is one that has a single point of failure. Therefore, we've shown that the probability of the initially decentralized system becoming centralized increases at scale. It is also worth considering our definition of \\hyperref[sec:triangle]{scale} and the implications of (Axiom~\\ref{Ax2}). Per (Axiom~\\ref{Ax2}), when a decentralized consensus system $S_1$ scales to the size of a similar centralized consensus system $S_2$, it will experience the same transactional demands as $S_2$. However, $S_2$ may scale to a size that would guarantee cartel formation in $S_1$ if it were to scale to the same size. Therefore, $S_1$ cannot scale to such a size while remaining decentralized, and therefore $S_1$ cannot satisfy our definition of scale. \\end{proof}","title":"Proof"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#getting-around-the-dcs-triangle","text":"\\label{AppendixA} As mentioned, the DCS triangle applies to systems employing \"full consensus\", or in other words, when all consensus participants are required to fully and independently verify the entire state of the system. It may be possible to \"get around\" the DCS Triangle by relaxing our definition of consensus. In this section we'll consider two such approaches.","title":"Getting around the DCS Triangle"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#combining-dc-and-ds-systems","text":"Let us suppose we have a DC-system that we wish to scale while preserving its decentralization. An example of such a system is Bitcoin.[@Bitcoin2008] Per the triangle, we know that increasing the system's throughput, $T(S)$, via any mechanism that requires all consensus participants to process the additional data, will result in a reduction in the number of independent consensus participants. And so, instead, we may choose to pair our DC-system with a DS-system in some clever way. \\begin{figure} \\centering \\begin{tikzpicture} \\draw (0,0) node[anchor=north east]{\\textbf D} -- (1,2) node[anchor=south]{S} -- (2,0) node[anchor=north west]{\\textbf C} -- (0,0); \\draw [line width=3pt,cap=round,xshift=0.2cm] (0,0) -- (1.6,0); \\draw (3.5,1) node {\\textbf +}; \\draw [dashed,yshift=-0.25cm] (2.6,0) -- (4.4,0); \\draw [xshift=5cm] (0,0) node[anchor=north east]{C} -- (1,2) node[anchor=south]{\\textbf S} -- (2,0) node[anchor=north west]{\\textbf D} -- (0,0); \\draw [xshift=5cm,line width=3pt,cap=round,rotate around={-63.435:(2,0)}] (0,0) -- (1.8,0); \\end{tikzpicture} \\end{figure} Our DS-system will give us the scale we're looking for, while our DC-system provides a stable and secure source of \"ultimate truth\" on an as-needed basis. We can connect the two systems in such a way that our DS-system only requires consensus in rare moments, and when it does it may communicate with our DC-system. The Lightning Network[@Poon2016] is a real-world example of such a pairing.","title":"Combining DC and DS systems"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#combining-multiple-dc-systems","text":"Yet another possibility is to combine multiple DC systems to create a super-system of DC groups . This approach explores a middle-ground within the DCS triangle, and is the approach taken by systems like OmniLedger.[@Eleftherios2017] \\begin{figure} \\centering \\begin{tikzpicture} % See \"tikz pgf manual.pdf\" for info on options % \\draw is an abbreviation for \\path[draw] % \\shade is an abbreviation for \\path[shade] % \\shadedraw is an abbreviation for \\path[shade,draw] \\shadedraw[top color=gray!30, middle color=white, shading angle=180] (0,0) node[anchor=north east]{Consensus} -- (1,2) node[anchor=south]{Scale} -- (2,0) node[anchor=north west]{Decentralized} -- (0,0); \\draw[->,thick] (1,0.3) -- (1,1.6); \\end{tikzpicture} \\end{figure} Also known as sharding, each group (or shard ) of consensus participants no longer has complete knowledge of the entire system state, and therefore must (at least partially) trust the other consensus groups. Transparency techniques, such as merkle tree logs, make it possible to minimize the amount of \"faith\" groups must place in each other. Overall system consensus is progressively \"sacrificed\" as the system scales, but only in small, manageable increments. If the system does not need much inter-group consensus, it can scale significantly without issue. If necessary, a DS-system can be added for additional scale.","title":"Combining multiple DC systems"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#acknowledgements","text":"Thanks to Trent McConaghy and Andrea Devers for their feedback.","title":"Acknowledgements"},{"location":"RWoT5/dcs-theorem/The-DCS-Theorem/#references","text":"","title":"References"},{"location":"RWoT6/","text":"Rebooting the Web of Trust VI: Santa Barbara (March 2018) This repository contains documents related to RWOT6, the sixth Rebooting the Web of Trust design workshop, which ran in Santa Barbara, California, on March 6th to 8th, 2018. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Topics & Advance Readings directory for a listing of all the papers. Complete Papers The goal for each Rebooting the Web of Trust workshop is publication of three to five white papers: BTCR DID Resolver Specification (Text) Kim Hamilton Duffy, Christopher Allen, Ryan Grant, and Dan Pape This describes the process of resolving a BTCR DID into a DID Document. The draft reference implementation is available at https://github.com/WebOfTrustInfo/btcr-did-tools-js (see didFormatter.js). Note that not all steps described in this document are implemented yet. Decentralized Autonomic Data (DAD) and the three R's of Key Management (Text) by Samuel M. Smith Ph.D. with Vishal Gupta This paper proposes a new class of data called decentralized autonomic data (DAD). The term decentralized means that the governance of the data may not reside with a single party. A related concept is that the trust in the data provenance is diffuse in nature. Central to the approach is leveraging the emerging DID (decentralized identifier) standard. The term autonomic means self-managing or self-regulating. In the context of data, we crystalize the meaning of self-managing to include cryptographic techniques for maintaining data provenance that make the data self-identifying, self-certifying, and self-securing. Implied thereby is the use of cryptographic keys and signatures to provide a root of trust for data integrity and to maintain that trust over transformation of that data, e.g. provenance. Thus key management must be a first order property of DADs. This includes key reproduction, rotation, and recovery. The pre-rotation and hybrid recovery methods presented herein are somewhat novel. Decentralized Identifiers v1.0 (Text) A Status Note The Decentralized Identifiers specification editors and implementers spent some time at Rebooting the Web of Trust 6 processing the remaining issues in the issue tracker. This document summarizes the proposed resolutions that the group has put forward to resolve all of the DID specification issues that were submitted before 2018-03-05. Exploring Sustainable Technology Commons using Appreciative Inquiry (Text) by Heather Vescent, Kaliya \u201cIdentity Woman\u201d Young, Adrian Gropper, and Juan Caballero Technology commons come in a variety of flavors and have achieved varying levels of financial success. For-profit corporate activities have in few historical cases been set up with a financial feedback mechanism to support the commons upon which they depend and capitalize. Why do the commons and the technology sectors\u2019 available forms of capitalism act as incompatible as oil and water, even though they support each other\u2019s aims? When capitalist benefactors support the technology commons that they utilize, it creates a sustainable and thriving commons which enables and supports additional capitalistic technology innovation. Having worked on both sides of the equation, the authors of this piece propose a vocabulary to nourish these interactions between the two sides; identified characteristics of a sustainable technology commons; identified commons models and variations; applied Appreciative Inquiry principles to one commons model; and identified future research areas. Identity Hub Attestation Flows and Components (Text) by Daniel Buchner, Cherie Duncan, John Toohey, Ron Kreutzer, and Stephen Curran In this document, we define a set of user flows and describe the associated Action Objects that support a Hub-centric approach to the request, issuance, presentation, verification, and revocation of interoperable attestations. This document extends the Identity Hub Explainer . Introduction to DID Auth (Text) by Markus Sabadello, Kyle Den Hartog, Christian Lundkvist, Cedric Franz, Alberto Elias, Andrew Hughes, John Jordan & Dmitri Zagidulin The term DID Auth has been used in different ways and is currently not well-defined. We define DID Auth as a ceremony where an identity owner, with the help of various components such as web browsers, mobile devices, and other agents, proves to a relying party that they are in control of a DID. This means demonstrating control of the DID using the mechanism specified in the DID Document's \"authentication\" object. This could take place using a number of different data formats, protocols, and flows. DID Auth includes the ability to establish mutually authenticated communication channels and to authenticate to web sites and applications. Authorization, Verifiable Credentials, and Capabilities are built on top of DID Auth and are out of scope for this document. This paper gives on overview of the scope of DID Auth, supported protocols and flows, and the use of components of the DID Documents that are relevant to authentication, as well as formats for challenges and responses. Open Badges are Verifiable Credentials (Text) By Nate Otto & Kim Hamilton Duffy We identify use cases and requirements that connect threads of work happening in the Rebooting Web of Trust community around: educational achievement claims (particularly using the Open Badges vocabulary); use of decentralized identifiers (DIDs) within web services where educational claims circulate; and integrating blockchain-reliant verification layers. We illustrate each of these cases with a set of example documents and describe user stories for Open Badges ecosystem software in the roles of Issuer, Host/Backpack, Displayer, and Verifier that need to be implemented in order to enable the capabilities described. SSI: A Roadmap for Adoption (Text) By Moses Ma, Claire Rumore, Dan Gisolfi, Wes Kussmaul & Dan Greening (Senex Rex) This document proposes the formation of a short-term team to develop consistent messaging for the Self-Sovereign Identity (SSI) market. It will target key stakeholders who would actively promote SSI adoption. The goal is to create an SSI market roadmap. This roadmap will help SSI leaders, standards bodies, developers, academics, media, and investors coordinate and clarify their messaging for the market, to accelerate the SSI adoption. Complete Rebooting the Web of Trust Listing A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 - New York City (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018) License All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"Index"},{"location":"RWoT6/#rebooting-the-web-of-trust-vi-santa-barbara-march-2018","text":"This repository contains documents related to RWOT6, the sixth Rebooting the Web of Trust design workshop, which ran in Santa Barbara, California, on March 6th to 8th, 2018. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community.","title":"Rebooting the Web of Trust VI: Santa Barbara (March 2018)"},{"location":"RWoT6/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Please see the Topics & Advance Readings directory for a listing of all the papers.","title":"Topics &amp; Advance Readings"},{"location":"RWoT6/#complete-papers","text":"The goal for each Rebooting the Web of Trust workshop is publication of three to five white papers:","title":"Complete Papers"},{"location":"RWoT6/#btcr-did-resolver-specification-text","text":"","title":"BTCR DID Resolver Specification (Text)"},{"location":"RWoT6/#kim-hamilton-duffy-christopher-allen-ryan-grant-and-dan-pape","text":"This describes the process of resolving a BTCR DID into a DID Document. The draft reference implementation is available at https://github.com/WebOfTrustInfo/btcr-did-tools-js (see didFormatter.js). Note that not all steps described in this document are implemented yet.","title":"Kim Hamilton Duffy, Christopher Allen, Ryan Grant, and Dan Pape"},{"location":"RWoT6/#decentralized-autonomic-data-dad-and-the-three-rs-of-key-management-text","text":"","title":"Decentralized Autonomic Data (DAD) and the three R's of Key Management (Text)"},{"location":"RWoT6/#by-samuel-m-smith-phd-with-vishal-gupta","text":"This paper proposes a new class of data called decentralized autonomic data (DAD). The term decentralized means that the governance of the data may not reside with a single party. A related concept is that the trust in the data provenance is diffuse in nature. Central to the approach is leveraging the emerging DID (decentralized identifier) standard. The term autonomic means self-managing or self-regulating. In the context of data, we crystalize the meaning of self-managing to include cryptographic techniques for maintaining data provenance that make the data self-identifying, self-certifying, and self-securing. Implied thereby is the use of cryptographic keys and signatures to provide a root of trust for data integrity and to maintain that trust over transformation of that data, e.g. provenance. Thus key management must be a first order property of DADs. This includes key reproduction, rotation, and recovery. The pre-rotation and hybrid recovery methods presented herein are somewhat novel.","title":"by Samuel M. Smith Ph.D. with Vishal Gupta"},{"location":"RWoT6/#decentralized-identifiers-v10-text","text":"","title":"Decentralized Identifiers v1.0 (Text)"},{"location":"RWoT6/#a-status-note","text":"The Decentralized Identifiers specification editors and implementers spent some time at Rebooting the Web of Trust 6 processing the remaining issues in the issue tracker. This document summarizes the proposed resolutions that the group has put forward to resolve all of the DID specification issues that were submitted before 2018-03-05.","title":"A Status Note"},{"location":"RWoT6/#exploring-sustainable-technology-commons-using-appreciative-inquiry-text","text":"","title":"Exploring Sustainable Technology Commons using Appreciative Inquiry (Text)"},{"location":"RWoT6/#by-heather-vescent-kaliya-identity-woman-young-adrian-gropper-and-juan-caballero","text":"Technology commons come in a variety of flavors and have achieved varying levels of financial success. For-profit corporate activities have in few historical cases been set up with a financial feedback mechanism to support the commons upon which they depend and capitalize. Why do the commons and the technology sectors\u2019 available forms of capitalism act as incompatible as oil and water, even though they support each other\u2019s aims? When capitalist benefactors support the technology commons that they utilize, it creates a sustainable and thriving commons which enables and supports additional capitalistic technology innovation. Having worked on both sides of the equation, the authors of this piece propose a vocabulary to nourish these interactions between the two sides; identified characteristics of a sustainable technology commons; identified commons models and variations; applied Appreciative Inquiry principles to one commons model; and identified future research areas.","title":"by Heather Vescent, Kaliya \u201cIdentity Woman\u201d Young, Adrian Gropper, and Juan Caballero"},{"location":"RWoT6/#identity-hub-attestation-flows-and-components-text","text":"","title":"Identity Hub Attestation Flows and Components (Text)"},{"location":"RWoT6/#by-daniel-buchner-cherie-duncan-john-toohey-ron-kreutzer-and-stephen-curran","text":"In this document, we define a set of user flows and describe the associated Action Objects that support a Hub-centric approach to the request, issuance, presentation, verification, and revocation of interoperable attestations. This document extends the Identity Hub Explainer .","title":"by Daniel Buchner, Cherie Duncan, John Toohey, Ron Kreutzer, and Stephen Curran"},{"location":"RWoT6/#introduction-to-did-auth-text","text":"","title":"Introduction to DID Auth (Text)"},{"location":"RWoT6/#by-markus-sabadello-kyle-den-hartog-christian-lundkvist-cedric-franz-alberto-elias-andrew-hughes-john-jordan-dmitri-zagidulin","text":"The term DID Auth has been used in different ways and is currently not well-defined. We define DID Auth as a ceremony where an identity owner, with the help of various components such as web browsers, mobile devices, and other agents, proves to a relying party that they are in control of a DID. This means demonstrating control of the DID using the mechanism specified in the DID Document's \"authentication\" object. This could take place using a number of different data formats, protocols, and flows. DID Auth includes the ability to establish mutually authenticated communication channels and to authenticate to web sites and applications. Authorization, Verifiable Credentials, and Capabilities are built on top of DID Auth and are out of scope for this document. This paper gives on overview of the scope of DID Auth, supported protocols and flows, and the use of components of the DID Documents that are relevant to authentication, as well as formats for challenges and responses.","title":"by Markus Sabadello, Kyle Den Hartog, Christian Lundkvist, Cedric Franz, Alberto Elias, Andrew Hughes, John Jordan &amp; Dmitri Zagidulin"},{"location":"RWoT6/#open-badges-are-verifiable-credentials-text","text":"","title":"Open Badges are Verifiable Credentials (Text)"},{"location":"RWoT6/#by-nate-otto-kim-hamilton-duffy","text":"We identify use cases and requirements that connect threads of work happening in the Rebooting Web of Trust community around: educational achievement claims (particularly using the Open Badges vocabulary); use of decentralized identifiers (DIDs) within web services where educational claims circulate; and integrating blockchain-reliant verification layers. We illustrate each of these cases with a set of example documents and describe user stories for Open Badges ecosystem software in the roles of Issuer, Host/Backpack, Displayer, and Verifier that need to be implemented in order to enable the capabilities described.","title":"By Nate Otto &amp; Kim Hamilton Duffy"},{"location":"RWoT6/#ssi-a-roadmap-for-adoption-text","text":"","title":"SSI: A Roadmap for Adoption (Text)"},{"location":"RWoT6/#by-moses-ma-claire-rumore-dan-gisolfi-wes-kussmaul-dan-greening-senex-rex","text":"This document proposes the formation of a short-term team to develop consistent messaging for the Self-Sovereign Identity (SSI) market. It will target key stakeholders who would actively promote SSI adoption. The goal is to create an SSI market roadmap. This roadmap will help SSI leaders, standards bodies, developers, academics, media, and investors coordinate and clarify their messaging for the market, to accelerate the SSI adoption.","title":"By Moses Ma, Claire Rumore, Dan Gisolfi, Wes Kussmaul &amp; Dan Greening (Senex Rex)"},{"location":"RWoT6/#complete-rebooting-the-web-of-trust-listing","text":"A different repository is available for each of the Rebooting the Web of Trust design workshops: Rebooting the Web of Trust I: San Francisco (November 2015) Rebooting the Web of Trust II: ID2020 - New York City (May 2016) Rebooting the Web of Trust III: San Francisco (October 2016) Rebooting the Web of Trust IV: Paris (April 2017) Rebooting the Web of Trust V: Boston (October 2017) Rebooting the Web of Trust VI: Santa Barbara (March 2018) Rebooting the Web of Trust VII: Toronto (September 2018)","title":"Complete Rebooting the Web of Trust Listing"},{"location":"RWoT6/#license","text":"All of the contents of this directory are licensed Creative Commons CC-BY their contributors.","title":"License"},{"location":"RWoT6/DecentralizedAutonomicData/","text":"Decentralized Autonomic Data (DAD) and the three R's of Key Management Author: Samuel M. Smith Ph.D. Contributor: Vishal Gupta 2018/03/07 Abstract This paper proposes a new class of data called decentralized autonomic data (DAD). The term decentralized means that the governance of the data may not reside with a single party. A related concept is that the trust in the data provenance is diffuse in nature. Central to the approach is leveraging the emerging DID (decentralized identifier) standard. The term autonomic means self-managing or self-regulating. In the context of data, we crystalize the meaning of self-managing to include cryptographic techniques for maintaining data provenance that make the data self-identifying, self-certifying, and self-securing. Implied thereby is the use of cryptographic keys and signatures to provide a root of trust for data integrity and to maintain that trust over transformation of that data, e.g. provenance. Thus key management must be a first order property of DADs. This includes key reproduction, rotation, and recovery. The pre-rotation and hybrid recovery methods presented herein are somewhat novel. The motivating use of DAD is to provide provenance for streaming data that is generated and processed in a distributed manner with decentralized governance. Streaming data are typically measurements that are collected and aggregated to form higher level constructs. Applications include analytics and instrumentation of distributed web or internet of things (IoT) applications. Of particular interest is the use of DADs in self-sovereign reputation systems. A DAD seeks to maintain a provenance chain for data undergoing various processing stages that follows diffuse trust security principles including signed at rest and in motion. Streaming data applications may impose significant performance demands on the processing of the associated data. Consequently one major goal is to use efficient mechanisms for providing the autonomic properties. This means finding minimally sufficient means for managing keys and cryptographic integrity. Importantly this paper provides detailed descriptions of the minimally sufficient means for key reproduction, rotation, and recovery for DID leveraged DADS. Overview A decentralized autonomic data (DAD) item is associated with a decentralized identifier, ( DID ). This paper does not provided a detailed definition of DIDs but does describe how DIDs are used by a DAD. The DID syntax specification is a modification of standard URL syntax per RFC-3986 . As such, it benefits from familiarity, which is a boon to adoption. One of the features of a DID is that it is a self certifying identifier in that a DID includes either a public key or a fingerprint of a public key from a cryptographic public/private key pair. Thereby a signature created with the private key can be verified using the public key provided by the DID. The inclusion of the public part of a cyptographic key pair in the DID give the DID other desirable properties. These include universal uniqueness and pseuodnynmity. Because a cryptographic key pair is generated from a large random number there is an infinitessimal chance that any two DIDs are the same (collision resistance). Another way to describe a DID is that it is a cryptonym, a cryptographically derived pseudonym. Associated with a DID is a DID Document (DDO). The DDO provides meta-data about the DID that can be used to manage the DID as well as discover services affiliated with the DID. Typically the DDO is meant to be provided by some service. The DID/DDO model is not a good match for streaming data especially if a new DID/DDO pair would need to be created for each new DAD item. But a DID/DDO is a good match when used as the root or master identifier from which an identifier for the DAD is derived. This derived identifier is called a derived DID or DDID . Thus only one DID/DDO paring is required to manage a large number of DADs where each DAD may have a unique DDID. The syntax for a DDID is identical for a DID. The difference is that only one DDO with meta-data is needed for the root DID and all the DAD items carry any additional DAD-specific meta-data, thus making them self-contained (autonomic). DID Syntax A DID or DDID has the following required syntax: did: method : idstring The method is some short string that namespaces the DID and provides for unique behavior in the associated method specification. In this paper we will use the method dad . The idstring must be universally unique. The idstring can have multiple colon \":\" separated parts, thus allowing for namespacing. In this document the first part of the idstring is linked to the public member of a cryptographic key pair that is defined by the method. In this paper we will use a 44-character Base64 URL-File safe encoding as per RFC-4648 , with one trailing pad byte of the 32-byte public verification key for an EdDSA (Ed25519) signing key pair. Unless otherwise specified Base64 in this document refers to the URL-File safe version of Base64. The URL-File safe version of Base64 encoding replaces plus \"+\" with minus \u201c-\u201d and slash \u201c\\\u201d with underscore \u201c_\u201d. As an example a DID using this format would be as follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148= An example DID with namespaced idstring follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:blue A DID may have optional parts including a path, query, or fragment. These use the same syntax as a URL, that is, the path is delimited with slashes, / , the query with a question mark, ? , and the fragment with a pound sign, # . When the path part is provided then the query applies to the resource referenced by the path and the fragment refers to an element in the document referenced by the path. An example follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=/mom?who=me#blue In contrast, when the path part is missing but either the query or fragment part is provided then the query and/or fragment parts have special meaning. A query without a path means the the query is an operation on either the DID itself or the DID document (DDO). Likewise when a fragment is provided then the fragment is referencing an elemet of the DDO. An example of a DID without a path but with a query follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?who=me As will be described later, a query part on a DID expression without a path part will enable the generation of DDIDs (derived DIDs) Minimal DAD A minimal DAD (decentralized autonomic data) item is a data item that contains a DID or DDID that helps uniquely identify that data item or affiliated data stream. In this paper JSON is used to represent serialized DAD items but other formats could be used instead. An example minimal trivial DAD is provided below. It is trivial because there is no data payload. { \"id\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\" } To ensure data integrity (i.e. that the data has not been tampered with) a signature that is verifiable as being generated by the private key associated with the public key in the id field value is appended to the DAD item. This signature verifies that the DAD item was created by the holder of the associated private key The DAD item is both self-identifing and self-certifying because the identifier value given by the id field is included in the signed data and is verifiable against the private key associated with the public key obtained from the associated DID in the id field. In the example below is a trivial DAD with an appended signature. The signature is separated from the JSON serialization with characters that may not appear in the JSON. { \"id\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\" } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== An example DAD with a payload follows: { \"id\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"data\": { \"name\": \"John Smith\", \"nation\": \"USA\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== While the simple DADs given in the examples above are minimally self-identifying and self-certifying, they do not provide support for other self-management properties such as key management. In other words, because each DID (Decentralized Identifer) references a public signing key with its associated private key, it needs to be managed as a key not just as an identifier. The following sections will introduce the core key-management properties and the associated meta-data that a DAD needs in order to support those properties. Key Management The three main key management operations are: Reproduction Rotation Recovery We call these the essential three R's of key management. Key Reproduction Key reproduction is all about managing the creation of new or derived keys. Each new DID requires a new public/private key pair. The private keys must be kept in a secured location. One reason to create unique public/private key pairs for each pair-wise relationship is to minimize the risk of exposure to exploits from the repeated use of a given key pair. Another reason to create unique key pairs for each interaction between parties is as a means for maintaining privacy through pseudonymity . This is discussed in more detail below. Minimizing the number of private keys that must be securely preserved for a given number of public keys simplifies management and reduces both expense and risk of exposure. To reiterate, there are two key-storage issues, one is storing public keys and the other is securely storing private keys. An exploit that captures a store of public keys may mean a loss of privacy because the expoiter can now correlate activity associated with those public keys. An exploit that captures a store of private keys means that the exploiter many now be able to use those private keys to take control of any associated resources. Consequently, one wants to avoid storing private keys as much as possible. Privacy and Confidentiality One desirable feature of a DAD is that it be privacy preserving. A simplified definition of privacy is that if two parties are participating in an exchange of data in a given context then the parties should not be linked to other interactions with other parties in other contexts. A simplified definition of confidentiality is that the content of the data exchanged is not disclosed to a third party. Confidentiality is usually obtained by encrypting the data. This paper does not specifically cover encryption but in general the mechanisms for managing encryption keys are very similar to those for managing signing keys. An exchange can be private but not confidential, confidential but not private, both, or neither. A minimally sufficent means for preserving privacy is to use a DID as a pseudonymous identifier of each party to the exchange. A pseudonynm is a manufactured alias (e.g. identifier) that is under the control of its creator and that is used to identify a given interaction but is not linkable to other interactions by its owner. The ability of a third party to correlate an entity's behavior across contexts is reduced when the entity uses a unique DID for each context. Although there are more sophisticated methods for preserving privacy such as zero-knowledge proofs, the goal here is to use methods that are compatible with the performance demands of streaming data. As mentioned above, the problem with using unique pseudonyms/cryptonyms for each exchange is that a large number of such identifiers may need to be maintained. Fortunately hierachically derived keychains provide a way to manage these cryptonyms with a reasonable level of effort. Hierachical Deterministic Key Generation As previously mentioned, reproduction has to do with the generation of new keys. One way to accomplish this is with a deterministic procedure for generating new public/private keys pairs where the private keys may be reproduced securely from some public information without having to be stored. A hierarchically deterministic (HD) key-generation algorithm does this by using a master or root private key and then generating new key pairs using a deterministic key-derivation algorithm. A derived key is expressed as a branch in a tree of parent/child keys. Each public key includes the path to its location in the tree. The private key for a given public key in the tree can be securely regenerated using the root private key and the key path, also called a chain code. Only one private key, the root, needs to be stored. The BIP-32 specification, for example, uses an indexed path representation for its HD chain code, such as, \"0/1/2/0\". The BIP-32 algorithm needs a master or root key pair and a chain code for each derived key. Then only the master key pair needs to be saved and only the master private key needs to be kept securely secret. The other private keys can be reproduced on the fly given the key generation algorithm and the chain code. An extended public key would include the chain code in its representation so that the associated private key can be derived by the holder of the master private key any time the extended public key is presented. This is the procedure for hardened keys. The query part of the DID syntax may be used to represent an HD chain code or HD key path for an HD key that is derived from a root DID. This provides an economical way to specify derived DIDs (DDIDs) that are used to identify DADS. An example follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2 This expression above discloses the root public DID as well as the key derivation path or chain via the query part. For the sake of brevity this will be call an extended DID. The actual derived DDID is create by applying the HD algorithm such as: did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE= Thus a database of DDIDs could be indexed by DDID expressions with each value being the extended DID. Looking up the extended DID allows the holder to recreate on the fly the associated private key for the DDID without ever having to store the private key. This might look like the following: { \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2\", ... } Or given that the same DID method is used throughout: { \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\": \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2\", ... } The namespacing of the DID idstring also provides information that could be used to help formulate an HD path to generate a DDID. The following example shows two different DDIDs using the same public key and the same chain code but with a different extended idstring. did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:blue?chain=0/1 did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:red?chain=0/1 Some refinements to this approach may be useful. One is the granularity of DDID allocation. A unique DDID could be used for each unique DAD or a unique DDID could be used for each unique destination party that is receiving a data stream. In this case each DAD would need an additional identifier to disambiguate each DAD sent to the same party. This can be provided with an additional field or by using the DID path part to provide a sequence number. This is shown in the following example: did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057 The associated DAD is as follows: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== Change Detection Stale DAD items must often be detectable to prevent replay attacks. A later re-transmission of an old copy of the DAD item must not supercede a newer copy. Using a sequence number or some other identifier could provide change detection. Another way to provide change detection is for the DAD item to include a changed field whose value is monotonically increasing and changes every time the data is changed. The souce of the data can enforce that the changed field value is monotonically increasing. Typical approaches include a monotonically increasing date-time stamp or sequence number. Any older data items resent or replayed would have older date-time stamps or lower sequence numbers and would thus be detectable as stale. Below is an example of an non-trivial data item that has a changed field for change detection. { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== Change detection prevents replay attacks in the following manner. A second party receives DAD updates that are each signed by the associated private key. Each update has a monitonically increasing changed field. The source signer controls the contents of the data wrapped by the signature. Therefore the signer controls any changed field. A consistent signer will use a monotonically increasing changed value whenever the data wrapped by the signature is changed. Thus a malicious third party cannot replay earlier instances of the DAD wrapped by a valid signature to the orginal second party because the second party knows to discard any receptions that have older changed fields than the latest one they have already received. On the Fly DDIDS in DADs One important use case for DDIDs in DADs is to identify data that is received from a source that is not providing identifying information with the data. The receiver then creates an associated DID and DDIDs to identify the data. At some later point the receiver may be able to link this data with some other identifying information or the source may \"claim\" this data by supplying identifying information. In this case the DDIDs are private to the receiver but can later be used to credibly provenance the internal use of the data. This may be extremely beneficial when shared amongst the entities in the processing chain as a way to manage the entailed proliferation of keys that may all be claimed later as a hierarchial group. The DIDs and associated derivation operations for DDIDS may be shared amongst a group of more-or-less trusted entities that are involved in the processing chain. Public Derivation Another important used case for DDIDS in DADS is to avoid storing even the DDID with its derivation chain. This may be an issue when a client wishes to communicate with a potenially very large number of public services. Each public service would be a new pairing with a unique DDID. If the derivation algorithm for an HD-Key DDID could use the public key or public DID of the public service to generate the DDID then the client need not store the actual DDID but can recover the DDID by using the public DID of the server to re-derive the associated DDID. This can be done by creating a hash of the root DID private key and the remote server public DID to create the seed used to generate the DDID for the DAD. This also means that the DDIDs or chain codes do not have to be included in the keys preserved by a key-recovery system. Key Rotation The simplest approach to key rotation is to revoke and replace the key in one operation. In some cases revocation without replacement is warranted. But this is the same as revoking and then replacing with a null key. Key rotation without revocation usually poses a security risk so it is not needed. Hence we simplify key management to include revocation as a subset of rotation. Key rotation is necessary because keys used for signing (and/or encryption) may suffer increased risk of becoming compromised due to continued use over time, may be vulnerable to brute force attack merely due to advances in computing technology over time, or may become compromised due to misuse or a specific exploit. Periodically rotating the key bounds the risk of compromise resulting from exposure over time. The more difficult problem to solve is secure rotation after a specific exploit may have already occurred. In this case, the receiving party may recieve a valid signed rotation operation from the exploiter prior to the orignal holding entity sending a valid rotation operation. The receiver may erroneously accept a rotation operation that transfers control of the data to the exploiter. A subsequent rotation operation from the original holder would either create a conflict or a race condition for the receiver. Although there are several ways to solve the early rotation exploit problem described above, the goal is to find the minimally sufficient means for preventing that exploit that is compatible with the demands of streaming data applications for which DADs are well suited. Basic Pre-rotation A complication with DADs is that there are two types of keys being used: the keys for the root DIDs and the keys for the derived DIDS (DDIDS). Generating a derived key pair requires using the private root key. The process for pre-rotating the root DID is described first, followed by the additional measures for DDID pre-rotation. The approach presented here is to pre-rotate the DID key and declare the pre-rotation at the inception of the DID. This pre-rotation is declared at initialization. This may be done with an inception event. A later rotation operation event creates the next pre-rotated key thus propogating a new set of current key and pre-rotated key. Shown below is an example inception-event data structure with a signing key in the signer field and a pre-rotated next signing key in the ensuer field. The signature is generated using the signer key. Example inception event: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"ensuer\": \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\" } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== A useful convention would be that if a signer field is not provided then the signer is given by the id field. { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"ensuer\": \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\" } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== When rotation occurs sometime later, the rotation operation atomically indicates that the key in the signer field is to be replaced with the pre-declared rotation key in the ensuer field and also declares the next rotation key to be placed in the ensuer field. One way to keep track of this is to provide three keys in the rotation event, the former signer in a new erster field, the former ensuer in the signer field and a new pre-rotated key in the ensuer field. The rotation operation has two signatures. The first signature is created with the former signer key (now erster field). The second signature with the former ensuer key (now signer field). This establishes provenance of the rotation operation. Example rotation event: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"erster\": \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"signer\": \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"ensuer\": \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=\" } \\r\\n\\r\\n jc3ZXMA5GuypGWFEsxrGVOBmKDtd0J34UKZyTIYUMohoMYirR8AgH5O28PSHyUB-UlwfWaJlibIPUmZVPTG1DA== \\r\\n\\r\\n efIU4jplMtZzjgaWc85gLjJpmmay6QoFvApMuinHn67UkQZ2it17ZPebYFvmCEKcd0weWQONaTO-ajwQxJe2DA== Instead of three fields in the structure a list or tuple of three fields could be used where the order corresponds to [erster, signer, ensuer] . In order to verify provenance over multiple rotation operations, the receiver needs to be able to replay the history of rotation operations. The pre-rotation approach has some useful features. For many exploits, the likelihood of exploit is a function of exposure to continued monitoring or probing. Narrowly resticting the opportunity for exploits in terms of time, place, and method, especially if the time and place is a one-time event, makes exploits extremely difficult. The exploiter has to either predict the time and place of the event or has to have continuous universal monitoring of all events. By declaring the pre-rotation at the inception event of the associated DAD, the window for exploits is as narrow as possible. Pre-rotation does not require any additional keys or special purpose keys for rotation. This makes the approach self-contained. Because the rotation-operation event requires two signatures, one using the current key and the other using the pre-rotated key, an exploiter would have to exploit both keys. This is extremely difficult because the only times the private side of the pre-rotated key is used are (1) at its creation in order to make the associated public key, and (2) at the later signing of the rotation operation event. This minimizes the times and places to a narrow sample. Listed Rotation Key Structure Another approach to declaring rotation events is to provide the full rotation history in the rotation operation and/or to use a list structure for providing the keys. In many cases, rotations are a rare event so the number of entries in the rotation history would be small. In the associated data structure a list of all the signers both former and future to date is provided in the signers field. The current signer is indicated by an index into the list in the signer field. The list index is zero based. The pre-rotated next signer or ensuer is the following entry in the signers list. A rotation event then changes the signer field index, which implies that the former signer ( erster ) is the previous entry and the next pre-rotated signer ( ensuer ) is the subsequent entry after the signer index. This is shown in the following examples. Example pre-rotated inception event with list structure for signing keys: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": 0, \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", ] } \\r\\n\\r\\n jc3ZXMA5GuypGWFEsxrGVOBmKDtd0J34UKZyTIYUMohoMYirR8AgH5O28PSHyUB-UlwfWaJlibIPUmZVPTG1DA== The signature above is with key at index = signer = 0. Example rotation event with list structure for signing keys: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": 1, \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=\" ] } \\r\\n\\r\\n jc3ZXMA5GuypGWFEsxrGVOBmKDtd0J34UKZyTIYUMohoMYirR8AgH5O28PSHyUB-UlwfWaJlibIPUmZVPTG1DA== \\r\\n\\r\\n efIU4jplMtZzjgaWc85gLjJpmmay6QoFvApMuinHn67UkQZ2it17ZPebYFvmCEKcd0weWQONaTO-ajwQxJe2DA== The first signature is with key at index = signer - 1 = 0. The second signature is with key at index = signer = 1. A subsequent rotation would add another key to the signers list and increment the signer index as follows: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": 2, \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=\", \"3syVH2woCpOvPF0SD9Z0bu_OxNe2ZgxKjTQ961LlMnA=\" ] } \\r\\n\\r\\n AeYbsHot0pmdWAcgTo5sD8iAuSQAfnH5U6wiIGpVNJQQoYKBYrPPxAoIc1i5SHCIDS8KFFgf8i0tDq8XGizaCg== \\r\\n\\r\\n o9yjuKHHNJZFi0QD9K6Vpt6fP0XgXlj8z_4D-7s3CcYmuoWAh6NVtYaf_GWw_2sCrHBAA2mAEsml3thLmu50Dw== Multi-signature Pre-rotation The list structure enables the declaration of several pre-rotations in advance by providing several future pre-rotation keys in the inception event. A rotation event then could include several rotations at once. Each rotation event would require a signature per each of the multiple rotations in the event thus allowing for multi-signature inception and rotations. If each key is from a different entity, then the rotation would require multiple entities to agree. Thus a DAD could be multi-signature and support multi-signature rotations. In this case the signer field would be a list of indices into the signers list. This approach could be further extended to support an M-of-N signature scheme where any M-of-N signatures are required to incept or rotate where M < N, and M, N are integers. The total number of keys in the list is a multiple of N. The following examples provide an inception and rotation event for a two signature pre-rotation. A namespaced key with a colon-separated idstring, as per the DID syntax, could be used to allow for signers using a different DID method or for namespacing within a given DID method. Example of a pre-rotated two-signature inception event with list structure for signing keys where \" blue \" indicates one source and \" red \" indicates another source: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": [0,1], \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=:blue\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:red\", \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=:blue\", \"3syVH2woCpOvPF0SD9Z0bu_OxNe2ZgxKjTQ961LlMnA=:red\" ] } \\r\\n\\r\\n AeYbsHot0pmdWAcgTo5sD8iAuSQAfnH5U6wiIGpVNJQQoYKBYrPPxAoIc1i5SHCIDS8KFFgf8i0tDq8XGizaCg== \\r\\n\\r\\n o9yjuKHHNJZFi0QD9K6Vpt6fP0XgXlj8z_4D-7s3CcYmuoWAh6NVtYaf_GWw_2sCrHBAA2mAEsml3thLmu50Dw== The signatures above are generated with the keys at indices 0 and 1 in the signers list respectively. Example of a two-signature rotation event with list structure for signing keys where \" blue \" indicates one source and \" red \" indicates another source: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": [2,3], \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=:blue\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:red\", \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=:blue\", \"3syVH2woCpOvPF0SD9Z0bu_OxNe2ZgxKjTQ961LlMnA=:red\" \"rTkep6H-4HA8tr54sHON1vWl6FEQt27fThWoNZsa88V=:blue\", \"7IUhL0JRaU2_RxFP0AL43wYn148Xq5YqaL6L48pf0fu=:red\", ] } \\r\\n\\r\\n AeYbsHot0pmdWAcgTo5sD8iAuSQAfnH5U6wiIGpVNJQQoYKBYrPPxAoIc1i5SHCIDS8KFFgf8i0tDq8XGizaCg== \\r\\n\\r\\n o9yjuKHHNJZFi0QD9K6Vpt6fP0XgXlj8z_4D-7s3CcYmuoWAh6NVtYaf_GWw_2sCrHBAA2mAEsml3thLmu50Dw== \\r\\n\\r\\n GpVNJQQoYKBYrPPxAoIc1i5SHCIDS8KFFgf8i0tDq8XGizaCgAeYbsHot0pmdWAcgTo5sD8iAuSQAfnH5U6wiI== \\r\\n\\r\\n 8z_4D-7s3CcYmuoWAh6NVtYaf_GWw_2sCrHBAA2mAEsml3thLmu50Dwo9yjuKHHNJZFi0QD9K6Vpt6fP0XgXlj== The signatures above are generated with the keys at indices 0 through 3 in the signers list respectively. Collective Signatures This multi-signature scheme suffers from the significant increase in the length of the attached signature block. One way to ameliorate this \"bloat\" is to use collective multi-signatures. A collective signature has the property that its length is not a multiple of the number of signatures it holds. Typically the maximum length of a collective signature is about double the length of a non-collective signature and does not increase significantly as more signatures are added to the collective. There is a draft IETF standard for collective signatures CoSi that might be useful for multi-signature rotation. Some useful references are here project , paper , slides . Collective signatures are a type of Schnorr multi-signature or Schnorr threshold signature. DDID Pre-rotation The complication for DDIDs (Derived DIDs) is that each DAD stream for each pairing of sender and receiver may have a unique DDID. Rotation of the root DID also requires rotating the DDIDs. The same pre-rotation approach, however, can be used for the DDIDs. At the inception event the root key and pre-rotation root keys are created. These keys are then used to created a set of DDIDS and pre-rotated derived keys using the root and pre-rotated root keys respectively. This does not significantly change the exploit vulnerability as the inception event is still one event. Although the pre-rotated root DID key is used to create a set of pre-rotated derived keys, it does not signicantly increase its exposure. Each rotation event then involves rotating the root DID key and all the DDID keys. The important consideration is that the number of DDIDs in the set must be determined in advance in order to create all the pre-rotated derived keys at one time. This can be managed by creating extra DDIDs and pre-rotated derived keys at the inception event. Only the public half of each of the key pairs need to be stored. In contrast, creating additional DDIDs with pre-rotated keys at a later time requires using the pre-rotated root private key. This increases the exposure of that private key to exploits and makes it less secure for pre-rotation. When the set of pre-rotated DDIDs is consumed, a rotation-operation event may be triggered, thereby rotating the existing DDIDs and then allowing additional DDIDs to be created. Alternatively if the pre-rotated set of DDIDs is consumed then a new DDID tree may be created with a unique new pre-rotated root key. This would create a hierachy of groups of pre-rotated DDIDs and derived keys. Moreover, when the re-establishment and re-initialization of a DAD stream is not a high-cost or high-risk endeavor then instead of pre-rotating the DDIDs, only pre-rotate the root DID and just close down the current DAD stream and re-establish with a new DDID created by the pre-rotated key as part of the rotation event. Finally if the exposure of the root DID is insignificant compared the exposure of the DDIDs then another approach to DDID pre-rotation could be employed. This requires a trade-off between convenience and privacy. A group of receivers could all have knowledge of the root public DID key and its pre-rotated public DID key for their unique DDIDs. This means that the members of the group could leak correlation information about the group via the shared root DID. However each member of the group could still maintain security via its unique DDID. In this case the root private DID is used to derive both the inception DDID and the pre-rotated derived key of each member. The individual members could then undergo DDID key rotation but only using the root DID not its pre-rotated key. In the rare event that the root DID needs to be rotated then each of the DDID members performs a double rotation within a rotation event. The first rotation rotates to the pre-rotated key generated using the original root DID, the second rotation is to a new set of derived and pre-rotated derived keys, each generated using the new pre-rotated root key. The first derived key in the pair is the new signer key, the second is the new pre-rotated signer key. A receiver must have knowledge of the root DID and pre-rotated root key in order to verify that the second rotation is not a forgery. This approach enables the organization and managment of DDIDs in heirarchical groups where the members of each group know about their group-root DID but that group-root DID could be a DDID of a higher level group and so on. Lower level groups only know about thier group root DID, but not any sibling groups so it can't leak information about sibling or parent groups only child groups. Replayability The constraint on pre-rotation is that the receiving party be able to replay the rotation events to ensure that it did not miss a rotation. This replay allows the receiver to verify the provenance chain of rotations. The question then is what are minimally sufficient means for enabling this replay capability? There are two use cases for providing this replay capability. The first case is for online one-to-one or pairwise interactions and the other case is for offline one-to-one or equivalently one-to-many or public interactions. In the one-to-one case, there is the sender of a DAD stream and the reciever of the stream. The initiation of the stream would involve exchanging keys for pairwise communication and would also include the establishment of the DDID used for the DAD items sent. The first DAD sent would include the DDID for the DAD as will as the pre-rotated DDID. This is the inception event. The receiver then merely needs to maintain a running log of DAD items that contain rotation events. As long as reliable communications are used between the sender and receiver, then the receiver can ensure that it has observed all rotation events by keeping its log and no imposter can later send an undetectable forged inception or rotation event. If the reciever loses its history then it must re-establish its communications channel and re-initialize. Alternatively the sender could maintain a copy of the inception and rotation event history and then provide it to the receiver upon request. The receiver would cache this history for speedier lookup. An imposter attempting to send an earlier forged inception event would be unsuccessful because only the first inception event is considered valid. In the one-to-many, public, or offline case, the rotation history is maintained by a service. While a decentralized distributed consensus blockchain ledger could provide this service it is not the minimally sufficient means of providing this capability. The minimally sufficient means is a redundant immutable event log of inception and rotation events indexed by the DDID associated with the DAD for the given DAD stream. The constraint is that a sufficient majority of the log hosts must be non-faulty at any point in time. This includes Byzantine faults. Is is also assumed that the sender communicates with the hosts using a reliable end-to-end signed protocol. The sender broadcasts the inception event to all the redundant hosts that provide copies of the log. These hosts are called Replicants. Then either the Replicants respond to the sender with a confirmation that the event is written to their log or the sender reads the log to verify. The event history is indexed by the DDID. Each Replicant timestamps and signs each entry in each event history. Each Replicant only allows one and only one inception event per event history. Attempts by imposters to forge an earlier inception event would be denied by honest Replicants. The sender can then verify that a sufficient majority of the Replicants have captured each event and have consistent event histories. Subsequent rotation events are redundantly appended to the DDID indexed log in the same way. The receiver can then broadcast a query to the Replicants and verify via their responses that a sufficient majority of the Replicants have the same DDID indexed event log. This eanbles both offline and one-to-many event streams. This approach is more scalable than using a distributed consensus ledger because the Replicants do not need to communicate with each other. The inter-host agreement of the members of a distributed consensus pool is usually the limiting factor in scalablity. Morever a given receiver could be completely responsible for providing the immutable log service for its own data stream with the sender. Each receiver could choose to implement a different level of reliability. Loss of the event log means that the sender and receiver have to re-initialize and re-establish the DAD stream. Alternatively the sender could be responsible for providing a set of Replicants and make the event log available to the receiver upon request. Key Recovery Key recovery is about providing a secure way of recovering a lost private key. The important consideration here is that the recovery mechanism be compatible with streaming data applications as per DADs. Keys recovery tends to be a rare occurrence so performance demands may be less constraining. Nonetheless, finding the minimally sufficient means for key recovery is still the goal. Moreover, to be secure the private key needs to be kept secret. Because cryptographic keys are long strings of numbers they are extremely hard to remember, this means that typically private keys are stored some place besides a person's memory and are therefore subject to being lost or stolen. If it is required or at least desirable that the DAD stream not be reinitialized due to the loss of the rotation-event history then a key-recovery mechanism would also need to provide recovery of the key-rotation history. To restate, it is not enough to just recover the original root DID but every rotated root DID must be recovered as well. Given that typically rotations happen rarely, the rotation-event history should be small in size and not pose a storage-size problem for recovery. Thus key recovery for DADs needs to at least recover the original root key and any rotations. DDIDs can be regenerated from the root DID given the HD-derivation code. In the case where the the DDID stream may not be easily reestablished but must resume given the latest rotated DDID then the HD chain code must also be preserved and recovered. If the number of DDIDs is very large then the storage requirements for chain codes may also be large relative to the storage requirement for key recovery. The DID root public key and DDID derivation chain codes do not expose the private keys. However, although disclosing the root public key and chain code for a DDID is not a security risk, it could be a privacy risk. A third party could correlate data streams from the associated DDIDs should the root public key used by multiple DDIDs be exposed. One way to address this is to encrypt the chain codes with an encryption key derived from the root signing key. The chain codes can then be stored outside of the core recovery system. The worst case exploit then is a loss of privacy should the encryption be broken but not a loss of control of the resources owned by the private key. When the DDID for communicating with a public service is derived from the public key of a server then the client does not need to preserve and recover the HD chain code. Instead it can regenerate the DDID using a hash of the root private DID and the public DID of the server. A complication occurs when the root private key has been rotated and the server was not made aware of the rotation. The client can still recover the current root DID used by the server using a trial and error approach by going through the list of rotated root DIDs, generating the associated DDID or derived key, verifying if the server will accept it, and if not incrementing to the next rotated root. Eventually the client will discover the last rotated DDID or derived key recognized by the serve. As a result the client can recover the appropriate DDID or derived key for a given service without having to preserve anything but the history of rotated root DIDs. This approach may provide meaningful storage savings when the number of external services is large. Cryptographic Strength Information Theoretic Security and Perfect Security With respect to DAD , key recovery deals with the recovery of the private half of signing and/or encryption keys in public/private key pairs. Given that once an adversary has the private key, security is completely broken, the cryptosystems used to backup and recover private keys needs to be as secure as is practically possible. The highest level of crypto-graphic secruity is called information-theoretic security . A cryptosystem that has this level of security cannot be broken algorithimically even if the adversary has nearly unlimited computing power including quantum computing. It must be broken by brute force if at all. Brute force means that in order to guarantee success the adversary must search every combination of key or seed. A special case of information-theoretic security is called perfect security . Perfect security means that the cipher text provides no information about the key. There are two well-known cryptosystems that can exhibit perfect sercurity. One is secret sharing or splitting (see also ss ). The other is a one-time pad (see also otp . Correct implementation of either/or a combination of these two approaches is appropriate for private-key recovery. Sufficient Cryptographic Strength to Withstand a Brute-force Attack For cryptosystems with perfect security, the fundamental parameter is the number of bits of entropy needed to resist any practical brute force attack. In other words, when a large random number is used as a seed/key to a cryptosystem that has perfect security, the question to be answered is how large does the random number need to be to withstand a brute force attack? In Shannon information theory the entropy of a message is measured in bits. The randomness of a number or message can measured by the number of bits of entropy in the number. A cryptographic quality random number will have as many bits of entropy as the number of bits in the number. Assuming conventional non-quantum computers, the convention wisdom is that, for systems with information theoretic or perfect security, the seed/key needs to have on the order of 128 bits (16 bytes) to practically withstand any brute force attack. For other cryptosystems that do not have perfect security the size of the seed/key may need to be much larger. Theoretically, quantum computers, using Grover's Algorithm might be able to brute force a 2 N random number with only 2 N/2 trials. Thus once quantum computers exists the size of N might need to increase from 128 to 256. An N-bit long base-2 random number has 2 N different possible values. Given that with perfect security no other information is available to an attacker, the attacker may need to try every possible value before finding the correct one. Thus the number of attempts that the attacker would have to test may be as much as 2 N-1 . Given available computing power, one can estimate if 128 is a large enought N to make brute force attack impractical. Let's suppose that the adversary has access to supercomputers. Current supercomputers can perform on the order of one quadrillion operations per second. Individual CPU cores can only perform about 4 billion operatons per second but a supercomputer will employ many cores in parallel. A quadrillion is approximately 2 50 = 1,125,899,906,842,624. Suppose somehow an adversary had a million (2 20 = 1,048,576) super computers to employ in parallel. The adversary could then try 2 50 * 2 20 = 2 70 values per second (assuming that each try only took one operation). There are about 3600 * 24 * 365 = 313,536,000 = 2 log 2 313536000 =2 24.91 ~= 2 25 seconds in a year. Thus this set of a million super computers could try 2 50+20+25 = 2 95 values per year. For a 128-bit random number this means that the adversary would need on the order of 2 128-95 = 2 33 = 8,589,934,592 years to find the right value. This assumes that the value of breaking the cryptosystem is worth the expense of that much computing power. Consequently, a cryptosystem with perfect security and 128 bits of cryptographic strength is practically impossible to break. Recovery Methods Fundamentally key recovery involves shifting the burden of remembering a cryptographic key made of a long random string of numbers to some other task that is less onerous. Physical Security One approach to recovery is to shift the burden of recovery from remembering a private key or keys to protecting physical copies of the keys. This is called physical security. Recovery first involves creating a hard copy of the key(s) such as a printed piece of paper or a \"hard\" electronic wallet and then hiding the hard copy. The memory task now becomes remembering where the hard copy was hidden. The security of the approach is now based on the physical security of the hidden location (under the bed, in the safety deposit box, in a hole in the backyard). The assumption is that remembering where something is hidden is assumed to be relatively reliable. Most important is that physical security is not vulnerable to remote attacks over the internet nor computational attacks where the attacker can employ resources and time to break a key. The attacker must have physical access and may be physically at risk. A weakness of this approach is that recovery may take time. Moreover if the person with the knowledge of the key location is incapacitated then recovery may be impossible unless the location of hard copy or another hard copy is shared with someone else, thus exposing a vulnerability. One way to address this is to use a legal mechanism such as power of attorney, a will, or another guardian who is authorized to reveal the hard copy given predefined circumstances. This can be ameliorated by using tamper-resistant envelopes and physical access logs to increase the risk of discovery. In any event physical recovery is useful as a backup to non-physical security recovery methods but may be too inconvenient as the primary form of recovery for the managers of streaming data applications. In general physical security may be a good backup for any of the other recovery methods. Mnemonics A mnemonic is a device or technique to aid human memory. The memory task in this case is to remember a 128-bit random number as a key or seed. This is further complicated for DAD recovery as it is not sufficient to just recover a single private key but instead requires the recovery of the whole key rotation history. One way to accomplish this is to use a 128-bit random number as a seed to a system that hides and recovers the whole rotation history. This will be discussed in more detail below. One well-known mnemonic is to use a phrase of random words from a word list. The user can create a story or imaginery visualation of a situation in which the words are all represented. An example would be the words, blue cat house eat pudding . Visualizing and rehearsing a fantastic situation that includes objects and actions corresponding to the words makes is much easier to remember. The DiceWare (see also wk and pp approach consists of a word list of 7776 words that are selected at random (using dice). The user must remember the words and their order to form a phrase that can be used to generate a random number. The EFF has produced modified versions of the word list ( EFF word list ) that have beneficial properties. Given a total of 7776 words, then each randomly selected word is one of 7776 choices, which provides log 2 (7776) = 12.9 bits of entropy per word. To get a 128 bits of entroy the phrase would need to include ten words. This is pretty long for a mnemonic but not impractical as long as the user is willing to do some rehearsal. More problematic is recovering not just one key but multiple keys from a key rotation history. Secret Sharing Another approach is to shift the task of recovery to other parties. This can be done securely using a secret sharing or \"splitting\" approach. The secret information is split into what are sometimes called shards. Each shard is then shared with another party called a shard holder. Later the shards are collected and combined to reproduce the secret. The shard holders must either keep the shard secret or if they are going to store it online they need to encrypt the shard and must then remember their encryption key. As mentioned above, secret sharing may have perfect security. This means that storing encrypted copies of the shards online may still be perfectly secure as long as an adversay cannot correlate the shards as belonging to the same secret information. If correlation does occur then the security is limited to the type of encryption and might be more vulnerable to exploits. In order to recover the secret information the user must interact with the shard holders to get them to provide their shard; that is, the recovery is multi-party interactive. The user then combines the shards to reconstitute the shared secret. This interaction may take time and may not be reliable. A useful variation on this approach is called threshold or Shamir sharing where only a subset of all the shards is needed to reconstitute the secret. For example an M of N threshold secret sharing (M < N) algorithm would share shards with N parties. Any combination of a subset of M parties can reconstitute the secret. This allows some of the parties to not be available or to lose their shard and still have successful recovery. Typically, to maintain secrecy the N parties do not know of each other. Although the security properties of Secret sharing make it an attractive approach for key recovery, secret sharing can be complicated, especially because it requires interaction with multiple parties. The secret owner must recall who the N parties are or at least M non-faulty parties. In an organizational setting, however, there may be a designated group of individuals who know about and hold the shards and have a policy for circumstances under which they can share the shards. One-time Pad As mentioned previously, the one-time pad (OTP) (see also otp ) may exhibit perfect security. The OTP is a venerable cyphersystem that has the advantage that it can be used manually without a computer. Basically a long string of random characters forms the pad . Someone can use the pad to encrypt a plain-text message. The procedure is to combine each plain-text character in order with the corresponding character from the pad. The combination is typically performed using modulo addition of the two characters but can be performed with a bitwise XOR. Because characters from the pad may only be used once, the pad must be at least as long as the plain-text message. The one time use of a random string of characters from the pad is what gives the system its perfect security property. If two parties wish to exchange multiple messages, then the pad must be at least as long as the sum of the length of all the messages. The main disadvantage of a one-time pad is that the two parties must each obtain a copy of the same pad . This is less of a disadvantage for key recovery because the the encrypted message (keys) does not need to be exchanged with another for decryption but are decrypted by the self-same party so only one copy of the pad is needed. Suppose for example, a OTP is used to encrypt the key or key history. Given that the adversary does not have access to the OTP then the encryption has perfect secrecy which means that the only viable attack is via brute force. If the encrypted key or key history is at least 128 bits long then brute force is practicaly impossible. Consequently the OTP encrypted key history could be safely stored in a public immutable database. The remaining problem is management of the OTP. Using an OTP to encrypt the key history just creates a new problem, that of securing the OTP itself. But the main advantage of a OTP over secret sharing described above for key recovery is that a OTP approach is non-multi-party interactive. It can be self-contained which is advantageous in data streaming applications. One common but weaker variant of the OTP is the book cyper. In this variant the OTP is a book. Because the characters in a book are not a random string there is some degree of correlation between characters that makes it less than perfectly secure. Thus two parties who each have a copy of the same book (same edition) can use the characters in the book as the OTP to encrypt messages without ever having to exchange copies of the book. Essentially using a book as OTP is an example of hiding the OTP in plain sight. An adversary would have to guess that a book was being used as a one-time pad and then figure out which book. For key recovery, the key owner merely needs to remember which book and edition. Should the book used by the key owner be lost, the key owner can get another copy from a bookstore. The book cypher is an interesting example due to the combination of simplicity, the use of existing but readily available sources of information, and the ability to hide the OTP as book in plain sight. This has the advantage that the only the title and edition of a book need to be remembered thus making light demand on human memory. The primary disadvantage of the book cypher is that the text is not random and its difficult to calculate how many bits of entropy are lost for a given book. Hybrid Key Recovery Method One of the main attractions of using a one-time pad (OTP) for key recovery, in contrast to secret sharing, is that it is non-multi-party interactive. A hybrid approach that makes a beneficial trade-off is to use a mnemonic merely to generate a seed for a cryptographic strength psuedorandom number generator (CSPRNG). The seed is then used via the CSPRNG to generate a OTP that is then used to encrypt the key-rotation history. The cryptographic strength of the OTP is now governed by the length of the seed not the length of the pad. But key-rotation histories are relatively short compared to the period of CSPRNG so a strong enough seed (128 bits of entrophy) would still be sufficient for this task. The PRNG algorithm must be of cryptographic quality otherwise it could become a source of vulnerability. A recent advancement in CSPRNG algorithms is a chaotic iteration psuedorandom number generators (CIPRNG) . These are of cryptographic quality have extremely high statistical randomness. They pass both the NIST and DieHard tests for PRNG with periods on the order of 10 9 opt . The basic concept is a chaotic finite state machine cfsm . Unfortunately there do not yet appear to be any open source implementations of this algorithm. A more practial CSPRNG that could be used to generate a OTP from a seed is the libsodium randombytes_buf_deterministic function. This uses ChaCha20 under the hood. The advantage of this hybrid approach is that the key recovery memory task is now limited to merely recovering the seed that would then be used to reproduce the OTP that would then be used in turn to decrypt the key history. This approach does not require multi-party interaction like secret sharing as the seed is directly recovered by the owner via a mnemonic device, not from others. This hybrid approach still benefits from the properties of the OTP for encryption so that the key-rotation history can be encrypted and stored online for recovery. What remains then is the selection of a mnemonic for generating the seed. It may be difficult for a single mnemonic to provide a random source of seed material at the required strength of 128 bits. Concatenating several sources of mnemonically derived seed material, however, could produce the required strength. This is akin to the DiceWare approach to passphrase generation. One problem with concatenation of seed material is that the order of concatenation must also be remembered. One way to avoid having to remember the order when combining multiple sources of seed material is to use the simple version of secret splitting . In this form of secret splitting, the secret is divided into shards and each shard is XORed together to recover the secret. In this case the secret is the seed and each shard contributes a certain amount of entropy to the final seed. This allows a mnemonic for each shard that may have much less than the required 128 bits of entropy but the combination of shards could have the required entropy and the order of the shards is not important. A non-ordered combination loses some cryptographic strength because the number of possibilties is no longer merely the multiple of the independant possibilities from each shard (permutations) but is instead the number of combinations of the shards. Suppose that there are four shards that each contribute 35 bits of entropy or in other words each shard is randomly chosen from 2 35 possibilities. Then the combined number of possibilities is 2 35 taken four at a time. The exact formula for the combination of N things taken K at a time is given by: N!/(K!*(N-K)!) Computing factorials for very large numbers is a computationally intensive task. For the sake of analysis an approximation is sufficient. A lower bound on the number of combinations of N things taken K at a time is (N/K) K (see bounds ). The bits of cryptographic strength of the combination of four shards each with 35 bits is where N = 2 35 and K = 4. Using the approximation gives the number of possiblities to be at least (2 35 /2 2 ) 4 = 2 33*4 =2 132 . This corresponds to 132 bits of entropy, which is greater than the required 128. The one remaining challenge then is to find good mnemonically recoverable sources of random seed material. One feature that makes the The book cypher was attractive because it took advantage of information that was highly available but hidden in plain sight and whose source was easy to remember (a book title). The problem with books is that the content is not highly random so it in itself is not a good source of seed material. In other words, the challenge is to find sources of information for seed material that have much higher degree entropy than a book but are still easy to remember. More specifically this means finding sources of highly random seed material that are highly available (thus do not require additional infrastructure to backup) but are also essentially hidden in plain sight and easy to recall via a mnemonic device. What follows are several viable sources of mnemonically recoverable sources of random seed material. DiceWare Seed Recovery The DiceWare approach can be repurposed to provide a mnemonic source of seed material. These can be used to recover the seed for the one-time pad used to encrypt the key-rotation history. Ten randomly selected words from a DiceWare-compatible wordlist could be used to generate the seed for the one-time pad. Ten randomly selected words in order provide the required 128 bits of entropy (recall that each DiceWare word provides 12.9 bits of entropy). The order of the words is important. Each word would be hashed using SHA-2 or Blake to generate a 16-byte string. The seed is created by concatenating the hashes in the defined order. Once the seed for the OTP is generated, the rest of the recovery method follows the process described above for generating the OTP using a CSPRNG and then using that to encrypt/decrypt the key rotation history. The mnemonic load for this method is the recall the order of ten words from the DiceWare or EFF wordlist. This has a large mnemonic load so it would require some rehearsal and might not be very practical. In addition to the mnemonic at least a physical backup of the ten words should also be created. The physical backup of the ten words could be split into parts to make it more secure. If practical, a multi-party threshold secret sharing backup could also be created. GitHub Seed Recovery Github.com stores versioned code repositories. The associated git utility automatically calculates a 160 it (20 byte) SHA-1 hash of each commit to a repository. These hashes are easily readable from the GitHub.com web site. Several Github commit hashes can be used to create the seed to generate the OTP for encrypting the key rotation history. In order to recover a commit hash one must remember the project and repository name, and the date of the commit. If there are multiple commits on the same date then one must also remember which commit, like the last or the first. This is not an onerous memory task but not a trivial one. There are over 80 million GitHub repositories. A reasonable estimate of the average number of commits per repository is over 1000. This means that there are about 80,000,000 * 1,000 = 80,000,000,000 = 2 36.22 possibilities to choose from. If a repository/commit is selected randomly then the number of bits of entropy represented by a single choice is about 36. To get 128 bits of security one would need to randomly select four repository/commits. A permutation of 4 gives 4 (36) = 144 bits of entropy. Remembering the order of the four repositories adds another memory task. If instead the four choices were combined using the simple version of secret splitting described above, where each shard is XORed together to recover the secret, then the number of random possibilities is reduced to the number of combinations of 80,000,000,000 items taken four at a time. As previously described, the lower bound on the number of combinations of N things taken K at a time is (N/K) K . In this case K = 4 and N = 2 36 . This gives the number of possibilities to be (2 36 /2 2 ) 4 = 2 34 4 =2 136 . This corresponds to 136 bits of entropy which is still greater than the required 128. The GitHub.com based recovery mechanism can be summarized as follows: Randomly choose four GitHub.com repository commits. For each commit, the pairing of a project name, repository name and commit date must be remembered and/or backed up using a hardware backup. Generate a seed by XORing together the 20-byte commit SHA-1 commit hash from each of the four repositories. Use this seed with a deterministic CSPRNG to generate a one-time pad of length at least as long as the key rotation history. Encrypt the key rotation history by bitwise XORing each byte in the history with the corresponding byte from the one-time pad. Securely discard the one-time pad. Store the encrypted key-rotation history in a highly available database. This encrypted history should be impervious to attack so it can be stored online. When recovery is required, remember the four project/repository/commit-date pairings or restore from a hardware backup. Use the pairings to lookup the SHA-1 commit hashes from GitHub.com for each. Then recreate the seed by XORing the four commit hashes. Use the seed and the same CRPRNG to regenerate the one-time pad. Retrieve the key history from the database. Use the one-time pad to bitwise XOR each byte of the saved encrypted key history to unencrypt it. The key history is now recovered. The memory load is four triples of a project name, a repository name, and a date, or twelve items total, but the order of the triples is not important. Given that typically each GitHub project has a small number of repositories, merely remembering the project should make remembering the repository much easier by going to the project page and looking at the choices for repositories. The date is the hardest memory task. There are several well known mnemonic techniques for remembering dates. In addition to the mnemonic, a physical backup of the hashes should also be created. The physical backup could be split into four parts to make it more secure. If practical a threshold multi-party secrete sharing system could provide additional backup. FlickR.com Seed Recovery The FlickR.com-based recovery mechanism is similar to the Github.com based one. There are over 10 billion primary photos on FlickR. Each primary photo may come in multiple resolutions. A given photo is displyed on the FlickR.com web page using a low-resolution copy. This displayed version can be scraped from the page. The Flickr.com website does not provide hashes of the images, so one would have to scrape or download an image and then calculate the hash after the fact. A viable approach would be to use SHA-2 from the OpenSSL library or Blake from the libsodium library. Ten billion is about 2 33.22 which corresponds to about 33 bits of entropy when randomly selected. Four randomly selected images are needed to get the required 128 bits of entropy, that is, 4 * 33 = 132. If we combine the hashes from four images by XORing (i.e. simple secret splitting) then the number of choices becomes the combinations of 10 billion things taken four at a time. As described above, the lower bound on the number of combinations of N things taken K at a time is (N/K) K . This gives the number of possibilities to be (2 33 /2 2 ) 4 = 2 31*4 =2 124 . This corresponds to 124 bits of entropy which is close enough to the required 128. (2 4 = 16, which is not meaningfully weaker as it would still take 500,000,000 years to break). The proceedure for recovery is essentially the same as the GitHub example above, once the hashes for each photo have been generated. The mnemonic task is remembering four images. Humans are very good at remembering images given a selection. The hard mnemonic task is searching on FlickR for a given image using tags. It takes about four or five tags to get the list of images to under 100 for a given tag set. The mnemonic task is then to remember four sets of four to five tags each, where the tags are not in any order. Remembering which photo is helped by the fact that the tag set typically corresponds to features of the photo. Moreover, images provide an opportunity to hide them in plain sight. In addition to the mnemonic, a physical backup of the hashes should also be created. The physical backup could be split into four parts to make it more secure. If practical a threshold multi-party secret sharing system could provide additional backup. Geneological Database Seed Recovery FamilySearch.org has over six billion genealogical records indexed by name and life-event type, event date, and event place. There are seven standard event types such as birth, death, marriage, census, military service, immigration, and probate. A randomly selected record can be recovered with a name and the event details of event type, date, and place. With six billion records and seven event types there are over 42 billion choices. The number of bits of entropy for one randomly selected record is log 2 (42,000,000,000) = 35.29. Suppose four records are randomly selected. hTe OTP seed is created by XORing a SHA-2 or Blake hash from each record where the hash is computed from the record name and event details. This produces (2 35 /2 2 ) 4 = 2 33*4 =2 132 combinations which corresponds to 132 bits of entropy. This exceeds the desired 128. The mnemonic task is to remember the name, event type, event date, and event place for four different records. The records can be in any order. In addition to the mnemonic a physical backup of the hashes should also be created. The physical backup could be split into four parts to make it more secure. If practical a threshold multi-party secrete sharing system could provide additional backup. Google Maps Seed Recovery The Google Maps database covers the entire globe with high resolution imagery of the land area. The world's land area is approximately 150,000,000 km 2 . It has been estimated that 90% of the landmass is inhabited although only 10% is considered urban. Lightly populated areas still have memorable identifiable features suitable for map based mnemonics such as roads, fences, and buildings (farms, huts, etc). The estimated inhabited surface area is 0.9 * 150,000,000 km 2 = 135,000,0000 km 2 . The resolution of Google Maps' georeferenced satellite photos is given in decimal degrees to six decimal places. For example, clicking on a map gives the location in (degrees latitude, degrees longitude) as (45.348807, -105.709547). Six decimal places is about one tenth of a meter. This is too small to reliably reproduce merely by clicking on the satellite view. Five decimal places is about one meter. This is big enough that it can be reproduced reliably albeit carefully by clicking on the satellite view. A conservative approach would be four decimal places which is about 10 meters. This is easily large enough that it is trivial to reproduce reliably by clicking on the satellite view. A resolution of approximately one square dekameter (10m) 2 or 4 decimal places per location gives a total of 135,000,000 * 10,000 = 1,350,000,000,000 = 2 40.3 unique locations. When selected randomly this corresponds to over 40 bits of entropy per location. A resolution of a square meter per (1m) 2 or 5 decimal places per location gives a total of 135,000,000 * 1,000,000 = 135,000,000,000,000 = 2 46.94 unique locations. When selected randomly this corresponds to over 46 bits of entropy per location. At a resolution of a square dekameter four randomly chosen locations are needed to reach over 128 bits of entropy, (4 * 40.3 = 160.9). At a square meter resolution only three randomly chosen locations are needed to reach over 128 bits of entropy, (3 * 46.94 = 140.82. When locations are combined using a secret splitting approach, the total number of combined unique locations in combination is reduced. As described above, a lower bound on the number of combinations of N things taken K at a time is (N/K) K . At the square dekameter resolution, K = 4 and N = 2 40 . This gives the number of possibilities to be (2 40 /2 2 ) 4 = 2 38 4 =2 152 . This corresponds to 152 bits of entropy which is greater than the required 128. At the square meter resolution, K = 3 and N = 2 46 . This gives the number of possibilities to be (2 46 /2 1.59 ) 3 = 2 44.41 3 ~= 2 133 . This corresponds to 133 bits of entropy which is still greater than the required 128. Consequently with Google Maps either three or four unique locations are needed to achieve the desired cryptographic strength for seed generation. Memorable locations could include the corner of a building or or a doorway or roofline or road intersection or fenceline intersection or pole. The mnemonic load for a site is the address of the site. Because humans are adept at remembering locations visually by familiarity with the surroundings, exact addresses may not be needed. Merely enough of an address to move the view within the neighborhood of a location may be enough. Once in the neighborhood, terminal navigation may be performed via visual interaction with the Maps app. Alternatively, landmarks, business or other nearby features could be used as the search parameters. In addition the user has to remember what exact feature of the structure is used for the location. Recovery Summary All of the hybrid recovery methods allow for rapid recovery that does not require multi-party interaction. They all depend on a non-trival but not onerous mnemonics for rapid recovery but may fall back to a physical or threshold secret sharing multi-party interactive copy for slower recovery. Rapid recovery using the online databases (GitHub.com , FlickR.com, FamilySearch.org, or Google maps) depends on the availability of the databases maintained by the corresponding entities. In each case, should one of the selected records be deleted then the only recourse would be one of the backups. In order to achieve the required 128 bits of security, the DiceWare approach requires recalling 10 words in order, whereas the GitHub.com, Flickr.com, FamilySearch.org and Google maps (at 1 dekameter) approaches require recalling four records. All five methods could be mixed. Using a mixture adds some security (more choices) but not enough to reduce the number of records required. Alternatively, at one meter resolution the Google maps approach only needs three records. The Google maps approach (either four locations or three locations) may have the lightest memory load because the exploits the high human capacity for visual-geospatial recall. The secret splitting used to combine records could be augmented to use a threshhold scheme to make it more resilient to record loss but at the cost of needing more than records. If multi-party interactive recovery is acceptable then using threshold secret sharing could be a better approach. Even when multi-pary interactive is not the preferred approach it could be another backup in addition the a physical backup. This novel hybrid approach combines multiple cryptographic techiques to provide a viable non-multi-party interactive rapid key recovery method that is well suited to data streaming applications. It combines hiding in plain site, mnemonics, DiceWare-like selection, secret splitting, CSPRNG, and one-time pads. The method is a practical trade-off between the features of the different approaches. Virtual World Game as Hierarchically Deterministic Seed Mnemonic Looking to the future, it would be possible to create a mnemonic-seed generating mobile or desktop application that is completely self-contained and does not require any external online databases for random key material. Humans have an innate ability to remember complex visual geo-spatially related information such as is encountered in everyday life when walking from one place to another without getting lost. Humans are particularly adept at remembering how to retrace the path they followed on a journey through a city, or countryside. Humans are also adept at remembering when the memory is associated with familiar spacial surroundings. The well known method of loci , more commonly known as the memory palace mnemonic, associates a sequence of items to be remembered with locations in one's house or other familiar structure. When a spatial mnemonic is enhanced with what is called [elaborative encoding] (https://en.wikipedia.org/wiki/Elaborative_encoding), that is, adding visual, auditory or other sensory cues, it becomes particularly powerful. Humans are also adept at learning complex mental models via hierarchical decomposition . Various other mnemonic devices take advantage or combinations of familiar, spatial, hierarchical and sensory cues to make the learning and recall task easier. An application that exploited multiple mnemonic devices in combination could minimize the memory load required to recover seed material. Indeed games that involve recalling complex sequences of movement and action within a simulated graphical world can be successfully played by young children. This level of mnemonic capability in demonstrated by young children when playing games like The Legend of Zelda . What is being proposed is a hierarchical deterministic seed mnemonic (HDSM) as a type of hierachical spatial elaborative encoded mnemonic. Lets call this hypothetical mnemonic seed generating game Quest for the Mnemon Seed for lack of a better title. A notional description follows: The game is based on a graphical virtual world map such as one might encounter in an online role playing game. In the game, the user starts at the entrance and is presented with a map of a locale such as a village containing unique sites including buildings, parks, roads etc. Each site within the locale has memorably unique visual features such as floor plan, architectural style, period, color, material, flora, fauna, characters, objects etc. The user then walks down roads and paths to get to the different sites. Upon entry to a site the user is presented with a choice of actions to perform such as picking up an object or interacting with a character. Thus the process of selecting a site and then selecting an action at the site constitues a choice. If the choice is selected at random then it becomes the source of random seed material. The mnemonic is remembering where the site is placed within the locale and how to get there and then remember the action(s) performed at the site. A sequence of site visits with actions then provides an extended source of key material. Playing the game provides rehearsal so that a specific set of actions can be recalled in order, thereby recovering the seed. The site options, both exterior and interior, such as location, layout, style, material, color, etc, are specified as a data structure represented as a sequence of bit fields. A single long string of bytes such as might be generated with a deterministic hash can then be used to generate a uniquely configured locale. A set of sites and actions can also be encoded as a sequence of bit fields. A path through the locale with visits and actions at each site can then be generated from a large random number. The game is then played in two modes. The first mode generates a random seed and then rehearses the mnemonic for the random seed. The second mode recovers the random seed with the mnemonic. In the first, generative, mode, the user inputs a string that is the customization phrase. The cryptographic strength of the customization phrase is not important, it just allows the user to have a custom configured locale that is compatible with the user preference. The customization phrase is hashed (with Blake or Sha2) to generate a sequence of bytes used to specify the local options. The locale is then generated. A 2D or 3D display of the locale map is then presented to the user. The game then uses a cryptographic class random number generator to create the 128-bit random seed. This seed will be used to generate the one-time pad for encrypting the key-rotation history. Using the seed and a CSPRNG, a sequence of sites and actions is created deterministically from the seed as the mnemonic. The user is then shown on the map this mnemonic path through the locale. The user follows the path through the locale, visiting each site in turn, where the user is prompted to perform the selected action or actions. Once complete the user continues to rehearse the mnemonic, only now the path is not shown. The user must recall it from memory. If the user makes a wrong choice, the game reminds the user with a prompt. Rehearsal repeats until the user can successfully retrace the path and actions from memory without any prompts. At this point the user has memorized the mnemonic and can print out copies of the random seed for backup, use it for generating a one-time encryption pad, and then instruct the application to forget the random seed. In the second, recovery, mode, the user inputs the customization phrase to generate the locale map. The user then visits sites in turn and performs actions at each site. The sequence of site visits and actions deterministically regenerates a seed. When the user completes a sequence the game displays the associated seed. If the user correctly replayed the sequence then the user will recover the correct seed. If the user does not, then the seed provided by the game will not be the one the user was trying to recover. Suppose that each locale contains 256 = 2 8 sites. This is comparable to a small village of population about 1000. Randomly selecting a site then provides 8 bits of entropy. Suppose that inside each site there are 8 = 2 3 spots, such as cupboard east wall, shelf north wall, barrel northeast corner, etc. Random selection of a spot would provide 3 more bits of entropy. Suppose that at each interior location the user has 2 = 2 1 choices of action such as, pick up hammer, drink vial of liquid, answer question from inn keeper, etc. Random selection of an action would provide another 1 bit of entropy. Suppose then that after completing the first spot-action the user has to select another spot and make another binary choice of action. The second spot-action provides yet another 4 bits of entropy. This given a total of 8 + 4 + 4 = 16 bits of entropy per site-spot-action-spot-action sequence. To provide the total of 128 = 8 * 16 bits of entropy needed for the random seed requires that the user visits 8 sites in order while selecting two successive actions at each site. Alternatively the game could provide some other mix of interior location and interaction choices to get 8 bits of entropy. Suppose for example that at each of the 256 sites there are 32 = 2 5 spots. Random selection of a spot provides 5 bits of entropy. At each spot there are 8 = 2 3 action choices. Random action selection provides another 3 bits of entropy. So each spot-action selection provides 5 + 3 = 8 bits of entropy. If at each site the user must make 3 spot-action selections then that provides a total of 3*8 = 24 bits of entropy. Thus each site-spot-action-spot-action-spot-action combintion or site + (spot-action) * 3 combination provides 32 = 8 + (3 * 8) bits of entropy. A 128 = 4 * 32 bit seed can then be generated from only four site-(spot-action) 3 combinations, that is, 128 = 4 * (8 + (3 * 8)). An area of research would be to find the optimal decomposition and combination of site-spot-action sequences. Either of the eight-site or four-site examples above are well within the mnemonic capabilities of the general population given the dense hierarchical geospatial sensory cues that such a graphical virtual game world journey provides and would only take a few minutes to replay for recovery. The app would run self contained on the user's mobile device or desktop computer and would make seed recovery fun. Any computing device could be engaged to play the app so it would not require a specific mobile device or computer and therefore loss of the user's mobile device would not impede seed recovery. A variation of the game would be to allows some sites to have a portal that transports the user to a new locale with a new unique map. The configuration of the new locale is determined by a hash of the site/action visit selections that were performed prior to entry of the portal. This would add additional variety to the game and help differentiate the mnemonics required for the create of multiple unique seeds. This makes the game a recursively hierarchical deterministic seed mnemonic (RHDSM). This hierarchically deterministic seed mnuemonic (HDSM) could become a standard feature for primary key recovery for any decentralized identity based cryptographic system where the user must generate and manage their private keys. Once users become familiar with this approach to key recovery it could open the door to more rapid adoption of decentralized approaches to online interactions where security is based on user managed public/private key pairs. Summary A new data type called a DAD for decentralized autonomic data has been presented that is derived from decentralized identifiers, DIDs. DADs are suitable for streaming applications. Methods for the three basic key management operations, namely, reproduction, rotation, and recovery have been presented that are compatible with DAD stream-data applications. The pre-rotation and hybrid recovery methods presented in this paper including the hierarchically deterministic seed mnuemonic (HDSM) are somewhat novel. They all provide what could be considered minimally sufficient means for key management operations. Appendices Support for DAD Signatures in HTTP In web applications that use HTTP, the simplest most compatible way to associate or attach a signature to an HTTP packet is to include it in a custom HTTP header. Standad JSON parsers raise an error if there are additional characters after a closing object bracket thus one cannot simply append the signature after the JSON serialization in the message body. Another approach would be to use a custom JSON parser that guarantees a cononical representation of a JSON serialization (including white space) and then wrap the data item and the signature in another JSON object, where the signature and the data item are both field in the wrapper object. This is more verbose and is not compatible with the vast majority of web application framework tools for handling JSON serialized message bodies. Thus it is non-trivial to include the signature in the message body. Using a custome HTTP header is relatively easy and has the advantage that is is compatible with the vast majority of existing web frameworks. A suggested header name is Signature header that provides one or more signatures of the request/response body text. The format of the custom Signature header follows the conventions of RFC 7230 Signature header has format: Signature: headervalue Headervalue: tag = \"signature\" or tag = \"signature\"; tag = \"signature\" ... where tag is replaced with a unique string for each signature value An example is shown below where one tag is the string signer and the other tag is the string current . Signature: signer=\"Y5xTb0_jTzZYrf5SSEK2f3LSLwIwhOX7GEj6YfRWmGViKAesa08UkNWukUkPGuKuu-EAH5U-sdFPPboBAsjRBw==\"; current=\"Xhh6WWGJGgjU5V-e57gj4HcJ87LLOhQr2Sqg5VToTSg-SI1W3A8lgISxOjAI5pa2qnonyz3tpGvC2cmf1VTpBg==\" Where tag is the name of a field in the body of the request whose value is a DID from which the public key for the signature can be obtained. If the same tag appears multiple times then only the last occurrence is used. Each signature value is a doubly quoted string \"\" that contains the actual signature in Base64 url safe format. But the signatures should use an intelligent default cryptographic suite such as 64-byte Ed25519 signatures that have been encoded into BASE64 url-file safe format. The encoded signatures are 88 characters in length and include two trailing pad characters = . An optional tag name = kind may be present to specify the cryptographic suite and version of the signatures. The kind tag field value specifies the type of signature. All signatures within the header must be of the same kind. Signature: signer=\"B0Qc72RP5IOodsQRQ_s4MKMNe0PIAqwjKsBl4b6lK9co2XPZHLmzQFHWzjA2PvxWso09cEkEHIeet5pjFhLUDg==\"; did=\"B0Qc72RP5IOodsQRQ_s4MKMNe0PIAqwjKsBl4b6lK9co2XPZHLmzQFHWzjA2PvxWso09cEkEHIeet5pjFhLUDg==\"; kind=\"ed25519:1.0\" Cryptographic Suite Representation Best practices cryptography limits the options that user may choose from for the various cryptographic operations, such as signing, encrypting, and hashing to a suite of balanced and tuned set of protocols, one for each operation. Each member of the set should be the one and only one best suited to that operation. This prevents the user from making bad choices. In most key-representation schemes each operation is completely free to be specified independent of the others. This is a very bad idea. Users should not be custom combining different protocols that are not part of a best practices cypher suite. Each custom configuration may be vunerable to potential attack vectors for exploit. The suggested approach is to specify a cypher suite with a version. If an exploit is discovered for a member of a suite and then fixed, the suite is updated totally to a new version. The number of cypher suites should be minimized to those essential for compatibility but no more. This approach increases expressive power because only one element is needed to specify a whole suite of operations instead of a different element per operation. See this article for a detailed explanation on how standards such as JOSE expose vulnerabilities due to too much flexibility in how cryptographic operations are specified. Example cypher suites: v1: Ed25519, X25519, XSalsa20poly1305, HMAC-SHA-512-256 v2: Ed448, X448, XChaCha20Poly1305, keyed BLAKE2b v3: SPHINCS-256, SIDH, NORX64-4-1, keyed BLAKE2x Canonical Data Serialization Canonical data serialization means that there is a universally defined way of serializing the data that is to be cyptographically signed. The are few typical approaches to achieving data canonicalization. The advantages of compatibility, flexibility, and modularity that come from using a key/value store serialization such as JSON usually makes 1) the preferred approach. Store the serialization and signature as a chunk. The simplest is that the signer is the only entity that actually serializes the data. All other users of the data only deserialize. This simplifies the work to guarantee canonization. For example, JSON is the typical data format used to serialize key:value or structured data. But the JSON specifcation for ser/deser treats whitespace characters and the order of appearance of keys as semantically unimportant. For a dictionary (key:value) data structure the typical approach is to represent it internally as a hash table. Most hash algorithms do not store data ordered in any predictable way (Python and other languages have support for Ordered Dicts or Ordered Hashes, which can be used to partially ameliorate this problem). But from the perspective of equivalence, key:value data structures are \"dict\" equal if they have the same set of keys with the same values for each key. Thus deserialization can produce uniform equivalent \"dict equal\" results from multiple but differing serializations (that differ in whitespace and order of appearance of fields). JSON only guarantees dict equivalent not serialization equivalence. Unfortunately the signatures for the differing but equivalent serializations will not match. But in signed at rest data only the signer ever needs to serialize the data. Indeed, only the signer may serialize the data because only the signer has the private key. So deserialization and reserialization by others is of limited value. The primary value appears to be either schema completeness where signatures are included as fields in a wrapper object or the ability to nest signatures or signed data with signatures. Because it is simple to convert a JSON serialization to a coded serializaiton such as Base64, nested coded JSON serialization without canonicalization can be trivially supported. After expansion and decoding, readers of the data can see the uncoded underlying data in a schema complete representation. The signer's serialization is always canonical for the signature. Users of the data merely need to use a \"dict equal\" deserialization which is provided by any compliant JSON deserializer. So no additional work is required to support it across multiple languages etc. If the associated data also needs to be stored, unserialized then validation and extraction of the data is performed by first verifying the signature on the stored serialization and then deserializing it in memory. Implement perfectly canonical universally reproducibly serialization. In this approach all implementations of the protocol or service use the exact same serialization method that is canonical including white space and field order so that they can reproduce the exact same serialization that the original signer created when originally signing the data. This is difficult to achieve with something like JSON across multiple languages, platforms, and tool kits. It's usually more work to implement and more work to support because it usually means either using something other than JSON for serialization or writing from scratch conformant JSON implementations or at the very least having tight control of how white space and order occurs and ensuring accross updates that this does not change. Unfortunately many overly schematizied standards are based on this approach. This approach typically breaks web application frameworks. Use binary data structures With binary data structures the canonical form is well defined but it is also highly inflexible. Relative Expressive Power One way to measure and compare different knowledge representations is called relative expressive power . In the physics world power is defined as work done per unit time. It is a ratio. Expressive power is similary defined as the ratio of meaning conveyed per dependency, where dependency is something that must be kept track of or transmitted to convey the meaningful information. Because dependencies are a measure of complexity, relatively higher expressive power conveys more meaning relatively more simply. Intelligent Defaults One approach to acheiving higher expressive power in a data representation specification is the use of intelligent defaults. An intelligent default assigns meaning to the absence of data. For example, if there are several options for a given data item value such as the type of a data item, an intelligent default would assign the type to a predetermined default if no type is provided in the data. This provides high expressive power as the type meaning is conveyed without the transmission of any bytes to represent type. Typically in any given knowledge representation application the relative frequency of the appearance of optional values is not evenly distributed, but follows a Pareto distribution. This means that if an intelligent default (the Pareto optimal value) is specified as part of the schema the average expressive power of data items will be increased. A practical example of this is the RAET (Reliable Asynchronous Event Transport) protocol header (see RAET ). Typically in protocols the header has a fixed format binary representation for two reasons. The first is that every packet includes the header, so a verbose header reduces the payload capacity of each packet, thereby making the protocol comsume more bandwidth. The second is that the header is used to interpret the rest of the packet and therefore must be consistenly parsable which is easier if the format is fixed. The problem with fixed format headers is that they are not extensible. To make the extensible usually means adding additional fields to the header to indicate the presence of additional extended fields. RAET used an intelligent default policy to achieve a completely flexible extensible header that on average is the size of a non-extensible fixed format header. In RAET the header is composed of a serialized list of key-value pairs where each key is the field name of the associated field value. This makes it easy to add new key-value pairs as needed to extend the protocol to different uses and with different behavior. Unfortunately, transmitting the keys makes the header much larger relative to a fixed format header where the offset of the value in the header determines the associated field. RAET overcomes this problem by defining a default value for each key-value pair. When a header is generated on the transmit side, the actual key-value pairs are compared against the default set. Any pair where the value matches the default is not included in the list of key-value pairs in the transmitted header. On the recieve side a default header is created with every key value pair set to the default. The received header's key-value pairs are used to update the default header with the non-defaulted values. Because the optional fields are seldomly used by most packets the average header size is comparable to a fixed format header. When viewing the header after expansion and update, all the fields are present, so there is no hidden information. All the meaning is apparently conveyed. RAET header field defaults PACKET_DEFAULTS = odict([ ('sh', DEFAULT_SRC_HOST), ('sp', RAET_PORT), ('dh', DEFAULT_DST_HOST), ('dp', RAET_PORT), ('ri', 'RAET'), ('vn', 0), ('pk', 0), ('pl', 0), ('hk', 0), ('hl', 0), ('se', 0), ('de', 0), ('cf', False), ('bf', False), ('nf', False), ('df', False), ('vf', False), ('si', 0), ('ti', 0), ('tk', 0), ('dt', 0), ('oi', 0), ('wf', False), ('sn', 0), ('sc', 1), ('ml', 0), ('sf', False), ('af', False), ('bk', 0), ('ck', 0), ('fk', 0), ('fl', 0), ('fg', '00'), ]) Any key-value based schema standard specification may benefit from an intelligent default policy to greatly increase the expressive power of the schema. This becomes even more important where security is concerned as the intelligent default might be the most secure set of options thus helping the user be more secure and more expressive. Moreover expressive power is about conveying meaning more simply which makes it easier to implement and incentivizes adoption. Essential vs. Optional Elements Another related technique for increasing expressive power is to distinguish between essential and optional elements in a given representation. Any essential elements should be expressed as explicitly as possible (when not defaulted); that is, it should not be looked up and should either not be indirected or have minimal indirection. External lookups are expensive. Moreover, hiding essential elements behind multiple levels of indirection may make it harder to understand the conveyed meaning (adding dependencies and hence complexity). An important meaningful difference that should be apparent is whenever an essential element is not set to a default value. This difference should not be hidden behind indirection.","title":"Decentralized Autonomic Data (DAD) and the three R's of Key Management"},{"location":"RWoT6/DecentralizedAutonomicData/#decentralized-autonomic-data-dad-and-the-three-rs-of-key-management","text":"Author: Samuel M. Smith Ph.D. Contributor: Vishal Gupta 2018/03/07","title":"Decentralized Autonomic Data (DAD) and the three R's of Key Management"},{"location":"RWoT6/DecentralizedAutonomicData/#abstract","text":"This paper proposes a new class of data called decentralized autonomic data (DAD). The term decentralized means that the governance of the data may not reside with a single party. A related concept is that the trust in the data provenance is diffuse in nature. Central to the approach is leveraging the emerging DID (decentralized identifier) standard. The term autonomic means self-managing or self-regulating. In the context of data, we crystalize the meaning of self-managing to include cryptographic techniques for maintaining data provenance that make the data self-identifying, self-certifying, and self-securing. Implied thereby is the use of cryptographic keys and signatures to provide a root of trust for data integrity and to maintain that trust over transformation of that data, e.g. provenance. Thus key management must be a first order property of DADs. This includes key reproduction, rotation, and recovery. The pre-rotation and hybrid recovery methods presented herein are somewhat novel. The motivating use of DAD is to provide provenance for streaming data that is generated and processed in a distributed manner with decentralized governance. Streaming data are typically measurements that are collected and aggregated to form higher level constructs. Applications include analytics and instrumentation of distributed web or internet of things (IoT) applications. Of particular interest is the use of DADs in self-sovereign reputation systems. A DAD seeks to maintain a provenance chain for data undergoing various processing stages that follows diffuse trust security principles including signed at rest and in motion. Streaming data applications may impose significant performance demands on the processing of the associated data. Consequently one major goal is to use efficient mechanisms for providing the autonomic properties. This means finding minimally sufficient means for managing keys and cryptographic integrity. Importantly this paper provides detailed descriptions of the minimally sufficient means for key reproduction, rotation, and recovery for DID leveraged DADS.","title":"Abstract"},{"location":"RWoT6/DecentralizedAutonomicData/#overview","text":"A decentralized autonomic data (DAD) item is associated with a decentralized identifier, ( DID ). This paper does not provided a detailed definition of DIDs but does describe how DIDs are used by a DAD. The DID syntax specification is a modification of standard URL syntax per RFC-3986 . As such, it benefits from familiarity, which is a boon to adoption. One of the features of a DID is that it is a self certifying identifier in that a DID includes either a public key or a fingerprint of a public key from a cryptographic public/private key pair. Thereby a signature created with the private key can be verified using the public key provided by the DID. The inclusion of the public part of a cyptographic key pair in the DID give the DID other desirable properties. These include universal uniqueness and pseuodnynmity. Because a cryptographic key pair is generated from a large random number there is an infinitessimal chance that any two DIDs are the same (collision resistance). Another way to describe a DID is that it is a cryptonym, a cryptographically derived pseudonym. Associated with a DID is a DID Document (DDO). The DDO provides meta-data about the DID that can be used to manage the DID as well as discover services affiliated with the DID. Typically the DDO is meant to be provided by some service. The DID/DDO model is not a good match for streaming data especially if a new DID/DDO pair would need to be created for each new DAD item. But a DID/DDO is a good match when used as the root or master identifier from which an identifier for the DAD is derived. This derived identifier is called a derived DID or DDID . Thus only one DID/DDO paring is required to manage a large number of DADs where each DAD may have a unique DDID. The syntax for a DDID is identical for a DID. The difference is that only one DDO with meta-data is needed for the root DID and all the DAD items carry any additional DAD-specific meta-data, thus making them self-contained (autonomic).","title":"Overview"},{"location":"RWoT6/DecentralizedAutonomicData/#did-syntax","text":"A DID or DDID has the following required syntax: did: method : idstring The method is some short string that namespaces the DID and provides for unique behavior in the associated method specification. In this paper we will use the method dad . The idstring must be universally unique. The idstring can have multiple colon \":\" separated parts, thus allowing for namespacing. In this document the first part of the idstring is linked to the public member of a cryptographic key pair that is defined by the method. In this paper we will use a 44-character Base64 URL-File safe encoding as per RFC-4648 , with one trailing pad byte of the 32-byte public verification key for an EdDSA (Ed25519) signing key pair. Unless otherwise specified Base64 in this document refers to the URL-File safe version of Base64. The URL-File safe version of Base64 encoding replaces plus \"+\" with minus \u201c-\u201d and slash \u201c\\\u201d with underscore \u201c_\u201d. As an example a DID using this format would be as follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148= An example DID with namespaced idstring follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:blue A DID may have optional parts including a path, query, or fragment. These use the same syntax as a URL, that is, the path is delimited with slashes, / , the query with a question mark, ? , and the fragment with a pound sign, # . When the path part is provided then the query applies to the resource referenced by the path and the fragment refers to an element in the document referenced by the path. An example follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=/mom?who=me#blue In contrast, when the path part is missing but either the query or fragment part is provided then the query and/or fragment parts have special meaning. A query without a path means the the query is an operation on either the DID itself or the DID document (DDO). Likewise when a fragment is provided then the fragment is referencing an elemet of the DDO. An example of a DID without a path but with a query follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?who=me As will be described later, a query part on a DID expression without a path part will enable the generation of DDIDs (derived DIDs)","title":"DID Syntax"},{"location":"RWoT6/DecentralizedAutonomicData/#minimal-dad","text":"A minimal DAD (decentralized autonomic data) item is a data item that contains a DID or DDID that helps uniquely identify that data item or affiliated data stream. In this paper JSON is used to represent serialized DAD items but other formats could be used instead. An example minimal trivial DAD is provided below. It is trivial because there is no data payload. { \"id\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\" } To ensure data integrity (i.e. that the data has not been tampered with) a signature that is verifiable as being generated by the private key associated with the public key in the id field value is appended to the DAD item. This signature verifies that the DAD item was created by the holder of the associated private key The DAD item is both self-identifing and self-certifying because the identifier value given by the id field is included in the signed data and is verifiable against the private key associated with the public key obtained from the associated DID in the id field. In the example below is a trivial DAD with an appended signature. The signature is separated from the JSON serialization with characters that may not appear in the JSON. { \"id\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\" } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== An example DAD with a payload follows: { \"id\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"data\": { \"name\": \"John Smith\", \"nation\": \"USA\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== While the simple DADs given in the examples above are minimally self-identifying and self-certifying, they do not provide support for other self-management properties such as key management. In other words, because each DID (Decentralized Identifer) references a public signing key with its associated private key, it needs to be managed as a key not just as an identifier. The following sections will introduce the core key-management properties and the associated meta-data that a DAD needs in order to support those properties.","title":"Minimal DAD"},{"location":"RWoT6/DecentralizedAutonomicData/#key-management","text":"The three main key management operations are: Reproduction Rotation Recovery We call these the essential three R's of key management.","title":"Key Management"},{"location":"RWoT6/DecentralizedAutonomicData/#key-reproduction","text":"Key reproduction is all about managing the creation of new or derived keys. Each new DID requires a new public/private key pair. The private keys must be kept in a secured location. One reason to create unique public/private key pairs for each pair-wise relationship is to minimize the risk of exposure to exploits from the repeated use of a given key pair. Another reason to create unique key pairs for each interaction between parties is as a means for maintaining privacy through pseudonymity . This is discussed in more detail below. Minimizing the number of private keys that must be securely preserved for a given number of public keys simplifies management and reduces both expense and risk of exposure. To reiterate, there are two key-storage issues, one is storing public keys and the other is securely storing private keys. An exploit that captures a store of public keys may mean a loss of privacy because the expoiter can now correlate activity associated with those public keys. An exploit that captures a store of private keys means that the exploiter many now be able to use those private keys to take control of any associated resources. Consequently, one wants to avoid storing private keys as much as possible.","title":"Key Reproduction"},{"location":"RWoT6/DecentralizedAutonomicData/#privacy-and-confidentiality","text":"One desirable feature of a DAD is that it be privacy preserving. A simplified definition of privacy is that if two parties are participating in an exchange of data in a given context then the parties should not be linked to other interactions with other parties in other contexts. A simplified definition of confidentiality is that the content of the data exchanged is not disclosed to a third party. Confidentiality is usually obtained by encrypting the data. This paper does not specifically cover encryption but in general the mechanisms for managing encryption keys are very similar to those for managing signing keys. An exchange can be private but not confidential, confidential but not private, both, or neither. A minimally sufficent means for preserving privacy is to use a DID as a pseudonymous identifier of each party to the exchange. A pseudonynm is a manufactured alias (e.g. identifier) that is under the control of its creator and that is used to identify a given interaction but is not linkable to other interactions by its owner. The ability of a third party to correlate an entity's behavior across contexts is reduced when the entity uses a unique DID for each context. Although there are more sophisticated methods for preserving privacy such as zero-knowledge proofs, the goal here is to use methods that are compatible with the performance demands of streaming data. As mentioned above, the problem with using unique pseudonyms/cryptonyms for each exchange is that a large number of such identifiers may need to be maintained. Fortunately hierachically derived keychains provide a way to manage these cryptonyms with a reasonable level of effort.","title":"Privacy and Confidentiality"},{"location":"RWoT6/DecentralizedAutonomicData/#hierachical-deterministic-key-generation","text":"As previously mentioned, reproduction has to do with the generation of new keys. One way to accomplish this is with a deterministic procedure for generating new public/private keys pairs where the private keys may be reproduced securely from some public information without having to be stored. A hierarchically deterministic (HD) key-generation algorithm does this by using a master or root private key and then generating new key pairs using a deterministic key-derivation algorithm. A derived key is expressed as a branch in a tree of parent/child keys. Each public key includes the path to its location in the tree. The private key for a given public key in the tree can be securely regenerated using the root private key and the key path, also called a chain code. Only one private key, the root, needs to be stored. The BIP-32 specification, for example, uses an indexed path representation for its HD chain code, such as, \"0/1/2/0\". The BIP-32 algorithm needs a master or root key pair and a chain code for each derived key. Then only the master key pair needs to be saved and only the master private key needs to be kept securely secret. The other private keys can be reproduced on the fly given the key generation algorithm and the chain code. An extended public key would include the chain code in its representation so that the associated private key can be derived by the holder of the master private key any time the extended public key is presented. This is the procedure for hardened keys. The query part of the DID syntax may be used to represent an HD chain code or HD key path for an HD key that is derived from a root DID. This provides an economical way to specify derived DIDs (DDIDs) that are used to identify DADS. An example follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2 This expression above discloses the root public DID as well as the key derivation path or chain via the query part. For the sake of brevity this will be call an extended DID. The actual derived DDID is create by applying the HD algorithm such as: did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE= Thus a database of DDIDs could be indexed by DDID expressions with each value being the extended DID. Looking up the extended DID allows the holder to recreate on the fly the associated private key for the DDID without ever having to store the private key. This might look like the following: { \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2\", ... } Or given that the same DID method is used throughout: { \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\": \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2\", ... } The namespacing of the DID idstring also provides information that could be used to help formulate an HD path to generate a DDID. The following example shows two different DDIDs using the same public key and the same chain code but with a different extended idstring. did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:blue?chain=0/1 did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:red?chain=0/1 Some refinements to this approach may be useful. One is the granularity of DDID allocation. A unique DDID could be used for each unique DAD or a unique DDID could be used for each unique destination party that is receiving a data stream. In this case each DAD would need an additional identifier to disambiguate each DAD sent to the same party. This can be provided with an additional field or by using the DID path part to provide a sequence number. This is shown in the following example: did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057 The associated DAD is as follows: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ==","title":"Hierachical Deterministic Key Generation"},{"location":"RWoT6/DecentralizedAutonomicData/#change-detection","text":"Stale DAD items must often be detectable to prevent replay attacks. A later re-transmission of an old copy of the DAD item must not supercede a newer copy. Using a sequence number or some other identifier could provide change detection. Another way to provide change detection is for the DAD item to include a changed field whose value is monotonically increasing and changes every time the data is changed. The souce of the data can enforce that the changed field value is monotonically increasing. Typical approaches include a monotonically increasing date-time stamp or sequence number. Any older data items resent or replayed would have older date-time stamps or lower sequence numbers and would thus be detectable as stale. Below is an example of an non-trivial data item that has a changed field for change detection. { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== Change detection prevents replay attacks in the following manner. A second party receives DAD updates that are each signed by the associated private key. Each update has a monitonically increasing changed field. The source signer controls the contents of the data wrapped by the signature. Therefore the signer controls any changed field. A consistent signer will use a monotonically increasing changed value whenever the data wrapped by the signature is changed. Thus a malicious third party cannot replay earlier instances of the DAD wrapped by a valid signature to the orginal second party because the second party knows to discard any receptions that have older changed fields than the latest one they have already received.","title":"Change Detection"},{"location":"RWoT6/DecentralizedAutonomicData/#on-the-fly-ddids-in-dads","text":"One important use case for DDIDs in DADs is to identify data that is received from a source that is not providing identifying information with the data. The receiver then creates an associated DID and DDIDs to identify the data. At some later point the receiver may be able to link this data with some other identifying information or the source may \"claim\" this data by supplying identifying information. In this case the DDIDs are private to the receiver but can later be used to credibly provenance the internal use of the data. This may be extremely beneficial when shared amongst the entities in the processing chain as a way to manage the entailed proliferation of keys that may all be claimed later as a hierarchial group. The DIDs and associated derivation operations for DDIDS may be shared amongst a group of more-or-less trusted entities that are involved in the processing chain.","title":"On the Fly DDIDS in DADs"},{"location":"RWoT6/DecentralizedAutonomicData/#public-derivation","text":"Another important used case for DDIDS in DADS is to avoid storing even the DDID with its derivation chain. This may be an issue when a client wishes to communicate with a potenially very large number of public services. Each public service would be a new pairing with a unique DDID. If the derivation algorithm for an HD-Key DDID could use the public key or public DID of the public service to generate the DDID then the client need not store the actual DDID but can recover the DDID by using the public DID of the server to re-derive the associated DDID. This can be done by creating a hash of the root DID private key and the remote server public DID to create the seed used to generate the DDID for the DAD. This also means that the DDIDs or chain codes do not have to be included in the keys preserved by a key-recovery system.","title":"Public Derivation"},{"location":"RWoT6/DecentralizedAutonomicData/#key-rotation","text":"The simplest approach to key rotation is to revoke and replace the key in one operation. In some cases revocation without replacement is warranted. But this is the same as revoking and then replacing with a null key. Key rotation without revocation usually poses a security risk so it is not needed. Hence we simplify key management to include revocation as a subset of rotation. Key rotation is necessary because keys used for signing (and/or encryption) may suffer increased risk of becoming compromised due to continued use over time, may be vulnerable to brute force attack merely due to advances in computing technology over time, or may become compromised due to misuse or a specific exploit. Periodically rotating the key bounds the risk of compromise resulting from exposure over time. The more difficult problem to solve is secure rotation after a specific exploit may have already occurred. In this case, the receiving party may recieve a valid signed rotation operation from the exploiter prior to the orignal holding entity sending a valid rotation operation. The receiver may erroneously accept a rotation operation that transfers control of the data to the exploiter. A subsequent rotation operation from the original holder would either create a conflict or a race condition for the receiver. Although there are several ways to solve the early rotation exploit problem described above, the goal is to find the minimally sufficient means for preventing that exploit that is compatible with the demands of streaming data applications for which DADs are well suited.","title":"Key Rotation"},{"location":"RWoT6/DecentralizedAutonomicData/#basic-pre-rotation","text":"A complication with DADs is that there are two types of keys being used: the keys for the root DIDs and the keys for the derived DIDS (DDIDS). Generating a derived key pair requires using the private root key. The process for pre-rotating the root DID is described first, followed by the additional measures for DDID pre-rotation. The approach presented here is to pre-rotate the DID key and declare the pre-rotation at the inception of the DID. This pre-rotation is declared at initialization. This may be done with an inception event. A later rotation operation event creates the next pre-rotated key thus propogating a new set of current key and pre-rotated key. Shown below is an example inception-event data structure with a signing key in the signer field and a pre-rotated next signing key in the ensuer field. The signature is generated using the signer key. Example inception event: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"ensuer\": \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\" } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== A useful convention would be that if a signer field is not provided then the signer is given by the id field. { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"ensuer\": \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\" } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== When rotation occurs sometime later, the rotation operation atomically indicates that the key in the signer field is to be replaced with the pre-declared rotation key in the ensuer field and also declares the next rotation key to be placed in the ensuer field. One way to keep track of this is to provide three keys in the rotation event, the former signer in a new erster field, the former ensuer in the signer field and a new pre-rotated key in the ensuer field. The rotation operation has two signatures. The first signature is created with the former signer key (now erster field). The second signature with the former ensuer key (now signer field). This establishes provenance of the rotation operation. Example rotation event: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"erster\": \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"signer\": \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"ensuer\": \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=\" } \\r\\n\\r\\n jc3ZXMA5GuypGWFEsxrGVOBmKDtd0J34UKZyTIYUMohoMYirR8AgH5O28PSHyUB-UlwfWaJlibIPUmZVPTG1DA== \\r\\n\\r\\n efIU4jplMtZzjgaWc85gLjJpmmay6QoFvApMuinHn67UkQZ2it17ZPebYFvmCEKcd0weWQONaTO-ajwQxJe2DA== Instead of three fields in the structure a list or tuple of three fields could be used where the order corresponds to [erster, signer, ensuer] . In order to verify provenance over multiple rotation operations, the receiver needs to be able to replay the history of rotation operations. The pre-rotation approach has some useful features. For many exploits, the likelihood of exploit is a function of exposure to continued monitoring or probing. Narrowly resticting the opportunity for exploits in terms of time, place, and method, especially if the time and place is a one-time event, makes exploits extremely difficult. The exploiter has to either predict the time and place of the event or has to have continuous universal monitoring of all events. By declaring the pre-rotation at the inception event of the associated DAD, the window for exploits is as narrow as possible. Pre-rotation does not require any additional keys or special purpose keys for rotation. This makes the approach self-contained. Because the rotation-operation event requires two signatures, one using the current key and the other using the pre-rotated key, an exploiter would have to exploit both keys. This is extremely difficult because the only times the private side of the pre-rotated key is used are (1) at its creation in order to make the associated public key, and (2) at the later signing of the rotation operation event. This minimizes the times and places to a narrow sample.","title":"Basic Pre-rotation"},{"location":"RWoT6/DecentralizedAutonomicData/#listed-rotation-key-structure","text":"Another approach to declaring rotation events is to provide the full rotation history in the rotation operation and/or to use a list structure for providing the keys. In many cases, rotations are a rare event so the number of entries in the rotation history would be small. In the associated data structure a list of all the signers both former and future to date is provided in the signers field. The current signer is indicated by an index into the list in the signer field. The list index is zero based. The pre-rotated next signer or ensuer is the following entry in the signers list. A rotation event then changes the signer field index, which implies that the former signer ( erster ) is the previous entry and the next pre-rotated signer ( ensuer ) is the subsequent entry after the signer index. This is shown in the following examples. Example pre-rotated inception event with list structure for signing keys: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": 0, \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", ] } \\r\\n\\r\\n jc3ZXMA5GuypGWFEsxrGVOBmKDtd0J34UKZyTIYUMohoMYirR8AgH5O28PSHyUB-UlwfWaJlibIPUmZVPTG1DA== The signature above is with key at index = signer = 0. Example rotation event with list structure for signing keys: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": 1, \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=\" ] } \\r\\n\\r\\n jc3ZXMA5GuypGWFEsxrGVOBmKDtd0J34UKZyTIYUMohoMYirR8AgH5O28PSHyUB-UlwfWaJlibIPUmZVPTG1DA== \\r\\n\\r\\n efIU4jplMtZzjgaWc85gLjJpmmay6QoFvApMuinHn67UkQZ2it17ZPebYFvmCEKcd0weWQONaTO-ajwQxJe2DA== The first signature is with key at index = signer - 1 = 0. The second signature is with key at index = signer = 1. A subsequent rotation would add another key to the signers list and increment the signer index as follows: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": 2, \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=\", \"3syVH2woCpOvPF0SD9Z0bu_OxNe2ZgxKjTQ961LlMnA=\" ] } \\r\\n\\r\\n AeYbsHot0pmdWAcgTo5sD8iAuSQAfnH5U6wiIGpVNJQQoYKBYrPPxAoIc1i5SHCIDS8KFFgf8i0tDq8XGizaCg== \\r\\n\\r\\n o9yjuKHHNJZFi0QD9K6Vpt6fP0XgXlj8z_4D-7s3CcYmuoWAh6NVtYaf_GWw_2sCrHBAA2mAEsml3thLmu50Dw==","title":"Listed Rotation Key Structure"},{"location":"RWoT6/DecentralizedAutonomicData/#multi-signature-pre-rotation","text":"The list structure enables the declaration of several pre-rotations in advance by providing several future pre-rotation keys in the inception event. A rotation event then could include several rotations at once. Each rotation event would require a signature per each of the multiple rotations in the event thus allowing for multi-signature inception and rotations. If each key is from a different entity, then the rotation would require multiple entities to agree. Thus a DAD could be multi-signature and support multi-signature rotations. In this case the signer field would be a list of indices into the signers list. This approach could be further extended to support an M-of-N signature scheme where any M-of-N signatures are required to incept or rotate where M < N, and M, N are integers. The total number of keys in the list is a multiple of N. The following examples provide an inception and rotation event for a two signature pre-rotation. A namespaced key with a colon-separated idstring, as per the DID syntax, could be used to allow for signers using a different DID method or for namespacing within a given DID method. Example of a pre-rotated two-signature inception event with list structure for signing keys where \" blue \" indicates one source and \" red \" indicates another source: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": [0,1], \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=:blue\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:red\", \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=:blue\", \"3syVH2woCpOvPF0SD9Z0bu_OxNe2ZgxKjTQ961LlMnA=:red\" ] } \\r\\n\\r\\n AeYbsHot0pmdWAcgTo5sD8iAuSQAfnH5U6wiIGpVNJQQoYKBYrPPxAoIc1i5SHCIDS8KFFgf8i0tDq8XGizaCg== \\r\\n\\r\\n o9yjuKHHNJZFi0QD9K6Vpt6fP0XgXlj8z_4D-7s3CcYmuoWAh6NVtYaf_GWw_2sCrHBAA2mAEsml3thLmu50Dw== The signatures above are generated with the keys at indices 0 and 1 in the signers list respectively. Example of a two-signature rotation event with list structure for signing keys where \" blue \" indicates one source and \" red \" indicates another source: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"signer\": [2,3], \"signers\": [ \"Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=:blue\", \"Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:red\", \"dZ74MLZXD-1QHoa73w9pQ9GroAvxqFi2RTZWlkC0raY=:blue\", \"3syVH2woCpOvPF0SD9Z0bu_OxNe2ZgxKjTQ961LlMnA=:red\" \"rTkep6H-4HA8tr54sHON1vWl6FEQt27fThWoNZsa88V=:blue\", \"7IUhL0JRaU2_RxFP0AL43wYn148Xq5YqaL6L48pf0fu=:red\", ] } \\r\\n\\r\\n AeYbsHot0pmdWAcgTo5sD8iAuSQAfnH5U6wiIGpVNJQQoYKBYrPPxAoIc1i5SHCIDS8KFFgf8i0tDq8XGizaCg== \\r\\n\\r\\n o9yjuKHHNJZFi0QD9K6Vpt6fP0XgXlj8z_4D-7s3CcYmuoWAh6NVtYaf_GWw_2sCrHBAA2mAEsml3thLmu50Dw== \\r\\n\\r\\n GpVNJQQoYKBYrPPxAoIc1i5SHCIDS8KFFgf8i0tDq8XGizaCgAeYbsHot0pmdWAcgTo5sD8iAuSQAfnH5U6wiI== \\r\\n\\r\\n 8z_4D-7s3CcYmuoWAh6NVtYaf_GWw_2sCrHBAA2mAEsml3thLmu50Dwo9yjuKHHNJZFi0QD9K6Vpt6fP0XgXlj== The signatures above are generated with the keys at indices 0 through 3 in the signers list respectively.","title":"Multi-signature Pre-rotation"},{"location":"RWoT6/DecentralizedAutonomicData/#collective-signatures","text":"This multi-signature scheme suffers from the significant increase in the length of the attached signature block. One way to ameliorate this \"bloat\" is to use collective multi-signatures. A collective signature has the property that its length is not a multiple of the number of signatures it holds. Typically the maximum length of a collective signature is about double the length of a non-collective signature and does not increase significantly as more signatures are added to the collective. There is a draft IETF standard for collective signatures CoSi that might be useful for multi-signature rotation. Some useful references are here project , paper , slides . Collective signatures are a type of Schnorr multi-signature or Schnorr threshold signature.","title":"Collective Signatures"},{"location":"RWoT6/DecentralizedAutonomicData/#ddid-pre-rotation","text":"The complication for DDIDs (Derived DIDs) is that each DAD stream for each pairing of sender and receiver may have a unique DDID. Rotation of the root DID also requires rotating the DDIDs. The same pre-rotation approach, however, can be used for the DDIDs. At the inception event the root key and pre-rotation root keys are created. These keys are then used to created a set of DDIDS and pre-rotated derived keys using the root and pre-rotated root keys respectively. This does not significantly change the exploit vulnerability as the inception event is still one event. Although the pre-rotated root DID key is used to create a set of pre-rotated derived keys, it does not signicantly increase its exposure. Each rotation event then involves rotating the root DID key and all the DDID keys. The important consideration is that the number of DDIDs in the set must be determined in advance in order to create all the pre-rotated derived keys at one time. This can be managed by creating extra DDIDs and pre-rotated derived keys at the inception event. Only the public half of each of the key pairs need to be stored. In contrast, creating additional DDIDs with pre-rotated keys at a later time requires using the pre-rotated root private key. This increases the exposure of that private key to exploits and makes it less secure for pre-rotation. When the set of pre-rotated DDIDs is consumed, a rotation-operation event may be triggered, thereby rotating the existing DDIDs and then allowing additional DDIDs to be created. Alternatively if the pre-rotated set of DDIDs is consumed then a new DDID tree may be created with a unique new pre-rotated root key. This would create a hierachy of groups of pre-rotated DDIDs and derived keys. Moreover, when the re-establishment and re-initialization of a DAD stream is not a high-cost or high-risk endeavor then instead of pre-rotating the DDIDs, only pre-rotate the root DID and just close down the current DAD stream and re-establish with a new DDID created by the pre-rotated key as part of the rotation event. Finally if the exposure of the root DID is insignificant compared the exposure of the DDIDs then another approach to DDID pre-rotation could be employed. This requires a trade-off between convenience and privacy. A group of receivers could all have knowledge of the root public DID key and its pre-rotated public DID key for their unique DDIDs. This means that the members of the group could leak correlation information about the group via the shared root DID. However each member of the group could still maintain security via its unique DDID. In this case the root private DID is used to derive both the inception DDID and the pre-rotated derived key of each member. The individual members could then undergo DDID key rotation but only using the root DID not its pre-rotated key. In the rare event that the root DID needs to be rotated then each of the DDID members performs a double rotation within a rotation event. The first rotation rotates to the pre-rotated key generated using the original root DID, the second rotation is to a new set of derived and pre-rotated derived keys, each generated using the new pre-rotated root key. The first derived key in the pair is the new signer key, the second is the new pre-rotated signer key. A receiver must have knowledge of the root DID and pre-rotated root key in order to verify that the second rotation is not a forgery. This approach enables the organization and managment of DDIDs in heirarchical groups where the members of each group know about their group-root DID but that group-root DID could be a DDID of a higher level group and so on. Lower level groups only know about thier group root DID, but not any sibling groups so it can't leak information about sibling or parent groups only child groups.","title":"DDID Pre-rotation"},{"location":"RWoT6/DecentralizedAutonomicData/#replayability","text":"The constraint on pre-rotation is that the receiving party be able to replay the rotation events to ensure that it did not miss a rotation. This replay allows the receiver to verify the provenance chain of rotations. The question then is what are minimally sufficient means for enabling this replay capability? There are two use cases for providing this replay capability. The first case is for online one-to-one or pairwise interactions and the other case is for offline one-to-one or equivalently one-to-many or public interactions. In the one-to-one case, there is the sender of a DAD stream and the reciever of the stream. The initiation of the stream would involve exchanging keys for pairwise communication and would also include the establishment of the DDID used for the DAD items sent. The first DAD sent would include the DDID for the DAD as will as the pre-rotated DDID. This is the inception event. The receiver then merely needs to maintain a running log of DAD items that contain rotation events. As long as reliable communications are used between the sender and receiver, then the receiver can ensure that it has observed all rotation events by keeping its log and no imposter can later send an undetectable forged inception or rotation event. If the reciever loses its history then it must re-establish its communications channel and re-initialize. Alternatively the sender could maintain a copy of the inception and rotation event history and then provide it to the receiver upon request. The receiver would cache this history for speedier lookup. An imposter attempting to send an earlier forged inception event would be unsuccessful because only the first inception event is considered valid. In the one-to-many, public, or offline case, the rotation history is maintained by a service. While a decentralized distributed consensus blockchain ledger could provide this service it is not the minimally sufficient means of providing this capability. The minimally sufficient means is a redundant immutable event log of inception and rotation events indexed by the DDID associated with the DAD for the given DAD stream. The constraint is that a sufficient majority of the log hosts must be non-faulty at any point in time. This includes Byzantine faults. Is is also assumed that the sender communicates with the hosts using a reliable end-to-end signed protocol. The sender broadcasts the inception event to all the redundant hosts that provide copies of the log. These hosts are called Replicants. Then either the Replicants respond to the sender with a confirmation that the event is written to their log or the sender reads the log to verify. The event history is indexed by the DDID. Each Replicant timestamps and signs each entry in each event history. Each Replicant only allows one and only one inception event per event history. Attempts by imposters to forge an earlier inception event would be denied by honest Replicants. The sender can then verify that a sufficient majority of the Replicants have captured each event and have consistent event histories. Subsequent rotation events are redundantly appended to the DDID indexed log in the same way. The receiver can then broadcast a query to the Replicants and verify via their responses that a sufficient majority of the Replicants have the same DDID indexed event log. This eanbles both offline and one-to-many event streams. This approach is more scalable than using a distributed consensus ledger because the Replicants do not need to communicate with each other. The inter-host agreement of the members of a distributed consensus pool is usually the limiting factor in scalablity. Morever a given receiver could be completely responsible for providing the immutable log service for its own data stream with the sender. Each receiver could choose to implement a different level of reliability. Loss of the event log means that the sender and receiver have to re-initialize and re-establish the DAD stream. Alternatively the sender could be responsible for providing a set of Replicants and make the event log available to the receiver upon request.","title":"Replayability"},{"location":"RWoT6/DecentralizedAutonomicData/#key-recovery","text":"Key recovery is about providing a secure way of recovering a lost private key. The important consideration here is that the recovery mechanism be compatible with streaming data applications as per DADs. Keys recovery tends to be a rare occurrence so performance demands may be less constraining. Nonetheless, finding the minimally sufficient means for key recovery is still the goal. Moreover, to be secure the private key needs to be kept secret. Because cryptographic keys are long strings of numbers they are extremely hard to remember, this means that typically private keys are stored some place besides a person's memory and are therefore subject to being lost or stolen. If it is required or at least desirable that the DAD stream not be reinitialized due to the loss of the rotation-event history then a key-recovery mechanism would also need to provide recovery of the key-rotation history. To restate, it is not enough to just recover the original root DID but every rotated root DID must be recovered as well. Given that typically rotations happen rarely, the rotation-event history should be small in size and not pose a storage-size problem for recovery. Thus key recovery for DADs needs to at least recover the original root key and any rotations. DDIDs can be regenerated from the root DID given the HD-derivation code. In the case where the the DDID stream may not be easily reestablished but must resume given the latest rotated DDID then the HD chain code must also be preserved and recovered. If the number of DDIDs is very large then the storage requirements for chain codes may also be large relative to the storage requirement for key recovery. The DID root public key and DDID derivation chain codes do not expose the private keys. However, although disclosing the root public key and chain code for a DDID is not a security risk, it could be a privacy risk. A third party could correlate data streams from the associated DDIDs should the root public key used by multiple DDIDs be exposed. One way to address this is to encrypt the chain codes with an encryption key derived from the root signing key. The chain codes can then be stored outside of the core recovery system. The worst case exploit then is a loss of privacy should the encryption be broken but not a loss of control of the resources owned by the private key. When the DDID for communicating with a public service is derived from the public key of a server then the client does not need to preserve and recover the HD chain code. Instead it can regenerate the DDID using a hash of the root private DID and the public DID of the server. A complication occurs when the root private key has been rotated and the server was not made aware of the rotation. The client can still recover the current root DID used by the server using a trial and error approach by going through the list of rotated root DIDs, generating the associated DDID or derived key, verifying if the server will accept it, and if not incrementing to the next rotated root. Eventually the client will discover the last rotated DDID or derived key recognized by the serve. As a result the client can recover the appropriate DDID or derived key for a given service without having to preserve anything but the history of rotated root DIDs. This approach may provide meaningful storage savings when the number of external services is large.","title":"Key Recovery"},{"location":"RWoT6/DecentralizedAutonomicData/#cryptographic-strength","text":"","title":"Cryptographic Strength"},{"location":"RWoT6/DecentralizedAutonomicData/#information-theoretic-security-and-perfect-security","text":"With respect to DAD , key recovery deals with the recovery of the private half of signing and/or encryption keys in public/private key pairs. Given that once an adversary has the private key, security is completely broken, the cryptosystems used to backup and recover private keys needs to be as secure as is practically possible. The highest level of crypto-graphic secruity is called information-theoretic security . A cryptosystem that has this level of security cannot be broken algorithimically even if the adversary has nearly unlimited computing power including quantum computing. It must be broken by brute force if at all. Brute force means that in order to guarantee success the adversary must search every combination of key or seed. A special case of information-theoretic security is called perfect security . Perfect security means that the cipher text provides no information about the key. There are two well-known cryptosystems that can exhibit perfect sercurity. One is secret sharing or splitting (see also ss ). The other is a one-time pad (see also otp . Correct implementation of either/or a combination of these two approaches is appropriate for private-key recovery.","title":"Information Theoretic Security and Perfect Security"},{"location":"RWoT6/DecentralizedAutonomicData/#sufficient-cryptographic-strength-to-withstand-a-brute-force-attack","text":"For cryptosystems with perfect security, the fundamental parameter is the number of bits of entropy needed to resist any practical brute force attack. In other words, when a large random number is used as a seed/key to a cryptosystem that has perfect security, the question to be answered is how large does the random number need to be to withstand a brute force attack? In Shannon information theory the entropy of a message is measured in bits. The randomness of a number or message can measured by the number of bits of entropy in the number. A cryptographic quality random number will have as many bits of entropy as the number of bits in the number. Assuming conventional non-quantum computers, the convention wisdom is that, for systems with information theoretic or perfect security, the seed/key needs to have on the order of 128 bits (16 bytes) to practically withstand any brute force attack. For other cryptosystems that do not have perfect security the size of the seed/key may need to be much larger. Theoretically, quantum computers, using Grover's Algorithm might be able to brute force a 2 N random number with only 2 N/2 trials. Thus once quantum computers exists the size of N might need to increase from 128 to 256. An N-bit long base-2 random number has 2 N different possible values. Given that with perfect security no other information is available to an attacker, the attacker may need to try every possible value before finding the correct one. Thus the number of attempts that the attacker would have to test may be as much as 2 N-1 . Given available computing power, one can estimate if 128 is a large enought N to make brute force attack impractical. Let's suppose that the adversary has access to supercomputers. Current supercomputers can perform on the order of one quadrillion operations per second. Individual CPU cores can only perform about 4 billion operatons per second but a supercomputer will employ many cores in parallel. A quadrillion is approximately 2 50 = 1,125,899,906,842,624. Suppose somehow an adversary had a million (2 20 = 1,048,576) super computers to employ in parallel. The adversary could then try 2 50 * 2 20 = 2 70 values per second (assuming that each try only took one operation). There are about 3600 * 24 * 365 = 313,536,000 = 2 log 2 313536000 =2 24.91 ~= 2 25 seconds in a year. Thus this set of a million super computers could try 2 50+20+25 = 2 95 values per year. For a 128-bit random number this means that the adversary would need on the order of 2 128-95 = 2 33 = 8,589,934,592 years to find the right value. This assumes that the value of breaking the cryptosystem is worth the expense of that much computing power. Consequently, a cryptosystem with perfect security and 128 bits of cryptographic strength is practically impossible to break.","title":"Sufficient Cryptographic Strength to Withstand a Brute-force Attack"},{"location":"RWoT6/DecentralizedAutonomicData/#recovery-methods","text":"Fundamentally key recovery involves shifting the burden of remembering a cryptographic key made of a long random string of numbers to some other task that is less onerous.","title":"Recovery Methods"},{"location":"RWoT6/DecentralizedAutonomicData/#physical-security","text":"One approach to recovery is to shift the burden of recovery from remembering a private key or keys to protecting physical copies of the keys. This is called physical security. Recovery first involves creating a hard copy of the key(s) such as a printed piece of paper or a \"hard\" electronic wallet and then hiding the hard copy. The memory task now becomes remembering where the hard copy was hidden. The security of the approach is now based on the physical security of the hidden location (under the bed, in the safety deposit box, in a hole in the backyard). The assumption is that remembering where something is hidden is assumed to be relatively reliable. Most important is that physical security is not vulnerable to remote attacks over the internet nor computational attacks where the attacker can employ resources and time to break a key. The attacker must have physical access and may be physically at risk. A weakness of this approach is that recovery may take time. Moreover if the person with the knowledge of the key location is incapacitated then recovery may be impossible unless the location of hard copy or another hard copy is shared with someone else, thus exposing a vulnerability. One way to address this is to use a legal mechanism such as power of attorney, a will, or another guardian who is authorized to reveal the hard copy given predefined circumstances. This can be ameliorated by using tamper-resistant envelopes and physical access logs to increase the risk of discovery. In any event physical recovery is useful as a backup to non-physical security recovery methods but may be too inconvenient as the primary form of recovery for the managers of streaming data applications. In general physical security may be a good backup for any of the other recovery methods.","title":"Physical Security"},{"location":"RWoT6/DecentralizedAutonomicData/#mnemonics","text":"A mnemonic is a device or technique to aid human memory. The memory task in this case is to remember a 128-bit random number as a key or seed. This is further complicated for DAD recovery as it is not sufficient to just recover a single private key but instead requires the recovery of the whole key rotation history. One way to accomplish this is to use a 128-bit random number as a seed to a system that hides and recovers the whole rotation history. This will be discussed in more detail below. One well-known mnemonic is to use a phrase of random words from a word list. The user can create a story or imaginery visualation of a situation in which the words are all represented. An example would be the words, blue cat house eat pudding . Visualizing and rehearsing a fantastic situation that includes objects and actions corresponding to the words makes is much easier to remember. The DiceWare (see also wk and pp approach consists of a word list of 7776 words that are selected at random (using dice). The user must remember the words and their order to form a phrase that can be used to generate a random number. The EFF has produced modified versions of the word list ( EFF word list ) that have beneficial properties. Given a total of 7776 words, then each randomly selected word is one of 7776 choices, which provides log 2 (7776) = 12.9 bits of entropy per word. To get a 128 bits of entroy the phrase would need to include ten words. This is pretty long for a mnemonic but not impractical as long as the user is willing to do some rehearsal. More problematic is recovering not just one key but multiple keys from a key rotation history.","title":"Mnemonics"},{"location":"RWoT6/DecentralizedAutonomicData/#secret-sharing","text":"Another approach is to shift the task of recovery to other parties. This can be done securely using a secret sharing or \"splitting\" approach. The secret information is split into what are sometimes called shards. Each shard is then shared with another party called a shard holder. Later the shards are collected and combined to reproduce the secret. The shard holders must either keep the shard secret or if they are going to store it online they need to encrypt the shard and must then remember their encryption key. As mentioned above, secret sharing may have perfect security. This means that storing encrypted copies of the shards online may still be perfectly secure as long as an adversay cannot correlate the shards as belonging to the same secret information. If correlation does occur then the security is limited to the type of encryption and might be more vulnerable to exploits. In order to recover the secret information the user must interact with the shard holders to get them to provide their shard; that is, the recovery is multi-party interactive. The user then combines the shards to reconstitute the shared secret. This interaction may take time and may not be reliable. A useful variation on this approach is called threshold or Shamir sharing where only a subset of all the shards is needed to reconstitute the secret. For example an M of N threshold secret sharing (M < N) algorithm would share shards with N parties. Any combination of a subset of M parties can reconstitute the secret. This allows some of the parties to not be available or to lose their shard and still have successful recovery. Typically, to maintain secrecy the N parties do not know of each other. Although the security properties of Secret sharing make it an attractive approach for key recovery, secret sharing can be complicated, especially because it requires interaction with multiple parties. The secret owner must recall who the N parties are or at least M non-faulty parties. In an organizational setting, however, there may be a designated group of individuals who know about and hold the shards and have a policy for circumstances under which they can share the shards.","title":"Secret Sharing"},{"location":"RWoT6/DecentralizedAutonomicData/#one-time-pad","text":"As mentioned previously, the one-time pad (OTP) (see also otp ) may exhibit perfect security. The OTP is a venerable cyphersystem that has the advantage that it can be used manually without a computer. Basically a long string of random characters forms the pad . Someone can use the pad to encrypt a plain-text message. The procedure is to combine each plain-text character in order with the corresponding character from the pad. The combination is typically performed using modulo addition of the two characters but can be performed with a bitwise XOR. Because characters from the pad may only be used once, the pad must be at least as long as the plain-text message. The one time use of a random string of characters from the pad is what gives the system its perfect security property. If two parties wish to exchange multiple messages, then the pad must be at least as long as the sum of the length of all the messages. The main disadvantage of a one-time pad is that the two parties must each obtain a copy of the same pad . This is less of a disadvantage for key recovery because the the encrypted message (keys) does not need to be exchanged with another for decryption but are decrypted by the self-same party so only one copy of the pad is needed. Suppose for example, a OTP is used to encrypt the key or key history. Given that the adversary does not have access to the OTP then the encryption has perfect secrecy which means that the only viable attack is via brute force. If the encrypted key or key history is at least 128 bits long then brute force is practicaly impossible. Consequently the OTP encrypted key history could be safely stored in a public immutable database. The remaining problem is management of the OTP. Using an OTP to encrypt the key history just creates a new problem, that of securing the OTP itself. But the main advantage of a OTP over secret sharing described above for key recovery is that a OTP approach is non-multi-party interactive. It can be self-contained which is advantageous in data streaming applications. One common but weaker variant of the OTP is the book cyper. In this variant the OTP is a book. Because the characters in a book are not a random string there is some degree of correlation between characters that makes it less than perfectly secure. Thus two parties who each have a copy of the same book (same edition) can use the characters in the book as the OTP to encrypt messages without ever having to exchange copies of the book. Essentially using a book as OTP is an example of hiding the OTP in plain sight. An adversary would have to guess that a book was being used as a one-time pad and then figure out which book. For key recovery, the key owner merely needs to remember which book and edition. Should the book used by the key owner be lost, the key owner can get another copy from a bookstore. The book cypher is an interesting example due to the combination of simplicity, the use of existing but readily available sources of information, and the ability to hide the OTP as book in plain sight. This has the advantage that the only the title and edition of a book need to be remembered thus making light demand on human memory. The primary disadvantage of the book cypher is that the text is not random and its difficult to calculate how many bits of entropy are lost for a given book.","title":"One-time Pad"},{"location":"RWoT6/DecentralizedAutonomicData/#hybrid-key-recovery-method","text":"One of the main attractions of using a one-time pad (OTP) for key recovery, in contrast to secret sharing, is that it is non-multi-party interactive. A hybrid approach that makes a beneficial trade-off is to use a mnemonic merely to generate a seed for a cryptographic strength psuedorandom number generator (CSPRNG). The seed is then used via the CSPRNG to generate a OTP that is then used to encrypt the key-rotation history. The cryptographic strength of the OTP is now governed by the length of the seed not the length of the pad. But key-rotation histories are relatively short compared to the period of CSPRNG so a strong enough seed (128 bits of entrophy) would still be sufficient for this task. The PRNG algorithm must be of cryptographic quality otherwise it could become a source of vulnerability. A recent advancement in CSPRNG algorithms is a chaotic iteration psuedorandom number generators (CIPRNG) . These are of cryptographic quality have extremely high statistical randomness. They pass both the NIST and DieHard tests for PRNG with periods on the order of 10 9 opt . The basic concept is a chaotic finite state machine cfsm . Unfortunately there do not yet appear to be any open source implementations of this algorithm. A more practial CSPRNG that could be used to generate a OTP from a seed is the libsodium randombytes_buf_deterministic function. This uses ChaCha20 under the hood. The advantage of this hybrid approach is that the key recovery memory task is now limited to merely recovering the seed that would then be used to reproduce the OTP that would then be used in turn to decrypt the key history. This approach does not require multi-party interaction like secret sharing as the seed is directly recovered by the owner via a mnemonic device, not from others. This hybrid approach still benefits from the properties of the OTP for encryption so that the key-rotation history can be encrypted and stored online for recovery. What remains then is the selection of a mnemonic for generating the seed. It may be difficult for a single mnemonic to provide a random source of seed material at the required strength of 128 bits. Concatenating several sources of mnemonically derived seed material, however, could produce the required strength. This is akin to the DiceWare approach to passphrase generation. One problem with concatenation of seed material is that the order of concatenation must also be remembered. One way to avoid having to remember the order when combining multiple sources of seed material is to use the simple version of secret splitting . In this form of secret splitting, the secret is divided into shards and each shard is XORed together to recover the secret. In this case the secret is the seed and each shard contributes a certain amount of entropy to the final seed. This allows a mnemonic for each shard that may have much less than the required 128 bits of entropy but the combination of shards could have the required entropy and the order of the shards is not important. A non-ordered combination loses some cryptographic strength because the number of possibilties is no longer merely the multiple of the independant possibilities from each shard (permutations) but is instead the number of combinations of the shards. Suppose that there are four shards that each contribute 35 bits of entropy or in other words each shard is randomly chosen from 2 35 possibilities. Then the combined number of possibilities is 2 35 taken four at a time. The exact formula for the combination of N things taken K at a time is given by: N!/(K!*(N-K)!) Computing factorials for very large numbers is a computationally intensive task. For the sake of analysis an approximation is sufficient. A lower bound on the number of combinations of N things taken K at a time is (N/K) K (see bounds ). The bits of cryptographic strength of the combination of four shards each with 35 bits is where N = 2 35 and K = 4. Using the approximation gives the number of possiblities to be at least (2 35 /2 2 ) 4 = 2 33*4 =2 132 . This corresponds to 132 bits of entropy, which is greater than the required 128. The one remaining challenge then is to find good mnemonically recoverable sources of random seed material. One feature that makes the The book cypher was attractive because it took advantage of information that was highly available but hidden in plain sight and whose source was easy to remember (a book title). The problem with books is that the content is not highly random so it in itself is not a good source of seed material. In other words, the challenge is to find sources of information for seed material that have much higher degree entropy than a book but are still easy to remember. More specifically this means finding sources of highly random seed material that are highly available (thus do not require additional infrastructure to backup) but are also essentially hidden in plain sight and easy to recall via a mnemonic device. What follows are several viable sources of mnemonically recoverable sources of random seed material.","title":"Hybrid Key Recovery Method"},{"location":"RWoT6/DecentralizedAutonomicData/#diceware-seed-recovery","text":"The DiceWare approach can be repurposed to provide a mnemonic source of seed material. These can be used to recover the seed for the one-time pad used to encrypt the key-rotation history. Ten randomly selected words from a DiceWare-compatible wordlist could be used to generate the seed for the one-time pad. Ten randomly selected words in order provide the required 128 bits of entropy (recall that each DiceWare word provides 12.9 bits of entropy). The order of the words is important. Each word would be hashed using SHA-2 or Blake to generate a 16-byte string. The seed is created by concatenating the hashes in the defined order. Once the seed for the OTP is generated, the rest of the recovery method follows the process described above for generating the OTP using a CSPRNG and then using that to encrypt/decrypt the key rotation history. The mnemonic load for this method is the recall the order of ten words from the DiceWare or EFF wordlist. This has a large mnemonic load so it would require some rehearsal and might not be very practical. In addition to the mnemonic at least a physical backup of the ten words should also be created. The physical backup of the ten words could be split into parts to make it more secure. If practical, a multi-party threshold secret sharing backup could also be created.","title":"DiceWare Seed Recovery"},{"location":"RWoT6/DecentralizedAutonomicData/#github-seed-recovery","text":"Github.com stores versioned code repositories. The associated git utility automatically calculates a 160 it (20 byte) SHA-1 hash of each commit to a repository. These hashes are easily readable from the GitHub.com web site. Several Github commit hashes can be used to create the seed to generate the OTP for encrypting the key rotation history. In order to recover a commit hash one must remember the project and repository name, and the date of the commit. If there are multiple commits on the same date then one must also remember which commit, like the last or the first. This is not an onerous memory task but not a trivial one. There are over 80 million GitHub repositories. A reasonable estimate of the average number of commits per repository is over 1000. This means that there are about 80,000,000 * 1,000 = 80,000,000,000 = 2 36.22 possibilities to choose from. If a repository/commit is selected randomly then the number of bits of entropy represented by a single choice is about 36. To get 128 bits of security one would need to randomly select four repository/commits. A permutation of 4 gives 4 (36) = 144 bits of entropy. Remembering the order of the four repositories adds another memory task. If instead the four choices were combined using the simple version of secret splitting described above, where each shard is XORed together to recover the secret, then the number of random possibilities is reduced to the number of combinations of 80,000,000,000 items taken four at a time. As previously described, the lower bound on the number of combinations of N things taken K at a time is (N/K) K . In this case K = 4 and N = 2 36 . This gives the number of possibilities to be (2 36 /2 2 ) 4 = 2 34 4 =2 136 . This corresponds to 136 bits of entropy which is still greater than the required 128. The GitHub.com based recovery mechanism can be summarized as follows: Randomly choose four GitHub.com repository commits. For each commit, the pairing of a project name, repository name and commit date must be remembered and/or backed up using a hardware backup. Generate a seed by XORing together the 20-byte commit SHA-1 commit hash from each of the four repositories. Use this seed with a deterministic CSPRNG to generate a one-time pad of length at least as long as the key rotation history. Encrypt the key rotation history by bitwise XORing each byte in the history with the corresponding byte from the one-time pad. Securely discard the one-time pad. Store the encrypted key-rotation history in a highly available database. This encrypted history should be impervious to attack so it can be stored online. When recovery is required, remember the four project/repository/commit-date pairings or restore from a hardware backup. Use the pairings to lookup the SHA-1 commit hashes from GitHub.com for each. Then recreate the seed by XORing the four commit hashes. Use the seed and the same CRPRNG to regenerate the one-time pad. Retrieve the key history from the database. Use the one-time pad to bitwise XOR each byte of the saved encrypted key history to unencrypt it. The key history is now recovered. The memory load is four triples of a project name, a repository name, and a date, or twelve items total, but the order of the triples is not important. Given that typically each GitHub project has a small number of repositories, merely remembering the project should make remembering the repository much easier by going to the project page and looking at the choices for repositories. The date is the hardest memory task. There are several well known mnemonic techniques for remembering dates. In addition to the mnemonic, a physical backup of the hashes should also be created. The physical backup could be split into four parts to make it more secure. If practical a threshold multi-party secrete sharing system could provide additional backup.","title":"GitHub Seed Recovery"},{"location":"RWoT6/DecentralizedAutonomicData/#flickrcom-seed-recovery","text":"The FlickR.com-based recovery mechanism is similar to the Github.com based one. There are over 10 billion primary photos on FlickR. Each primary photo may come in multiple resolutions. A given photo is displyed on the FlickR.com web page using a low-resolution copy. This displayed version can be scraped from the page. The Flickr.com website does not provide hashes of the images, so one would have to scrape or download an image and then calculate the hash after the fact. A viable approach would be to use SHA-2 from the OpenSSL library or Blake from the libsodium library. Ten billion is about 2 33.22 which corresponds to about 33 bits of entropy when randomly selected. Four randomly selected images are needed to get the required 128 bits of entropy, that is, 4 * 33 = 132. If we combine the hashes from four images by XORing (i.e. simple secret splitting) then the number of choices becomes the combinations of 10 billion things taken four at a time. As described above, the lower bound on the number of combinations of N things taken K at a time is (N/K) K . This gives the number of possibilities to be (2 33 /2 2 ) 4 = 2 31*4 =2 124 . This corresponds to 124 bits of entropy which is close enough to the required 128. (2 4 = 16, which is not meaningfully weaker as it would still take 500,000,000 years to break). The proceedure for recovery is essentially the same as the GitHub example above, once the hashes for each photo have been generated. The mnemonic task is remembering four images. Humans are very good at remembering images given a selection. The hard mnemonic task is searching on FlickR for a given image using tags. It takes about four or five tags to get the list of images to under 100 for a given tag set. The mnemonic task is then to remember four sets of four to five tags each, where the tags are not in any order. Remembering which photo is helped by the fact that the tag set typically corresponds to features of the photo. Moreover, images provide an opportunity to hide them in plain sight. In addition to the mnemonic, a physical backup of the hashes should also be created. The physical backup could be split into four parts to make it more secure. If practical a threshold multi-party secret sharing system could provide additional backup.","title":"FlickR.com Seed Recovery"},{"location":"RWoT6/DecentralizedAutonomicData/#geneological-database-seed-recovery","text":"FamilySearch.org has over six billion genealogical records indexed by name and life-event type, event date, and event place. There are seven standard event types such as birth, death, marriage, census, military service, immigration, and probate. A randomly selected record can be recovered with a name and the event details of event type, date, and place. With six billion records and seven event types there are over 42 billion choices. The number of bits of entropy for one randomly selected record is log 2 (42,000,000,000) = 35.29. Suppose four records are randomly selected. hTe OTP seed is created by XORing a SHA-2 or Blake hash from each record where the hash is computed from the record name and event details. This produces (2 35 /2 2 ) 4 = 2 33*4 =2 132 combinations which corresponds to 132 bits of entropy. This exceeds the desired 128. The mnemonic task is to remember the name, event type, event date, and event place for four different records. The records can be in any order. In addition to the mnemonic a physical backup of the hashes should also be created. The physical backup could be split into four parts to make it more secure. If practical a threshold multi-party secrete sharing system could provide additional backup.","title":"Geneological Database Seed Recovery"},{"location":"RWoT6/DecentralizedAutonomicData/#google-maps-seed-recovery","text":"The Google Maps database covers the entire globe with high resolution imagery of the land area. The world's land area is approximately 150,000,000 km 2 . It has been estimated that 90% of the landmass is inhabited although only 10% is considered urban. Lightly populated areas still have memorable identifiable features suitable for map based mnemonics such as roads, fences, and buildings (farms, huts, etc). The estimated inhabited surface area is 0.9 * 150,000,000 km 2 = 135,000,0000 km 2 . The resolution of Google Maps' georeferenced satellite photos is given in decimal degrees to six decimal places. For example, clicking on a map gives the location in (degrees latitude, degrees longitude) as (45.348807, -105.709547). Six decimal places is about one tenth of a meter. This is too small to reliably reproduce merely by clicking on the satellite view. Five decimal places is about one meter. This is big enough that it can be reproduced reliably albeit carefully by clicking on the satellite view. A conservative approach would be four decimal places which is about 10 meters. This is easily large enough that it is trivial to reproduce reliably by clicking on the satellite view. A resolution of approximately one square dekameter (10m) 2 or 4 decimal places per location gives a total of 135,000,000 * 10,000 = 1,350,000,000,000 = 2 40.3 unique locations. When selected randomly this corresponds to over 40 bits of entropy per location. A resolution of a square meter per (1m) 2 or 5 decimal places per location gives a total of 135,000,000 * 1,000,000 = 135,000,000,000,000 = 2 46.94 unique locations. When selected randomly this corresponds to over 46 bits of entropy per location. At a resolution of a square dekameter four randomly chosen locations are needed to reach over 128 bits of entropy, (4 * 40.3 = 160.9). At a square meter resolution only three randomly chosen locations are needed to reach over 128 bits of entropy, (3 * 46.94 = 140.82. When locations are combined using a secret splitting approach, the total number of combined unique locations in combination is reduced. As described above, a lower bound on the number of combinations of N things taken K at a time is (N/K) K . At the square dekameter resolution, K = 4 and N = 2 40 . This gives the number of possibilities to be (2 40 /2 2 ) 4 = 2 38 4 =2 152 . This corresponds to 152 bits of entropy which is greater than the required 128. At the square meter resolution, K = 3 and N = 2 46 . This gives the number of possibilities to be (2 46 /2 1.59 ) 3 = 2 44.41 3 ~= 2 133 . This corresponds to 133 bits of entropy which is still greater than the required 128. Consequently with Google Maps either three or four unique locations are needed to achieve the desired cryptographic strength for seed generation. Memorable locations could include the corner of a building or or a doorway or roofline or road intersection or fenceline intersection or pole. The mnemonic load for a site is the address of the site. Because humans are adept at remembering locations visually by familiarity with the surroundings, exact addresses may not be needed. Merely enough of an address to move the view within the neighborhood of a location may be enough. Once in the neighborhood, terminal navigation may be performed via visual interaction with the Maps app. Alternatively, landmarks, business or other nearby features could be used as the search parameters. In addition the user has to remember what exact feature of the structure is used for the location.","title":"Google Maps Seed Recovery"},{"location":"RWoT6/DecentralizedAutonomicData/#recovery-summary","text":"All of the hybrid recovery methods allow for rapid recovery that does not require multi-party interaction. They all depend on a non-trival but not onerous mnemonics for rapid recovery but may fall back to a physical or threshold secret sharing multi-party interactive copy for slower recovery. Rapid recovery using the online databases (GitHub.com , FlickR.com, FamilySearch.org, or Google maps) depends on the availability of the databases maintained by the corresponding entities. In each case, should one of the selected records be deleted then the only recourse would be one of the backups. In order to achieve the required 128 bits of security, the DiceWare approach requires recalling 10 words in order, whereas the GitHub.com, Flickr.com, FamilySearch.org and Google maps (at 1 dekameter) approaches require recalling four records. All five methods could be mixed. Using a mixture adds some security (more choices) but not enough to reduce the number of records required. Alternatively, at one meter resolution the Google maps approach only needs three records. The Google maps approach (either four locations or three locations) may have the lightest memory load because the exploits the high human capacity for visual-geospatial recall. The secret splitting used to combine records could be augmented to use a threshhold scheme to make it more resilient to record loss but at the cost of needing more than records. If multi-party interactive recovery is acceptable then using threshold secret sharing could be a better approach. Even when multi-pary interactive is not the preferred approach it could be another backup in addition the a physical backup. This novel hybrid approach combines multiple cryptographic techiques to provide a viable non-multi-party interactive rapid key recovery method that is well suited to data streaming applications. It combines hiding in plain site, mnemonics, DiceWare-like selection, secret splitting, CSPRNG, and one-time pads. The method is a practical trade-off between the features of the different approaches.","title":"Recovery Summary"},{"location":"RWoT6/DecentralizedAutonomicData/#virtual-world-game-as-hierarchically-deterministic-seed-mnemonic","text":"Looking to the future, it would be possible to create a mnemonic-seed generating mobile or desktop application that is completely self-contained and does not require any external online databases for random key material. Humans have an innate ability to remember complex visual geo-spatially related information such as is encountered in everyday life when walking from one place to another without getting lost. Humans are particularly adept at remembering how to retrace the path they followed on a journey through a city, or countryside. Humans are also adept at remembering when the memory is associated with familiar spacial surroundings. The well known method of loci , more commonly known as the memory palace mnemonic, associates a sequence of items to be remembered with locations in one's house or other familiar structure. When a spatial mnemonic is enhanced with what is called [elaborative encoding] (https://en.wikipedia.org/wiki/Elaborative_encoding), that is, adding visual, auditory or other sensory cues, it becomes particularly powerful. Humans are also adept at learning complex mental models via hierarchical decomposition . Various other mnemonic devices take advantage or combinations of familiar, spatial, hierarchical and sensory cues to make the learning and recall task easier. An application that exploited multiple mnemonic devices in combination could minimize the memory load required to recover seed material. Indeed games that involve recalling complex sequences of movement and action within a simulated graphical world can be successfully played by young children. This level of mnemonic capability in demonstrated by young children when playing games like The Legend of Zelda . What is being proposed is a hierarchical deterministic seed mnemonic (HDSM) as a type of hierachical spatial elaborative encoded mnemonic. Lets call this hypothetical mnemonic seed generating game Quest for the Mnemon Seed for lack of a better title. A notional description follows: The game is based on a graphical virtual world map such as one might encounter in an online role playing game. In the game, the user starts at the entrance and is presented with a map of a locale such as a village containing unique sites including buildings, parks, roads etc. Each site within the locale has memorably unique visual features such as floor plan, architectural style, period, color, material, flora, fauna, characters, objects etc. The user then walks down roads and paths to get to the different sites. Upon entry to a site the user is presented with a choice of actions to perform such as picking up an object or interacting with a character. Thus the process of selecting a site and then selecting an action at the site constitues a choice. If the choice is selected at random then it becomes the source of random seed material. The mnemonic is remembering where the site is placed within the locale and how to get there and then remember the action(s) performed at the site. A sequence of site visits with actions then provides an extended source of key material. Playing the game provides rehearsal so that a specific set of actions can be recalled in order, thereby recovering the seed. The site options, both exterior and interior, such as location, layout, style, material, color, etc, are specified as a data structure represented as a sequence of bit fields. A single long string of bytes such as might be generated with a deterministic hash can then be used to generate a uniquely configured locale. A set of sites and actions can also be encoded as a sequence of bit fields. A path through the locale with visits and actions at each site can then be generated from a large random number. The game is then played in two modes. The first mode generates a random seed and then rehearses the mnemonic for the random seed. The second mode recovers the random seed with the mnemonic. In the first, generative, mode, the user inputs a string that is the customization phrase. The cryptographic strength of the customization phrase is not important, it just allows the user to have a custom configured locale that is compatible with the user preference. The customization phrase is hashed (with Blake or Sha2) to generate a sequence of bytes used to specify the local options. The locale is then generated. A 2D or 3D display of the locale map is then presented to the user. The game then uses a cryptographic class random number generator to create the 128-bit random seed. This seed will be used to generate the one-time pad for encrypting the key-rotation history. Using the seed and a CSPRNG, a sequence of sites and actions is created deterministically from the seed as the mnemonic. The user is then shown on the map this mnemonic path through the locale. The user follows the path through the locale, visiting each site in turn, where the user is prompted to perform the selected action or actions. Once complete the user continues to rehearse the mnemonic, only now the path is not shown. The user must recall it from memory. If the user makes a wrong choice, the game reminds the user with a prompt. Rehearsal repeats until the user can successfully retrace the path and actions from memory without any prompts. At this point the user has memorized the mnemonic and can print out copies of the random seed for backup, use it for generating a one-time encryption pad, and then instruct the application to forget the random seed. In the second, recovery, mode, the user inputs the customization phrase to generate the locale map. The user then visits sites in turn and performs actions at each site. The sequence of site visits and actions deterministically regenerates a seed. When the user completes a sequence the game displays the associated seed. If the user correctly replayed the sequence then the user will recover the correct seed. If the user does not, then the seed provided by the game will not be the one the user was trying to recover. Suppose that each locale contains 256 = 2 8 sites. This is comparable to a small village of population about 1000. Randomly selecting a site then provides 8 bits of entropy. Suppose that inside each site there are 8 = 2 3 spots, such as cupboard east wall, shelf north wall, barrel northeast corner, etc. Random selection of a spot would provide 3 more bits of entropy. Suppose that at each interior location the user has 2 = 2 1 choices of action such as, pick up hammer, drink vial of liquid, answer question from inn keeper, etc. Random selection of an action would provide another 1 bit of entropy. Suppose then that after completing the first spot-action the user has to select another spot and make another binary choice of action. The second spot-action provides yet another 4 bits of entropy. This given a total of 8 + 4 + 4 = 16 bits of entropy per site-spot-action-spot-action sequence. To provide the total of 128 = 8 * 16 bits of entropy needed for the random seed requires that the user visits 8 sites in order while selecting two successive actions at each site. Alternatively the game could provide some other mix of interior location and interaction choices to get 8 bits of entropy. Suppose for example that at each of the 256 sites there are 32 = 2 5 spots. Random selection of a spot provides 5 bits of entropy. At each spot there are 8 = 2 3 action choices. Random action selection provides another 3 bits of entropy. So each spot-action selection provides 5 + 3 = 8 bits of entropy. If at each site the user must make 3 spot-action selections then that provides a total of 3*8 = 24 bits of entropy. Thus each site-spot-action-spot-action-spot-action combintion or site + (spot-action) * 3 combination provides 32 = 8 + (3 * 8) bits of entropy. A 128 = 4 * 32 bit seed can then be generated from only four site-(spot-action) 3 combinations, that is, 128 = 4 * (8 + (3 * 8)). An area of research would be to find the optimal decomposition and combination of site-spot-action sequences. Either of the eight-site or four-site examples above are well within the mnemonic capabilities of the general population given the dense hierarchical geospatial sensory cues that such a graphical virtual game world journey provides and would only take a few minutes to replay for recovery. The app would run self contained on the user's mobile device or desktop computer and would make seed recovery fun. Any computing device could be engaged to play the app so it would not require a specific mobile device or computer and therefore loss of the user's mobile device would not impede seed recovery. A variation of the game would be to allows some sites to have a portal that transports the user to a new locale with a new unique map. The configuration of the new locale is determined by a hash of the site/action visit selections that were performed prior to entry of the portal. This would add additional variety to the game and help differentiate the mnemonics required for the create of multiple unique seeds. This makes the game a recursively hierarchical deterministic seed mnemonic (RHDSM). This hierarchically deterministic seed mnuemonic (HDSM) could become a standard feature for primary key recovery for any decentralized identity based cryptographic system where the user must generate and manage their private keys. Once users become familiar with this approach to key recovery it could open the door to more rapid adoption of decentralized approaches to online interactions where security is based on user managed public/private key pairs.","title":"Virtual World Game as Hierarchically Deterministic Seed Mnemonic"},{"location":"RWoT6/DecentralizedAutonomicData/#summary","text":"A new data type called a DAD for decentralized autonomic data has been presented that is derived from decentralized identifiers, DIDs. DADs are suitable for streaming applications. Methods for the three basic key management operations, namely, reproduction, rotation, and recovery have been presented that are compatible with DAD stream-data applications. The pre-rotation and hybrid recovery methods presented in this paper including the hierarchically deterministic seed mnuemonic (HDSM) are somewhat novel. They all provide what could be considered minimally sufficient means for key management operations.","title":"Summary"},{"location":"RWoT6/DecentralizedAutonomicData/#appendices","text":"","title":"Appendices"},{"location":"RWoT6/DecentralizedAutonomicData/#support-for-dad-signatures-in-http","text":"In web applications that use HTTP, the simplest most compatible way to associate or attach a signature to an HTTP packet is to include it in a custom HTTP header. Standad JSON parsers raise an error if there are additional characters after a closing object bracket thus one cannot simply append the signature after the JSON serialization in the message body. Another approach would be to use a custom JSON parser that guarantees a cononical representation of a JSON serialization (including white space) and then wrap the data item and the signature in another JSON object, where the signature and the data item are both field in the wrapper object. This is more verbose and is not compatible with the vast majority of web application framework tools for handling JSON serialized message bodies. Thus it is non-trivial to include the signature in the message body. Using a custome HTTP header is relatively easy and has the advantage that is is compatible with the vast majority of existing web frameworks. A suggested header name is Signature header that provides one or more signatures of the request/response body text. The format of the custom Signature header follows the conventions of RFC 7230 Signature header has format: Signature: headervalue Headervalue: tag = \"signature\" or tag = \"signature\"; tag = \"signature\" ... where tag is replaced with a unique string for each signature value An example is shown below where one tag is the string signer and the other tag is the string current . Signature: signer=\"Y5xTb0_jTzZYrf5SSEK2f3LSLwIwhOX7GEj6YfRWmGViKAesa08UkNWukUkPGuKuu-EAH5U-sdFPPboBAsjRBw==\"; current=\"Xhh6WWGJGgjU5V-e57gj4HcJ87LLOhQr2Sqg5VToTSg-SI1W3A8lgISxOjAI5pa2qnonyz3tpGvC2cmf1VTpBg==\" Where tag is the name of a field in the body of the request whose value is a DID from which the public key for the signature can be obtained. If the same tag appears multiple times then only the last occurrence is used. Each signature value is a doubly quoted string \"\" that contains the actual signature in Base64 url safe format. But the signatures should use an intelligent default cryptographic suite such as 64-byte Ed25519 signatures that have been encoded into BASE64 url-file safe format. The encoded signatures are 88 characters in length and include two trailing pad characters = . An optional tag name = kind may be present to specify the cryptographic suite and version of the signatures. The kind tag field value specifies the type of signature. All signatures within the header must be of the same kind. Signature: signer=\"B0Qc72RP5IOodsQRQ_s4MKMNe0PIAqwjKsBl4b6lK9co2XPZHLmzQFHWzjA2PvxWso09cEkEHIeet5pjFhLUDg==\"; did=\"B0Qc72RP5IOodsQRQ_s4MKMNe0PIAqwjKsBl4b6lK9co2XPZHLmzQFHWzjA2PvxWso09cEkEHIeet5pjFhLUDg==\"; kind=\"ed25519:1.0\"","title":"Support for DAD Signatures in HTTP"},{"location":"RWoT6/DecentralizedAutonomicData/#cryptographic-suite-representation","text":"Best practices cryptography limits the options that user may choose from for the various cryptographic operations, such as signing, encrypting, and hashing to a suite of balanced and tuned set of protocols, one for each operation. Each member of the set should be the one and only one best suited to that operation. This prevents the user from making bad choices. In most key-representation schemes each operation is completely free to be specified independent of the others. This is a very bad idea. Users should not be custom combining different protocols that are not part of a best practices cypher suite. Each custom configuration may be vunerable to potential attack vectors for exploit. The suggested approach is to specify a cypher suite with a version. If an exploit is discovered for a member of a suite and then fixed, the suite is updated totally to a new version. The number of cypher suites should be minimized to those essential for compatibility but no more. This approach increases expressive power because only one element is needed to specify a whole suite of operations instead of a different element per operation. See this article for a detailed explanation on how standards such as JOSE expose vulnerabilities due to too much flexibility in how cryptographic operations are specified. Example cypher suites: v1: Ed25519, X25519, XSalsa20poly1305, HMAC-SHA-512-256 v2: Ed448, X448, XChaCha20Poly1305, keyed BLAKE2b v3: SPHINCS-256, SIDH, NORX64-4-1, keyed BLAKE2x","title":"Cryptographic Suite Representation"},{"location":"RWoT6/DecentralizedAutonomicData/#canonical-data-serialization","text":"Canonical data serialization means that there is a universally defined way of serializing the data that is to be cyptographically signed. The are few typical approaches to achieving data canonicalization. The advantages of compatibility, flexibility, and modularity that come from using a key/value store serialization such as JSON usually makes 1) the preferred approach. Store the serialization and signature as a chunk. The simplest is that the signer is the only entity that actually serializes the data. All other users of the data only deserialize. This simplifies the work to guarantee canonization. For example, JSON is the typical data format used to serialize key:value or structured data. But the JSON specifcation for ser/deser treats whitespace characters and the order of appearance of keys as semantically unimportant. For a dictionary (key:value) data structure the typical approach is to represent it internally as a hash table. Most hash algorithms do not store data ordered in any predictable way (Python and other languages have support for Ordered Dicts or Ordered Hashes, which can be used to partially ameliorate this problem). But from the perspective of equivalence, key:value data structures are \"dict\" equal if they have the same set of keys with the same values for each key. Thus deserialization can produce uniform equivalent \"dict equal\" results from multiple but differing serializations (that differ in whitespace and order of appearance of fields). JSON only guarantees dict equivalent not serialization equivalence. Unfortunately the signatures for the differing but equivalent serializations will not match. But in signed at rest data only the signer ever needs to serialize the data. Indeed, only the signer may serialize the data because only the signer has the private key. So deserialization and reserialization by others is of limited value. The primary value appears to be either schema completeness where signatures are included as fields in a wrapper object or the ability to nest signatures or signed data with signatures. Because it is simple to convert a JSON serialization to a coded serializaiton such as Base64, nested coded JSON serialization without canonicalization can be trivially supported. After expansion and decoding, readers of the data can see the uncoded underlying data in a schema complete representation. The signer's serialization is always canonical for the signature. Users of the data merely need to use a \"dict equal\" deserialization which is provided by any compliant JSON deserializer. So no additional work is required to support it across multiple languages etc. If the associated data also needs to be stored, unserialized then validation and extraction of the data is performed by first verifying the signature on the stored serialization and then deserializing it in memory. Implement perfectly canonical universally reproducibly serialization. In this approach all implementations of the protocol or service use the exact same serialization method that is canonical including white space and field order so that they can reproduce the exact same serialization that the original signer created when originally signing the data. This is difficult to achieve with something like JSON across multiple languages, platforms, and tool kits. It's usually more work to implement and more work to support because it usually means either using something other than JSON for serialization or writing from scratch conformant JSON implementations or at the very least having tight control of how white space and order occurs and ensuring accross updates that this does not change. Unfortunately many overly schematizied standards are based on this approach. This approach typically breaks web application frameworks. Use binary data structures With binary data structures the canonical form is well defined but it is also highly inflexible.","title":"Canonical Data Serialization"},{"location":"RWoT6/DecentralizedAutonomicData/#relative-expressive-power","text":"One way to measure and compare different knowledge representations is called relative expressive power . In the physics world power is defined as work done per unit time. It is a ratio. Expressive power is similary defined as the ratio of meaning conveyed per dependency, where dependency is something that must be kept track of or transmitted to convey the meaningful information. Because dependencies are a measure of complexity, relatively higher expressive power conveys more meaning relatively more simply.","title":"Relative Expressive Power"},{"location":"RWoT6/DecentralizedAutonomicData/#intelligent-defaults","text":"One approach to acheiving higher expressive power in a data representation specification is the use of intelligent defaults. An intelligent default assigns meaning to the absence of data. For example, if there are several options for a given data item value such as the type of a data item, an intelligent default would assign the type to a predetermined default if no type is provided in the data. This provides high expressive power as the type meaning is conveyed without the transmission of any bytes to represent type. Typically in any given knowledge representation application the relative frequency of the appearance of optional values is not evenly distributed, but follows a Pareto distribution. This means that if an intelligent default (the Pareto optimal value) is specified as part of the schema the average expressive power of data items will be increased. A practical example of this is the RAET (Reliable Asynchronous Event Transport) protocol header (see RAET ). Typically in protocols the header has a fixed format binary representation for two reasons. The first is that every packet includes the header, so a verbose header reduces the payload capacity of each packet, thereby making the protocol comsume more bandwidth. The second is that the header is used to interpret the rest of the packet and therefore must be consistenly parsable which is easier if the format is fixed. The problem with fixed format headers is that they are not extensible. To make the extensible usually means adding additional fields to the header to indicate the presence of additional extended fields. RAET used an intelligent default policy to achieve a completely flexible extensible header that on average is the size of a non-extensible fixed format header. In RAET the header is composed of a serialized list of key-value pairs where each key is the field name of the associated field value. This makes it easy to add new key-value pairs as needed to extend the protocol to different uses and with different behavior. Unfortunately, transmitting the keys makes the header much larger relative to a fixed format header where the offset of the value in the header determines the associated field. RAET overcomes this problem by defining a default value for each key-value pair. When a header is generated on the transmit side, the actual key-value pairs are compared against the default set. Any pair where the value matches the default is not included in the list of key-value pairs in the transmitted header. On the recieve side a default header is created with every key value pair set to the default. The received header's key-value pairs are used to update the default header with the non-defaulted values. Because the optional fields are seldomly used by most packets the average header size is comparable to a fixed format header. When viewing the header after expansion and update, all the fields are present, so there is no hidden information. All the meaning is apparently conveyed. RAET header field defaults PACKET_DEFAULTS = odict([ ('sh', DEFAULT_SRC_HOST), ('sp', RAET_PORT), ('dh', DEFAULT_DST_HOST), ('dp', RAET_PORT), ('ri', 'RAET'), ('vn', 0), ('pk', 0), ('pl', 0), ('hk', 0), ('hl', 0), ('se', 0), ('de', 0), ('cf', False), ('bf', False), ('nf', False), ('df', False), ('vf', False), ('si', 0), ('ti', 0), ('tk', 0), ('dt', 0), ('oi', 0), ('wf', False), ('sn', 0), ('sc', 1), ('ml', 0), ('sf', False), ('af', False), ('bk', 0), ('ck', 0), ('fk', 0), ('fl', 0), ('fg', '00'), ]) Any key-value based schema standard specification may benefit from an intelligent default policy to greatly increase the expressive power of the schema. This becomes even more important where security is concerned as the intelligent default might be the most secure set of options thus helping the user be more secure and more expressive. Moreover expressive power is about conveying meaning more simply which makes it easier to implement and incentivizes adoption.","title":"Intelligent Defaults"},{"location":"RWoT6/DecentralizedAutonomicData/#essential-vs-optional-elements","text":"Another related technique for increasing expressive power is to distinguish between essential and optional elements in a given representation. Any essential elements should be expressed as explicitly as possible (when not defaulted); that is, it should not be looked up and should either not be indirected or have minimal indirection. External lookups are expensive. Moreover, hiding essential elements behind multiple levels of indirection may make it harder to understand the conveyed meaning (adding dependencies and hence complexity). An important meaningful difference that should be apparent is whenever an essential element is not set to a default value. This difference should not be hidden behind indirection.","title":"Essential vs. Optional Elements"},{"location":"RWoT6/a-roadmap-for-ssi/","text":"SSI: A Roadmap for Adoption A Journey from huh? to DUH! Moses Ma (FutureLab), Claire Rumore (FutureLab), Dan Gisolfi (IBM), Wes Kussmaul (Reliable Identities, Inc), Dan Greening (Senex Rex) Abstract This document proposes the formation of a short-term team to develop consistent messaging for the Self-Sovereign Identity (SSI) market. It will target key stakeholders who would actively promote SSI adoption. The goal is to create an SSI market roadmap. This roadmap will help SSI leaders, standards bodies, developers, academics, media, and investors coordinate and clarify their messaging for the market, to accelerate the SSI adoption. We illustrate the need for this by summarizing our market strengths, weaknesses, opportunities, and threats (SWOT analysis), and show how this simple analysis exposes important marketing goals. Further projects will include market explainers, frequently asked questions, market glossaries, market research, and go-to-market (GTM) materials for developers. We wrote this paper to find more people who value a healthy SSI market. By working with us on this short-term project, you will help to grow the overall SSI market. Please join us! 1. Self-Sovereign Identity Group Charter This section describes the mission, stakeholders, goals, and initial structure of the proposed Self-Sovereign Identity Group . Mission We improve social stability, economic productivity, and individual freedom by promoting broad use of self-sovereign identity. Stakeholders Our market development efforts will target these stakeholders. Role Benefit Technical leaders Focus innovation on market-enabling technology Standards participants Develop standards that promote high usage Media Develop more compelling, meaningful, and accurate stories Developers Create applications and services for rapid adoption Businesses, Governments, Non-profits Develop early customers and adoption Venture capitalists Make better decisions to reduce risk and increase IRR Academics Target hot research opportunities Membership Members in this roadmap team must contribute to content development and distribution, stakeholder engagement, and more. Individuals who contribute will learn an enormous amount about the SSI market, identity stakeholders, standards organizations, and marketing. Finally, to the extent possible, we want members to have autonomy; this will be accomplished by providing them with measurable goals and allowing them to achieve those goals as they see fit (within reason). We expect the team to ultimately self-organize to maximize market expansion. Initially, a single Product Owner may prioritize work, and a Scrum Master may be assigned to facilitate communication and work, manage the workflow, and seek timely results. These focus areas will likely dominate our activities. Content : Create informational content (presentation, videos, etc) suitable for stakeholders Portal : Create and maintain the team's website Communications : Propagate SSI messaging to passive stakeholders Contact Moses Ma or Dan Greening to become a member. Goals Get ourselves organized (DONE) Agree on mission (see above) (DONE) Setup tools (i.e. Trello) (DONE) Establish web site (see Self Sovereign ID (in progress) Complete the final draft of this document (not doing until bigger) Discuss project management Schedule cadence (scrum) call Review and refine tribe structure Establish site login tiers (public, developer, marketer, press) and tracking to measure reach and engagement Create and promulgate a realistic technology and market roadmap Recruit initial foundational partners as adoption catalysts Identify minimum viable ecosystem (DID network, wallet, browser integration, blog integration?) * Build Drupal plugin for SuperSignOn , a decentralized authentication system based on DID-Auth * Allow developers and others to SuperSignOn to our (Drupal) site Recruit and help influential leaders (e.g. Reid Hoffman, Fred Wilson, etc) to speak favorably about SSI Access to subject matter experts Sample presentations Speakers bureau Recruit/produce minimum viable ecosystem participants Identify willing networks, wallets, feasible browser plugins, feasible blog plugins Convince key communication technologies from Slack, Telegram, RocketChat, Wikipedia to support DIDs Recruit developer support services, such as GitHub, CircleCI, BitBucket, to support DIDs Produce Go To Market Resources to support developers by year end 2018 (in progress) Write glossary of market-relevant terms (in progress) Develop FAQs that debunk myths and promote adoption Consolidate technical primers (tutorials) into a single \"getting started\" kit Develop a Communications Kit Simplify and articulate the concepts and benefits of SSI for the masses Provide a cohesive narrative about SSI and its goals for the Media Offer common baseline talking points for SSI developers Explain wallets and DID distribution Explain who the wallet makers are Explain how wallets will be interoperable and secure Create high-impact videos and other demo recordings (i.e.: RWOT) Assets This document was developed at RWOT6 (Rebooting Web of Trust, Spring 2018) group. Document source is HERE A team web site has been created at selfsovereign.id . An initial glossary is at SSI Glossary . Group work is prioritized and tracked at Trello . Members communicate through Slack . Contributors Some people have contributed substantially so far. They are Person Company Contribution Dan Gisolfi IBM Content Moses Ma Futurelab Content, Management Darrell Duane Crypto UBI Portal Wes Kussmaul Reliable Identities Content Dan Greening Senex Rex Content Possible volunteers People who have expressed interest in the team include: Person Company Possible Role Vishal Gupta Diro Foundation Portal (Design) Chandran Gaurav Diro Foundation Portal (Design) Kate Sills Communications Kaliya Young Communications Remy Lyon Communications People who have been invited are: Person Company Possible Role Alex Preukschat (Invited) Globatalent GTM Development Sean Bohan (Invited) Evernym Stakeholder Engagement David Crocker (Invited) Brandenburg InternetWorking Stakeholder Engagement Nathan George (Invited) Sovrin Stakeholder Engagement 2. Existing Market Motivation (SWOT) This section describes the state of the SSI industry as of 7 April 2018. We can gain a broad understanding of the maturity and vulnerability of our industry by listing strengths, weaknesses, opportunities and threats (SWOT analysis). Strengths SSI networks provide affordability, scalability, reliability, trustability, privacy, security, and portability superior to traditional centralized and siloed identity networks. For decades, identity leaders have avoided partisanship and collaborated on identity fundamentals and standards. Our community has largely completed the SSI technical infrastructure to support SSI applications. There are several operational and competing SSI distributed identity networks and wallets in beta. Weaknesses Our own complacency could derail or delay SSI adoption. Self-sovereign identity concepts can be confusing, especially when we lead with technology and not societal benefits. We do not discuss deployment schedules for SSI networks and APIs, so many application developers refrain from investing effort. We do not broadly discuss expected costs for DIDs and for SSI services, which creates financial risk for startups. We provide no SSI reference applications and libraries, which limits developers ability to produce compelling applications. We don't list SSI network and wallet vendors, and so application developers worry that they will be locked in. Our narrow focus on technical and philosophical exploration is distracting us and delaying the societal and individual benefits of SSI. Our individual non-specific identity concerns \u2014 such as whether biometric data might enable despotic states to track our movements \u2014 distract us from solving clear and present dangers, such as putting development work toward eliminating non-state-originated financial, identity, and privacy theft (which will actually help us to better understand and address the challenges of despotic states); Opportunities Society is turning a corner in demanding better identity solutions: Fear of identity hacks and theft are now pervasive. Privately-maintained identity systems have exposed businesses and governments to hacking, leaks, and lawsuits. Foreign enemies have destabilized nations through election and infrastructure hacking. Malware-borne DDS attacks have damaged individual companies and degraded the internet. Social media and advertisers have enabled private data misuse (sometimes unintentionally). Inexpensive SSI-based solutions can resolve each of these problems. If we direct some of our attention to expanding the SSI market, we can likely gain rapid traction. Threats Despite the many advantages of SSI, competitive threats could derail or significantly delay SSI adoption. Standards bodies could produce standards incompatible with SSI. A specific concern is the mDL (mobile drivers license) standard being developed by the ISO/IEC JTC 1/SC 17/SG 1 working group . Key customers could adopt inferior non-SSI alternatives. A poor identity decision by a customer will take years and cash to correct, due to high switching costs for identity. The low agility of government agencies compounds the effect: their bad decisions could affect everyone for decades. If government agencies don't participate in distributed identity networks for licenses, passports, and national identity it will degrade citizen security, financial tracability, and information verifiability for a long time. False assumptions about SSI are rife: Some believe blockchain solutions store personally identifiable information (PII) on the distributed ledger. Some believe that participation in SSI networks is involuntary. Some believe that a decentralized ID could \" never be turned off or blocked \" due to the immutability of the distributed ledgers. Some believe it will be impossible to prevent anyone from publishing anything they want about you, without the standard societal repercussions (libel, etc.), and that immutable negative ledger reviews become part of an indelible permanent record. Few people know the privacy benefits of Selective Disclosure (or the Principle of Minimum Disclosure) and how SSI supports it. Most of these threats can be disarmed by informing key influencers. Industry Organizations There are many groups and organizations focused on distributed identity, but there are many disparate voices. We could all benefit from consistent messaging about market development. A quick overview of the landscape includes: Rebooting Web of Trust (RWOT) Rebooting Web of Trust is a group that meets twice yearly to develop position papers and kick off more significant efforts. It seeks to create the next generation of decentralized web-of-trust-based identity systems. Each event generates roughly five technical white papers on topics decided by the group that will have the greatest impact on the future. RWOT may also use hackathons to implement those ideas. This document is a \"technical white paper\" written at RWOT6 (Spring 2018). Internet Identity Workshop (IIW) Internet Identity Workshop is a group that meets twice yearly to share and refine ideas about identity and to forge working relationships. It is organized as an unconference using Open Space Technology. It is highly effective, and that effectiveness is largely due to its insistence on note-taking and collaboration. World Wide Web Consortium (W3C) World Wide Web Consortium (W3C) is an international community that develops open standards to ensure the long-term growth of the Web. Two working groups are particularly relevant to self-sovereign identity: Verified Claims Working Group (VC) seeks to make it easier and more secure to express and exchange credentials on the web that have been verified by a third party. Credentials Community Group (CCG) explores the creation, storage, presentation, verification, and user control of credentials. The group drafts and incubates internet specifications for further standardization and prototyping and tests reference implementations. CCG was the original source of material for the official Verified Claims Working Group. Distributed Identity Foundation (DIF) Distributed Identity Foundation (DIF) is an engineering-focused non-profit organization composed of individuals and companies who are collaboratively developing an interoperable set of decentralized identity protocols, specs, and reference implementations that run across chains and service providers. Its goal is user-enablement via the creation of a ubiquitous decentralized identity ecosystem that benefits every person and company worldwide. Sovrin Foundation (Sovrin) Sovrin Foundation (Sovrin) is a non-profit organization that manages a permissioned blockchain identity network. Because the network is permissioned, participants must agree to maintain identity security and privacy (otherwise a collection of members could subvert the network). Authoritative network participants must sign the Sovrin Trust Framework, which gives them permission to operate a node. The Sovrin Foundation is a spinoff of the company Evernym . The Sovrin Foundation has produced a substantial body of market-relevant material, but it is targeted toward one specific network. Other competing networks include Veres One (with its own network) and uPort (based on the Ethereum contract network ). Information Trust Exchange Governing Association (ITEGA) Information Trust Exchange Governing Association provides Internet stakeholders with a forum to convene, develop, and implement governing protocols and business rules for protecting and balancing trust, privacy, identity, and information commerce. It seeks to balance privacy, personalization, and payment to improve journalism and publishing. One of its first projects is the deployment of a proof-of-concept for a first-party-user data exchange that would incorporate privacy-by-design. Industry Goals Here are the goals these other organizations have articulated: One million public (open or pseudonymous) DIDs issued by March 2019 Formalized plans by year-end 2018 for foundational SSI specifications W3C DID W3C VC Oasis DKMS Public release of a handful of reference applications that can help jumpstart developer applications 3. Summary The distributed identity market is at a crossroads: Market forces are demanding secure, privacy-respecting solutions to identity. DID infrastructure is in beta or better. Whether we have a minimum viable ecosystem is debatable. Our market communications, so far, have been weak. This paper argues for the formation of an unbiased team to develop the Self-Sovereign Identity (SSI) market across all participants. We will educate and support a variety of stakeholders to promote SSI. We will provide infrastructure and application developers with unbiased information and tools to support their go-to-market efforts. We seek your help. Appendix A: WOWs to Consider Another requirement for success is the creation of a design process that would lead to a sustainable flow of compelling technologies that provide a \u201cwow\u201d factor, forming a pipeline of compelling functionality to fortify the value proposition for decentralized identity. An initial set of projects and ideas for \u201cwow prototypes\u201d include: Industry Demos : Develop a number of vertical industry references. For example: FinTech: A demo of the CULedger CUID Trust Framework that uses the Hyperledger Indy framework. Travel and Transportation: A Dapps that implements one or more concepts outlined in the World Economic Forum\u2019s Known Traveler Report . Community Badges Toolkit : Produce a Starter Kit that would allow any small or large community to begin to issue verifiable credentials in the form of OpenBadges . Such a toolkit would be a perfect Getting Started toolkit for a go-to-market (GTM) package of resources. NOTE: The final paper submitted by the Open Badges are Verifiable Credentials RWOT Workgroup is supposed to produce a working prototype that can be used to seed this toolkit. Other Interesting Concepts : There are a number of other potential reference applications that could be interesting to develop and deploy. SuperSignOn , a decentralized authentication system based on the DID-Auth specification. GitHub Authentication using Verifiable Credentials. ICO-LegalAssist , which would use Verifiable Claims as a basis for attestations by attorneys in ICOs. IDESG\u2019s IDEF assessment , a tool for receiving badges . Appendix B: Frequently Asked Questions Naysayers illustrate the urgency for this effort as they create myths that could be discredited by a strong communication plan. Here are a few examples of Q&As that address myths already circulating in the media and amongst analysts: Do blockchain SSI solutions store personally identifiable information (PII)? No. Commercial blockchain identity systems do not store personally identifiable information (PII) on blockchains because it is unnecessary, expensive, and limits portability. Your DID entry on the blockchain may point to data, if there is any, stored elsewhere, typically in encrypted form. The SSI standards discourage storing personal data on blockchains, and the industry has established several principles against it. Can I prevent someone from correlating my information from different sites to find out more about me? Yes. Once you create a \"public DID,\" you create \"pseudonymous DIDs\" whenever you establish a new online relationship (such as a new login). Pseudonymous DIDs cannot be correlated to your public DID by others without your involvement (and rarely need to be). See decentralized identifier (DID) and internet standard Universally Unique Identifiers (UUIDs). Can I revoke an SSI-distributed ID? Yes. A decentralized ID is turned off by invalidating the data that the DID points to. Can someone publish something permanently about me on an SSI blockchain? No. Blockchains are \"immutable,\" meaning data published there are never erased. However, the only things published on SSI blockchains are DIDs, encryption keys (not decryption keys ), and pointers to outside data (which can be erased or changed). The DID specification states that the user/owner explicitly controls and administers the publication of their decentralized IDs. This is actually the meaning of the phrase \"self-sovereign ID.\" The proposed Verifiable Credentials specification states plainly that SSI credentials are revocable, expirable, and that an SSI network must enforce the data policies of both the issuer and holder. Existing SSI networks require an ID holder to approve before sharing an ID. This means that a competitor, who would not be trusted to make a claim about your business, could not append a negative claim about your business without your explicit approval. This is because the issuer and holder both have the ability to revoke any claim. When I use a DID with a provider, is my personal information exchanged? If the DID is a new pseudonymous DID (easy to create from your public DID), the answer is \"No.\" Furthermore, the SSI industry encourages applications to uses something called \"Zero Knowledge Proofs\" to limit the amount of information shared. For example, when you assure a bartender that you can legally drink, you typically show a drivers license and reveal your birthdate. With most SSI networks, an identity supplier that knows your birthdate can reveal your legal drinking status without sharing your birthdate. How does SSI protect my privacy? The SSI industry promotes the use of pseudonymous IDs and encourages Selective Disclosure (or the Principle of Minimum Disclosure) to keep your personal data secure and private. Those practices also improve compliance with privacy regulations like HIPAA (Health Insurance Portability and Accountability Act) in the US and GDPR (Global Data Protection Regulation) in the EU. With SSI networks, users control their personally identifiable data and data privacy at levels they've never experienced before.","title":"SSI: A Roadmap for Adoption"},{"location":"RWoT6/a-roadmap-for-ssi/#ssi-a-roadmap-for-adoption","text":"A Journey from huh? to DUH! Moses Ma (FutureLab), Claire Rumore (FutureLab), Dan Gisolfi (IBM), Wes Kussmaul (Reliable Identities, Inc), Dan Greening (Senex Rex)","title":"SSI: A Roadmap for Adoption"},{"location":"RWoT6/a-roadmap-for-ssi/#abstract","text":"This document proposes the formation of a short-term team to develop consistent messaging for the Self-Sovereign Identity (SSI) market. It will target key stakeholders who would actively promote SSI adoption. The goal is to create an SSI market roadmap. This roadmap will help SSI leaders, standards bodies, developers, academics, media, and investors coordinate and clarify their messaging for the market, to accelerate the SSI adoption. We illustrate the need for this by summarizing our market strengths, weaknesses, opportunities, and threats (SWOT analysis), and show how this simple analysis exposes important marketing goals. Further projects will include market explainers, frequently asked questions, market glossaries, market research, and go-to-market (GTM) materials for developers. We wrote this paper to find more people who value a healthy SSI market. By working with us on this short-term project, you will help to grow the overall SSI market. Please join us!","title":"Abstract"},{"location":"RWoT6/a-roadmap-for-ssi/#1-self-sovereign-identity-group-charter","text":"This section describes the mission, stakeholders, goals, and initial structure of the proposed Self-Sovereign Identity Group .","title":"1. Self-Sovereign Identity Group Charter"},{"location":"RWoT6/a-roadmap-for-ssi/#mission","text":"We improve social stability, economic productivity, and individual freedom by promoting broad use of self-sovereign identity.","title":"Mission"},{"location":"RWoT6/a-roadmap-for-ssi/#stakeholders","text":"Our market development efforts will target these stakeholders. Role Benefit Technical leaders Focus innovation on market-enabling technology Standards participants Develop standards that promote high usage Media Develop more compelling, meaningful, and accurate stories Developers Create applications and services for rapid adoption Businesses, Governments, Non-profits Develop early customers and adoption Venture capitalists Make better decisions to reduce risk and increase IRR Academics Target hot research opportunities","title":"Stakeholders"},{"location":"RWoT6/a-roadmap-for-ssi/#membership","text":"Members in this roadmap team must contribute to content development and distribution, stakeholder engagement, and more. Individuals who contribute will learn an enormous amount about the SSI market, identity stakeholders, standards organizations, and marketing. Finally, to the extent possible, we want members to have autonomy; this will be accomplished by providing them with measurable goals and allowing them to achieve those goals as they see fit (within reason). We expect the team to ultimately self-organize to maximize market expansion. Initially, a single Product Owner may prioritize work, and a Scrum Master may be assigned to facilitate communication and work, manage the workflow, and seek timely results. These focus areas will likely dominate our activities. Content : Create informational content (presentation, videos, etc) suitable for stakeholders Portal : Create and maintain the team's website Communications : Propagate SSI messaging to passive stakeholders Contact Moses Ma or Dan Greening to become a member.","title":"Membership"},{"location":"RWoT6/a-roadmap-for-ssi/#goals","text":"Get ourselves organized (DONE) Agree on mission (see above) (DONE) Setup tools (i.e. Trello) (DONE) Establish web site (see Self Sovereign ID (in progress) Complete the final draft of this document (not doing until bigger) Discuss project management Schedule cadence (scrum) call Review and refine tribe structure Establish site login tiers (public, developer, marketer, press) and tracking to measure reach and engagement Create and promulgate a realistic technology and market roadmap Recruit initial foundational partners as adoption catalysts Identify minimum viable ecosystem (DID network, wallet, browser integration, blog integration?) * Build Drupal plugin for SuperSignOn , a decentralized authentication system based on DID-Auth * Allow developers and others to SuperSignOn to our (Drupal) site Recruit and help influential leaders (e.g. Reid Hoffman, Fred Wilson, etc) to speak favorably about SSI Access to subject matter experts Sample presentations Speakers bureau Recruit/produce minimum viable ecosystem participants Identify willing networks, wallets, feasible browser plugins, feasible blog plugins Convince key communication technologies from Slack, Telegram, RocketChat, Wikipedia to support DIDs Recruit developer support services, such as GitHub, CircleCI, BitBucket, to support DIDs Produce Go To Market Resources to support developers by year end 2018 (in progress) Write glossary of market-relevant terms (in progress) Develop FAQs that debunk myths and promote adoption Consolidate technical primers (tutorials) into a single \"getting started\" kit Develop a Communications Kit Simplify and articulate the concepts and benefits of SSI for the masses Provide a cohesive narrative about SSI and its goals for the Media Offer common baseline talking points for SSI developers Explain wallets and DID distribution Explain who the wallet makers are Explain how wallets will be interoperable and secure Create high-impact videos and other demo recordings (i.e.: RWOT)","title":"Goals"},{"location":"RWoT6/a-roadmap-for-ssi/#assets","text":"This document was developed at RWOT6 (Rebooting Web of Trust, Spring 2018) group. Document source is HERE A team web site has been created at selfsovereign.id . An initial glossary is at SSI Glossary . Group work is prioritized and tracked at Trello . Members communicate through Slack .","title":"Assets"},{"location":"RWoT6/a-roadmap-for-ssi/#contributors","text":"Some people have contributed substantially so far. They are Person Company Contribution Dan Gisolfi IBM Content Moses Ma Futurelab Content, Management Darrell Duane Crypto UBI Portal Wes Kussmaul Reliable Identities Content Dan Greening Senex Rex Content","title":"Contributors"},{"location":"RWoT6/a-roadmap-for-ssi/#possible-volunteers","text":"People who have expressed interest in the team include: Person Company Possible Role Vishal Gupta Diro Foundation Portal (Design) Chandran Gaurav Diro Foundation Portal (Design) Kate Sills Communications Kaliya Young Communications Remy Lyon Communications People who have been invited are: Person Company Possible Role Alex Preukschat (Invited) Globatalent GTM Development Sean Bohan (Invited) Evernym Stakeholder Engagement David Crocker (Invited) Brandenburg InternetWorking Stakeholder Engagement Nathan George (Invited) Sovrin Stakeholder Engagement","title":"Possible volunteers"},{"location":"RWoT6/a-roadmap-for-ssi/#2-existing-market","text":"","title":"2. Existing Market"},{"location":"RWoT6/a-roadmap-for-ssi/#motivation-swot","text":"This section describes the state of the SSI industry as of 7 April 2018. We can gain a broad understanding of the maturity and vulnerability of our industry by listing strengths, weaknesses, opportunities and threats (SWOT analysis).","title":"Motivation (SWOT)"},{"location":"RWoT6/a-roadmap-for-ssi/#strengths","text":"SSI networks provide affordability, scalability, reliability, trustability, privacy, security, and portability superior to traditional centralized and siloed identity networks. For decades, identity leaders have avoided partisanship and collaborated on identity fundamentals and standards. Our community has largely completed the SSI technical infrastructure to support SSI applications. There are several operational and competing SSI distributed identity networks and wallets in beta.","title":"Strengths"},{"location":"RWoT6/a-roadmap-for-ssi/#weaknesses","text":"Our own complacency could derail or delay SSI adoption. Self-sovereign identity concepts can be confusing, especially when we lead with technology and not societal benefits. We do not discuss deployment schedules for SSI networks and APIs, so many application developers refrain from investing effort. We do not broadly discuss expected costs for DIDs and for SSI services, which creates financial risk for startups. We provide no SSI reference applications and libraries, which limits developers ability to produce compelling applications. We don't list SSI network and wallet vendors, and so application developers worry that they will be locked in. Our narrow focus on technical and philosophical exploration is distracting us and delaying the societal and individual benefits of SSI. Our individual non-specific identity concerns \u2014 such as whether biometric data might enable despotic states to track our movements \u2014 distract us from solving clear and present dangers, such as putting development work toward eliminating non-state-originated financial, identity, and privacy theft (which will actually help us to better understand and address the challenges of despotic states);","title":"Weaknesses"},{"location":"RWoT6/a-roadmap-for-ssi/#opportunities","text":"Society is turning a corner in demanding better identity solutions: Fear of identity hacks and theft are now pervasive. Privately-maintained identity systems have exposed businesses and governments to hacking, leaks, and lawsuits. Foreign enemies have destabilized nations through election and infrastructure hacking. Malware-borne DDS attacks have damaged individual companies and degraded the internet. Social media and advertisers have enabled private data misuse (sometimes unintentionally). Inexpensive SSI-based solutions can resolve each of these problems. If we direct some of our attention to expanding the SSI market, we can likely gain rapid traction.","title":"Opportunities"},{"location":"RWoT6/a-roadmap-for-ssi/#threats","text":"Despite the many advantages of SSI, competitive threats could derail or significantly delay SSI adoption. Standards bodies could produce standards incompatible with SSI. A specific concern is the mDL (mobile drivers license) standard being developed by the ISO/IEC JTC 1/SC 17/SG 1 working group . Key customers could adopt inferior non-SSI alternatives. A poor identity decision by a customer will take years and cash to correct, due to high switching costs for identity. The low agility of government agencies compounds the effect: their bad decisions could affect everyone for decades. If government agencies don't participate in distributed identity networks for licenses, passports, and national identity it will degrade citizen security, financial tracability, and information verifiability for a long time. False assumptions about SSI are rife: Some believe blockchain solutions store personally identifiable information (PII) on the distributed ledger. Some believe that participation in SSI networks is involuntary. Some believe that a decentralized ID could \" never be turned off or blocked \" due to the immutability of the distributed ledgers. Some believe it will be impossible to prevent anyone from publishing anything they want about you, without the standard societal repercussions (libel, etc.), and that immutable negative ledger reviews become part of an indelible permanent record. Few people know the privacy benefits of Selective Disclosure (or the Principle of Minimum Disclosure) and how SSI supports it. Most of these threats can be disarmed by informing key influencers.","title":"Threats"},{"location":"RWoT6/a-roadmap-for-ssi/#industry-organizations","text":"There are many groups and organizations focused on distributed identity, but there are many disparate voices. We could all benefit from consistent messaging about market development. A quick overview of the landscape includes:","title":"Industry Organizations"},{"location":"RWoT6/a-roadmap-for-ssi/#rebooting-web-of-trust-rwot","text":"Rebooting Web of Trust is a group that meets twice yearly to develop position papers and kick off more significant efforts. It seeks to create the next generation of decentralized web-of-trust-based identity systems. Each event generates roughly five technical white papers on topics decided by the group that will have the greatest impact on the future. RWOT may also use hackathons to implement those ideas. This document is a \"technical white paper\" written at RWOT6 (Spring 2018).","title":"Rebooting Web of Trust (RWOT)"},{"location":"RWoT6/a-roadmap-for-ssi/#internet-identity-workshop-iiw","text":"Internet Identity Workshop is a group that meets twice yearly to share and refine ideas about identity and to forge working relationships. It is organized as an unconference using Open Space Technology. It is highly effective, and that effectiveness is largely due to its insistence on note-taking and collaboration.","title":"Internet Identity Workshop (IIW)"},{"location":"RWoT6/a-roadmap-for-ssi/#world-wide-web-consortium-w3c","text":"World Wide Web Consortium (W3C) is an international community that develops open standards to ensure the long-term growth of the Web. Two working groups are particularly relevant to self-sovereign identity: Verified Claims Working Group (VC) seeks to make it easier and more secure to express and exchange credentials on the web that have been verified by a third party. Credentials Community Group (CCG) explores the creation, storage, presentation, verification, and user control of credentials. The group drafts and incubates internet specifications for further standardization and prototyping and tests reference implementations. CCG was the original source of material for the official Verified Claims Working Group.","title":"World Wide Web Consortium (W3C)"},{"location":"RWoT6/a-roadmap-for-ssi/#distributed-identity-foundation-dif","text":"Distributed Identity Foundation (DIF) is an engineering-focused non-profit organization composed of individuals and companies who are collaboratively developing an interoperable set of decentralized identity protocols, specs, and reference implementations that run across chains and service providers. Its goal is user-enablement via the creation of a ubiquitous decentralized identity ecosystem that benefits every person and company worldwide.","title":"Distributed Identity Foundation (DIF)"},{"location":"RWoT6/a-roadmap-for-ssi/#sovrin-foundation-sovrin","text":"Sovrin Foundation (Sovrin) is a non-profit organization that manages a permissioned blockchain identity network. Because the network is permissioned, participants must agree to maintain identity security and privacy (otherwise a collection of members could subvert the network). Authoritative network participants must sign the Sovrin Trust Framework, which gives them permission to operate a node. The Sovrin Foundation is a spinoff of the company Evernym . The Sovrin Foundation has produced a substantial body of market-relevant material, but it is targeted toward one specific network. Other competing networks include Veres One (with its own network) and uPort (based on the Ethereum contract network ).","title":"Sovrin Foundation (Sovrin)"},{"location":"RWoT6/a-roadmap-for-ssi/#information-trust-exchange-governing-association-itega","text":"Information Trust Exchange Governing Association provides Internet stakeholders with a forum to convene, develop, and implement governing protocols and business rules for protecting and balancing trust, privacy, identity, and information commerce. It seeks to balance privacy, personalization, and payment to improve journalism and publishing. One of its first projects is the deployment of a proof-of-concept for a first-party-user data exchange that would incorporate privacy-by-design.","title":"Information Trust Exchange Governing Association (ITEGA)"},{"location":"RWoT6/a-roadmap-for-ssi/#industry-goals","text":"Here are the goals these other organizations have articulated: One million public (open or pseudonymous) DIDs issued by March 2019 Formalized plans by year-end 2018 for foundational SSI specifications W3C DID W3C VC Oasis DKMS Public release of a handful of reference applications that can help jumpstart developer applications","title":"Industry Goals"},{"location":"RWoT6/a-roadmap-for-ssi/#3-summary","text":"The distributed identity market is at a crossroads: Market forces are demanding secure, privacy-respecting solutions to identity. DID infrastructure is in beta or better. Whether we have a minimum viable ecosystem is debatable. Our market communications, so far, have been weak. This paper argues for the formation of an unbiased team to develop the Self-Sovereign Identity (SSI) market across all participants. We will educate and support a variety of stakeholders to promote SSI. We will provide infrastructure and application developers with unbiased information and tools to support their go-to-market efforts. We seek your help.","title":"3. Summary"},{"location":"RWoT6/a-roadmap-for-ssi/#appendix-a-wows-to-consider","text":"Another requirement for success is the creation of a design process that would lead to a sustainable flow of compelling technologies that provide a \u201cwow\u201d factor, forming a pipeline of compelling functionality to fortify the value proposition for decentralized identity. An initial set of projects and ideas for \u201cwow prototypes\u201d include: Industry Demos : Develop a number of vertical industry references. For example: FinTech: A demo of the CULedger CUID Trust Framework that uses the Hyperledger Indy framework. Travel and Transportation: A Dapps that implements one or more concepts outlined in the World Economic Forum\u2019s Known Traveler Report . Community Badges Toolkit : Produce a Starter Kit that would allow any small or large community to begin to issue verifiable credentials in the form of OpenBadges . Such a toolkit would be a perfect Getting Started toolkit for a go-to-market (GTM) package of resources. NOTE: The final paper submitted by the Open Badges are Verifiable Credentials RWOT Workgroup is supposed to produce a working prototype that can be used to seed this toolkit. Other Interesting Concepts : There are a number of other potential reference applications that could be interesting to develop and deploy. SuperSignOn , a decentralized authentication system based on the DID-Auth specification. GitHub Authentication using Verifiable Credentials. ICO-LegalAssist , which would use Verifiable Claims as a basis for attestations by attorneys in ICOs. IDESG\u2019s IDEF assessment , a tool for receiving badges .","title":"Appendix A: WOWs to Consider"},{"location":"RWoT6/a-roadmap-for-ssi/#appendix-b-frequently-asked-questions","text":"Naysayers illustrate the urgency for this effort as they create myths that could be discredited by a strong communication plan. Here are a few examples of Q&As that address myths already circulating in the media and amongst analysts:","title":"Appendix B: Frequently Asked Questions"},{"location":"RWoT6/a-roadmap-for-ssi/#do-blockchain-ssi-solutions-store-personally-identifiable-information-pii","text":"No. Commercial blockchain identity systems do not store personally identifiable information (PII) on blockchains because it is unnecessary, expensive, and limits portability. Your DID entry on the blockchain may point to data, if there is any, stored elsewhere, typically in encrypted form. The SSI standards discourage storing personal data on blockchains, and the industry has established several principles against it.","title":"Do blockchain SSI solutions store personally identifiable information (PII)?"},{"location":"RWoT6/a-roadmap-for-ssi/#can-i-prevent-someone-from-correlating-my-information-from-different-sites-to-find-out-more-about-me","text":"Yes. Once you create a \"public DID,\" you create \"pseudonymous DIDs\" whenever you establish a new online relationship (such as a new login). Pseudonymous DIDs cannot be correlated to your public DID by others without your involvement (and rarely need to be). See decentralized identifier (DID) and internet standard Universally Unique Identifiers (UUIDs).","title":"Can I prevent someone from correlating my information from different sites to find out more about me?"},{"location":"RWoT6/a-roadmap-for-ssi/#can-i-revoke-an-ssi-distributed-id","text":"Yes. A decentralized ID is turned off by invalidating the data that the DID points to.","title":"Can I revoke an SSI-distributed ID?"},{"location":"RWoT6/a-roadmap-for-ssi/#can-someone-publish-something-permanently-about-me-on-an-ssi-blockchain","text":"No. Blockchains are \"immutable,\" meaning data published there are never erased. However, the only things published on SSI blockchains are DIDs, encryption keys (not decryption keys ), and pointers to outside data (which can be erased or changed). The DID specification states that the user/owner explicitly controls and administers the publication of their decentralized IDs. This is actually the meaning of the phrase \"self-sovereign ID.\" The proposed Verifiable Credentials specification states plainly that SSI credentials are revocable, expirable, and that an SSI network must enforce the data policies of both the issuer and holder. Existing SSI networks require an ID holder to approve before sharing an ID. This means that a competitor, who would not be trusted to make a claim about your business, could not append a negative claim about your business without your explicit approval. This is because the issuer and holder both have the ability to revoke any claim.","title":"Can someone publish something permanently about me on an SSI blockchain?"},{"location":"RWoT6/a-roadmap-for-ssi/#when-i-use-a-did-with-a-provider-is-my-personal-information-exchanged","text":"If the DID is a new pseudonymous DID (easy to create from your public DID), the answer is \"No.\" Furthermore, the SSI industry encourages applications to uses something called \"Zero Knowledge Proofs\" to limit the amount of information shared. For example, when you assure a bartender that you can legally drink, you typically show a drivers license and reveal your birthdate. With most SSI networks, an identity supplier that knows your birthdate can reveal your legal drinking status without sharing your birthdate.","title":"When I use a DID with a provider, is my personal information exchanged?"},{"location":"RWoT6/a-roadmap-for-ssi/#how-does-ssi-protect-my-privacy","text":"The SSI industry promotes the use of pseudonymous IDs and encourages Selective Disclosure (or the Principle of Minimum Disclosure) to keep your personal data secure and private. Those practices also improve compliance with privacy regulations like HIPAA (Health Insurance Portability and Accountability Act) in the US and GDPR (Global Data Protection Regulation) in the EU. With SSI networks, users control their personally identifiable data and data privacy at levels they've never experienced before.","title":"How does SSI protect my privacy?"},{"location":"RWoT6/btcr-resolver/","text":"BTCR DID Resolver Specification By: Kim Hamilton Duffy, Christopher Allen, Ryan Grant, Dan Pape This describes the process of resolving a BTCR DID into a DID Document. The draft reference implementation is available at https://github.com/WebOfTrustInfo/btcr-did-tools-js (see didFormatter.js). Note that not all steps described in this document are implemented yet. See the BTCR playground for a live demonstration. The BTCR playground uses the draft reference implementation BTCR DID resolver. Input: BTCR DID The input to a BTCR DID resolver is a BTCR DID. Format: did:btcr:<specific-idstring> Example: did:btcr:xkyt-fzgq-qq87-xnhn Resolution Phase 1: Construct \"implicit\" DID document Terminology \"Extended transaction reference\": refers to our specific transaction reference customizations for the BTCR DID method spec Issue #1 \"txref-ext\": abbreviation for above \"Constructed\" DID Document: what the resolver generates \"Continuation\" DID Document: a referenced DID document to be merged into the constructed DID document We used to call this a \"DID Fragment\" but this is confusing because of DID Fragments defined in the spec Goal This phase constructs the \"implicit\" DID Document from Bitcoin transaction data. Steps Confirm method from the DID is btcr . Fail if not From the BTCR DID, extract the extended transaction reference: this is the <specific-idstring> component of did:btcr:<specific-idstring> = did:btcr:<TXREF-EXT(TX)> Extract transaction details from the txref-ext encoding: txref-ext encodes these transaction details: bitcoin network (mainnet, testnet, ..) the transaction block height and position Example: in the BTCR Playground note that did:btcr:xkyt-fzgq-qq87-xnhn resolves to: network = testnet3 transaction id = 67c0ee676221d9e0e08b98a55a8bf8add9cba854f13dda393e38ffa1b982b833 blockheight = 1201739, position = 2 Reference implementation: https://github.com/WebOfTrustInfo/txref-conversion-js Note that txref-ext deviates from txrefs Issue #1 The most significant difference at the moment is that the network prefix is removed. So for example, a txref of txtest1-xkyt-fzgq-qq87-xnhn converts to a txref-ext of xkyt-fzgq-qq87-xnhn . For now, calling libraries handle this conversion by adding back the txref prefix. In the btcr-did-tools library , see util.ensureTxref Look up transaction by height and position. Is the transaction output spent? no: this is the latest version of the DID. From this we can construct the DID Document yes: keep following transaction chain until the latest with an unspent output is found Extract the hex-encoded public key that signed the transaction and update the DID document with default authentication capability Populate the first entry of the publicKey array in the DID document. The id will have a fragment of #keys-1 , so that the full id is did:btcr:<specific-idstring>#keys-1 . This is a BTCR method spec convention that #keys-1 corresponds to the transaction signing key. We'll see in the next section that overriding this path in the supplementary DID document data is not allowed Encode the key material according to the Koblitz Elliptic Curve Signature 2016 signature suite. Issue #5 Populate the first entry of the authentication array in the DID document, referencing the key above Alternate representation note: a public key can be inlined if there is only one reference in the DID document (as opposed to the representation above, in which there is a publicKey array and a reference from authentication ) If the transaction contains an OP_RETURN field, populate the serviceEndpoint in the DID document. This is assumed to reference supplementary DID document data Add an entry to the service section of the DID document type is BTCREndpoint serviceEndpoint is the value in the OP_RETURN field, e.g. \"https://github.com/myopreturnpointer\" Add SatoshiAuditTrail , which contains additional metadata available from the Bitcoin transaction. This is still being defined; Issue #3 If the transaction contained no OP_RETURN data (and therefore no serviceEndpoint was added), the resolution process is done. Otherwise, proceed to phase 2. Output of Phase 1 The output of this resolution phase is referred to as the \"implicit\" DID Document; it is derived exclusively from Bitcoin transaction data. If the transaction has no OP_RETURN data, then the service array would have no entries. The only default capabilities would be to authenticate with the transaction signing key. Example: in the BTCR Playground note that did:btcr:xkyt-fzgq-qq87-xnhn Phase 1 output is: { \"@context\": \"https://w3id.org/btcr/v1\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"publicKey\": [ { \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn#keys-1\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"type\": \"EdDsaSAPublicKeySecp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" } ], \"authentication\": [ { \"type\": \"EdDsaSAPublicKeySecp256k1Authentication\", \"publicKey\": \"#keys-1\" } ], \"service\": [ { \"type\": \"BTCREndpoint\", \"serviceEndpoint\": \"https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld\" } ], \"SatoshiAuditTrail\": [ { \"chain\": \"testnet\", \"blockhash\": \"0000000000000722ded9d85d67e145ba41c53ef2e8680f75540a08b885febba5\", \"blockindex\": 2, \"outputindex\": 1, \"blocktime\": \"2017-09-23T17:27:56.682Z\", \"time\": 1499501000, \"timereceived\": \"2017-09-23T17:27:56.682Z\", \"burn-fee\": -0.05 } ] } Resolution Phase 2: Populate DID document with supplementary DID document data Steps Retrieve the continuation DID document from serviceEndpoint.BTCREndpoint and extract the portions of type \"DIDDocument\". Issue #6 If URL doesn't exist, ERROR Verify the continuation DID Document Issue #2 If the content is in an immutable store: full id s are not required (but a fragment is? -- Issue #2 ) a signature is not required Otherwise: id s must be fully specified signature is required resolver must check signature Merge in continuation DID document entries (keys, authorizations, etc -- as appropriate) into the constructed DID document. Additive only! Merge items that are part of the DID specification ( publicKey , authentication , service ) into the constructed DID document by appending their entries to the arrays of the matching term If any new id s are already used in the constructed DID document, ERROR For immutable stores, merge id fragments with the DID value into the constructed DID document Append unknown terms to the constructed DID Document Issue #6 Repeat steps 7-9 for additional referenced continuation DID documents Issue #4 Proposed but not shown here: wrap the DID document in resolver envelope with additional metadata Output of Phase 2 This resolution phase returns a final constructed JSON-LD DID Document to caller, which can use the keys to authenticate data such as the signature on a verifiable claim, or perform other application tasks. Let's assume the supplementary DID document (from the OP_RETURN data) is stored in an immutable store and contains the following didDocument . { ... \"didDocument\": { \"@context\": \"https://w3id.org/did/v1\", \"publicKey\": [{ \"id\": \"#keys-2\", \"type\": \"RsaVerificationKey2018\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }], \"authentication\": [{ \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"#keys-2\" }], ... } Note that the id is not known yet, because the transaction referencing this supplementary document has not occurred. Example: in the BTCR Playground the final output for did:btcr:xkyt-fzgq-qq87-xnhn is: { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"publicKey\": [ { \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn#keys-1\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"type\": \"EdDsaSAPublicKeySecp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" }, { \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn#keys-2\", \"type\": \"RsaVerificationKey2018\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\" } ], \"authentication\": [ { \"type\": \"EdDsaSAPublicKeySecp256k1Authentication\", \"publicKey\": \"#keys-1\" }, { \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"#keys-2\" } ], \"service\": [ { \"type\": \"BTCREndpoint\", \"serviceEndpoint\": \"https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld\" } ], \"SatoshiAuditTrail\": [ { \"chain\": \"testnet\", \"blockhash\": \"0000000000000722ded9d85d67e145ba41c53ef2e8680f75540a08b885febba5\", \"blockindex\": 2, \"outputindex\": 1, \"blocktime\": \"2017-09-23T17:27:56.682Z\", \"time\": 1499501000, \"timereceived\": \"2017-09-23T17:27:56.682Z\", \"burn-fee\": -0.05 } ]","title":"BTCR DID Resolver Specification"},{"location":"RWoT6/btcr-resolver/#btcr-did-resolver-specification","text":"By: Kim Hamilton Duffy, Christopher Allen, Ryan Grant, Dan Pape This describes the process of resolving a BTCR DID into a DID Document. The draft reference implementation is available at https://github.com/WebOfTrustInfo/btcr-did-tools-js (see didFormatter.js). Note that not all steps described in this document are implemented yet. See the BTCR playground for a live demonstration. The BTCR playground uses the draft reference implementation BTCR DID resolver.","title":"BTCR DID Resolver Specification"},{"location":"RWoT6/btcr-resolver/#input-btcr-did","text":"The input to a BTCR DID resolver is a BTCR DID. Format: did:btcr:<specific-idstring> Example: did:btcr:xkyt-fzgq-qq87-xnhn","title":"Input: BTCR DID"},{"location":"RWoT6/btcr-resolver/#resolution-phase-1-construct-implicit-did-document","text":"","title":"Resolution Phase 1: Construct \"implicit\" DID document"},{"location":"RWoT6/btcr-resolver/#terminology","text":"\"Extended transaction reference\": refers to our specific transaction reference customizations for the BTCR DID method spec Issue #1 \"txref-ext\": abbreviation for above \"Constructed\" DID Document: what the resolver generates \"Continuation\" DID Document: a referenced DID document to be merged into the constructed DID document We used to call this a \"DID Fragment\" but this is confusing because of DID Fragments defined in the spec","title":"Terminology"},{"location":"RWoT6/btcr-resolver/#goal","text":"This phase constructs the \"implicit\" DID Document from Bitcoin transaction data.","title":"Goal"},{"location":"RWoT6/btcr-resolver/#steps","text":"Confirm method from the DID is btcr . Fail if not From the BTCR DID, extract the extended transaction reference: this is the <specific-idstring> component of did:btcr:<specific-idstring> = did:btcr:<TXREF-EXT(TX)> Extract transaction details from the txref-ext encoding: txref-ext encodes these transaction details: bitcoin network (mainnet, testnet, ..) the transaction block height and position Example: in the BTCR Playground note that did:btcr:xkyt-fzgq-qq87-xnhn resolves to: network = testnet3 transaction id = 67c0ee676221d9e0e08b98a55a8bf8add9cba854f13dda393e38ffa1b982b833 blockheight = 1201739, position = 2 Reference implementation: https://github.com/WebOfTrustInfo/txref-conversion-js Note that txref-ext deviates from txrefs Issue #1 The most significant difference at the moment is that the network prefix is removed. So for example, a txref of txtest1-xkyt-fzgq-qq87-xnhn converts to a txref-ext of xkyt-fzgq-qq87-xnhn . For now, calling libraries handle this conversion by adding back the txref prefix. In the btcr-did-tools library , see util.ensureTxref Look up transaction by height and position. Is the transaction output spent? no: this is the latest version of the DID. From this we can construct the DID Document yes: keep following transaction chain until the latest with an unspent output is found Extract the hex-encoded public key that signed the transaction and update the DID document with default authentication capability Populate the first entry of the publicKey array in the DID document. The id will have a fragment of #keys-1 , so that the full id is did:btcr:<specific-idstring>#keys-1 . This is a BTCR method spec convention that #keys-1 corresponds to the transaction signing key. We'll see in the next section that overriding this path in the supplementary DID document data is not allowed Encode the key material according to the Koblitz Elliptic Curve Signature 2016 signature suite. Issue #5 Populate the first entry of the authentication array in the DID document, referencing the key above Alternate representation note: a public key can be inlined if there is only one reference in the DID document (as opposed to the representation above, in which there is a publicKey array and a reference from authentication ) If the transaction contains an OP_RETURN field, populate the serviceEndpoint in the DID document. This is assumed to reference supplementary DID document data Add an entry to the service section of the DID document type is BTCREndpoint serviceEndpoint is the value in the OP_RETURN field, e.g. \"https://github.com/myopreturnpointer\" Add SatoshiAuditTrail , which contains additional metadata available from the Bitcoin transaction. This is still being defined; Issue #3 If the transaction contained no OP_RETURN data (and therefore no serviceEndpoint was added), the resolution process is done. Otherwise, proceed to phase 2.","title":"Steps"},{"location":"RWoT6/btcr-resolver/#output-of-phase-1","text":"The output of this resolution phase is referred to as the \"implicit\" DID Document; it is derived exclusively from Bitcoin transaction data. If the transaction has no OP_RETURN data, then the service array would have no entries. The only default capabilities would be to authenticate with the transaction signing key. Example: in the BTCR Playground note that did:btcr:xkyt-fzgq-qq87-xnhn Phase 1 output is: { \"@context\": \"https://w3id.org/btcr/v1\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"publicKey\": [ { \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn#keys-1\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"type\": \"EdDsaSAPublicKeySecp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" } ], \"authentication\": [ { \"type\": \"EdDsaSAPublicKeySecp256k1Authentication\", \"publicKey\": \"#keys-1\" } ], \"service\": [ { \"type\": \"BTCREndpoint\", \"serviceEndpoint\": \"https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld\" } ], \"SatoshiAuditTrail\": [ { \"chain\": \"testnet\", \"blockhash\": \"0000000000000722ded9d85d67e145ba41c53ef2e8680f75540a08b885febba5\", \"blockindex\": 2, \"outputindex\": 1, \"blocktime\": \"2017-09-23T17:27:56.682Z\", \"time\": 1499501000, \"timereceived\": \"2017-09-23T17:27:56.682Z\", \"burn-fee\": -0.05 } ] }","title":"Output of Phase 1"},{"location":"RWoT6/btcr-resolver/#resolution-phase-2-populate-did-document-with-supplementary-did-document-data","text":"","title":"Resolution Phase 2: Populate DID document with supplementary DID document data"},{"location":"RWoT6/btcr-resolver/#steps_1","text":"Retrieve the continuation DID document from serviceEndpoint.BTCREndpoint and extract the portions of type \"DIDDocument\". Issue #6 If URL doesn't exist, ERROR Verify the continuation DID Document Issue #2 If the content is in an immutable store: full id s are not required (but a fragment is? -- Issue #2 ) a signature is not required Otherwise: id s must be fully specified signature is required resolver must check signature Merge in continuation DID document entries (keys, authorizations, etc -- as appropriate) into the constructed DID document. Additive only! Merge items that are part of the DID specification ( publicKey , authentication , service ) into the constructed DID document by appending their entries to the arrays of the matching term If any new id s are already used in the constructed DID document, ERROR For immutable stores, merge id fragments with the DID value into the constructed DID document Append unknown terms to the constructed DID Document Issue #6 Repeat steps 7-9 for additional referenced continuation DID documents Issue #4 Proposed but not shown here: wrap the DID document in resolver envelope with additional metadata","title":"Steps"},{"location":"RWoT6/btcr-resolver/#output-of-phase-2","text":"This resolution phase returns a final constructed JSON-LD DID Document to caller, which can use the keys to authenticate data such as the signature on a verifiable claim, or perform other application tasks. Let's assume the supplementary DID document (from the OP_RETURN data) is stored in an immutable store and contains the following didDocument . { ... \"didDocument\": { \"@context\": \"https://w3id.org/did/v1\", \"publicKey\": [{ \"id\": \"#keys-2\", \"type\": \"RsaVerificationKey2018\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }], \"authentication\": [{ \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"#keys-2\" }], ... } Note that the id is not known yet, because the transaction referencing this supplementary document has not occurred. Example: in the BTCR Playground the final output for did:btcr:xkyt-fzgq-qq87-xnhn is: { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"publicKey\": [ { \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn#keys-1\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"type\": \"EdDsaSAPublicKeySecp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" }, { \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn#keys-2\", \"type\": \"RsaVerificationKey2018\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\" } ], \"authentication\": [ { \"type\": \"EdDsaSAPublicKeySecp256k1Authentication\", \"publicKey\": \"#keys-1\" }, { \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"#keys-2\" } ], \"service\": [ { \"type\": \"BTCREndpoint\", \"serviceEndpoint\": \"https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld\" } ], \"SatoshiAuditTrail\": [ { \"chain\": \"testnet\", \"blockhash\": \"0000000000000722ded9d85d67e145ba41c53ef2e8680f75540a08b885febba5\", \"blockindex\": 2, \"outputindex\": 1, \"blocktime\": \"2017-09-23T17:27:56.682Z\", \"time\": 1499501000, \"timereceived\": \"2017-09-23T17:27:56.682Z\", \"burn-fee\": -0.05 } ]","title":"Output of Phase 2"},{"location":"RWoT6/did-auth/","text":"Introduction to DID Auth Authors: Markus Sabadello, Kyle Den Hartog, Christian Lundkvist, Cedric Franz, Alberto Elias, Andrew Hughes, John Jordan, Dmitri Zagidulin Contributors: Eugeniu Rusu, Adam Powers, John Callahan, Joe Andrieu Abstract The term DID Auth has been used in different ways and is currently not well-defined. We define DID Auth as a ceremony where an identity owner , with the help of various components such as web browsers, mobile devices, and other agents, proves to a relying party that they are in control of a DID. This means demonstrating control of the DID using the mechanism specified in the DID Document's \"authentication\" object. This could take place using a number of different data formats, protocols, and flows. DID Auth includes the ability to establish mutually authenticated communication channels and to authenticate to web sites and applications. Authorization, Verifiable Credentials, and Capabilities are built on top of DID Auth and are out of scope for this document. This paper gives on overview of the scope of DID Auth, supported protocols and flows, and the use of components of the DID Documents that are relevant to authentication, as well as formats for challenges and responses. Resources This paper is a continuation of ongoing work by Rebooting the Web of Trust and other communities. Previous work includes: RWoT IV: James Monaghan's topic paper \"DID Auth\" RWoT VI: Markus Sabadello's topic paper \"DID Auth: Scope, Formats, and Protocols\" RWoT VI: Kyle Den Hartog's topic paper \"DID-Auth protocol\" Portions of the work on this paper have been funded through a Code With Us contract awarded by the BC Developers' Exchange (https://bcdevexchange.org/) of the Province of British Columbia, Canada. Definitions Authentication : The ceremony where an identity owner proves to a relying party that the identity owner controls a DID, using a mechanism that is described in the DID's associated DID Document. Authorization : A process of establishing the rights and privileges of an identity owner to perform certain actions, including operations on a DID itself, or in another context. Decentralized Identifier (DID) : A globally unique identifier that does not require a centralized registration authority because it is registered with distributed ledger technology or another form of decentralized network. (see here ) DID Document : A structured document containing metadata that describes a DID, including authentication materials such as public keys and pseudonymous biometrics, that an entity can use to authenticate, i.e. to prove control of the DID. A DID Document may also contain other attributes or claims describing the entity. (see here ) DID Record : The combination of a DID and its associated DID Document. Identity Owner : The individual, organization or thing who created the DID, is identified by the DID that is the subject of the DID Document, and who has the ultimate authority to update or revoke the DID. Relying Party : The individual, organization or thing that authenticates an identity owner using a DID Auth protocol. Also called \"Verifier\" in other specifications. Verifiable Credentials : A set of one or more claims that are statements made by an issuer about a subject that is tamper-resistant and whose authorship can be cryptographically verified (see here ). Introduction Scope This paper defines data formats and challenge and response transports allowing an identity owner to prove control of a DID to a relying party . Proof of control of a DID is a technical interaction that may be a precursor to establishing a longer term relationship between two parties. A successful DID Auth interaction may create the required conditions to allow the parties to exchange further data in a trustworthy way. This further data could include streams of raw data, from sensors to the exchange of Verifiable Credentials. This further exchange of data is out of scope of the DID Auth protocol itself. DID Auth may be a one-way interaction where party A proves control of a DID A to party B, or a two-way interaction where mutual proof of control of DIDs is achieved. In the latter case, party A proves control of DID A to party B and party B proves control of DID B to party A. It is in the purview of the two parties engaged in the interaction to determine the need to have a one-way or two-way DID Auth interaction. It is also in the purview of the two parties to determine if further exchanges of data such as Verifiable Credentials may be necessary to establish the nature of the relationship between the two parties. Implementers may decide to subsume a DID Auth interaction within a higher layer interaction such as the exchange of Verifiable Credentials, which could simultaneously prove control of a DID and offer Verifiable Credentials for some transaction-specific purpose. DID Auth and Verifiable Credentials Even though DID Auth is about proving control of a DID, the exchange of Verifiable Credentials associated with a DID is related to DID Auth. The relationship between DID Auth and Verifiable Credentials could be thought of in several conceptual ways: DID Auth and Verifiable Credentials exchange are separate: At the beginning of an interaction between two parties, they need to authenticate (mutually, or just in one direction). After this is done, a protocol for exchange of Verifiable Credentials can be executed, so that the two parties can learn more about each other (and then perhaps make authorization decisions). Verifiable Credentials exchange is an extension to (or part of) DID Auth: In this approach, proving control of an identifier, and proving possession of Verifiable Credentials are closely related, and a single protocol is used for both purposes. The Verifiable Credentials are an \"optional field\" in the protocol. In order to \"only\" prove control of an identifier, an empty set of Verifiable Credentials is exchanged. DID Auth is a certain kind of Verifiable Credential: It is possible to think of DID Auth as an exchange of the most trivial Verifiable Credential imaginable: a self-issued Verifiable Credential that states \"I am me\". From this perspective, the separation between DID Auth and exchange of \"other\" Verifiable Credentials is blurred, and both are part of a single universal protocol. DID Record Creation DID Auth requires authentication material that is generated during DID Record Creation. As stated in the DID specification, the steps to create a DID Record compliant with DID Auth are: Generate a NEW_DID as specified in the relevant DID method specification. Generate a NEW_DID_DOCUMENT as specified in the relevant DID method specification. Set the id property to the value of NEW_DID (the DID subject). Choose one or more authentication type(s) from the array of proof mechanisms. Record the type property in an authentication object of the NEW_DID_DOCUMENT . Generate authentication material for use at a later time during authentication of the NEW_DID . The authentication type determines how to generate authentication material for a proof mechanism. Communicate and store the authentication material, either directly or as derived material, in the NEW_DID_DOCUMENT and for storage by the identity owner . If the chosen proof mechanism is based on asymmetric keys, the authentication material in the NEW_DID_DOCUMENT is recorded in a publicKey object in the DID Document. Example authentication and publicKey objects in a DID Document: { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:example:123456789abcdefghi\", \"authentication\": [{ \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"did:example:123456789abcdefghi#keys-1\" }, { \"type\": \"Ed25519SignatureAuthentication2018\", \"publicKey\": \"did:example:123456789abcdefghi#keys-2\" }], \"publicKey\": [{ \"id\": \"did:example:123456789abcdefghi#keys-1\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:example:123456789abcdefghi#keys-2\", \"type\": \"Ed25519VerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyBase58\": \"H3C2AVvLMv6gmMNam3uVAjZpfkcJCwDwnZn6z3wXmqPV\" }] } DID Auth can use different transports for exchanging challenges and responses between an identity owner and a relying party . One such transport uses an HTTP POST call to a DID Auth service endpoint. This service endpoint can be discovered from a DID Document. Example DID Auth service endpoint in a DID Document: { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:example:123456789abcdefghi\", \"service\": { \"type\": \"DidAuthService\", \"serviceEndpoint\": \"https://auth.example.com/did:example:123456789abcdefg\" } } Authentication of a DID This section describes a conceptual authentication framework of the DID Auth protocol. It relates general concepts of authentication to specific concepts of DID Auth. Authentication of a DID: Similar to other authentication methods, DID Auth relies on a challenge-response cycle in which a relying party authenticates the DID of an identity owner . During this cycle, an identity owner demonstrates control of their authentication material that was generated and distributed during DID Record Creation through execution of the authentication-proof mechanism. Challenge: The way an identity owner or their agent encounters an authentication challenge, as well as the format of the challenge, will vary depending on the situation. For example, they can come across a \"Sign in with DID Auth\" button or a QR code on a website. Or, in the case of an API call, the relying party may respond to a request by asking for authentication (the HTTP 401 Unauthorized response is a classic example, although DID Auth covers many use cases beyond HTTP). Challenge principles: The relying party may or may not know the identity owner 's DID at the time the challenge is constructed, and therefore the identity owner 's DID may or may not be included in the challenge. If the DID is known at the time of challenge construction, then the relying party may use the contents of the DID Document to select preferred authentication methods or service endpoints. The challenge that is sent by the relying party may or may not itself contain a proof of the relying party 's control of a DID. The relying party may or may not need additional transport-specific information about the identity owner in order to be able to deliver the challenge (e.g. a DID Auth service endpoint). This additional protocol-specific information may be discoverable from the identity owner's DID if it is known to the relying party . The relying party should include a nonce, to prevent replay attacks and to help link the challenge to a subsequent response. Response: Based on the challenge, the identity owner then constructs a response that proves control of their DID. This often involves a cryptographic signature, but can include other proof mechanisms. (As mentioned earlier, the response may also contain Verifiable Credentials that the relying party asked for in the challenge.) After receiving the response, the relying party resolves the identity owner 's DID, and verifies that the response is valid for a prior challenge (for example, verifying the response signature by using a publicKey object contained in the DID Document). Response principles: The identity owner may or may not need additional transport-specific information about the relying party in order to be able to deliver the response (e.g. a callback URL). This additional protocol-specific information may be included in the challenge, or it may be discoverable from the relying party 's DID that is included in the challenge. The relying party must be able to internally link a response to a prior challenge. This can be done with a nonce or message identifier in the challenge that must also be included in the response. It can also be done by including the entire original challenge in the response. Multiple devices, user agents, and other technical components may act on behalf of the identity owner to receive and process the challenge. For example, an identity owner's DID Auth service endpoint may receive the challenge and relay it to the identity owner 's mobile app. The identity owner's component that sends the response may or may not be the same component as the one that received the challenge. E.g. the challenge may be received as HTTP POST by a DID Auth service, but the response may be sent as HTTP POST by a mobile app (see DID Auth Architecture 3 ). The relying party's component that receives the response may or may not be the same component as the one that sent the challenge. E.g. the challenge may be sent as deep link by a mobile web page, but the response may be received as HTTP POST by a web server (see DID Auth Architecture 2 ). Generic DID Auth architecture: Challenge Formats In a DID Auth interaction, a challenge is transmitted by a relying party to an identity owner , asking the identity owner to return a response that proves their control of a DID. This section documents data formats for such challenges. JWT format Example: uPort eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NksifQ.eyJpc3MiOiIyb2VYdWZIR0RwVTUxYmZLQnNaRGR1N0plOXdlSjNyN3NWRyIsImlhdCI6MTUyNTg2NTM5OCwicmVxdWVzdGVkIjpbIm5hbWUiLCJwaG9uZSIsImNvdW50cnkiLCJhdmF0YXIiXSwicGVybWlzc2lvbnMiOlsibm90aWZpY2F0aW9ucyJdLCJjYWxsYmFjayI6Imh0dHBzOi8vY2hhc3F1aS51cG9ydC5tZS9hcGkvdjEvdG9waWMvR29EVGtmV2hvQmZ0N1BaOSIsIm5ldCI6IjB4NCIsImV4cCI6MTUyNTg2NTk5OCwidHlwZSI6InNoYXJlUmVxIn0.sQnZv63KaiWj7uQzLHLZ2jfZCZTOPz-ei7zqiUeDpjCmAdrKhO_uBujYhERJ6m-vuHcoAKuibXAPoBVHI7_H4A Header and payload decodes to: { \"typ\": \"JWT\", \"alg\": \"ES256K\" } { \"iss\": \"2oeXufHGDpU51bfKBsZDdu7Je9weJ3r7sVG\", \"iat\": 1525865398, \"requested\": [ \"name\", \"phone\", \"country\", \"avatar\" ], \"permissions\": [ \"notifications\" ], \"callback\": \"https://chasqui.uport.me/api/v1/topic/GoDTkfWhoBft7PZ9\", \"net\": \"0x4\", \"exp\": 1525865998, \"type\": \"shareReq\" } Example: Jolocom eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NksifQ.eyJpYXQiOjE1Mjg5OTc4NDIyNzUsInJlcXVlc3RlZENyZWRlbnRpYWxzIjpbeyJ0eXBlIjpbIkNyZWRlbnRpYWwiLCJQcm9vZk9mRW1haWxDcmVkZW50aWFsIl0sImNvbnN0cmFpbnRzIjp7ImFuZCI6W3siPT0iOlt0cnVlLHRydWVdfSx7IiE9IjpbeyJ2YXIiOiJpc3N1ZXIifSx7InZhciI6ImNsYWltLmlkIn1dfV19fV0sInJlcXVlc3RlcklkZW50aXR5IjoiZGlkOmpvbG86YjMxMGQyOTNhZWFjOGE1Y2E2ODAyMzJiOTY5MDFmZTg1OTg4ZmRlMjg2MGExYTVkYjY5YjQ5NzYyOTIzY2M4OCIsImNhbGxiYWNrVVJMIjoiaHR0cHM6Ly9kZW1vLXNzby5qb2xvY29tLmNvbS9wcm94eS9hdXRoZW50aWNhdGlvbi9hd3M2aSJ9.TZwB6_XMXFm_SjIv_PSangYNb9ldAQPzlEln8iBdcaSPDyU1A7kuJzJIaI0ykZnJED_vagvLB3TMMHQYPXmxOA Header and payload decodes to: { \"typ\": \"JWT\", \"alg\": \"ES256K\" } { \"iat\": 1528997842275, \"requestedCredentials\": [ { \"type\": [\"Credential\", \"ProofOfEmailCredential\"], \"constraints\": { \"and\": [ { \"==\": [ true, true ] }, { \"!=\": [ { \"var\": \"issuer\" }, { \"var\": \"claim.id\" } ] } ] } } ], \"requesterIdentity\": \"did:jolo:b310d293aeac8a5ca680232b96901fe85988fde2860a1a5db69b49762923cc88\", \"callbackURL\": \"https://demo-sso.jolocom.com/proxy/authentication/aws6i\" } References: https://github.com/uport-project/specs/blob/develop/messages/sharereq.md https://demo-sso.jolocom.com/ JSON-LD Verifiable Credentials format Example: Verifiable Credentials { \"type\": [\"Credential\"], \"claim\": { \"publicKey\" } } Response Formats A DID Auth response is constructed by an identity owner after reception of a DID Auth challenge. This section documents data formats for such responses. JWT format Example: uPort { \"iat\": 1525865451, \"exp\": 1525951851, \"aud\": \"2oeXufHGDpU51bfKBsZDdu7Je9weJ3r7sVG\", \"type\": \"shareResp\", \"nad\": \"2osC1TQ52MRTiRih1LP2tSB7R5FAibR3Ftr\", \"own\": { \"name\": \"Markus\", \"phone\": \"+436643154848\", \"country\": \"AT\" }, \"req\": \"eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NksifQ.eyJpc3MiOiIyb2VYdWZIR0RwVTUxYmZLQnNaRGR1N0plOXdlSjNyN3NWRyIsImlhdCI6MTUyNTg2NTM5OCwicmVxdWVzdGVkIjpbIm5hbWUiLCJwaG9uZSIsImNvdW50cnkiLCJhdmF0YXIiXSwicGVybWlzc2lvbnMiOlsibm90aWZpY2F0aW9ucyJdLCJjYWxsYmFjayI6Imh0dHBzOi8vY2hhc3F1aS51cG9ydC5tZS9hcGkvdjEvdG9waWMvR29EVGtmV2hvQmZ0N1BaOSIsIm5ldCI6IjB4NCIsImV4cCI6MTUyNTg2NTk5OCwidHlwZSI6InNoYXJlUmVxIn0.sQnZv63KaiWj7uQzLHLZ2jfZCZTOPz-ei7zqiUeDpjCmAdrKhO_uBujYhERJ6m-vuHcoAKuibXAPoBVHI7_H4A\", \"capabilities\": [ \"eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NksifQ.eyJpYXQiOjE1MjU4NjU0NTEsImV4cCI6MTUyNzE2MTQ1MSwiYXVkIjoiMm9lWHVmSEdEcFU1MWJmS0JzWkRkdTdKZTl3ZUozcjdzVkciLCJ0eXBlIjoibm90aWZpY2F0aW9ucyIsInZhbHVlIjoiYXJuOmF3czpzbnM6dXMtd2VzdC0yOjExMzE5NjIxNjU1ODplbmRwb2ludC9HQ00vdVBvcnQvNTcxNzViMDgtMzc3Mi0zNDY5LWEzZDEtYzhhYzg2MzkyNTQ3IiwiaXNzIjoiMm9zQzFUUTUyTVJUaVJpaDFMUDJ0U0I3UjVGQWliUjNGdHIifQ.Ldy1Ficyxo9gFM6dwpz8IanNDjHbqTKmbn-bI7GOAA9QlWaTaMiha0GnTBFewWeam1ioZKSTbbaoRILQI8FysA\" ], \"publicEncKey\": \"PZyTtN0EKJLFJyC/hqbYhz/UCUSYk9l1eFTd9cp4Tj8=\", \"iss\": \"2osC1TQ52MRTiRih1LP2tSB7R5FAibR3Ftr\" } References: https://github.com/uport-project/specs/blob/develop/messages/shareresp.md JSON-LD Verifiable Credentials format The following Verifiable Credential contains a claim of a certain public key as a property of the identity owner . { \"type\": [\"VerifiablePresentation\"], \"issuer\": \"did:example:123456789abcdefghi\", \"issued\": \"2018-03-07\", \"claim\": { \"id\": \"did:example:123456789abcdefghi\", \"publicKey\": \"did:example:123456789abcdefghi#keys-2\" }, \"proof\": { \"type\": \"Ed25519Signature2018\", \"created\": \"2018-01-01T21:19:10Z\", \"creator\": \"did:example:123456789abcdefghi#keys-2\", \"nonce\": \"c0ae1c8e-c7e7-469f-b252-86e6a0e7387e\", \"signatureValue\": \"...\" } } Challenge Transports A DID Auth challenge may be delivered by a relying party to an identity owner in different ways. DID Auth defines a few common ways that this can be done. DID Auth Service Endpoint If the identity owner 's DID is known to the relying party , it resolves it to its associated DID Document to find a DID Auth service endpoint and sends an HTTP POST with the challenge to that endpoint. Scan QR Code from Mobile App The relying party may display a DID Auth challenge encoded as a QR code, which is delivered to the identity owner by scanning it with a DID-Auth-compatible mobile app. Mobile Deep Link A mobile app or webpage may itself be a relying party and deliver a DID Auth challenge to another mobile app that represents the identity owner . Example: did-auth:jwt/... Custom Protocol Handler Similar to a mobile deep link, a web page can contain a web-based protocol link that contains a DID Auth challenge. Example: <a href=\"did-auth:jwt/...\">Login with DID Auth</a> Invoke User Agent's JavaScript API A relying party web site may deliver a DID Auth challenge to the identity owner by invoking an API via a JavaScript function in the identity owner 's user agent. Example: Browser Credential Handler API const credential = await navigator.credentials.get({ web: { VerifiableProfile: { name: true } } }); console.log('credential received', credential); References: https://credential-verifier.demo.digitalbazaar.com/ Form Redirect A relying party web site may deliver a DID Auth challenge to the identity owner by redirecting to a DID Auth web site that acts on the identity owner 's behalf (similar to a classic IdP in OpenID Connect or SAML protocols). Example: <form action=\"https://auth.example.com/did:example:123456789abcdefg\" method=\"post\"> <input type=\"hidden\" name=\"challenge\" value=\"...\"> <input type=\"submit\" value=\"Submit!\"> </form> Device-to-device Communication If both the relying party and identity owner meet physically with devices they control, the relying party can deliver a DID Auth challenge to the identity owner via direct communication between devices, using Bluetooth, NFC, WiFi, etc. Response Transports HTTP POST to Callback URL The DID Auth response may be delivered to the relying party in the form of an HTTP POST to a callback URL. This callback URL may be known to the identity owner in advance, it may have been included in the challenge, or it may be discoverable from the relying party 's DID. Scan QR Code from Mobile App The identity owner may encode a DID Auth response as a QR code, which is delivered to the relying party by scanning it with a DID-Auth-compatible mobile app. Fulfill JavaScript Promise If the DID Auth challenge was delivered to the identity owner via a JavaScript API, then the response may be returned to the relying party via fulfillment of a JavaScript promise. Device-to-device Communication If both the relying party and identity owner meet physically with devices they control, the identity owner can deliver a DID Auth response to the relying party via direct communication between devices, using Bluetooth, NFC, WiFi, etc. Architectures Based on the above challenge and response formats and transports, it is possible to construct architectures for various complete DID Auth interactions. When selecting an appropriate combination, it may be useful to keep in mind that the example architectures in this section mainly differ along four dimensions: Is the DID known to the relying party at the time of challenge construction? What is the transport mechanism of the DID Auth challenge? What is the location of authentication material? (i.e. where are the secrets stored?) What is the transport mechanism of the DID Auth response? Therefore, the following DID Auth architectures should be understood only as some examples of possible combinations of design options. DID Auth Architecture 1: Web Page and Mobile App Relying party's web page displays a QR code (with challenge) to identity owner's web browser . { Identity owner's mobile app scans QR code (with challenge) from identity owner's web browser . } Identity owner's mobile app sends HTTP POST (with response) to relying party's web server . { Relying party's web server is polled with HTTP GET (with response) by relying party's web page . } References: https://github.com/uport-project/specs/blob/develop/messages/sharereq.md https://demo-sso.jolocom.com/ DID Auth Architecture 2: Mobile Web Page and Mobile App Relying party's mobile web page redirects via deep link (with challenge) to identity owner's mobile app . Identity owner's mobile app opens return link (with response) to relying party's web server . { Relying party's web server updates relying party's mobile web page . } References: https://github.com/uport-project/specs/blob/develop/messages/sharereq.md DID Auth Architecture 3: Web Page and DID Auth Service (1) { Relying party's web page contains a link or button that calls the relying party's web server . } Relying party's web server sends HTTP POST (with challenge) to identity owner's DID Auth service . { Identity owner's DID Auth service sends push notification (with challenge) to identity owner's mobile app . } Identity owner's mobile app sends HTTP POST (with response) to relying party's web server . { Relying party's web server is polled with HTTP GET (with response) by relying party's web page . } References: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2018/blob/master/draft-documents/Identity%20Hub%20Attestation%20Handling.md DID Auth Architecture 4: Web Page and DID Auth Service (2) { Relying party's web page contains a link or button that calls the relying party's web server . } Relying party's web server sends HTTP POST (with challenge) to identity owner's DID Auth service . { Identity owner's DID Auth service sends push notification (with challenge) to identity owner's mobile app . } { Identity owner's mobile app sends HTTP POST (with response) to identity owner's DID Auth service . } Identity owner's DID Auth service sends HTTP POST (with response) to relying party's web server . { Relying party's web server is polled with HTTP GET (with response) by relying party's web page . } DID Auth Architecture 5: Web Page and DID Auth Web Page { Relying party's web page contains a link or button that calls the relying party's web server . } Relying party's web server sends HTTP Redirect (with challenge) to identity owner's web browser . { Identity owner's web browser follows HTTP Redirect (with challenge) to identity owner's DID Auth web page . } { Identity owner's DID Auth web page optionally interacts (with challenge and response) with identity owner's mobile app or other device . } Identity owner's DID Auth web page follows HTTP Redirect (with response) to relying party's web server . { Relying party's web server updates relying party's web page . } DID Auth Architecture 6: Web Page and Web Browser Relying party's web page calls JavaScript function (with challenge) on identity owner's web browser . { Identity owner's web browser optionally interacts (with challenge and response) with identity owner's mobile app or other device . } Identity owner's web browser triggers JavaScript promise (with response) on relying party's web page . { Relying party's web page sends HTTP POST (with response) to relying party's web server . } References: Credential Handler API DID Auth browser add-on DID Auth Architecture 7: Mobile Apps and DID Auth Service Relying party's mobile app displays a QR code (with challenge) that is scanned by identity owner's mobile app . Identity owner's mobile app sends HTTP POST (with response) to relying party's DID Auth service . { Relying party's DID Auth service sends push notification (with response) to relying party's mobile app . } DID Auth Architecture 8: DID-TLS Two services (or agents, or hubs, etc.) act as both identity owner and relying party and engage in TLS handshake where mutual challenges and responses are exchanged and verified. Note that while TLS specifications support mutual authentication handshakes, in practice some implementations are not well-suited for trust models that are not based on traditional certificate authorities. Besides TLS, other protocols can also be used to establish secure channels with mutual authentication based on DIDs, such as CurveZMQ, or custom handshake protocols. References: TLS Flex DID-Auth Protocol DID Auth Architecture 9: HTTP Signatures Relying party's service (or agent, or hub, etc.) answers an HTTP request by returning HTTP code 401 with a WWW-Authenticate header to identity owner's service (or agent, or hub, etc.) . Identity owner's service (or agent, or hub, etc.) sends an HTTP request to relying party 's service (or agent, or hub, etc.) and includes an HTTP Signature. Example: POST /api/v1/issuerservices HTTP/1.1 Host: testhost.gov.bc.ca Accept: */* Authorization: Signature keyId=\"did:example:123456789abcdefghi#keys-1\",algorithm=\"rsa-sha256\",headers=\"(request-target) accept user-agent\",signature=\"214BeK0YJ9P2wmMXBjZNNXDMT4prNlc32ZkslillkJYkJeLp3zbz4r1WfgCltd103m7AyY734qbau+GsWENDXaqxeTaP6LSMLWr6FexWMVgBbMzH1KDMhJlozTMFPkMsGlbuDpRKwEPqnX1Yy6ldHLe8mIJfSAEUy5P/Hf3y1b1kI8XyHNVbChFJLiUkOocF7XsFuTfoB+MJSEUqJDnuKibiF+Ap9rxI7J7Uroe6EjaVYqLXnGbpu8j8Oxn5QzGBZFCA/j6XgHy4NK9fG9pcCyyAPGzSYi1RWjDWFyS0RDQAXFBBNgyskXAgssKuVS2AFwPvXcHb5mhvKFUYMvMESg==\" References: HTTP Signatures DID Auth HTTP Proxy DID Auth Architecture 10: Authenticated Encryption Relying party's service (or agent, or hub, etc.) sends an auth-encrypted challenge to identity owner 's service (or agent, or hub, etc.) . This challenge is then decrypted and authenticated by identity owner 's service (or agent, or hub, etc.) as part of the decryption process. Identity owner 's service (or agent, or hub, etc.) then auth-encrypts a response message and sends it to relying party's service (or agent, or hub, etc.) , which decrypts and authenticates the response of the challenge. References: NaCl crypto_box based on Curve25519, Salsa20, and Poly1305 Relation to Other Technologies Biometrics Biometrics are recognizable and verifiable data that is unique, specific to an identity owner , and available to every human being. Authentication with biometrics involves matching a non-reversible biometric template (or IBV, Initial Biometric Vector) against biometric input data (or CBV, Candidate Biometric Vector). Biometrics can be used in several ways in conjunction with DID Auth: Biometrics such as fingerprints or voice can protect secret authentication material (such as a private key) on an identity owner 's physical device, which is used to construct a response to the relying party 's challenge. In this case, biometrics are only used locally to protect the identity owner 's part of the DID Auth challenge-response cycle; the proof mechanism listed in a DID Document does not directly involve biometrics. Biometric protocols such as IEEE 2410-2015 \"BOPS\" or Web Authentication can be adapted to augment DID Auth flows by using biometrics in a standard way. In both protocols, no biometric data is exchanged between the relying party and identity owner . Biometrics can also play a more direct role as a proof mechanism that is listed in a DID Document and used during a DID Auth challenge-response cycle to prove control of a DID. In this case, certain biometric data may be exchanged between a relying party and an identity owner ; biometric service providers such as iRespond can assist the process by offering to perform the biometric matching procedure via a remote service. In any case, since biometrics are sensitive data with special properties (i.e. their semi-public nature and the inability to revoke them), certain principles must be carefully followed (e.g. no storage of biometrics in centralized silos, no storage of biometrics on a blockchain). References: Six Principles for Self-Sovereign Biometrics Remove biometric templates from DID spec elements #62 Other Public Key Infrastructure (PGP, SSH, etc.) Many existing applications and services that today depend on public-key cryptography for authentication could be augmented to use DIDs as identifiers and to use DID resolution for retrieving a DID's associated public key instead of registering public keys as identifiers directly, e.g.: An e-mail may be signed using MIME Security and OpenPGP formats, but using key material associated with DIDs instead of static public keys that are exchanged in advance. An SSH server may rely on a list of DIDs in a ~/.ssh/authorized_dids file instead of relying on a list of public keys in the traditional ~/.ssh/authorized_keys file. WebAuthn WebAuthn is a specification for a JavaScript API that enables FIDO Authentication in the browser. The architecture resembles DID Auth Architecture 6 in this paper. In WebAuthn, a relying party associates a public key with an identity owner during a registration process. In subsequent login processes, the identity owner proves control of that same public key. Different public keys can be used for different relying parties based on an \"origin\". WebAuthn Registration: Register(Account, Origin) WebAuthn Registration Response: RegisterResponse(PublicKeyCredential, Attestation, Origin) WebAuthn Login: Sign(Challenge, Origin) WebAuthn Login Response: SignResponse(SignedChallenge, Origin) In order to adapt WebAuthn to support DIDs, a relying party should associate DIDs rather than public keys with identity owners (by using a DIDCredential instead of a PublicKeyCredential ). During the login process, the identity owner includes their DID in the login response, which the relying party uses to look up the current public key from the DID Document. WebAuthn+DID Registration: Register(Account, Origin) WebAuthn+DID Registration Response: RegisterResponse(DIDCredential, Attestation, Origin) WebAuthn+DID Login: Sign(Challenge, Origin) WebAuthn+DID Login Response: SignResponse(DID, SignedChallenge, Origin) Additional Notes: WebAuthn+DID must ensure that the publicKey objects in the DID Document correspond to the keys used by the FIDO authenticators. WebAuthn+DID requires no service endpoints in the DID Document. Ideally, a different DID should be used for each WebAuthn \"origin\". References: IIW #26 Session Notes \"WebAuthn + DID Auth\" Web Authentication: An API for accessing Public Key Credentials Level 1 OpenID Connect OpenID Connect (OIDC) is an authentication protocol built on the OAuth 2.0 protocol. In its most common web-based form, an end-user's user agent is redirected by a relying party (OAuth 2.0 client) to an OpenID Provider (OAuth 2.0 authorization server), which authenticates the end-user and redirects them back to the relying party . The architecture resembles DID Auth Architecture 5 in this paper. OpenID Connect is highly modular and customizable. It can potentially relate to DID Auth in the following ways: DID Auth as OIDC Local Authentication Method Within established OIDC deployments, DID Auth can serve as a drop-in \"local authentication method\", i.e. a method that determines how the OpenID Provider authenticates the end-user (to replace username and password). References: Authentication Context Class DID Auth as Alternative OIDC Provider Discovery For use cases where the DID is known beforehand, DID resolution can serve as an alternative mechanism for OIDC provider discovery (alternative to OpenID Connect Discovery / WebFinger) via service endpoint. Example OpenID Connect service endpoint in DID Document: { \"service\": [{ \"id\": \"did:example:123456789abcdefghi;openid\", \"type\": \"OpenIdConnectVersion1.0Service\", \"serviceEndpoint\": \"https://openid.example.com/\" }] } References: OpenID Connect Discovery WebFinger DID Auth with JWT Attributes and OIDC Session Management As a format for the DID Auth challenge-response cycle, DID Auth could re-use the well-specified JWT attributes used in OAuth2 and OIDC (e.g. exp , iat , and others) as well as the concept of OIDC request objects where an authentication request is expressed as a self-contained JWT. In addition, although session management is not a focus of this document, DID Auth implementers might find the OIDC Session Management specification and the OIDC User Consent step helpful (in terms of establishing a long-lived session after the DID Auth ceremony). References: OIDC: Passing Request Parameters as JWTs IIW #26 Session Notes \"Open ID v. FIDO v. SSI\" IIW #26 Session Notes \"DID Auth Workflows (Part 2)\" DID Auth as Token Endpoint Authentication Method As part of the OAuth2 / OIDC workflow, relying party clients must authenticate themselves to the authorization server (i.e. the token issuing endpoint) using a variety of methods. Since a PKI-based authentication method is already supported by OIDC (i.e. the private_key_jwt method), DID Auth can fulfill a role as a secure client-authentication mechanism during this step. Security and Privacy Considerations Support for Multiple DIDs During a DID Auth interaction, a relying party may request that the identity owner 's user agent display a DID selection dialog where the identity owner is able to pick the DID to authenticate with to the relying party . The DID Auth challenge and response are then done using that DID. An identity owner 's user agent may also automatically choose (or even dynamically create) different DIDs for each relying party . This idea is also known as directed identity. If a different DID is used for each relying party , then they are called pairwise-pseudonymous DIDs. This measure is helpful (but not always sufficient) to avoid correlation of the identity owner . An identity owner may also choose to use different DIDs for the same relying party depending on context. Automatic Authentication Depending on the challenge and response transports, and depending on an identity owner's user agent, it may be possible to automate some steps in a DID Auth interaction. For example, a DID Auth browser extension can automatically complete a DID Auth interaction to log in to a website using a single click on a button. Re-authentication and Step-up Even after a DID Auth challenge-response cycle has been completed, a relying party may require an identity owner to re-authenticate in certain cases, e.g. when a high-value transaction such as transferring money is initiated. In such cases, subsequent protocol interactions may also require the exchange of Verifiable Credentials in addition to basic DID Auth, in order to establish the desired level of trust. DID Resolution DID Auth depends on the ability to resolve a DID to its associated DID Document. Therefore all security considerations associated with DID resolution must be taken into account. Among other topics, this includes caching behavior of a DID resolver, as well as metadata about the DID resolution process (e.g. whether the DID Document has been retrieved via a blockchain full node or via an untrusted intermediary lookup service). References: DID Resolution v0.1 Owner vs. Controller This paper heavily uses the term identity owner . This helps to emphasize clearly how DID Auth is fundamentally different from earlier authentication protocols, which have traditionally revolved around \"identity providers\". Note however that the identity owner (the entity identified by the DID) may in some situations not be the same entity who controls the DID and who is able to perform authentication. This may happen in cases of security breaches (e.g. theft of a private key that controls the DID), or in intentional situations such as digital guardianship, where for some reason the identity owner is not able to control their own DID. Single Log-out DID Auth should provide ways to end all sessions and relationships that have been established for an authenticated DID. An identity owner should be able to trigger single log-out manually using their user agent. Single log-out should also happen automatically if a DID is revoked, which means that relying parties must monitor authenticated DIDs for revocation. Hardware Wallet The DID Auth challenge may be processed by a hardware wallet that upon an identity owner 's physical interaction will create a response and send it back to the relying party . DID Auth gateway services In order to ease adoption of DID Auth through the use of standard protocols, gateway services could be developed that expose the OpenID Connect and/or OAuth2 protocols and perform (parts of) a DID Auth relying party . In such cases, the privacy and trust implications of the use of a gateway service must be carefully considered.","title":"Introduction to DID Auth"},{"location":"RWoT6/did-auth/#introduction-to-did-auth","text":"Authors: Markus Sabadello, Kyle Den Hartog, Christian Lundkvist, Cedric Franz, Alberto Elias, Andrew Hughes, John Jordan, Dmitri Zagidulin Contributors: Eugeniu Rusu, Adam Powers, John Callahan, Joe Andrieu","title":"Introduction to DID Auth"},{"location":"RWoT6/did-auth/#abstract","text":"The term DID Auth has been used in different ways and is currently not well-defined. We define DID Auth as a ceremony where an identity owner , with the help of various components such as web browsers, mobile devices, and other agents, proves to a relying party that they are in control of a DID. This means demonstrating control of the DID using the mechanism specified in the DID Document's \"authentication\" object. This could take place using a number of different data formats, protocols, and flows. DID Auth includes the ability to establish mutually authenticated communication channels and to authenticate to web sites and applications. Authorization, Verifiable Credentials, and Capabilities are built on top of DID Auth and are out of scope for this document. This paper gives on overview of the scope of DID Auth, supported protocols and flows, and the use of components of the DID Documents that are relevant to authentication, as well as formats for challenges and responses.","title":"Abstract"},{"location":"RWoT6/did-auth/#resources","text":"This paper is a continuation of ongoing work by Rebooting the Web of Trust and other communities. Previous work includes: RWoT IV: James Monaghan's topic paper \"DID Auth\" RWoT VI: Markus Sabadello's topic paper \"DID Auth: Scope, Formats, and Protocols\" RWoT VI: Kyle Den Hartog's topic paper \"DID-Auth protocol\" Portions of the work on this paper have been funded through a Code With Us contract awarded by the BC Developers' Exchange (https://bcdevexchange.org/) of the Province of British Columbia, Canada.","title":"Resources"},{"location":"RWoT6/did-auth/#definitions","text":"Authentication : The ceremony where an identity owner proves to a relying party that the identity owner controls a DID, using a mechanism that is described in the DID's associated DID Document. Authorization : A process of establishing the rights and privileges of an identity owner to perform certain actions, including operations on a DID itself, or in another context. Decentralized Identifier (DID) : A globally unique identifier that does not require a centralized registration authority because it is registered with distributed ledger technology or another form of decentralized network. (see here ) DID Document : A structured document containing metadata that describes a DID, including authentication materials such as public keys and pseudonymous biometrics, that an entity can use to authenticate, i.e. to prove control of the DID. A DID Document may also contain other attributes or claims describing the entity. (see here ) DID Record : The combination of a DID and its associated DID Document. Identity Owner : The individual, organization or thing who created the DID, is identified by the DID that is the subject of the DID Document, and who has the ultimate authority to update or revoke the DID. Relying Party : The individual, organization or thing that authenticates an identity owner using a DID Auth protocol. Also called \"Verifier\" in other specifications. Verifiable Credentials : A set of one or more claims that are statements made by an issuer about a subject that is tamper-resistant and whose authorship can be cryptographically verified (see here ).","title":"Definitions"},{"location":"RWoT6/did-auth/#introduction","text":"","title":"Introduction"},{"location":"RWoT6/did-auth/#scope","text":"This paper defines data formats and challenge and response transports allowing an identity owner to prove control of a DID to a relying party . Proof of control of a DID is a technical interaction that may be a precursor to establishing a longer term relationship between two parties. A successful DID Auth interaction may create the required conditions to allow the parties to exchange further data in a trustworthy way. This further data could include streams of raw data, from sensors to the exchange of Verifiable Credentials. This further exchange of data is out of scope of the DID Auth protocol itself. DID Auth may be a one-way interaction where party A proves control of a DID A to party B, or a two-way interaction where mutual proof of control of DIDs is achieved. In the latter case, party A proves control of DID A to party B and party B proves control of DID B to party A. It is in the purview of the two parties engaged in the interaction to determine the need to have a one-way or two-way DID Auth interaction. It is also in the purview of the two parties to determine if further exchanges of data such as Verifiable Credentials may be necessary to establish the nature of the relationship between the two parties. Implementers may decide to subsume a DID Auth interaction within a higher layer interaction such as the exchange of Verifiable Credentials, which could simultaneously prove control of a DID and offer Verifiable Credentials for some transaction-specific purpose.","title":"Scope"},{"location":"RWoT6/did-auth/#did-auth-and-verifiable-credentials","text":"Even though DID Auth is about proving control of a DID, the exchange of Verifiable Credentials associated with a DID is related to DID Auth. The relationship between DID Auth and Verifiable Credentials could be thought of in several conceptual ways: DID Auth and Verifiable Credentials exchange are separate: At the beginning of an interaction between two parties, they need to authenticate (mutually, or just in one direction). After this is done, a protocol for exchange of Verifiable Credentials can be executed, so that the two parties can learn more about each other (and then perhaps make authorization decisions). Verifiable Credentials exchange is an extension to (or part of) DID Auth: In this approach, proving control of an identifier, and proving possession of Verifiable Credentials are closely related, and a single protocol is used for both purposes. The Verifiable Credentials are an \"optional field\" in the protocol. In order to \"only\" prove control of an identifier, an empty set of Verifiable Credentials is exchanged. DID Auth is a certain kind of Verifiable Credential: It is possible to think of DID Auth as an exchange of the most trivial Verifiable Credential imaginable: a self-issued Verifiable Credential that states \"I am me\". From this perspective, the separation between DID Auth and exchange of \"other\" Verifiable Credentials is blurred, and both are part of a single universal protocol.","title":"DID Auth and Verifiable Credentials"},{"location":"RWoT6/did-auth/#did-record-creation","text":"DID Auth requires authentication material that is generated during DID Record Creation. As stated in the DID specification, the steps to create a DID Record compliant with DID Auth are: Generate a NEW_DID as specified in the relevant DID method specification. Generate a NEW_DID_DOCUMENT as specified in the relevant DID method specification. Set the id property to the value of NEW_DID (the DID subject). Choose one or more authentication type(s) from the array of proof mechanisms. Record the type property in an authentication object of the NEW_DID_DOCUMENT . Generate authentication material for use at a later time during authentication of the NEW_DID . The authentication type determines how to generate authentication material for a proof mechanism. Communicate and store the authentication material, either directly or as derived material, in the NEW_DID_DOCUMENT and for storage by the identity owner . If the chosen proof mechanism is based on asymmetric keys, the authentication material in the NEW_DID_DOCUMENT is recorded in a publicKey object in the DID Document. Example authentication and publicKey objects in a DID Document: { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:example:123456789abcdefghi\", \"authentication\": [{ \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"did:example:123456789abcdefghi#keys-1\" }, { \"type\": \"Ed25519SignatureAuthentication2018\", \"publicKey\": \"did:example:123456789abcdefghi#keys-2\" }], \"publicKey\": [{ \"id\": \"did:example:123456789abcdefghi#keys-1\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:example:123456789abcdefghi#keys-2\", \"type\": \"Ed25519VerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyBase58\": \"H3C2AVvLMv6gmMNam3uVAjZpfkcJCwDwnZn6z3wXmqPV\" }] } DID Auth can use different transports for exchanging challenges and responses between an identity owner and a relying party . One such transport uses an HTTP POST call to a DID Auth service endpoint. This service endpoint can be discovered from a DID Document. Example DID Auth service endpoint in a DID Document: { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:example:123456789abcdefghi\", \"service\": { \"type\": \"DidAuthService\", \"serviceEndpoint\": \"https://auth.example.com/did:example:123456789abcdefg\" } }","title":"DID Record Creation"},{"location":"RWoT6/did-auth/#authentication-of-a-did","text":"This section describes a conceptual authentication framework of the DID Auth protocol. It relates general concepts of authentication to specific concepts of DID Auth. Authentication of a DID: Similar to other authentication methods, DID Auth relies on a challenge-response cycle in which a relying party authenticates the DID of an identity owner . During this cycle, an identity owner demonstrates control of their authentication material that was generated and distributed during DID Record Creation through execution of the authentication-proof mechanism. Challenge: The way an identity owner or their agent encounters an authentication challenge, as well as the format of the challenge, will vary depending on the situation. For example, they can come across a \"Sign in with DID Auth\" button or a QR code on a website. Or, in the case of an API call, the relying party may respond to a request by asking for authentication (the HTTP 401 Unauthorized response is a classic example, although DID Auth covers many use cases beyond HTTP). Challenge principles: The relying party may or may not know the identity owner 's DID at the time the challenge is constructed, and therefore the identity owner 's DID may or may not be included in the challenge. If the DID is known at the time of challenge construction, then the relying party may use the contents of the DID Document to select preferred authentication methods or service endpoints. The challenge that is sent by the relying party may or may not itself contain a proof of the relying party 's control of a DID. The relying party may or may not need additional transport-specific information about the identity owner in order to be able to deliver the challenge (e.g. a DID Auth service endpoint). This additional protocol-specific information may be discoverable from the identity owner's DID if it is known to the relying party . The relying party should include a nonce, to prevent replay attacks and to help link the challenge to a subsequent response. Response: Based on the challenge, the identity owner then constructs a response that proves control of their DID. This often involves a cryptographic signature, but can include other proof mechanisms. (As mentioned earlier, the response may also contain Verifiable Credentials that the relying party asked for in the challenge.) After receiving the response, the relying party resolves the identity owner 's DID, and verifies that the response is valid for a prior challenge (for example, verifying the response signature by using a publicKey object contained in the DID Document). Response principles: The identity owner may or may not need additional transport-specific information about the relying party in order to be able to deliver the response (e.g. a callback URL). This additional protocol-specific information may be included in the challenge, or it may be discoverable from the relying party 's DID that is included in the challenge. The relying party must be able to internally link a response to a prior challenge. This can be done with a nonce or message identifier in the challenge that must also be included in the response. It can also be done by including the entire original challenge in the response. Multiple devices, user agents, and other technical components may act on behalf of the identity owner to receive and process the challenge. For example, an identity owner's DID Auth service endpoint may receive the challenge and relay it to the identity owner 's mobile app. The identity owner's component that sends the response may or may not be the same component as the one that received the challenge. E.g. the challenge may be received as HTTP POST by a DID Auth service, but the response may be sent as HTTP POST by a mobile app (see DID Auth Architecture 3 ). The relying party's component that receives the response may or may not be the same component as the one that sent the challenge. E.g. the challenge may be sent as deep link by a mobile web page, but the response may be received as HTTP POST by a web server (see DID Auth Architecture 2 ). Generic DID Auth architecture:","title":"Authentication of a DID"},{"location":"RWoT6/did-auth/#challenge-formats","text":"In a DID Auth interaction, a challenge is transmitted by a relying party to an identity owner , asking the identity owner to return a response that proves their control of a DID. This section documents data formats for such challenges.","title":"Challenge Formats"},{"location":"RWoT6/did-auth/#jwt-format","text":"Example: uPort eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NksifQ.eyJpc3MiOiIyb2VYdWZIR0RwVTUxYmZLQnNaRGR1N0plOXdlSjNyN3NWRyIsImlhdCI6MTUyNTg2NTM5OCwicmVxdWVzdGVkIjpbIm5hbWUiLCJwaG9uZSIsImNvdW50cnkiLCJhdmF0YXIiXSwicGVybWlzc2lvbnMiOlsibm90aWZpY2F0aW9ucyJdLCJjYWxsYmFjayI6Imh0dHBzOi8vY2hhc3F1aS51cG9ydC5tZS9hcGkvdjEvdG9waWMvR29EVGtmV2hvQmZ0N1BaOSIsIm5ldCI6IjB4NCIsImV4cCI6MTUyNTg2NTk5OCwidHlwZSI6InNoYXJlUmVxIn0.sQnZv63KaiWj7uQzLHLZ2jfZCZTOPz-ei7zqiUeDpjCmAdrKhO_uBujYhERJ6m-vuHcoAKuibXAPoBVHI7_H4A Header and payload decodes to: { \"typ\": \"JWT\", \"alg\": \"ES256K\" } { \"iss\": \"2oeXufHGDpU51bfKBsZDdu7Je9weJ3r7sVG\", \"iat\": 1525865398, \"requested\": [ \"name\", \"phone\", \"country\", \"avatar\" ], \"permissions\": [ \"notifications\" ], \"callback\": \"https://chasqui.uport.me/api/v1/topic/GoDTkfWhoBft7PZ9\", \"net\": \"0x4\", \"exp\": 1525865998, \"type\": \"shareReq\" } Example: Jolocom eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NksifQ.eyJpYXQiOjE1Mjg5OTc4NDIyNzUsInJlcXVlc3RlZENyZWRlbnRpYWxzIjpbeyJ0eXBlIjpbIkNyZWRlbnRpYWwiLCJQcm9vZk9mRW1haWxDcmVkZW50aWFsIl0sImNvbnN0cmFpbnRzIjp7ImFuZCI6W3siPT0iOlt0cnVlLHRydWVdfSx7IiE9IjpbeyJ2YXIiOiJpc3N1ZXIifSx7InZhciI6ImNsYWltLmlkIn1dfV19fV0sInJlcXVlc3RlcklkZW50aXR5IjoiZGlkOmpvbG86YjMxMGQyOTNhZWFjOGE1Y2E2ODAyMzJiOTY5MDFmZTg1OTg4ZmRlMjg2MGExYTVkYjY5YjQ5NzYyOTIzY2M4OCIsImNhbGxiYWNrVVJMIjoiaHR0cHM6Ly9kZW1vLXNzby5qb2xvY29tLmNvbS9wcm94eS9hdXRoZW50aWNhdGlvbi9hd3M2aSJ9.TZwB6_XMXFm_SjIv_PSangYNb9ldAQPzlEln8iBdcaSPDyU1A7kuJzJIaI0ykZnJED_vagvLB3TMMHQYPXmxOA Header and payload decodes to: { \"typ\": \"JWT\", \"alg\": \"ES256K\" } { \"iat\": 1528997842275, \"requestedCredentials\": [ { \"type\": [\"Credential\", \"ProofOfEmailCredential\"], \"constraints\": { \"and\": [ { \"==\": [ true, true ] }, { \"!=\": [ { \"var\": \"issuer\" }, { \"var\": \"claim.id\" } ] } ] } } ], \"requesterIdentity\": \"did:jolo:b310d293aeac8a5ca680232b96901fe85988fde2860a1a5db69b49762923cc88\", \"callbackURL\": \"https://demo-sso.jolocom.com/proxy/authentication/aws6i\" } References: https://github.com/uport-project/specs/blob/develop/messages/sharereq.md https://demo-sso.jolocom.com/","title":"JWT format"},{"location":"RWoT6/did-auth/#json-ld-verifiable-credentials-format","text":"Example: Verifiable Credentials { \"type\": [\"Credential\"], \"claim\": { \"publicKey\" } }","title":"JSON-LD Verifiable Credentials format"},{"location":"RWoT6/did-auth/#response-formats","text":"A DID Auth response is constructed by an identity owner after reception of a DID Auth challenge. This section documents data formats for such responses.","title":"Response Formats"},{"location":"RWoT6/did-auth/#jwt-format_1","text":"Example: uPort { \"iat\": 1525865451, \"exp\": 1525951851, \"aud\": \"2oeXufHGDpU51bfKBsZDdu7Je9weJ3r7sVG\", \"type\": \"shareResp\", \"nad\": \"2osC1TQ52MRTiRih1LP2tSB7R5FAibR3Ftr\", \"own\": { \"name\": \"Markus\", \"phone\": \"+436643154848\", \"country\": \"AT\" }, \"req\": \"eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NksifQ.eyJpc3MiOiIyb2VYdWZIR0RwVTUxYmZLQnNaRGR1N0plOXdlSjNyN3NWRyIsImlhdCI6MTUyNTg2NTM5OCwicmVxdWVzdGVkIjpbIm5hbWUiLCJwaG9uZSIsImNvdW50cnkiLCJhdmF0YXIiXSwicGVybWlzc2lvbnMiOlsibm90aWZpY2F0aW9ucyJdLCJjYWxsYmFjayI6Imh0dHBzOi8vY2hhc3F1aS51cG9ydC5tZS9hcGkvdjEvdG9waWMvR29EVGtmV2hvQmZ0N1BaOSIsIm5ldCI6IjB4NCIsImV4cCI6MTUyNTg2NTk5OCwidHlwZSI6InNoYXJlUmVxIn0.sQnZv63KaiWj7uQzLHLZ2jfZCZTOPz-ei7zqiUeDpjCmAdrKhO_uBujYhERJ6m-vuHcoAKuibXAPoBVHI7_H4A\", \"capabilities\": [ \"eyJ0eXAiOiJKV1QiLCJhbGciOiJFUzI1NksifQ.eyJpYXQiOjE1MjU4NjU0NTEsImV4cCI6MTUyNzE2MTQ1MSwiYXVkIjoiMm9lWHVmSEdEcFU1MWJmS0JzWkRkdTdKZTl3ZUozcjdzVkciLCJ0eXBlIjoibm90aWZpY2F0aW9ucyIsInZhbHVlIjoiYXJuOmF3czpzbnM6dXMtd2VzdC0yOjExMzE5NjIxNjU1ODplbmRwb2ludC9HQ00vdVBvcnQvNTcxNzViMDgtMzc3Mi0zNDY5LWEzZDEtYzhhYzg2MzkyNTQ3IiwiaXNzIjoiMm9zQzFUUTUyTVJUaVJpaDFMUDJ0U0I3UjVGQWliUjNGdHIifQ.Ldy1Ficyxo9gFM6dwpz8IanNDjHbqTKmbn-bI7GOAA9QlWaTaMiha0GnTBFewWeam1ioZKSTbbaoRILQI8FysA\" ], \"publicEncKey\": \"PZyTtN0EKJLFJyC/hqbYhz/UCUSYk9l1eFTd9cp4Tj8=\", \"iss\": \"2osC1TQ52MRTiRih1LP2tSB7R5FAibR3Ftr\" } References: https://github.com/uport-project/specs/blob/develop/messages/shareresp.md","title":"JWT format"},{"location":"RWoT6/did-auth/#json-ld-verifiable-credentials-format_1","text":"The following Verifiable Credential contains a claim of a certain public key as a property of the identity owner . { \"type\": [\"VerifiablePresentation\"], \"issuer\": \"did:example:123456789abcdefghi\", \"issued\": \"2018-03-07\", \"claim\": { \"id\": \"did:example:123456789abcdefghi\", \"publicKey\": \"did:example:123456789abcdefghi#keys-2\" }, \"proof\": { \"type\": \"Ed25519Signature2018\", \"created\": \"2018-01-01T21:19:10Z\", \"creator\": \"did:example:123456789abcdefghi#keys-2\", \"nonce\": \"c0ae1c8e-c7e7-469f-b252-86e6a0e7387e\", \"signatureValue\": \"...\" } }","title":"JSON-LD Verifiable Credentials format"},{"location":"RWoT6/did-auth/#challenge-transports","text":"A DID Auth challenge may be delivered by a relying party to an identity owner in different ways. DID Auth defines a few common ways that this can be done.","title":"Challenge Transports"},{"location":"RWoT6/did-auth/#did-auth-service-endpoint","text":"If the identity owner 's DID is known to the relying party , it resolves it to its associated DID Document to find a DID Auth service endpoint and sends an HTTP POST with the challenge to that endpoint.","title":"DID Auth Service Endpoint"},{"location":"RWoT6/did-auth/#scan-qr-code-from-mobile-app","text":"The relying party may display a DID Auth challenge encoded as a QR code, which is delivered to the identity owner by scanning it with a DID-Auth-compatible mobile app.","title":"Scan QR Code from Mobile App"},{"location":"RWoT6/did-auth/#mobile-deep-link","text":"A mobile app or webpage may itself be a relying party and deliver a DID Auth challenge to another mobile app that represents the identity owner . Example: did-auth:jwt/...","title":"Mobile Deep Link"},{"location":"RWoT6/did-auth/#custom-protocol-handler","text":"Similar to a mobile deep link, a web page can contain a web-based protocol link that contains a DID Auth challenge. Example: <a href=\"did-auth:jwt/...\">Login with DID Auth</a>","title":"Custom Protocol Handler"},{"location":"RWoT6/did-auth/#invoke-user-agents-javascript-api","text":"A relying party web site may deliver a DID Auth challenge to the identity owner by invoking an API via a JavaScript function in the identity owner 's user agent. Example: Browser Credential Handler API const credential = await navigator.credentials.get({ web: { VerifiableProfile: { name: true } } }); console.log('credential received', credential); References: https://credential-verifier.demo.digitalbazaar.com/","title":"Invoke User Agent's JavaScript API"},{"location":"RWoT6/did-auth/#form-redirect","text":"A relying party web site may deliver a DID Auth challenge to the identity owner by redirecting to a DID Auth web site that acts on the identity owner 's behalf (similar to a classic IdP in OpenID Connect or SAML protocols). Example: <form action=\"https://auth.example.com/did:example:123456789abcdefg\" method=\"post\"> <input type=\"hidden\" name=\"challenge\" value=\"...\"> <input type=\"submit\" value=\"Submit!\"> </form>","title":"Form Redirect"},{"location":"RWoT6/did-auth/#device-to-device-communication","text":"If both the relying party and identity owner meet physically with devices they control, the relying party can deliver a DID Auth challenge to the identity owner via direct communication between devices, using Bluetooth, NFC, WiFi, etc.","title":"Device-to-device Communication"},{"location":"RWoT6/did-auth/#response-transports","text":"","title":"Response Transports"},{"location":"RWoT6/did-auth/#http-post-to-callback-url","text":"The DID Auth response may be delivered to the relying party in the form of an HTTP POST to a callback URL. This callback URL may be known to the identity owner in advance, it may have been included in the challenge, or it may be discoverable from the relying party 's DID.","title":"HTTP POST to Callback URL"},{"location":"RWoT6/did-auth/#scan-qr-code-from-mobile-app_1","text":"The identity owner may encode a DID Auth response as a QR code, which is delivered to the relying party by scanning it with a DID-Auth-compatible mobile app.","title":"Scan QR Code from Mobile App"},{"location":"RWoT6/did-auth/#fulfill-javascript-promise","text":"If the DID Auth challenge was delivered to the identity owner via a JavaScript API, then the response may be returned to the relying party via fulfillment of a JavaScript promise.","title":"Fulfill JavaScript Promise"},{"location":"RWoT6/did-auth/#device-to-device-communication_1","text":"If both the relying party and identity owner meet physically with devices they control, the identity owner can deliver a DID Auth response to the relying party via direct communication between devices, using Bluetooth, NFC, WiFi, etc.","title":"Device-to-device Communication"},{"location":"RWoT6/did-auth/#architectures","text":"Based on the above challenge and response formats and transports, it is possible to construct architectures for various complete DID Auth interactions. When selecting an appropriate combination, it may be useful to keep in mind that the example architectures in this section mainly differ along four dimensions: Is the DID known to the relying party at the time of challenge construction? What is the transport mechanism of the DID Auth challenge? What is the location of authentication material? (i.e. where are the secrets stored?) What is the transport mechanism of the DID Auth response? Therefore, the following DID Auth architectures should be understood only as some examples of possible combinations of design options.","title":"Architectures"},{"location":"RWoT6/did-auth/#did-auth-architecture-1-web-page-and-mobile-app","text":"Relying party's web page displays a QR code (with challenge) to identity owner's web browser . { Identity owner's mobile app scans QR code (with challenge) from identity owner's web browser . } Identity owner's mobile app sends HTTP POST (with response) to relying party's web server . { Relying party's web server is polled with HTTP GET (with response) by relying party's web page . } References: https://github.com/uport-project/specs/blob/develop/messages/sharereq.md https://demo-sso.jolocom.com/","title":"DID Auth Architecture 1: Web Page and Mobile App"},{"location":"RWoT6/did-auth/#did-auth-architecture-2-mobile-web-page-and-mobile-app","text":"Relying party's mobile web page redirects via deep link (with challenge) to identity owner's mobile app . Identity owner's mobile app opens return link (with response) to relying party's web server . { Relying party's web server updates relying party's mobile web page . } References: https://github.com/uport-project/specs/blob/develop/messages/sharereq.md","title":"DID Auth Architecture 2: Mobile Web Page and Mobile App"},{"location":"RWoT6/did-auth/#did-auth-architecture-3-web-page-and-did-auth-service-1","text":"{ Relying party's web page contains a link or button that calls the relying party's web server . } Relying party's web server sends HTTP POST (with challenge) to identity owner's DID Auth service . { Identity owner's DID Auth service sends push notification (with challenge) to identity owner's mobile app . } Identity owner's mobile app sends HTTP POST (with response) to relying party's web server . { Relying party's web server is polled with HTTP GET (with response) by relying party's web page . } References: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2018/blob/master/draft-documents/Identity%20Hub%20Attestation%20Handling.md","title":"DID Auth Architecture 3: Web Page and DID Auth Service (1)"},{"location":"RWoT6/did-auth/#did-auth-architecture-4-web-page-and-did-auth-service-2","text":"{ Relying party's web page contains a link or button that calls the relying party's web server . } Relying party's web server sends HTTP POST (with challenge) to identity owner's DID Auth service . { Identity owner's DID Auth service sends push notification (with challenge) to identity owner's mobile app . } { Identity owner's mobile app sends HTTP POST (with response) to identity owner's DID Auth service . } Identity owner's DID Auth service sends HTTP POST (with response) to relying party's web server . { Relying party's web server is polled with HTTP GET (with response) by relying party's web page . }","title":"DID Auth Architecture 4: Web Page and DID Auth Service (2)"},{"location":"RWoT6/did-auth/#did-auth-architecture-5-web-page-and-did-auth-web-page","text":"{ Relying party's web page contains a link or button that calls the relying party's web server . } Relying party's web server sends HTTP Redirect (with challenge) to identity owner's web browser . { Identity owner's web browser follows HTTP Redirect (with challenge) to identity owner's DID Auth web page . } { Identity owner's DID Auth web page optionally interacts (with challenge and response) with identity owner's mobile app or other device . } Identity owner's DID Auth web page follows HTTP Redirect (with response) to relying party's web server . { Relying party's web server updates relying party's web page . }","title":"DID Auth Architecture 5: Web Page and DID Auth Web Page"},{"location":"RWoT6/did-auth/#did-auth-architecture-6-web-page-and-web-browser","text":"Relying party's web page calls JavaScript function (with challenge) on identity owner's web browser . { Identity owner's web browser optionally interacts (with challenge and response) with identity owner's mobile app or other device . } Identity owner's web browser triggers JavaScript promise (with response) on relying party's web page . { Relying party's web page sends HTTP POST (with response) to relying party's web server . } References: Credential Handler API DID Auth browser add-on","title":"DID Auth Architecture 6: Web Page and Web Browser"},{"location":"RWoT6/did-auth/#did-auth-architecture-7-mobile-apps-and-did-auth-service","text":"Relying party's mobile app displays a QR code (with challenge) that is scanned by identity owner's mobile app . Identity owner's mobile app sends HTTP POST (with response) to relying party's DID Auth service . { Relying party's DID Auth service sends push notification (with response) to relying party's mobile app . }","title":"DID Auth Architecture 7: Mobile Apps and DID Auth Service"},{"location":"RWoT6/did-auth/#did-auth-architecture-8-did-tls","text":"Two services (or agents, or hubs, etc.) act as both identity owner and relying party and engage in TLS handshake where mutual challenges and responses are exchanged and verified. Note that while TLS specifications support mutual authentication handshakes, in practice some implementations are not well-suited for trust models that are not based on traditional certificate authorities. Besides TLS, other protocols can also be used to establish secure channels with mutual authentication based on DIDs, such as CurveZMQ, or custom handshake protocols. References: TLS Flex DID-Auth Protocol","title":"DID Auth Architecture 8: DID-TLS"},{"location":"RWoT6/did-auth/#did-auth-architecture-9-http-signatures","text":"Relying party's service (or agent, or hub, etc.) answers an HTTP request by returning HTTP code 401 with a WWW-Authenticate header to identity owner's service (or agent, or hub, etc.) . Identity owner's service (or agent, or hub, etc.) sends an HTTP request to relying party 's service (or agent, or hub, etc.) and includes an HTTP Signature. Example: POST /api/v1/issuerservices HTTP/1.1 Host: testhost.gov.bc.ca Accept: */* Authorization: Signature keyId=\"did:example:123456789abcdefghi#keys-1\",algorithm=\"rsa-sha256\",headers=\"(request-target) accept user-agent\",signature=\"214BeK0YJ9P2wmMXBjZNNXDMT4prNlc32ZkslillkJYkJeLp3zbz4r1WfgCltd103m7AyY734qbau+GsWENDXaqxeTaP6LSMLWr6FexWMVgBbMzH1KDMhJlozTMFPkMsGlbuDpRKwEPqnX1Yy6ldHLe8mIJfSAEUy5P/Hf3y1b1kI8XyHNVbChFJLiUkOocF7XsFuTfoB+MJSEUqJDnuKibiF+Ap9rxI7J7Uroe6EjaVYqLXnGbpu8j8Oxn5QzGBZFCA/j6XgHy4NK9fG9pcCyyAPGzSYi1RWjDWFyS0RDQAXFBBNgyskXAgssKuVS2AFwPvXcHb5mhvKFUYMvMESg==\" References: HTTP Signatures DID Auth HTTP Proxy","title":"DID Auth Architecture 9: HTTP Signatures"},{"location":"RWoT6/did-auth/#did-auth-architecture-10-authenticated-encryption","text":"Relying party's service (or agent, or hub, etc.) sends an auth-encrypted challenge to identity owner 's service (or agent, or hub, etc.) . This challenge is then decrypted and authenticated by identity owner 's service (or agent, or hub, etc.) as part of the decryption process. Identity owner 's service (or agent, or hub, etc.) then auth-encrypts a response message and sends it to relying party's service (or agent, or hub, etc.) , which decrypts and authenticates the response of the challenge. References: NaCl crypto_box based on Curve25519, Salsa20, and Poly1305","title":"DID Auth Architecture 10: Authenticated Encryption"},{"location":"RWoT6/did-auth/#relation-to-other-technologies","text":"","title":"Relation to Other Technologies"},{"location":"RWoT6/did-auth/#biometrics","text":"Biometrics are recognizable and verifiable data that is unique, specific to an identity owner , and available to every human being. Authentication with biometrics involves matching a non-reversible biometric template (or IBV, Initial Biometric Vector) against biometric input data (or CBV, Candidate Biometric Vector). Biometrics can be used in several ways in conjunction with DID Auth: Biometrics such as fingerprints or voice can protect secret authentication material (such as a private key) on an identity owner 's physical device, which is used to construct a response to the relying party 's challenge. In this case, biometrics are only used locally to protect the identity owner 's part of the DID Auth challenge-response cycle; the proof mechanism listed in a DID Document does not directly involve biometrics. Biometric protocols such as IEEE 2410-2015 \"BOPS\" or Web Authentication can be adapted to augment DID Auth flows by using biometrics in a standard way. In both protocols, no biometric data is exchanged between the relying party and identity owner . Biometrics can also play a more direct role as a proof mechanism that is listed in a DID Document and used during a DID Auth challenge-response cycle to prove control of a DID. In this case, certain biometric data may be exchanged between a relying party and an identity owner ; biometric service providers such as iRespond can assist the process by offering to perform the biometric matching procedure via a remote service. In any case, since biometrics are sensitive data with special properties (i.e. their semi-public nature and the inability to revoke them), certain principles must be carefully followed (e.g. no storage of biometrics in centralized silos, no storage of biometrics on a blockchain). References: Six Principles for Self-Sovereign Biometrics Remove biometric templates from DID spec elements #62","title":"Biometrics"},{"location":"RWoT6/did-auth/#other-public-key-infrastructure-pgp-ssh-etc","text":"Many existing applications and services that today depend on public-key cryptography for authentication could be augmented to use DIDs as identifiers and to use DID resolution for retrieving a DID's associated public key instead of registering public keys as identifiers directly, e.g.: An e-mail may be signed using MIME Security and OpenPGP formats, but using key material associated with DIDs instead of static public keys that are exchanged in advance. An SSH server may rely on a list of DIDs in a ~/.ssh/authorized_dids file instead of relying on a list of public keys in the traditional ~/.ssh/authorized_keys file.","title":"Other Public Key Infrastructure (PGP, SSH, etc.)"},{"location":"RWoT6/did-auth/#webauthn","text":"WebAuthn is a specification for a JavaScript API that enables FIDO Authentication in the browser. The architecture resembles DID Auth Architecture 6 in this paper. In WebAuthn, a relying party associates a public key with an identity owner during a registration process. In subsequent login processes, the identity owner proves control of that same public key. Different public keys can be used for different relying parties based on an \"origin\". WebAuthn Registration: Register(Account, Origin) WebAuthn Registration Response: RegisterResponse(PublicKeyCredential, Attestation, Origin) WebAuthn Login: Sign(Challenge, Origin) WebAuthn Login Response: SignResponse(SignedChallenge, Origin) In order to adapt WebAuthn to support DIDs, a relying party should associate DIDs rather than public keys with identity owners (by using a DIDCredential instead of a PublicKeyCredential ). During the login process, the identity owner includes their DID in the login response, which the relying party uses to look up the current public key from the DID Document. WebAuthn+DID Registration: Register(Account, Origin) WebAuthn+DID Registration Response: RegisterResponse(DIDCredential, Attestation, Origin) WebAuthn+DID Login: Sign(Challenge, Origin) WebAuthn+DID Login Response: SignResponse(DID, SignedChallenge, Origin) Additional Notes: WebAuthn+DID must ensure that the publicKey objects in the DID Document correspond to the keys used by the FIDO authenticators. WebAuthn+DID requires no service endpoints in the DID Document. Ideally, a different DID should be used for each WebAuthn \"origin\". References: IIW #26 Session Notes \"WebAuthn + DID Auth\" Web Authentication: An API for accessing Public Key Credentials Level 1","title":"WebAuthn"},{"location":"RWoT6/did-auth/#openid-connect","text":"OpenID Connect (OIDC) is an authentication protocol built on the OAuth 2.0 protocol. In its most common web-based form, an end-user's user agent is redirected by a relying party (OAuth 2.0 client) to an OpenID Provider (OAuth 2.0 authorization server), which authenticates the end-user and redirects them back to the relying party . The architecture resembles DID Auth Architecture 5 in this paper. OpenID Connect is highly modular and customizable. It can potentially relate to DID Auth in the following ways:","title":"OpenID Connect"},{"location":"RWoT6/did-auth/#did-auth-as-oidc-local-authentication-method","text":"Within established OIDC deployments, DID Auth can serve as a drop-in \"local authentication method\", i.e. a method that determines how the OpenID Provider authenticates the end-user (to replace username and password). References: Authentication Context Class","title":"DID Auth as OIDC Local Authentication Method"},{"location":"RWoT6/did-auth/#did-auth-as-alternative-oidc-provider-discovery","text":"For use cases where the DID is known beforehand, DID resolution can serve as an alternative mechanism for OIDC provider discovery (alternative to OpenID Connect Discovery / WebFinger) via service endpoint. Example OpenID Connect service endpoint in DID Document: { \"service\": [{ \"id\": \"did:example:123456789abcdefghi;openid\", \"type\": \"OpenIdConnectVersion1.0Service\", \"serviceEndpoint\": \"https://openid.example.com/\" }] } References: OpenID Connect Discovery WebFinger","title":"DID Auth as Alternative OIDC Provider Discovery"},{"location":"RWoT6/did-auth/#did-auth-with-jwt-attributes-and-oidc-session-management","text":"As a format for the DID Auth challenge-response cycle, DID Auth could re-use the well-specified JWT attributes used in OAuth2 and OIDC (e.g. exp , iat , and others) as well as the concept of OIDC request objects where an authentication request is expressed as a self-contained JWT. In addition, although session management is not a focus of this document, DID Auth implementers might find the OIDC Session Management specification and the OIDC User Consent step helpful (in terms of establishing a long-lived session after the DID Auth ceremony). References: OIDC: Passing Request Parameters as JWTs IIW #26 Session Notes \"Open ID v. FIDO v. SSI\" IIW #26 Session Notes \"DID Auth Workflows (Part 2)\"","title":"DID Auth with JWT Attributes and OIDC Session Management"},{"location":"RWoT6/did-auth/#did-auth-as-token-endpoint-authentication-method","text":"As part of the OAuth2 / OIDC workflow, relying party clients must authenticate themselves to the authorization server (i.e. the token issuing endpoint) using a variety of methods. Since a PKI-based authentication method is already supported by OIDC (i.e. the private_key_jwt method), DID Auth can fulfill a role as a secure client-authentication mechanism during this step.","title":"DID Auth as Token Endpoint Authentication Method"},{"location":"RWoT6/did-auth/#security-and-privacy-considerations","text":"","title":"Security and Privacy Considerations"},{"location":"RWoT6/did-auth/#support-for-multiple-dids","text":"During a DID Auth interaction, a relying party may request that the identity owner 's user agent display a DID selection dialog where the identity owner is able to pick the DID to authenticate with to the relying party . The DID Auth challenge and response are then done using that DID. An identity owner 's user agent may also automatically choose (or even dynamically create) different DIDs for each relying party . This idea is also known as directed identity. If a different DID is used for each relying party , then they are called pairwise-pseudonymous DIDs. This measure is helpful (but not always sufficient) to avoid correlation of the identity owner . An identity owner may also choose to use different DIDs for the same relying party depending on context.","title":"Support for Multiple DIDs"},{"location":"RWoT6/did-auth/#automatic-authentication","text":"Depending on the challenge and response transports, and depending on an identity owner's user agent, it may be possible to automate some steps in a DID Auth interaction. For example, a DID Auth browser extension can automatically complete a DID Auth interaction to log in to a website using a single click on a button.","title":"Automatic Authentication"},{"location":"RWoT6/did-auth/#re-authentication-and-step-up","text":"Even after a DID Auth challenge-response cycle has been completed, a relying party may require an identity owner to re-authenticate in certain cases, e.g. when a high-value transaction such as transferring money is initiated. In such cases, subsequent protocol interactions may also require the exchange of Verifiable Credentials in addition to basic DID Auth, in order to establish the desired level of trust.","title":"Re-authentication and Step-up"},{"location":"RWoT6/did-auth/#did-resolution","text":"DID Auth depends on the ability to resolve a DID to its associated DID Document. Therefore all security considerations associated with DID resolution must be taken into account. Among other topics, this includes caching behavior of a DID resolver, as well as metadata about the DID resolution process (e.g. whether the DID Document has been retrieved via a blockchain full node or via an untrusted intermediary lookup service). References: DID Resolution v0.1","title":"DID Resolution"},{"location":"RWoT6/did-auth/#owner-vs-controller","text":"This paper heavily uses the term identity owner . This helps to emphasize clearly how DID Auth is fundamentally different from earlier authentication protocols, which have traditionally revolved around \"identity providers\". Note however that the identity owner (the entity identified by the DID) may in some situations not be the same entity who controls the DID and who is able to perform authentication. This may happen in cases of security breaches (e.g. theft of a private key that controls the DID), or in intentional situations such as digital guardianship, where for some reason the identity owner is not able to control their own DID.","title":"Owner vs. Controller"},{"location":"RWoT6/did-auth/#single-log-out","text":"DID Auth should provide ways to end all sessions and relationships that have been established for an authenticated DID. An identity owner should be able to trigger single log-out manually using their user agent. Single log-out should also happen automatically if a DID is revoked, which means that relying parties must monitor authenticated DIDs for revocation.","title":"Single Log-out"},{"location":"RWoT6/did-auth/#hardware-wallet","text":"The DID Auth challenge may be processed by a hardware wallet that upon an identity owner 's physical interaction will create a response and send it back to the relying party .","title":"Hardware Wallet"},{"location":"RWoT6/did-auth/#did-auth-gateway-services","text":"In order to ease adoption of DID Auth through the use of standard protocols, gateway services could be developed that expose the OpenID Connect and/or OAuth2 protocols and perform (parts of) a DID Auth relying party . In such cases, the privacy and trust implications of the use of a gateway service must be carefully considered.","title":"DID Auth gateway services"},{"location":"RWoT6/did-spec-1.0/","text":"Decentralized Identifiers v1.0 The Decentralized Identifiers specification editors and implementers spent some time at Rebooting the Web of Trust 6 processing the remaining issues in the issue tracker. This document summarizes the proposed resolutions that the group has put forward to resolve all of the DID specification issues that were submitted before 2018-03-05. DID Specification RWoT6 Resolutions Describe how guardianship and delegation is provided for by Object Capabilities Use 'proof' instead of 'signature' DID Documents MUST be fully formed (not in intermediate form) DID Documents MUST contain an 'id' property Cryptographic keys MUST contain an 'owner' property Service endpoints will be defined per DID Reconciliation Draft DIDs MUST start with 'did:' References to JOSE/JWS/JWK will be made in Linked Data Proof/Signature/Suite specs, not DID spec See #38 See #39 DID Documents do not require an encapsulating signature Authorization best practices will request that developers follow the Object Capability pattern Add crypto material search algorithm to spec Allow use of publicKeyAddress instead of URL for Ethereum Use Cases Spec will explain graph data model approach Cryptographic Suites will be non-normative, but explained in examples Biometrics will be pseudonymous and no material will be on blockchain Revoked keys will be allowed in DID Document Next Steps The group expects that work will continue during the W3C Credentials Community Group meetings over the next several months, with a plan to take the specification on the W3C Standards track by Q4 2018. The following work items will be needed to progress onto the W3C Standards track: DID Primer for W3C Members W3C DID Working Group Charter Proposal DID Use Cases (focus on 3-5 use cases) DID Specification Test Suite for DID Specification","title":"Decentralized Identifiers v1.0"},{"location":"RWoT6/did-spec-1.0/#decentralized-identifiers-v10","text":"The Decentralized Identifiers specification editors and implementers spent some time at Rebooting the Web of Trust 6 processing the remaining issues in the issue tracker. This document summarizes the proposed resolutions that the group has put forward to resolve all of the DID specification issues that were submitted before 2018-03-05.","title":"Decentralized Identifiers v1.0"},{"location":"RWoT6/did-spec-1.0/#did-specification-rwot6-resolutions","text":"Describe how guardianship and delegation is provided for by Object Capabilities Use 'proof' instead of 'signature' DID Documents MUST be fully formed (not in intermediate form) DID Documents MUST contain an 'id' property Cryptographic keys MUST contain an 'owner' property Service endpoints will be defined per DID Reconciliation Draft DIDs MUST start with 'did:' References to JOSE/JWS/JWK will be made in Linked Data Proof/Signature/Suite specs, not DID spec See #38 See #39 DID Documents do not require an encapsulating signature Authorization best practices will request that developers follow the Object Capability pattern Add crypto material search algorithm to spec Allow use of publicKeyAddress instead of URL for Ethereum Use Cases Spec will explain graph data model approach Cryptographic Suites will be non-normative, but explained in examples Biometrics will be pseudonymous and no material will be on blockchain Revoked keys will be allowed in DID Document","title":"DID Specification RWoT6 Resolutions"},{"location":"RWoT6/did-spec-1.0/#next-steps","text":"The group expects that work will continue during the W3C Credentials Community Group meetings over the next several months, with a plan to take the specification on the W3C Standards track by Q4 2018. The following work items will be needed to progress onto the W3C Standards track: DID Primer for W3C Members W3C DID Working Group Charter Proposal DID Use Cases (focus on 3-5 use cases) DID Specification Test Suite for DID Specification","title":"Next Steps"},{"location":"RWoT6/identity-hub-attestations/","text":"Identity Hub Attestation Flows and Components Contributors: Daniel Buchner - Daniel.Buchner@microsoft.com Cherie Duncan - Cherie.Duncan@dominode.com John Toohey - john.toohey@dominode.com Ron Kreutzer - ron@pillarproject.io Stephen Curran - swcurran@cloudcompass.ca Abstract In this document, we define a set of user flows and describe the associated Action Objects that support a Hub-centric approach to the request, issuance, presentation, verification, and revocation of interoperable attestations. This document extends the Identity Hub Explainer . 1 Introduction In the digital identity space, Hubs let you securely store and share data. A Hub is a datastore containing semantic data objects at well-known locations. An identity needs to be able to prove that some data is true to another entity that requests it. These attestations are that method of proof. In the digital world, the requester may be software, and the response may or may not require involvement of the individual/identity who the proof is being made against. These examples and flows depict how attestations are requested and resolved. 2 Example Use Cases We use examples here to give guidance/suggestions for how attestations can be used with real-world examples. The overall use case is a person, Alice, who registers for College using a process that includes using an attestation she possesses to prove she has received some required immunizations. After graduation, Alice requests an attestation from the College that she has graduated, and presents that attestation to her professional profile on a professional network. Agents We use the term \u201cUser Agent\u201d (UA) to refer to an app on a smartphone or other device that has access to DID-linked keys and the power to do things on behalf of a DID owner (Alice). This could also be referred to as a digital wallet. Similarly, we use the term \u201cEnterprise Agent\u201d (EA) to refer to the comparable component representing an Organization \u2013 e.g. a College or professional network. A UA and EA are conceptually the same, but while the UA is likely a personal device (laptop, tablet, phone), an EA is likely a service that processes requests based on business rules and data held in back-end systems. Note that an EA might need input from a specific member of the organization to complete the processing of a request. In that case, the EA might contact that user through that person\u2019s User Agent (although there are many other possibilities). Sites In the examples below, \u201cSites\u201d are assumed to be Web or Mobile Site \u2013 user interfaces that allow a user (in our case, Alice) to trigger the start of a process. There are many other ways to trigger the start of such a process. Decentralized IDs (DIDs), Documents and Attestations Each of Alice's Decentralized Identifiers (DIDs) referenced in the scenarios is generated and held by her user agent (UA) and used for a specific purpose - for example her relationship with the College. Her DIDs are not necessarily correlated to any other identifiers that make up her identity. Per the W3C DID Specification , a DID Document is associated with a DID that contains information about the public keys and service endpoints for that DID. Thus, given a DID and DID Document for another Identity, an entity has a mechanism to resolve and communicate with the Identity Owner of the DID. DIDs may be public and stored on a publicly available Distributed Ledger, with their associated DID Document found via the DIF Universal Resolver , or may be pairwise private DIDs, where two Identities directly exchange DIDs/DID Documents. An attestation is something (such as a Verifiable Credential ) issued by an entity to a holder (often the subject of the attestation) so that the holder can prove to others that they hold the attestation. In one of the examples below, for instance, Alice wants to receive a graduation attestation from the College so that she can present (prove) that attestation to a professional network. Interface Guidelines: Hubs, Agents and Identity Owners Some basic guidelines are defined about Hubs, Agents, and their Identity Owners: Private keys are accessible only to Agents (User and Enterprise), thus any encrypting/signing of information must be done by an Agent. In general, Hubs are addressable using the service pointers located in a DID Document, and Agents are addressed via a user's Hub. The only exception is invocation of a User Agent through direct mechanisms, like a deep link on a mobile site, a QR code on a Web site scanned by a User Agent, or a Bluetooth/NFC data exchange. Hubs generally have limited, generic functionality, and any decision making must be made at the Agent level via a user app/device (User Agents) or more automated business services (Enterprise Agents). For simplicity, we show the Hub and Enterprise Agent as a single entity in the following scenarios. In typical implementations, they will be separate entities that communicate to accomplish their respective activities. 2.1 Alice Links to an Entity In order to communicate a request for attestation to an entity (in our examples, Alice), a user will first need to establish a connection between her user agent and the entity she will interact with. This is necessary for all follow-on scenarios. Alice wants to transact with the entities described in the scenarios with the intent to receive or exchange attestations. First and foremost, the entity must verify that Alice is the owner of the decentralized identifier she claims. In order to find Alice\u2019s user agent, we leverage the Universal Resolver (UR) to lookup Alice\u2019s Decentralized Identifier (DID) to find her DID Document (DDO). The keys located in Alice's DDO are used to authenticate Alice\u2019s ownership of the DID and to determine access to Alice\u2019s hub and user agent. participant Universal Resolver as UR participant Entity Hub / EA as EH participant Entity Site as ES participant Alice UA as AUA participant Alice Hub as AH AUA-->ES: 1 Initiates DID Linkage ES-->AUA: 2 Prompts to disclose DID AUA-->ES: 3 Discloses DID ES-->EH: 4 Relay DID EH-->UR: 5 Lookup DID UR-->EH: 6 Return DDO EH-->AH: 7 DID Auth Challenge AH-->AUA: 8 DID Auth Challenge AUA-->EH: 9 DID Auth Response EH-->ES: 10 DID Auth Complete Alice navigates to an entity\u2019s website and clicks a link to initiate a DID linkage with the entity. The content received from clicking the link includes DID information about the Enitity that Alice should use for the relationship. Alice may have to use the Universal Resolver to access the DID Document associated with the DID. The entity prompts for Alice to disclose a DID that represents her digital identity. If the website was accessed via a laptop/desktop, the website typically displays a QR Code, and Alice uses her mobile wallet app to scan the QR. If the website was accessed via her mobile device, a protocol handler raises Alice's AU app. Alice selects an existing DID or creates a new DID for this relationship and sends the DID to the Entity Site. The Entity Site passes the DID to the Entity\u2019s Enterprise Agent to initiate the DID Auth response. The EA uses the Universal Resolver (UR) to request retrieval of the DID Document that matches the provided DID. The DID Document is returned to the EA. The EA initiates the DID Auth process by issuing a challenge to Alice\u2019s Hub. Alice\u2019s Hub passes the DID Auth challenge to Alice\u2019s User Agent for signing. Alice\u2019s User Agent proves her identity with a signed response to the auth challenge. The Entity Hub confirms the response and notifies the Entity Site with a successful login. 2.1.1 DIF Identity Hub 2FA A second identity linking scenario to consider is when Alice is registering with the site using a device that is not a UA, yet she still wants to use her UA to establish the connection. In this case, Alice discloses a DID connected to her UA to the site, the site contacts the UA and the mobile device containing the UA displays a code for Alice to use. Alice enters the code into a form on the site, proving that she controls the DID. participant Universal Resolver as UR participant Entity Site as ES participant Alice Laptop as AD participant Alice Mobile UA as AM participant Alice Hub as AH AD-->ES: 1 Initiates DID Linkage ES-->AD: 2 Prompts to disclose DID AD-->ES: 3 Discloses DID ES-->UR: 4 Lookup DID UR-->ES: 5 Return DDO ES-->AH: 6 DID Auth challenge AH-->AM: 7 Challenge code displayed AD-->ES: 8 Enters challenge code Alice navigates to an entity\u2019s website and clicks a link to initiate a DID linkage with the entity. The entity prompts for Alice to disclose a DID that represents her digital identity. Alice selects an existing DID and sends the DID to the Entity Site. The Entity/EA uses the Universal Resolver (UR) to request retrieval of the DID Document that matches the provided DID. The DID Document is returned to the Entity/EA. The EA initiates the DID Auth process by issuing a challenge to Alice\u2019s Hub. Alice\u2019s Hub passes the DID Auth challenge to Alice\u2019s User Agent for signing. Alice\u2019s User Agent processes the challenge and displays a code expected by the Entity Site on the mobile device. Alice enters the code on her laptop and the Entity Site confirms the response, resulting in a successful login. 2.2 Alice Must Provide Preconditional Proof Alice is attempting to register for college and her DID is already linked to the College. In this example, for Alice to get admitted to the College, she must prove that she previously has received appropriate immunizations. Assumptions Alice is linked to the College via her DID. Alice has an Identity Hub accessed via an application on her mobile device. Alice has a verified digital attestation for her previous immunizations. participant College Hub / EA as CH participant College Site as CS participant Alice UA as AUA participant Alice Hub as AH AUA-->CS: 1 Triggers registration attestation request CS-->AH: 2 Action: PreconditionsAttestationAction (Immunizations) AH-->AUA: 3 Prompts with preconditions AUA-->CH: 4 Action: PresentAttestationAction (Immunizations) CH-->AH: 5 Action: DeliverAttestationAction (Registered) AH-->AUA: 6 Accepts Attestation? AUA-->AH: 7 Stores approved attestation Alice initiates a Registration request on the College Site. The College EA determines there are preconditions for Registration: she must prove she has the required immunizations. The College EA initiates a request for presentation of the preconditions. Alice is prompted by her UA to provide the preconditions. Alice selects the correct attestation to use and her UA sends them back to the College Hub. The College EA processes the preconditions and sends a Registered Student attestation to Alice's Hub. Alice accepts the request to accept/store the college registration attestation. Alice\u2019s Hub stores the Registered Student attestation and broadcasts it to her connected devices. Referenced Action Objects PreconditionsAttestationAction PresentAttestationAction DeliverAttestationAction 2.3 Alice Obtains a Diploma Attestation In this example, Alice has graduated from college and wants to acquire a digital diploma attestation. Assumptions Alice is linked to the College via her DID. Alice has an Identity Hub accessed via an application on her mobile device. Alice has graduated from College. participant College Hub / EA as CH participant College Site as CS participant Alice UA as AUA participant Alice Hub as AH AUA-->CS: 1 Triggers attestation request CS-->CH: 2 Determines available attestations CH-->AH: 3 Action: OfferAttestationAction AH-->AUA: 4 Selects from offered attestations AUA-->CH: 5 Action: RequestAttestationAction CH-->AH: 6 Action: DeliverAttestationAction AH-->AUA: 7 Accepts Attestation? AUA-->AH: 8 Approved for storage Alice initiates a request through the College website to obtain an attestation regarding her graduation. College website reaches out to its Enterprise Agent service to determine what attestations are available for Alice. The College's EA sends an attestation offer to Alice\u2019s Hub. Alice's UA receives a Action from the College EA that contains the attestations it can provide. Alice selects the attestations she wants. Alice's UA sends an attestation request for her selected attestations to the College's Hub. The EA delivers the attestations to Alice's Hub. Alice is prompted to accept or deny the attestation. Alice accepts the attestation and stores it across her Hubs and devices. Referenced Action Objects OfferAttestationAction RequestAttestationAction DeliverAttestationAction 2.4 Alice Shares Her Education Verification, and Future Updates, with a Professional Networking Site Alice has graduated from college, possesses an attestation from the College, and wants to share her existing and future education attestations with a professional networking site. Assumptions The site has linked Alice to her DID via DID Auth. Alice has an Identity Hub, accessible via an app on her mobile device. Alice possesses an attestation for her college diploma. participant Prof. Hub / EA as LH participant Prof. Site as LS participant Alice UA as AUA participant Alice Hub as AH AUA-->LS: 1 Triggers permission request LS-->AH: 2 Action: RequestPermissionAction AH-->AUA: 3 Prompts to grant permission AUA-->AH: 4 Generates permission and keys AH-->LH: 5 Action: GrantPermissionAction LH-->LS: 6 Notice of permission grant LS-->AH: 7 RetrieveAttestationsAction Alice navigates to the professional network site and initiates the flow to grant access to her educational attestations. The website sends a RequestPermissionAction to Alice\u2019s Hub. Alice\u2019s Hub relays the request to Alice's UA, which prompts her to grant/deny permission. Alice grants permission to access her current and future educational attestations by pushing a signed permission object and DID-specific keys to her Hub. Alice\u2019s Hub stores the keys she generated for the professional networking site and relays an Action to the professional network's Hub to provide notice that their permission request has been granted. The professional networking site is notified that the permission has been granted. At any time in the future, the professional networking site can retrieve Alice\u2019s education credentials from Alice\u2019s Hub, based on the permissions she provided and using the private key held by the professional networking site. Should the permission later be removed, the Prof Site's ability to retrieve updated credentials will be removed. Referenced Action Objects RequestPermissionAction GrantPermissionAction RetrieveAttestationsAction 2.5 Alice Applies for a Job and Refuses to Provide References Alice is applying for a job and has connected with the HR department via her DID. Alice has already provided some basic attestations about her right to work, name, address, etc. But when she receives a request for her references Alice refuses/denies the request as by the time this request comes in Alice has already accepted a position somewhere else (for example). Assumptions Alice is linked to Company\u2019s HR via her DID. Alice has an Identity Hub accessed via an application on her mobile device. Alice has a verified digital attestation for her references but does not wish to share them at this time, or Alice does not have references in her digital wallet yet. participant HR Hub / EA as LH participant HR Site as LS participant Alice UA as AUA participant Alice Hub as AH LS-->AH: 1 Action: PreconditionsAttestationAction (References) AH-->AUA: 2 Prompts for reference disclosure AUA->AH: 3 Refuses reference disclosure AH-->LH: 4 Action: DenyAttestationAction (Reason) LH-->LS: 5 Notice of refusal HR initiates a request for references via Alice\u2019s Hub. Alice's Hub finds appropriate Attestations and provides them to Alice's User Agent. For whatever reason, Alice refuses (via her agent) to provide references at this time. Alice\u2019s Hub notifies HR Hub of the refusal, with optional reason for refusal. A notification is sent to HR with the refusal details (generic or specific to the scenario). Referenced Action Objects PreconditionAttestationAction DenyAttestionAction 2.6 A Bank Sends Alice a Contract that Requires her DID signature, which She Signs and Delivers Back to the Bank. participant App Hub / EA as LH participant App Site as LS participant Alice UA as AUA participant Alice Hub as AH LS-->AH: 1 Action: SignAttestationAction (Contract) AH-->AUA: 2 Prompt for signature AUA-->LH: 3 Action: DeliverAttestationAction (Contract) LH-->LS: 4 Notice of signing Referenced Action Objects SignAttestionAction DeliverAttestationAction 2.7 The College Determines Alice Was Issued a Nursing Certificate Instead of Her CS diploma, so They Revoke the Attestation and Issue the Correct One. participant App Hub / EA as LH participant App Site as LS participant Alice UA as AUA participant Alice Hub as AH LS-->AH: 1 Action: RevokeAttestationAction (Old Diploma) AH-->AUA: 2 Notice of revocation LS-->AH: 3 Action: DeliverAttestationAction (New Diploma) AH-->AUA: 4 Accept Attestation? AUA-->AH: 5 Store accepted attestation Referenced Action Objects RevokeAttestationAction DeliverAttestationAction 2.8 Alice Retracts Data Access Permission from a Professional Networking Site. participant Prof. Hub / EA as PH participant Prof. Site as PS participant Alice UA as AUA participant Alice Hub as AH AUA-->AH: 1 Remove Prof. permissions AH-->PH: 2 Action: RetractPermissionAction PH-->PS: 3 Notice of retraction request PS-->PH: 4 Signed reciept of retraction request PH-->AH: 5 Signed reciept response Referenced Action Objects RetractPermissionAction 2.9 Alice's College Discovers they Made a Mistake on her Diploma Attestation, and Sends her an Amended Attestation with the Correct Info. participant College Site as CS participant Alice UA as AUA participant Alice Hub as AH CS-->AH: 1 Action: AmendAttestionAction (New + Old reference) AH-->AUA: 2 Accept Attestation? AUA-->AH: 3 Replace old with new attestation Referenced Action Objects AmendAttestionAction 3 Action Objects Identity Hub attestation handling relies on the passage and recognition of common Action types that Hubs, User Agents, and consuming apps/services understand. In order to ensure that the flows related to attestations are precise and maximally descriptive of their intent, the Identity Hub spec will define its own Action objects for each of the relevant attestation actions. These objects are extensions of the Schema.org Action object, the schema origin of which shall be schema.identity.foundation. These objects are strictly a shared means of communicating and facilitating the various activities related to attestations; they do not infer or require a specific type of proof format or material be used within them. Note that each Action returns only a status of whether the Action was successfully (or not) transmitted. The result of processing the request is conveyed to the caller via a subsequent Action. The following is a description of the objects and examples that encompass their structure and properties: 3.1 RequestAttestationAction The Holder requests an attestation from an Issuer. Type of attestation wanted List of tag strings to describe the attestation Detailed, human-readable description of the attestation being requested (mostly for UAs to display to users) Who is the attestation for? What format do you need it in? Enable passing of preconditions Option to set a deadline for issuance/fulfillment { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RequestAttestationAction\", \"identifier\": UNIQUE_ID, \"for\": [\"did:foo:123-456\"], \"format\": CLAIM_FORMAT, \"expiration\": EPOCH_TIME, \"description\": \"Province of British Columbia Driver\u2019s License\", \"tags\": [\"license\", \"driving\", \"permit\", \"DL\", \"driver\u2019s license\"], \"preconditions\": ARRAY_OF_PRECONDITION_PROOFS (optional) } 3.2 DenyAttestationAction In response to a request for an Attestation, a Verifier/Issuer informs a Holder that the attestation cannot be provided. This Action inherits from schema.org's RejectAction . Linked attestation action ID Reason for refusing the Request Attestation Action. { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"DenyAttestationAction\", \"identifier\": UNIQUE_ID, \"purpose\": \"We cannot issue your diploma, you have not graduated.\" } 3.3 PreconditionsAttestationAction In response to a request for an Attestation, a Verifier/Issuer informs a Holder a list of Pre-Conditions that must be met before the requested Attestation can be issued. Linked attestation action ID Specify set of preconditions, each with their own descriptors { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"PreconditionsAttestationAction\", \"identifier\": UNIQUE_ID, \"preconditions\": ARRAY_OF_PRECONDITION_DESCRIPTORS } 3.4 OfferAttestationAction In response to a request for an Attestation that cannot be issued because that type is not available, provide to the Holder a list of attestations that ARE available. For each attestation type available to the requester: Type of attestation List of tag strings to describe the attestation Detailed, human-readable description of the attestation being requested (mostly for UAs to display to users) Formats available for the attestation { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"OfferAttestationAction\", \"identifier\": UNIQUE_ID, \"availableAttestations\": ARRAY_OF_ATTESTATION_DESCRIPTORS } 3.5 DeliverAttestationAction Used by any party that delivers a finalized attestation to a target entity. This Action inherits from schema.org's SendAction . Linked attestation action ID Payload of the proof material Format of the proof material Time delivered { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"DeliverAttestationAction\", \"identifier\": \"UNIQUE_ID\", \"object\": ATTESTATION_PAYLOAD, \"description\": \"Province of British Columbia Driver\u2019s License\", \"tags\": [\"license\", \"driving\", \"permit\", \"DL\", \"driver\u2019s license\"] } 3.6 PresentAttestationAction This Action is the envelop used to present an attestation to an inspecting party. List of tag strings to describe the attestation Detailed, human-readable description of the attestation being requested (mostly for UAs and EAs to reason over and use in display) Format of the attestation payload The attestation payload { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"PresentAttestationAction\", \"object\": ATTESTATION_PAYLOAD, \"description\": \"MIT Diploma for B.S. in Computer Science\", \"tags\": [\"diploma\", \"degree\"] } 3.7 SignAttestationAction A party sends a Action to a target prompting them to sign the provided attestation payload. This Action inherits from schema.org's EndorseAction . Linked attestation action ID Payload of the proof material Format of the proof material Time delivered { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"SignAttestationAction\", \"identifier\": UNIQUE_ID, \"object\": ATTESTATION_PAYLOAD, \"description\": \"Loan for 123 Main Street, Anytown USA\" } 3.8 RevokeAttestationAction The party that previously supplied an attestation sends a notice to the attestation owner/holder that issuing party has revoked the attestation. This Action inherits from schema.org's DeactivateAction . Attestation ID Revocation code - array of revocation codes (look for an existing standard) Reason for revocation - array of human-readable descriptions of the reason, or URI { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RevokeAttestationAction\", \"identifier\": UNIQUE_ID, \"object\": ATTESTATION_PAYLOAD, \"result\": REVOCATION_RECORD, \"purpose\": \"Your driver's license was revoked.\" } 3.9 AmendAttestationAction Used to update an attestation. Requires past ID, optionally including previous attestation. This Action inherits from schema.org's ReplaceAction . Attestation ID Change delta of some kind Reason for amendment - array of human-readable descriptions of the reason, or URI { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"AmendAttestationAction\", \"identifier\": UNIQUE_ID, \"object\": ATTESTATION_PAYLOAD, \"purpose\": \"Your driver's license was amended with your latest picture\" } 3.10 RequestPermissionAction Request permission for access to a DID's Identity Hub data. This Action inherits from schema.org's AuthorizeAction . Permission being requested Intended use of data being requested { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RequestPermissionAction\", \"object\": PERMISSION_OCAP, \"purpose\": \"Display and filtering on a professional network\", } 3.11 GrantPermissionAction The party that allows a permission sends a notice to the requesting party to let them know the permission has been granted. This Action inherits from schema.org's AcceptAction . Permission being requested Intended use of data being requested { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"GrantPermissionAction\", \"object\": PERMISSION_OCAP } 3.12 DenyPermissionAction The party evaluating the permission request does not grant the permission and sends the requesting party a notice of the denial. This Action inherits from schema.org's RejectAction . There is not currently an example of this action in the scenarios in Section 2 of this document. Permission being requested Intended use of data being requested { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"DenyPermissionAction\", \"object\": PERMISSION_OCAP, \"purpose\": \"I do not want to allow you access at this time\", } 3.13 RetractPermissionAction The party that has previously issued a permission granting access sends a notice to the affected party to let them know the permission has been retracted. This Action inherits from schema.org's DeleteAction . Permission being retracted { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RetractPermissionAction\", \"object\": PERMISSION_OCAP, \"purpose\": \"I no longer want you to have access to my attestations\", } 3.14 RetrieveAttestationsAction Used by any party that has been granted permission access to a set of Attestations via the GrantPermissionAction to retrieve a set of Attestations. { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RetrieveAttestationsAction\", \"identifier\": \"UNIQUE_ID\", \"object\": ATTESTATION_PAYLOAD, \"description\": \"Province of British Columbia Driver\u2019s License\", \"tags\": [\"license\", \"driving\", \"permit\", \"DL\", \"driver\u2019s license\"] } 4 Glossary Decentralized Identifier : Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, \"self-sovereign\" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. DID : Decentralized Identifier DID Auth : Authentication of an Identity by verifying the Identity's control of its DID DID Document : The control document that specifies keys, service endpoints, and other basic details about a DID. DDO : Abbreviation for a DID Document EA : Enterprise Agent: a HUB-aware service that integrates with an Enterprise\u2019s backend systems and representatives to process HUB requests. Conceptually equivalent to a person\u2019s UA, but for an organization. UA : Abbreviation for User Agent Universal Resolver : A mechanism of getting the DID Document associated with a DID across any (supported) DID implementation UR : Abbreviation for Universal Resolver User Agent : a smartphone-based digital wallet, browser 5 Technical & Spec Implications For the Hub /permission spec: add optional timeout for permissions","title":"Identity Hub Attestation Flows and Components"},{"location":"RWoT6/identity-hub-attestations/#identity-hub-attestation-flows-and-components","text":"Contributors: Daniel Buchner - Daniel.Buchner@microsoft.com Cherie Duncan - Cherie.Duncan@dominode.com John Toohey - john.toohey@dominode.com Ron Kreutzer - ron@pillarproject.io Stephen Curran - swcurran@cloudcompass.ca","title":"Identity Hub Attestation Flows and Components"},{"location":"RWoT6/identity-hub-attestations/#abstract","text":"In this document, we define a set of user flows and describe the associated Action Objects that support a Hub-centric approach to the request, issuance, presentation, verification, and revocation of interoperable attestations. This document extends the Identity Hub Explainer .","title":"Abstract"},{"location":"RWoT6/identity-hub-attestations/#1-introduction","text":"In the digital identity space, Hubs let you securely store and share data. A Hub is a datastore containing semantic data objects at well-known locations. An identity needs to be able to prove that some data is true to another entity that requests it. These attestations are that method of proof. In the digital world, the requester may be software, and the response may or may not require involvement of the individual/identity who the proof is being made against. These examples and flows depict how attestations are requested and resolved.","title":"1\u00a0Introduction"},{"location":"RWoT6/identity-hub-attestations/#2-example-use-cases","text":"We use examples here to give guidance/suggestions for how attestations can be used with real-world examples. The overall use case is a person, Alice, who registers for College using a process that includes using an attestation she possesses to prove she has received some required immunizations. After graduation, Alice requests an attestation from the College that she has graduated, and presents that attestation to her professional profile on a professional network.","title":"2\u00a0Example Use Cases"},{"location":"RWoT6/identity-hub-attestations/#agents","text":"We use the term \u201cUser Agent\u201d (UA) to refer to an app on a smartphone or other device that has access to DID-linked keys and the power to do things on behalf of a DID owner (Alice). This could also be referred to as a digital wallet. Similarly, we use the term \u201cEnterprise Agent\u201d (EA) to refer to the comparable component representing an Organization \u2013 e.g. a College or professional network. A UA and EA are conceptually the same, but while the UA is likely a personal device (laptop, tablet, phone), an EA is likely a service that processes requests based on business rules and data held in back-end systems. Note that an EA might need input from a specific member of the organization to complete the processing of a request. In that case, the EA might contact that user through that person\u2019s User Agent (although there are many other possibilities).","title":"Agents"},{"location":"RWoT6/identity-hub-attestations/#sites","text":"In the examples below, \u201cSites\u201d are assumed to be Web or Mobile Site \u2013 user interfaces that allow a user (in our case, Alice) to trigger the start of a process. There are many other ways to trigger the start of such a process.","title":"Sites"},{"location":"RWoT6/identity-hub-attestations/#decentralized-ids-dids-documents-and-attestations","text":"Each of Alice's Decentralized Identifiers (DIDs) referenced in the scenarios is generated and held by her user agent (UA) and used for a specific purpose - for example her relationship with the College. Her DIDs are not necessarily correlated to any other identifiers that make up her identity. Per the W3C DID Specification , a DID Document is associated with a DID that contains information about the public keys and service endpoints for that DID. Thus, given a DID and DID Document for another Identity, an entity has a mechanism to resolve and communicate with the Identity Owner of the DID. DIDs may be public and stored on a publicly available Distributed Ledger, with their associated DID Document found via the DIF Universal Resolver , or may be pairwise private DIDs, where two Identities directly exchange DIDs/DID Documents. An attestation is something (such as a Verifiable Credential ) issued by an entity to a holder (often the subject of the attestation) so that the holder can prove to others that they hold the attestation. In one of the examples below, for instance, Alice wants to receive a graduation attestation from the College so that she can present (prove) that attestation to a professional network.","title":"Decentralized IDs (DIDs), Documents and Attestations"},{"location":"RWoT6/identity-hub-attestations/#interface-guidelines-hubs-agents-and-identity-owners","text":"Some basic guidelines are defined about Hubs, Agents, and their Identity Owners: Private keys are accessible only to Agents (User and Enterprise), thus any encrypting/signing of information must be done by an Agent. In general, Hubs are addressable using the service pointers located in a DID Document, and Agents are addressed via a user's Hub. The only exception is invocation of a User Agent through direct mechanisms, like a deep link on a mobile site, a QR code on a Web site scanned by a User Agent, or a Bluetooth/NFC data exchange. Hubs generally have limited, generic functionality, and any decision making must be made at the Agent level via a user app/device (User Agents) or more automated business services (Enterprise Agents). For simplicity, we show the Hub and Enterprise Agent as a single entity in the following scenarios. In typical implementations, they will be separate entities that communicate to accomplish their respective activities.","title":"Interface Guidelines: Hubs, Agents and Identity Owners"},{"location":"RWoT6/identity-hub-attestations/#21-alice-links-to-an-entity","text":"In order to communicate a request for attestation to an entity (in our examples, Alice), a user will first need to establish a connection between her user agent and the entity she will interact with. This is necessary for all follow-on scenarios. Alice wants to transact with the entities described in the scenarios with the intent to receive or exchange attestations. First and foremost, the entity must verify that Alice is the owner of the decentralized identifier she claims. In order to find Alice\u2019s user agent, we leverage the Universal Resolver (UR) to lookup Alice\u2019s Decentralized Identifier (DID) to find her DID Document (DDO). The keys located in Alice's DDO are used to authenticate Alice\u2019s ownership of the DID and to determine access to Alice\u2019s hub and user agent. participant Universal Resolver as UR participant Entity Hub / EA as EH participant Entity Site as ES participant Alice UA as AUA participant Alice Hub as AH AUA-->ES: 1 Initiates DID Linkage ES-->AUA: 2 Prompts to disclose DID AUA-->ES: 3 Discloses DID ES-->EH: 4 Relay DID EH-->UR: 5 Lookup DID UR-->EH: 6 Return DDO EH-->AH: 7 DID Auth Challenge AH-->AUA: 8 DID Auth Challenge AUA-->EH: 9 DID Auth Response EH-->ES: 10 DID Auth Complete Alice navigates to an entity\u2019s website and clicks a link to initiate a DID linkage with the entity. The content received from clicking the link includes DID information about the Enitity that Alice should use for the relationship. Alice may have to use the Universal Resolver to access the DID Document associated with the DID. The entity prompts for Alice to disclose a DID that represents her digital identity. If the website was accessed via a laptop/desktop, the website typically displays a QR Code, and Alice uses her mobile wallet app to scan the QR. If the website was accessed via her mobile device, a protocol handler raises Alice's AU app. Alice selects an existing DID or creates a new DID for this relationship and sends the DID to the Entity Site. The Entity Site passes the DID to the Entity\u2019s Enterprise Agent to initiate the DID Auth response. The EA uses the Universal Resolver (UR) to request retrieval of the DID Document that matches the provided DID. The DID Document is returned to the EA. The EA initiates the DID Auth process by issuing a challenge to Alice\u2019s Hub. Alice\u2019s Hub passes the DID Auth challenge to Alice\u2019s User Agent for signing. Alice\u2019s User Agent proves her identity with a signed response to the auth challenge. The Entity Hub confirms the response and notifies the Entity Site with a successful login.","title":"2.1\u00a0Alice Links to an Entity"},{"location":"RWoT6/identity-hub-attestations/#211-dif-identity-hub-2fa","text":"A second identity linking scenario to consider is when Alice is registering with the site using a device that is not a UA, yet she still wants to use her UA to establish the connection. In this case, Alice discloses a DID connected to her UA to the site, the site contacts the UA and the mobile device containing the UA displays a code for Alice to use. Alice enters the code into a form on the site, proving that she controls the DID. participant Universal Resolver as UR participant Entity Site as ES participant Alice Laptop as AD participant Alice Mobile UA as AM participant Alice Hub as AH AD-->ES: 1 Initiates DID Linkage ES-->AD: 2 Prompts to disclose DID AD-->ES: 3 Discloses DID ES-->UR: 4 Lookup DID UR-->ES: 5 Return DDO ES-->AH: 6 DID Auth challenge AH-->AM: 7 Challenge code displayed AD-->ES: 8 Enters challenge code Alice navigates to an entity\u2019s website and clicks a link to initiate a DID linkage with the entity. The entity prompts for Alice to disclose a DID that represents her digital identity. Alice selects an existing DID and sends the DID to the Entity Site. The Entity/EA uses the Universal Resolver (UR) to request retrieval of the DID Document that matches the provided DID. The DID Document is returned to the Entity/EA. The EA initiates the DID Auth process by issuing a challenge to Alice\u2019s Hub. Alice\u2019s Hub passes the DID Auth challenge to Alice\u2019s User Agent for signing. Alice\u2019s User Agent processes the challenge and displays a code expected by the Entity Site on the mobile device. Alice enters the code on her laptop and the Entity Site confirms the response, resulting in a successful login.","title":"2.1.1\u00a0DIF Identity Hub 2FA"},{"location":"RWoT6/identity-hub-attestations/#22-alice-must-provide-preconditional-proof","text":"Alice is attempting to register for college and her DID is already linked to the College. In this example, for Alice to get admitted to the College, she must prove that she previously has received appropriate immunizations.","title":"2.2\u00a0Alice Must Provide Preconditional Proof"},{"location":"RWoT6/identity-hub-attestations/#assumptions","text":"Alice is linked to the College via her DID. Alice has an Identity Hub accessed via an application on her mobile device. Alice has a verified digital attestation for her previous immunizations. participant College Hub / EA as CH participant College Site as CS participant Alice UA as AUA participant Alice Hub as AH AUA-->CS: 1 Triggers registration attestation request CS-->AH: 2 Action: PreconditionsAttestationAction (Immunizations) AH-->AUA: 3 Prompts with preconditions AUA-->CH: 4 Action: PresentAttestationAction (Immunizations) CH-->AH: 5 Action: DeliverAttestationAction (Registered) AH-->AUA: 6 Accepts Attestation? AUA-->AH: 7 Stores approved attestation Alice initiates a Registration request on the College Site. The College EA determines there are preconditions for Registration: she must prove she has the required immunizations. The College EA initiates a request for presentation of the preconditions. Alice is prompted by her UA to provide the preconditions. Alice selects the correct attestation to use and her UA sends them back to the College Hub. The College EA processes the preconditions and sends a Registered Student attestation to Alice's Hub. Alice accepts the request to accept/store the college registration attestation. Alice\u2019s Hub stores the Registered Student attestation and broadcasts it to her connected devices.","title":"Assumptions"},{"location":"RWoT6/identity-hub-attestations/#referenced-action-objects","text":"PreconditionsAttestationAction PresentAttestationAction DeliverAttestationAction","title":"Referenced Action Objects"},{"location":"RWoT6/identity-hub-attestations/#23-alice-obtains-a-diploma-attestation","text":"In this example, Alice has graduated from college and wants to acquire a digital diploma attestation.","title":"2.3\u00a0Alice Obtains a Diploma Attestation"},{"location":"RWoT6/identity-hub-attestations/#assumptions_1","text":"Alice is linked to the College via her DID. Alice has an Identity Hub accessed via an application on her mobile device. Alice has graduated from College. participant College Hub / EA as CH participant College Site as CS participant Alice UA as AUA participant Alice Hub as AH AUA-->CS: 1 Triggers attestation request CS-->CH: 2 Determines available attestations CH-->AH: 3 Action: OfferAttestationAction AH-->AUA: 4 Selects from offered attestations AUA-->CH: 5 Action: RequestAttestationAction CH-->AH: 6 Action: DeliverAttestationAction AH-->AUA: 7 Accepts Attestation? AUA-->AH: 8 Approved for storage Alice initiates a request through the College website to obtain an attestation regarding her graduation. College website reaches out to its Enterprise Agent service to determine what attestations are available for Alice. The College's EA sends an attestation offer to Alice\u2019s Hub. Alice's UA receives a Action from the College EA that contains the attestations it can provide. Alice selects the attestations she wants. Alice's UA sends an attestation request for her selected attestations to the College's Hub. The EA delivers the attestations to Alice's Hub. Alice is prompted to accept or deny the attestation. Alice accepts the attestation and stores it across her Hubs and devices. Referenced Action Objects OfferAttestationAction RequestAttestationAction DeliverAttestationAction","title":"Assumptions"},{"location":"RWoT6/identity-hub-attestations/#24-alice-shares-her-education-verification-and-future-updates-with-a-professional-networking-site","text":"Alice has graduated from college, possesses an attestation from the College, and wants to share her existing and future education attestations with a professional networking site.","title":"2.4\u00a0Alice Shares Her Education Verification, and Future Updates, with a Professional Networking Site"},{"location":"RWoT6/identity-hub-attestations/#assumptions_2","text":"The site has linked Alice to her DID via DID Auth. Alice has an Identity Hub, accessible via an app on her mobile device. Alice possesses an attestation for her college diploma. participant Prof. Hub / EA as LH participant Prof. Site as LS participant Alice UA as AUA participant Alice Hub as AH AUA-->LS: 1 Triggers permission request LS-->AH: 2 Action: RequestPermissionAction AH-->AUA: 3 Prompts to grant permission AUA-->AH: 4 Generates permission and keys AH-->LH: 5 Action: GrantPermissionAction LH-->LS: 6 Notice of permission grant LS-->AH: 7 RetrieveAttestationsAction Alice navigates to the professional network site and initiates the flow to grant access to her educational attestations. The website sends a RequestPermissionAction to Alice\u2019s Hub. Alice\u2019s Hub relays the request to Alice's UA, which prompts her to grant/deny permission. Alice grants permission to access her current and future educational attestations by pushing a signed permission object and DID-specific keys to her Hub. Alice\u2019s Hub stores the keys she generated for the professional networking site and relays an Action to the professional network's Hub to provide notice that their permission request has been granted. The professional networking site is notified that the permission has been granted. At any time in the future, the professional networking site can retrieve Alice\u2019s education credentials from Alice\u2019s Hub, based on the permissions she provided and using the private key held by the professional networking site. Should the permission later be removed, the Prof Site's ability to retrieve updated credentials will be removed. Referenced Action Objects RequestPermissionAction GrantPermissionAction RetrieveAttestationsAction","title":"Assumptions"},{"location":"RWoT6/identity-hub-attestations/#25-alice-applies-for-a-job-and-refuses-to-provide-references","text":"Alice is applying for a job and has connected with the HR department via her DID. Alice has already provided some basic attestations about her right to work, name, address, etc. But when she receives a request for her references Alice refuses/denies the request as by the time this request comes in Alice has already accepted a position somewhere else (for example).","title":"2.5\u00a0Alice Applies for a Job and Refuses to Provide References"},{"location":"RWoT6/identity-hub-attestations/#assumptions_3","text":"Alice is linked to Company\u2019s HR via her DID. Alice has an Identity Hub accessed via an application on her mobile device. Alice has a verified digital attestation for her references but does not wish to share them at this time, or Alice does not have references in her digital wallet yet. participant HR Hub / EA as LH participant HR Site as LS participant Alice UA as AUA participant Alice Hub as AH LS-->AH: 1 Action: PreconditionsAttestationAction (References) AH-->AUA: 2 Prompts for reference disclosure AUA->AH: 3 Refuses reference disclosure AH-->LH: 4 Action: DenyAttestationAction (Reason) LH-->LS: 5 Notice of refusal HR initiates a request for references via Alice\u2019s Hub. Alice's Hub finds appropriate Attestations and provides them to Alice's User Agent. For whatever reason, Alice refuses (via her agent) to provide references at this time. Alice\u2019s Hub notifies HR Hub of the refusal, with optional reason for refusal. A notification is sent to HR with the refusal details (generic or specific to the scenario).","title":"Assumptions"},{"location":"RWoT6/identity-hub-attestations/#referenced-action-objects_1","text":"PreconditionAttestationAction DenyAttestionAction","title":"Referenced Action Objects"},{"location":"RWoT6/identity-hub-attestations/#26-a-bank-sends-alice-a-contract-that-requires-her-did-signature-which-she-signs-and-delivers-back-to-the-bank","text":"participant App Hub / EA as LH participant App Site as LS participant Alice UA as AUA participant Alice Hub as AH LS-->AH: 1 Action: SignAttestationAction (Contract) AH-->AUA: 2 Prompt for signature AUA-->LH: 3 Action: DeliverAttestationAction (Contract) LH-->LS: 4 Notice of signing","title":"2.6\u00a0A Bank Sends Alice a Contract that Requires her DID signature, which She Signs and Delivers Back to the Bank."},{"location":"RWoT6/identity-hub-attestations/#referenced-action-objects_2","text":"SignAttestionAction DeliverAttestationAction","title":"Referenced Action Objects"},{"location":"RWoT6/identity-hub-attestations/#27-the-college-determines-alice-was-issued-a-nursing-certificate-instead-of-her-cs-diploma-so-they-revoke-the-attestation-and-issue-the-correct-one","text":"participant App Hub / EA as LH participant App Site as LS participant Alice UA as AUA participant Alice Hub as AH LS-->AH: 1 Action: RevokeAttestationAction (Old Diploma) AH-->AUA: 2 Notice of revocation LS-->AH: 3 Action: DeliverAttestationAction (New Diploma) AH-->AUA: 4 Accept Attestation? AUA-->AH: 5 Store accepted attestation","title":"2.7\u00a0The College Determines Alice Was Issued a Nursing Certificate Instead of Her CS diploma, so They Revoke the Attestation and Issue the Correct One."},{"location":"RWoT6/identity-hub-attestations/#referenced-action-objects_3","text":"RevokeAttestationAction DeliverAttestationAction","title":"Referenced Action Objects"},{"location":"RWoT6/identity-hub-attestations/#28-alice-retracts-data-access-permission-from-a-professional-networking-site","text":"participant Prof. Hub / EA as PH participant Prof. Site as PS participant Alice UA as AUA participant Alice Hub as AH AUA-->AH: 1 Remove Prof. permissions AH-->PH: 2 Action: RetractPermissionAction PH-->PS: 3 Notice of retraction request PS-->PH: 4 Signed reciept of retraction request PH-->AH: 5 Signed reciept response Referenced Action Objects RetractPermissionAction","title":"2.8\u00a0Alice Retracts Data Access Permission from a Professional Networking Site."},{"location":"RWoT6/identity-hub-attestations/#29-alices-college-discovers-they-made-a-mistake-on-her-diploma-attestation-and-sends-her-an-amended-attestation-with-the-correct-info","text":"participant College Site as CS participant Alice UA as AUA participant Alice Hub as AH CS-->AH: 1 Action: AmendAttestionAction (New + Old reference) AH-->AUA: 2 Accept Attestation? AUA-->AH: 3 Replace old with new attestation","title":"2.9\u00a0Alice's College Discovers they Made a Mistake on her Diploma Attestation, and Sends her an Amended Attestation with the Correct Info."},{"location":"RWoT6/identity-hub-attestations/#referenced-action-objects_4","text":"AmendAttestionAction","title":"Referenced Action Objects"},{"location":"RWoT6/identity-hub-attestations/#3-action-objects","text":"Identity Hub attestation handling relies on the passage and recognition of common Action types that Hubs, User Agents, and consuming apps/services understand. In order to ensure that the flows related to attestations are precise and maximally descriptive of their intent, the Identity Hub spec will define its own Action objects for each of the relevant attestation actions. These objects are extensions of the Schema.org Action object, the schema origin of which shall be schema.identity.foundation. These objects are strictly a shared means of communicating and facilitating the various activities related to attestations; they do not infer or require a specific type of proof format or material be used within them. Note that each Action returns only a status of whether the Action was successfully (or not) transmitted. The result of processing the request is conveyed to the caller via a subsequent Action. The following is a description of the objects and examples that encompass their structure and properties:","title":"3 Action Objects"},{"location":"RWoT6/identity-hub-attestations/#31-requestattestationaction","text":"The Holder requests an attestation from an Issuer. Type of attestation wanted List of tag strings to describe the attestation Detailed, human-readable description of the attestation being requested (mostly for UAs to display to users) Who is the attestation for? What format do you need it in? Enable passing of preconditions Option to set a deadline for issuance/fulfillment { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RequestAttestationAction\", \"identifier\": UNIQUE_ID, \"for\": [\"did:foo:123-456\"], \"format\": CLAIM_FORMAT, \"expiration\": EPOCH_TIME, \"description\": \"Province of British Columbia Driver\u2019s License\", \"tags\": [\"license\", \"driving\", \"permit\", \"DL\", \"driver\u2019s license\"], \"preconditions\": ARRAY_OF_PRECONDITION_PROOFS (optional) }","title":"3.1\u00a0RequestAttestationAction"},{"location":"RWoT6/identity-hub-attestations/#32-denyattestationaction","text":"In response to a request for an Attestation, a Verifier/Issuer informs a Holder that the attestation cannot be provided. This Action inherits from schema.org's RejectAction . Linked attestation action ID Reason for refusing the Request Attestation Action. { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"DenyAttestationAction\", \"identifier\": UNIQUE_ID, \"purpose\": \"We cannot issue your diploma, you have not graduated.\" }","title":"3.2\u00a0DenyAttestationAction"},{"location":"RWoT6/identity-hub-attestations/#33-preconditionsattestationaction","text":"In response to a request for an Attestation, a Verifier/Issuer informs a Holder a list of Pre-Conditions that must be met before the requested Attestation can be issued. Linked attestation action ID Specify set of preconditions, each with their own descriptors { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"PreconditionsAttestationAction\", \"identifier\": UNIQUE_ID, \"preconditions\": ARRAY_OF_PRECONDITION_DESCRIPTORS }","title":"3.3\u00a0PreconditionsAttestationAction"},{"location":"RWoT6/identity-hub-attestations/#34-offerattestationaction","text":"In response to a request for an Attestation that cannot be issued because that type is not available, provide to the Holder a list of attestations that ARE available. For each attestation type available to the requester: Type of attestation List of tag strings to describe the attestation Detailed, human-readable description of the attestation being requested (mostly for UAs to display to users) Formats available for the attestation { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"OfferAttestationAction\", \"identifier\": UNIQUE_ID, \"availableAttestations\": ARRAY_OF_ATTESTATION_DESCRIPTORS }","title":"3.4\u00a0OfferAttestationAction"},{"location":"RWoT6/identity-hub-attestations/#35-deliverattestationaction","text":"Used by any party that delivers a finalized attestation to a target entity. This Action inherits from schema.org's SendAction . Linked attestation action ID Payload of the proof material Format of the proof material Time delivered { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"DeliverAttestationAction\", \"identifier\": \"UNIQUE_ID\", \"object\": ATTESTATION_PAYLOAD, \"description\": \"Province of British Columbia Driver\u2019s License\", \"tags\": [\"license\", \"driving\", \"permit\", \"DL\", \"driver\u2019s license\"] }","title":"3.5\u00a0DeliverAttestationAction"},{"location":"RWoT6/identity-hub-attestations/#36-presentattestationaction","text":"This Action is the envelop used to present an attestation to an inspecting party. List of tag strings to describe the attestation Detailed, human-readable description of the attestation being requested (mostly for UAs and EAs to reason over and use in display) Format of the attestation payload The attestation payload { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"PresentAttestationAction\", \"object\": ATTESTATION_PAYLOAD, \"description\": \"MIT Diploma for B.S. in Computer Science\", \"tags\": [\"diploma\", \"degree\"] }","title":"3.6\u00a0PresentAttestationAction"},{"location":"RWoT6/identity-hub-attestations/#37-signattestationaction","text":"A party sends a Action to a target prompting them to sign the provided attestation payload. This Action inherits from schema.org's EndorseAction . Linked attestation action ID Payload of the proof material Format of the proof material Time delivered { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"SignAttestationAction\", \"identifier\": UNIQUE_ID, \"object\": ATTESTATION_PAYLOAD, \"description\": \"Loan for 123 Main Street, Anytown USA\" }","title":"3.7\u00a0SignAttestationAction"},{"location":"RWoT6/identity-hub-attestations/#38-revokeattestationaction","text":"The party that previously supplied an attestation sends a notice to the attestation owner/holder that issuing party has revoked the attestation. This Action inherits from schema.org's DeactivateAction . Attestation ID Revocation code - array of revocation codes (look for an existing standard) Reason for revocation - array of human-readable descriptions of the reason, or URI { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RevokeAttestationAction\", \"identifier\": UNIQUE_ID, \"object\": ATTESTATION_PAYLOAD, \"result\": REVOCATION_RECORD, \"purpose\": \"Your driver's license was revoked.\" }","title":"3.8\u00a0RevokeAttestationAction"},{"location":"RWoT6/identity-hub-attestations/#39-amendattestationaction","text":"Used to update an attestation. Requires past ID, optionally including previous attestation. This Action inherits from schema.org's ReplaceAction . Attestation ID Change delta of some kind Reason for amendment - array of human-readable descriptions of the reason, or URI { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"AmendAttestationAction\", \"identifier\": UNIQUE_ID, \"object\": ATTESTATION_PAYLOAD, \"purpose\": \"Your driver's license was amended with your latest picture\" }","title":"3.9\u00a0AmendAttestationAction"},{"location":"RWoT6/identity-hub-attestations/#310-requestpermissionaction","text":"Request permission for access to a DID's Identity Hub data. This Action inherits from schema.org's AuthorizeAction . Permission being requested Intended use of data being requested { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RequestPermissionAction\", \"object\": PERMISSION_OCAP, \"purpose\": \"Display and filtering on a professional network\", }","title":"3.10\u00a0RequestPermissionAction"},{"location":"RWoT6/identity-hub-attestations/#311-grantpermissionaction","text":"The party that allows a permission sends a notice to the requesting party to let them know the permission has been granted. This Action inherits from schema.org's AcceptAction . Permission being requested Intended use of data being requested { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"GrantPermissionAction\", \"object\": PERMISSION_OCAP }","title":"3.11\u00a0GrantPermissionAction"},{"location":"RWoT6/identity-hub-attestations/#312-denypermissionaction","text":"The party evaluating the permission request does not grant the permission and sends the requesting party a notice of the denial. This Action inherits from schema.org's RejectAction . There is not currently an example of this action in the scenarios in Section 2 of this document. Permission being requested Intended use of data being requested { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"DenyPermissionAction\", \"object\": PERMISSION_OCAP, \"purpose\": \"I do not want to allow you access at this time\", }","title":"3.12\u00a0DenyPermissionAction"},{"location":"RWoT6/identity-hub-attestations/#313-retractpermissionaction","text":"The party that has previously issued a permission granting access sends a notice to the affected party to let them know the permission has been retracted. This Action inherits from schema.org's DeleteAction . Permission being retracted { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RetractPermissionAction\", \"object\": PERMISSION_OCAP, \"purpose\": \"I no longer want you to have access to my attestations\", }","title":"3.13\u00a0RetractPermissionAction"},{"location":"RWoT6/identity-hub-attestations/#314-retrieveattestationsaction","text":"Used by any party that has been granted permission access to a set of Attestations via the GrantPermissionAction to retrieve a set of Attestations. { \"@context\": \"http://schema.identity.foundation/\", \"@type\": \"RetrieveAttestationsAction\", \"identifier\": \"UNIQUE_ID\", \"object\": ATTESTATION_PAYLOAD, \"description\": \"Province of British Columbia Driver\u2019s License\", \"tags\": [\"license\", \"driving\", \"permit\", \"DL\", \"driver\u2019s license\"] }","title":"3.14\u00a0RetrieveAttestationsAction"},{"location":"RWoT6/identity-hub-attestations/#4-glossary","text":"Decentralized Identifier : Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, \"self-sovereign\" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. DID : Decentralized Identifier DID Auth : Authentication of an Identity by verifying the Identity's control of its DID DID Document : The control document that specifies keys, service endpoints, and other basic details about a DID. DDO : Abbreviation for a DID Document EA : Enterprise Agent: a HUB-aware service that integrates with an Enterprise\u2019s backend systems and representatives to process HUB requests. Conceptually equivalent to a person\u2019s UA, but for an organization. UA : Abbreviation for User Agent Universal Resolver : A mechanism of getting the DID Document associated with a DID across any (supported) DID implementation UR : Abbreviation for Universal Resolver User Agent : a smartphone-based digital wallet, browser","title":"4\u00a0Glossary"},{"location":"RWoT6/identity-hub-attestations/#5-technical-spec-implications","text":"For the Hub /permission spec: add optional timeout for permissions","title":"5\u00a0Technical &amp; Spec Implications"},{"location":"RWoT6/open-badges-are-verifiable-credentials/","text":"Open Badges are Verifiable Credentials By Nate Otto (Concentric Sky), Kim Hamilton Duffy (Learning Machine) Contributors: Kulpreet Singh (Chlu) Luiz Gustavo Ferraz Aoqui (IBM) This document is expected to contribute to standards development at IMS Global and/or other standards bodies such as the W3C. Licensed CC-BY. The authors of this document commit to sign and abide by the W3C Community Group IPR Policy to facilitate this work being used in the web standards process. Abstract We identify use cases and requirements that connect threads of work happening in the Rebooting Web of Trust community around: educational achievement claims (particularly using the Open Badges vocabulary); use of decentralized identifiers (DIDs) within web services where educational claims circulate; and integrating blockchain-reliant verification layers. We illustrate each of these cases with a set of example documents and describe user stories for Open Badges ecosystem software in the roles of Issuer, Host/Backpack, Displayer, and Verifier that need to be implemented in order to enable the capabilities described. Table of Contents Open Badges are Verifiable Credentials Abstract Table of Contents Introduction Motivation Open Badges and Verifiable Credentials are compatible in purpose Similar data models Benefits of continued alignment to both communities The Open Badges Vocabulary The Verifiable Credentials Specification Implementation, present and future Implementation Options Option 1: A Verifiable Credential/Assertion claims that an entity holds an BadgeClass Option 2: An Assertion is a Verifiable Credential Claim Comparing Options 1 and 2 An Assertion Awarded to a Recipient Identified by a DID An Assertion Awarded by an Issuer identified with a DID Blockchain proof of existence with Blockcerts Implementing User Stories for Open Badges Tools Validator Validating Option 2 Validating Option 1 Issuer Host/Backpack Displayer Next Steps Introduction The Open Badges Specification is a vocabulary and set of protocols that describes credentials. The vocabulary can describe any achievement in terms of a common set of attributes and is most often used for educational or occupational credentials. At present in version 2.0, Open Badges defines two verification methods: HostedBadge (requiring resources hosted on HTTP in specific locations) and SignedBadge (using a JSON Web Signature, which references hosted Issuer Profile and CryptographicKey information). The Blockcerts Open Badges Draft Extension introduced a verification method based on those used by Verifiable Credentials for the specific use case of blockchain-anchored credentials. This paper expands that work and proposes a new option that can reside alongside existing Open Badges verification methods. This paper also explores new capabilities that the Open Badges ecosystem could gain by adopting some of the emerging technologies and conventions from the Verifiable Credentials specification and its communities of implementers. It will inform the authors' development of working prototypes to demonstrate the viability and utility of these methods for publishing and circulating Open Badges. Motivation In this section we provide the motivation for supporting Verifiable Credentials Specifications in Open Badges implementations. There are three key reasons for our work: these models are compatible in purpose; their data models are complementary; and there are benefits in alignment for the existing communities around both specifications. Open Badges and Verifiable Credentials are compatible in purpose Both Open Badges (OB) and Verifiable Credentials (VC) are capable of expressing a cryptographically verifiable statement about the subject, issuer, evidence, and status of a credential. The Verifiable Credentials specification provides a lightweight structure for expressing a wide range of credentials \u2014 including driver's licenses or passports. A VC implementer chooses which schema/vocabulary to use, depending on the use case or domain. The strength of Verifiable Credentials is its flexibility across a wide variety of use cases. At the same time, there is not yet general agreement on schema and vocabulary sets to use with Verifiable Credentials that will allow them to serve their varied purposes. In comparison, Open Badges have been used in production deployments for nearly a decade. This has established fitness-of-purpose for real-world educational/occupational scenarios, and has led to a rich set of conventions and vocabularies. Successful deployments range from low to high stakes contexts, including informal recognition of a valuable contribution, completion of training, or completion of coursework or a university degrees. Building on this success, the Open Badges community has driven related standards, such as stackable credentials leading to a larger goal (\"Open Pathways\", which could be used to represent the path from individual university courses to a degree). Similar data models There is significant cross-pollination in the development of Open Badges and Verifiable Credentials; accordingly, there is already some structural alignment: - Open Badge Endorsements employ the Verifiable Credentials structure. - Open Badges and Verifiable Credentials Issuer Profiles are aligned. - Open Badges and Verifiable Credentials both use JSON Linked Data (JSON-LD), allowing reuse among different contexts. They both allow the addition of terms beyond their own vocabulary, meaning components of each specification may be used in the other. Blockcerts/Open Badge signing and verification processes use the same methods as Verifiable Credentials to establish the authenticity and integrity of claims. This is because the Blockcerts extension draft for Open Badges 2.0 uses the JSON-LD signatures/verification method (the same used by Verifiable Credentials) to anchor an Open Badge to a blockchain. Benefits of continued alignment to both communities Further alignment of Open Badges and Verifiable Credentials provides benefits to the communities and recipients: - Verifiable Credentials benefit from the rich expressiveness and vocabulary vetted by years of real-world Open Badges deployments and the ability to use these claims quickly across deployed services that already understand Open Badges. - Open Badges can participate in the upcoming Verifiable Credentials ecosystem, which benefits recipients by allowing their Open Badges to function as Verifiable Credentials (via alignment of structural and verification mechanisms). - Recipients may share their Open Badges just like any Verifiable Credential, taking advantage of potentially broad interoperability of these claims. The Open Badges Vocabulary Understanding the alignment between these specifications is possible with a deeper examination of how Open Badges claims are described semantically. Open Badges defines three primary data classes, with one of each making up a valid badge awarded to a single recipient. The Open Badges Specification , currently in version 2.0, is published by the IMS Global Learning Consortium. The Issuer Profile describes the issuer entity, whether that is an individual, an organization, or something else, like an autonomous actor. The BadgeClass describes a particular achievement, the criteria that all recipients must pass in order to be awarded the badge. Each BadgeClass has an issuer, its creator. The Assertion describes an instance of the achievement as it applies to a single recipient. It identifies which BadgeClass is awarded, and describes the recipient of the award using a single string-type identifier (such as an email address). The Assertion does not currently identify its issuer directly, but only that the issuer of the BadgeClass may publish valid Assertions. The Open Badges validator(s) check to ensure the Assertion is either cryptographically signed with one of the Issuer's authorized keys or uses a hosted verification URI within the Issuer's allowed scope. Members of the Open Badges community have long talked about use cases for Assertions with a different issuer than the issuer of their BadgeClass, but this feature has not yet been implemented in the specification. The Verifiable Credentials Specification Members of the W3C and Rebooting Web of Trust community have been developing the idea of Verifiable Credentials (formerly Verifiable Claims) for several years, and it has now been picked up as official work of the W3C Verifiable Credentials Working Group . The current editor's draft of the VC Data Model describes that a Credential is a set of one or more claims about a subject . All of the claims made by traditional credentials such as driver's licenses may be made by a digital Verifiable Credential, and many novel claims may also be made. VCs become verifiable through proofs , such as cryptographic signatures. Implementation, present and future The Open Badges Specification defines everything needed to create an ecosystem where credentials circulate. The core vocabulary allows issuers to describe their profiles and achievements (available and awarded) in JSON-LD. The verification protocols for hosted and signed badges describe how to establish that a particular Assertion is the valid expression of a badge award published by an Issuer Profile, while the Baking Specification describes a method by which Assertion data may be embedded within a PNG or SVG image file, which is portable across many file systems and publishable on websites. There are dozens of implementers of Open Badges across the roles of Issuer, Host (commonly known as a \"backpack\"), and Displayer, including several platforms that are certified by IMS Global as compliant implementers of the 2.0 specification in these roles. Over 10 million credentials have been awarded as Open Badges. Going forward from 2.0, community members are interested in increased flexibility to describe credentials, better information about how specific badges and issuers are situated in networks of trust, and a growing ecosystem of adopters with good complementary integration with other specifications. The natural focus of the Open Badges Specification is as a vocabulary to describe defined achievements . There are advantages to be gained in consistent implementation of a common vocabulary for achievement credentials. Verifiable Credentials has a vibrant community of interested implementers, including several who are using draft versions of the specification in production systems. The Verifiable Credentials specification is a general purpose technology with a wide range of potential use cases. The specification's focus is not on a particular type of credential or specific set of terms and it does not have an inherent concept of a defined achievement like Open Badges does. The strength for Verifiable Credentials as a specification is to enable innovation in different types of proofs, claims, and methods of identifying credential subjects. Examples of these efforts include the use of blockchain pointers in proofs and Decentralized Identifiers (DIDs) for identifying subjects. There may be a clean fit between the focus of Open Badges as a controlled vocabulary for defined achievements and the innovation around proofs and subject identifiers occurring in the community of Verifiable Credentials implementers. This paper illustrates how these technologies may move closer to full compatibility by making careful choices at the integration surface. Effectively, the Open Badges ecosystem can implement the Verifiable Credentials data model as an option alongside its existing hosted and signed delivery/verification methods with minimal changes to specification and software in the Issuer, Host, and Displayer roles. Open Badges validation/verification software is most significantly affected, but the open source and shared nature of the official IMS Global Open Badges validator library means very few actual pieces of software must be updated in order to obtain wide-reaching availability of the techniques shown here across the Host and Displayer roles. Services in the Issuer Role have an easier and less critical adoption process, especially as long as existing verification methods remain in place as an option. Implementation Options We have identified two possible ways to connect the Open Badges and Verifiable Credentials specifications. We will circulate and compare the core options here and choose which to base our prototypes upon. Option 1: A Verifiable Credential/Assertion claims that an entity holds an BadgeClass The Verifiable Credentials Data model has moved to enable some \"credential metadata\" options that parallel attributes of Open Badges Assertions, including evidence. The core purpose of an Assertion is to describe the instance of an award, an issuer's claim that a particular recipient has met the criteria defined in a BadgeClass and to present the evidence related to that. While Open Badges have a badge property that points to \"the BadgeClass definition that is presently asserted\" and a separate recipient property that serves to identify the recipient by some string-based identifier, such as an email or a DID, Verifiable Credentials have a claim that an issuer makes to describe attributes of a recipient. In the Verifiable Credentials context, the way to describe the Open Badges concept of \"earning an instance of a BadgeClass definition\" is to claim that \"a recipient, identified by an IRI or by an attribute of a different type of string-type identifier, holds or has met the criteria of a BadgeClass definition\". Here is an example of a Verifiable Credential/Assertion that a recipient identified by a HTTP profile URI holds a certain BadgeClass. In this and future examples, id is an alias in the Open Badges and Verifiable Credentials context of the linked data subject identifier keyword, canonically @id . This requires the introduction of a new holds property in the Open Badges vocabulary in order to make the claim shown here \"The entity identified as https://example.com/profiles/bob holds the BadgeClass 'Certificate of Accomplishment'\". { \"@context\": [\"https://w3id.org/credentials/v1\", \"https://w3id.org/openbadges/v2\"], \"id\": \"https://example.com/assertions/1001\", \"type\": [\"Credential\", \"Assertion\"], \"issuer\": \"https://example.com/profiles/alice\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": { \"id\": \"https://example.com/profiles/bob\", \"obi:holds\": { \"id\": \"https://example.com/badgeclasses/123\", \"type\": \"BadgeClass\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"A badge describing great accomplishments\", \"criteria\": { \"narrative\": \"Perform tasks of valor and wit.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"https://example.com/profiles/alice\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } } }, \"obi:evidence\": { \"id\": \"https://example.org/portfolios/25\", \"name\": \"Bob's Portfolio\", \"narrative\": \"Bob worked hard to develop a good portfolio\", \"genre\": \"ePortfolio\" }, \"sec:proof\": { \"@graph\": { \"type\": \"sec:RsaSignature2018\", \"created\": \"2018-03-07T19:22:15Z\", \"creator\": \"https://example.com/profiles/alice/keys/1\", \"sec:jws\": \"eyJhbGciOiJQUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..p9RDJqwzXyxfC69dMEz503_ZZ_af1e_hV931dPlIdrofC6p2y_dcjjqIDysReJy6W_fnN_dZGVoXqFAg2OD_SmbDi5dNMOZILot-zJdDJCxXWuwZtiCFlt29KfLmJs6me0bD5pU4RbknXDoyBhA8muMby8j1fUeBDo3Ienmzv5UlB3v0f0-w5l6-z_cswHB_UXIlWw4EzcsmLvHzjB7TI76QLwq3KeVPSB3U9aM3o2Ejkq6Ygh5XxUGkXiZUQ5ungQ9Psy_VicjZyOc19LoBPoiPxDHQodTrqCFNH2qCNhDc4lg2zE8S9KNlQhUUFatzkTN70s23fhWBMKz2a5DWgQ\" } } } Values for the claim may vary in two meaningful cases: The issuer wishes to take advantage of the light anti-correlation mechanism enabled by Open Badges hashed recipient identifiers for a recipient who is identified by an id string. The issuer wishes to identify the recipient by a profile identifier property other than id , such as email . Use of hashed identifier may or may not be used. In either case, the implementation option is to make an additional claim about the recipient. In addition to the above claim that \"a recipient holds or has met the criteria of a BadgeClass definition\", the issuer makes a claim that the recipient \"holds an identifier of a particular type\" using the Open Badges IdentityObject : { ... \"claim\": { \"holds\": { \"type\": \"BadgeClass\" }, \"recipient\": { \"type\": \"email\", \"hashed\": false, \"identity\": \"testrecipient@example.com\" } } } This example does not use any id at all for the credential's claim. This is nonstandard in the VC ecosystem if not disallowed. If an issuer desires to not omit id while still identifying the recipient via the IdentityObject, a one-time IRI may be generated, such as a randomly UUID, using the urn:uuid: scheme. An Open Badges verifier should assume that if a claim is found where obi:holds and recipient are claimed, the recipient IdentityObject takes precedence over the claim subject id or that both are valid ways to refer to the recipient. Option 2: An Assertion is a Verifiable Credential Claim The Verifiable Credentials Specification allows issuers to make a claim about a subject while remaining agnostic to the content of that claim. It is possible to use an Open Badges Assertion as the claim of a Verifiable Credential, signed with a Linked Data Signature. The ID for the key and its owner are modeled as an HTTP-resolvable document as in the examples on the jsonld-signatures library. { \"@context\": \"https://w3id.org/credentials/v1\", \"id\": \"https://some.university.edu/credentials/9732\", \"type\": [\"Credential\", \"OpenBadgeCredential\"], \"issuer\": \"https://example.com/i/alice\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": [{ \"@context\": \"https://w3id.org/openbadges/v2\", \"id\": \"urn:uuid:437fc6ff-bb3c-4987-a4b7-be8661ff6f21\", \"type\": \"Assertion\", \"recipient\": { \"type\": \"email\", \"identity\": \"testrecipient@example.com\", \"hashed\": false }, \"badge\": { \"type\": \"BadgeClass\", \"id\": \"urn:uuid:7aad3c57-3bfb-45ea-ae79-5a6023cc62e4\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"A badge describing great accomplishments\", \"criteria\": { \"narrative\": \"Perform tasks of valor and wit.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"https://example.com/profiles/alice\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } }, \"verification\": { \"type\": \"VerifiableClaim2018\" } }], \"sec:proof\": { \"@graph\": { \"type\": \"sec:RsaSignature2018\", \"created\": \"2018-03-07T19:22:15Z\", \"creator\": \"https://example.com/profiles/alice/keys/1\", \"sec:jws\": \"eyJhbGciOiJQUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..p9RDJqwzXyxfC69dMEz503_ZZ_af1e_hV931dPlIdrofC6p2y_dcjjqIDysReJy6W_fnN_dZGVoXqFAg2OD_SmbDi5dNMOZILot-zJdDJCxXWuwZtiCFlt29KfLmJs6me0bD5pU4RbknXDoyBhA8muMby8j1fUeBDo3Ienmzv5UlB3v0f0-w5l6-z_cswHB_UXIlWw4EzcsmLvHzjB7TI76QLwq3KeVPSB3U9aM3o2Ejkq6Ygh5XxUGkXiZUQ5ungQ9Psy_VicjZyOc19LoBPoiPxDHQodTrqCFNH2qCNhDc4lg2zE8S9KNlQhUUFatzkTN70s23fhWBMKz2a5DWgQ\" } } } The example above illustrates the following salient features: The claim ID is the ID of the Assertion, not of the Recipient. Semantically, that means that the Verifiable Credential is effectively claiming \"This Assertion exists with content...\" instead of the Option 1 model of introducing a \"holds\" property making the claim about a recipient instead. For example, \"The subject identified as id holds the Assertion with content\". The above approach is a simple option where we can preserve Open Badges' ability to \"hash\" the recipient identifier. When the recipient id has been hashed in this manner, the Verifiable Credential will likely not be usable in a wallet application intended for VCs. But because there will remain a strong market for Open Badges-specific tooling to serve recipients, this type of VC with a hashed subject identifier will likely remain useful as a light layer of personally identifiable information protection above a base VC. There is some duplication in the data package around the issuer. Future relaxation of the Open Badges Specification may become possible when the Assertion issuer is defined elsewhere in the \"envelope\", but the relative cost of duplicating this data to achieve parity with the current Open Badges Specification is low and will minimize the work needed in Open Badges tools to implement this delivery method. Comparing Options 1 and 2 Option 1 uses a recipient subject identifier in the same way as other claims, which is expected to increase compatibility with VC-native tooling. On the other hand, Option 2 requires significantly less work on the part of the Open Badges validator(s) to return a result graph that matches its existing format implemented based on Open Badges 2.0. Option 2, in which the claim is roughly the same as the existing Assertion class, is a valid Verifiable Credential. But because the claim subject is the Assertion itself instead of the recipient, this is not assumed to be a very strong integration path that would allow Open Badges Verifiable Credentials to be circulated among general purpose Verifiable Credential \"wallet\" tools. These tools expect claims to be made about their users, not about entities that have relationships to their users. An Assertion Awarded to a Recipient Identified by a DID In the existing Open Badges ecosystem, recipients are identified by email address virtually all of the time. Open Badges 2.0 strengthened and clarified the ability to award to other types of identifiers, but Assertions far more often use email. An example implementation in an Assertion could identify a recipient like this: \"recipient\": { \"type\": \"email\", \"identity\": \"testrecipient@example.com\", \"hashed\": false } The Open Badges Specification allows for several string-type attributes of an entity to be used as an Assertion's Recipient Profile Identifier Property . Decentralized Identifiers (DIDs) are emerging as a new type of entity identifier that comes in an IRI string format. They have the ability to be deterministically \"resolved\" to a \"DID Document\". The resolved documents then describe the functionally useful aspects of its identity, such as the public keys and other authentication methods that can be used to connect a user in a web browser or a human sitting across the table from you to the DID identifier. As Open Badges can be awarded to other string-type identifiers, like @id (aliased as id in the Open Badges & Verifiable Credentials contexts). We can identify a recipient in an Option 2-style Assertion like: \"recipient\": { \"identity\": \"did:example:recipient_did\", \"type\": \"id\", \"hashed\": false } This is all that must be done to award an Assertion to a DID as recipient identifier. The verification that an assertion is awarded to the expected DID is already supported in the Open Badges Validator. The Specification can be clarified to add \"id\" to the list of identifier types \"considered serviceable\", but that will constitute a clarification rather than a change. An Assertion Awarded by an Issuer identified with a DID The primary use case for a Decentralized Identifier (DID) string is to resolve it to a DID Document , which describes the entity; specifically, it has attributes like public keys that allow for the authentication of messages sent by and under the auspices of the entity that controls the DID. Verifiable Credentials are one type of message that can be signed and delivered by a DID-identified entity, and Open Badges Verifiable Credentials are yet another type of Verifiable Credential that can be signed in the same way. With this model in place, viewers of Open Badges (through Verifiers, Backpacks/Hosts and Displayers) can resolve a DID to a DID Document, access the authorized public keys associated with that DID, determine which authorized key was identified as the creator of a Credential signature, and verify the authenticity of that signature. In the current Open Badges Specification, even when using the SignedBadge verification method, there are still at least two resources that must be hosted on HTTP(s) in order to be able to produce valid Open Badges Assertions: the Issuer Profile and the CryptographicKey (identified by the publicKey attribute of the Issuer Profile). Using a DID for the Issuer Profile id instead of an HTTP URI allows for open badge data to be entirely uncoupled from HTTP hosting dependencies. Here is an example DID for an issuer: did:example:issuer_did . The part of the DID between the first and second : characters identifies the resolver method . The part after the second : is the DID path . If a service has implemented the algorithm specified by the particular resolver method used, it can consistently retrieve a DID document identified by the DID. We now show an example DID document for the did:example:issuer_did issuer described above. An implementer of the example resolver method could have retrieved the DID document using the method specified by the method specified. For example, by fetching data from a specific blockchain and deriving the DID document from it. { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:example:issuer_did\", \"publicKey\": [{ \"id\": \"did:example:issuer_did#keys-1\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:issuer_did\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA2x3wWF6LY1pNxxTIUfPtHBGvPgcVvVDaoGpOjMk9+QfcXBVzcEnIlYenI8WfvoNSNUbblmDyn5sqWg9KyQpcoJ2IAoemjTcb+Skw9PaS2KAYjCYq9pmnvGhmmXJjk1xuT3gevG8K9XGJ2MmuqdKQ4yzfPhD5kHdLxV9Y9VY2rgZJzSNL83Oz596tcFbA1QB0p8wj7xQpLYRu5d1Mz+1qu1E8NM6HPgSBp54JHJF0yL3s39rGNbxmopwCq1Vw9E22ZnJpHQtc4nq4N3JksfVPeHhirC3eny0YS78Z6W7bGlVT+bf+T6r43Rq8kQ7N8hVLrGHc+NuHXm1JBIbpKwIDAQAB-----END PUBLIC KEY-----\\r\\n\" }], \"authentication\": [{ \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"did:example:issuer_did#keys-1\" }], \"service\": [{ \"type\": \"ExampleService\", \"serviceEndpoint\": \"https://example.com/endpoint/8377464\" }] } Within such a DID Document, one or more public keys can be identified. In the example above, a single RSA key is described. An Open Badges verifier that supports the required DID resolver method discovers keys that can be trusted to be under the control of the entity. The resolver then verifies that a particular Open Badges Verifiable Claim is signed by a key pair belonging to the entity. Here is an example of what such a signed Assertion looks like for this issuer: { \"@context\": \"https://w3id.org/credentials/v1\", \"id\": \"urn:uuid:01f0bb90-86ee-4469-9655-7ca6f4d591ae\", \"type\": [\"Credential\", \"OpenBadgeCredential\"], \"issuer\": \"did:example:issuer_did\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": [{ \"@context\": \"https://w3id.org/openbadges/v2\", \"id\": \"urn:uuid:437fc6ff-bb3c-4987-a4b7-be8661ff6f21\", \"type\": \"Assertion\", \"issuedOn\": \"2018-02-25T00:00:00+00:00\", \"recipient\": { \"type\": \"id\", \"identity\": \"did:example:recipient_did\", \"hashed\": false }, \"badge\": { \"type\": \"BadgeClass\", \"id\": \"urn:uuid:7aad3c57-3bfb-45ea-ae79-5a6023cc62e4\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"Lorem ipsum dolor sit amet, mei docendi concludaturque ad, cu nec partem graece. Est aperiam consetetur cu, expetenda moderatius neglegentur ei nam, suas dolor laudem eam an.\", \"criteria\": { \"narrative\": \"Nibh iriure ei nam, modo ridens neglegentur mel eu. At his cibo mucius.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"did:example:issuer_did\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } }, \"verification\": { \"type\": \"VerifiableClaim2018\" } }], \"sec:proof\": { \"@graph\": { \"type\": \"sec:RsaSignature2018\", \"created\": \"2018-03-07T22:26:57Z\", \"creator\": \"did:example:issuer_did#keys-1\", \"sec:jws\": \"eyJhbGciOiJQUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..GMPdCXn-qQcZHPoO6qHn6hBiysMNaZ7nSBx_e27LuDxJvRsCLbR1n7LGG7i8NVW1SVMwRjs8aJ3H2XXFphCZF_dGaueTsaehTzLQgh9n5imPgrQFsAKsRAKTJ_zpVL8JpsbPcrXbb-fkAcD52oDuYJg1uVr3MOhe4BzibDUKaFg5-cXZ-Gs8KcXrh_Ddqtd8CWw0zS3fRvI3SKbO6op6hNB1Jha4mfAn49Q0BRiSuCxbyPNy5MtX7FGoimvLhsluM7UAtPWHBi6iW8Nh57fk4uS5ZywHJSYS9-HPcvbDUGPHPHOnwq4qq7xc47yXveMmyo2VX4YSYe3LM-_9w1TnGg\" } } } The above example shows a signed message where the claim is the Assertion (Option 2), but the same type of signature could be applied to Option 1 and any other Verifiable Credential. The issuer id declared in the claim.badge.issuer is the DID that we expect the verifier will resolve to the did:example:issuer_did DID Document shown earlier in this section. When the verifier encounters this \"did:example\" IRI scheme, it will 1) resolve this DID to its corresponding DID Document, 2) verify the signature, and 3) verify that an authorized key created the signature. There is some repetition of data in the example above. The issuer id is included at both issuer and claim.badge.issuer . While this data can be normalized to only appear at issuer , leaving it in supports consistency within the Open Badges Specification \u2014 there are no breaking changes needed to add this new verification option. In addition, there are attributes of the Issuer Profile that are not assumed to be verifiable by DID resolver methods, such as the name of the issuer, which is used by the Open Badges ecosystem. A version of the claim using Option 1 looks very similar to non-DID-approach for the issuer, but the validator would be expected to resolve the issuer DID to the DID Document and to use that document to get the applicable signing keys for the issuer and confirm the Verifiable Credential was signed with one of those keys. { \"@context\": [\"https://w3id.org/credentials/v1\", \"https://w3id.org/openbadges/v2\"], \"id\": \"https://example.com/assertions/1001\", \"type\": [\"Credential\", \"Assertion\"], \"issuer\": \"did:example:issuer_did\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": { \"id\": \"did:example:recipient_did\", \"obi:holds\": { \"id\": \"urn:uuid:7aad3c57-3bfb-45ea-ae79-5a6023cc62e4\", \"type\": \"BadgeClass\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"A badge describing great accomplishments\", \"criteria\": { \"narrative\": \"Perform tasks of valor and wit.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"did:example:issuer_did\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } } }, \"obi:evidence\": { \"id\": \"https://example.org/portfolios/25\", \"name\": \"Bob's Portfolio\", \"narrative\": \"Bob worked hard to develop a good portfolio\", \"genre\": \"ePortfolio\" }, \"sec:proof\": { \"@graph\": { \"type\": \"sec:RsaSignature2018\", \"created\": \"2018-03-07T19:22:15Z\", \"creator\": \"did:example:issuer_did#keys-1\", \"sec:jws\": \"eyJhbGciOiJQUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..p9RDJqwzXyxfC69dMEz503_ZZ_af1e_hV931dPlIdrofC6p2y_dcjjqIDysReJy6W_fnN_dZGVoXqFAg2OD_SmbDi5dNMOZILot-zJdDJCxXWuwZtiCFlt29KfLmJs6me0bD5pU4RbknXDoyBhA8muMby8j1fUeBDo3Ienmzv5UlB3v0f0-w5l6-z_cswHB_UXIlWw4EzcsmLvHzjB7TI76QLwq3KeVPSB3U9aM3o2Ejkq6Ygh5XxUGkXiZUQ5ungQ9Psy_VicjZyOc19LoBPoiPxDHQodTrqCFNH2qCNhDc4lg2zE8S9KNlQhUUFatzkTN70s23fhWBMKz2a5DWgQ\" } } } An important capability enabled by using a DID instead of an HTTP URI as an Issuer id is that the issuer entity can authenticate into multiple applications that will then have equal ability to act on behalf of the issuer entity. There will be no implied primary/secondary distinction between these applications, because none of them will \"own\" the identifier the way DNS allows only one entity to authoritatively own an HTTP identifier. The DID Document doesn't have any ability to describe in a verifiable sense the issuer's name, URL or email, but these properties are desirable to display within badging systems as part of the Open Badges Profile class, so they are embedded in the claim in these examples. Open Badges ecosystem tools need to determine for themselves when and why they trust these values to be correctly associated with an issuer ID. We can authenticate the DID, but not these properties directly. However, they may be the claims of other Verifiable Credentials where the Issuer is the subject. If a consumer trusts one or more of these claims, they could trust this data wherever it is presented associated with the issuer. This is likely a case where an issuer would want to reference Endorsements (the Open Badges Vocabulary term for \"plain\" Verifiable Credentials) they have received within the Issuer Profile that gets embedded in claims like the above example. Blockchain proof of existence with Blockcerts Blockchain-tethered issuance of Open Badges/Verifiable Credentials is enabled by the same JSON-LD signature and verification framework described here. The MerkleProof2017 LD signature suite allows verification of data anchored to a blockchain. This is the same signature suite used by the current version of Blockcerts, The above Option 1 example modified to use this MerkleProof2017 signature suite looks the following: { \"@context\": [\"https://w3id.org/credentials/v1\", \"https://w3id.org/openbadges/v2\", \"https://w3id.org/blockcerts/v2\"], \"id\": \"https://example.com/assertions/1001\", \"type\": [\"Credential\", \"Assertion\"], \"issuer\": \"did:example:issuer_did\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": { \"id\": \"did:example:recipient_did\", \"obi:holds\": { \"id\": \"urn:uuid:7aad3c57-3bfb-45ea-ae79-5a6023cc62e4\", \"type\": \"BadgeClass\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"A badge describing great accomplishments\", \"criteria\": { \"narrative\": \"Perform tasks of valor and wit.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"did:example:issuer_did\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } } }, \"obi:evidence\": { \"id\": \"https://example.org/portfolios/25\", \"name\": \"Bob's Portfolio\", \"narrative\": \"Bob worked hard to develop a good portfolio\", \"genre\": \"ePortfolio\" }, \"sec:proof\": { \"type\": \"MerkleProof2017\", \"targetHash\": \"637ec732fa4b7b56f4c15a6a12680519a17a9e9eade09f5b424a48eb0e6f5ad0\", \"merkleRoot\": \"f029b45bb1a7b1f0b970f6de35344b73cccd16177b4c037acbc2541c7fc27078\", \"anchors\": [ { \"sourceId\": \"d75b7a5bdb3d5244b753e6b84e987267cfa4ffa7a532a2ed49ad3848be1d82f8\", \"type\": \"BTCOpReturn\" }], \"proof\": [{ \"right\": \"11174e220fe74de907d1107e2a357e41434123f2948fc6b946fbfd7e3e3eecd1\" }] } } } This demonstrates a special form of proof, applying to the transaction containing the data. This proof technique is known as a \"Merkle proof\" \u2014 a cryptographic data structure used to show that the present credential belongs to a structure that is anchored to a blockchain transaction. This provides proof of existence through confidence in the timestamp of the Assertion, via the timestamp (if supported by the blockchain) associated with the block's addition to the ledger (according to the blockchain's consensus protocol and confirmation threshold). This also allows more than one credential to be part of a single blockchain transaction; batching of credentials is commonly performed for cost savings alongside natural logical groupings of credentials (e.g. \"Graduates of 2018\"). Informally, the Merkle proof, along with JSON-LD normalization, allows you to confirm that the local data matches an expected hash, and that hash combined with redacted data (and functions of redacted data), combine to a value that matches the value on the blockchain. More precisely, during verification, the hash of the present credential is calculated to ensure it matches targetHash . The proof section is how to merge this local credential with one-way hashes of other credentials (to avoid revealing the contents) into a tree structure, the base of which should equal merkleRoot . Lastly, the merkleRoot value is the expected value on the blockchain, which we can independently confirm. In the anchors part of the proof, the type and sourceId incidate which blockchain, transaction id, and transaction field to fetch; this value should equal the merkleRoot value. This example shows an issuer identified with a DID ( did:example:issuer_did ). This enables an improved authenticity check during the Blockcerts verification process, which will ensure the public key that signed the issuing transaction is authorized by the DID document associated with the issuer's DID. This format is proposed for Blockcerts v3, and has the advantage of unifying Blockcerts more tightly into Verifiable Credentials / Open Badges, as opposed to a separate Open Badge extension. Implementing User Stories for Open Badges Tools The two different options outlined in this paper pose different distributions of cost and complexity on Open Badges Applications in the core roles of Validator, Issuer, Host, and Displayer. When choosing which option to pursue to implement the capabilities described above, understanding the implementation path for each option enables cost/benefit analysis. Validator The Open Badges Validator is the main application that needs to be updated to support these features. The validator will need to be modified to detect that an input string is valid JSON-LD and of type Credential . The validator then queues a relevant procedure to intake the document that will be an alternative path to the existing procedures for intake based on a hosted URL input, image input, JWS string input, and existing JSON strings of Assertion, BadgeClass or Profile. Validating Option 2 Because internally, Option 2 looks entirely like an existing Assertion, the validator will not need to make many modifications to its internal expectations about how Open Badges objects are published. Resolving DIDs in this example occurs in neat parallel to resolving HTTP(s) identifiers into usable documents, but instead of the HTTP-based integrity verification tasks, DID-specific tasks will be queued, including: Resolve document from the DID. Verify DID Document signature (if signed) and requirement for it to be signed for particular contexts. Verify proofs for a few relevant Linked Data Signature Suites. Confirm badge object signing key is authorized to sign on behalf of DID-identified entity. Validating Option 1 Option 1 requires some changes to the Validator in addition to the DID resolution and signature verification changes. Update expectation for client that the Verifiable Credential style class can appear in the resultant data graph in place of the \"bare\" Assertion class. This structure is fundamentally the same as that already appears in the graph for Open Badges Endorsements, which are referenced from within the objects discovered while processing the verification input. Add procedure to queue up tests for the obi:holds claim in an VC that is found as part of processing. For convenience, the validator should be able to deliver a version of the VC translated into the 2.0 Assertion style to serve Backpacks and Displayers that cannot yet adapt to the Option 2 VC as Assertion model. This legacy version should be set as the \"validationSubject\" in the validation report. Issuer The OpenBadgeCredential wrapper is an optional verification type for Open Badges. That means issuers don't need to implement it in order to continue to issue valid Open Badges. Once the validator is updated, issuers can publish badges using these options, with very little distinction between them. To use a DID as a recipient identifier, badges can be published with any method supported by the validator that uses the DID for recipient identification. It may be the subject identifier of a claim (as in Option 1) or it may be hashed in an IdentityObject (Option 1 or 2), for instance. In order to use a DID as an issuer identifier, the Issuer Service will need to create a new DID or gain access to an authorized signing key created by the DID Owner user (e.g. \"Alice\"), for which the validator will be able to confirm authorization. For example, Alice could use the features of her DID Resolver Method to add the Issuer Service's signing key to her DID Document. Some implementing Issuer services may register the DID themselves and act as its owner, instead of authenticating and gaining access to a DID created by a user. This would likely not allow the user of the Issuer System who created the Issuer Profile to independently act as the DID owner themselves outside of the system, but it is a potentially valid path to implementation. Extending the possibilities explored by example above, one emerging complementary technology is Linked Data Object Capabilities . Under a system that uses this, Alice will act as her own DID Owner and will grant the Issuer Service the capabilities to perform several relevant Open Badges actions on her behalf, such as Issuing. When the issuing system invokes these capabilities, it will reference the capability chain in its proof. Capabilities are safer than adding an external service's key to your DID Document as if it were your own, because Capabilities may be attenuated to only grant specific actions to the capability holder. To the validator, if the Validator supported Linked Data Capabilities, it would appear as if Alice issued a signed Verifiable Credential Open Badge herself, though the audit trail would be available to determine that a key held by the Issuer Service invoked an Issue capability in order to sign the valid proof. This topic is explored in more detail in a complementary paper about Open Badges Peer Claims . Another alternative would be for Alice to sign new Assertions interactively, using a private key held only on a trusted device. The issuer service could prepare the Assertions for signature and then obtain the signature from Alice directly. Host/Backpack Backpacks mostly rely on the verification information returned from a validator, so if the validator can be updated to present the Open Badges data in a predictable supported format, hosts don't need to specially support the Verifiable Credentials wrapper. Verifying that badges belong to users of the Host service is one of their most important capabilities, however, and the introduction of DIDs as recipient identifiers introduces significant new requirements around that, similar in scope to issuer support. There are some emerging capabilities being described for the Authentication of users identified by a DID who are interacting with a system via browser. These are in very early draft stages, so backpacks that wish to resolve and authenticate users as owners of DIDs will likely need to implement an interactive process. These features could be implemented in Host services or via external identity providers. Collaboration with implementers of DID-resolver and authentication services seems like a fruitful direction in order to achieve a good pace of adoption and even implementation support across an ecosystem for the various resolver methods and authentication methods that will begin to proliferate. If an external identity provider is trusted by an Open Badges Host, a connection could be made via OAuth2 or similar protocol that may already be built into a backpack product. DID owners may provide one or more methods of authentication in their DID document, and relying services may select an authentication method from those available. They would establish a communication channel with a user attempting to log in to send an authentication challenge to the user's selected authentication device. For example, if the user keeps their signing key(s) on a mobile phone, they could be directed to scan a QR Code to obtain authentication challenge data, which they would sign and submit, completing an authentication process on the device where they started the request, upon which, they could be sent back to the Host, authenticated as the owner of the DID. Once a Host has authenticated that a particular local user controls a specific DID, it may accept badges into that user's account that identify their recipient using that DID. Displayer Displayers, like Hosts, rely on the return value of the validator to display trustworthy data, so if the validator returns essentially the same data structure, very few updates will be needed to support awards in VC format. Prototypes should negotiate a balance between adapting Displayers to handle the VC that makes a obi:holds claim and the validator returning a 2.0 Assertion style object in the graph that it synthesizes. Next Steps An implementation of these capabilities will be best informed by prototypes of experimental branches of open source Open Badges software that fills the different ecosystem roles. Our primary goal with this proposal is to gain feedback from the Open Badges and Verifiable Credentials communities. We have planned implementations to prove out these concepts, including: - Open Badges Validator support for JSON-LD signature verification using Option 1 with Option 2 as a fallback. - Issuing Blockerts credentials that can be validated in the format described here. - Adding support for Blockcerts blockchain pointer in validator. - Collaborating with those in the Rebooting Web of Trust Community to integrate at least one method of DID Authentication into an Open Badges Host to connect reliably. The lessons learned from these prototypes will feed use cases to submit in the next stages of standardization of Open Badges and related technologies.","title":"Open Badges are Verifiable Credentials"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#open-badges-are-verifiable-credentials","text":"By Nate Otto (Concentric Sky), Kim Hamilton Duffy (Learning Machine) Contributors: Kulpreet Singh (Chlu) Luiz Gustavo Ferraz Aoqui (IBM) This document is expected to contribute to standards development at IMS Global and/or other standards bodies such as the W3C. Licensed CC-BY. The authors of this document commit to sign and abide by the W3C Community Group IPR Policy to facilitate this work being used in the web standards process.","title":"Open Badges are Verifiable Credentials"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#abstract","text":"We identify use cases and requirements that connect threads of work happening in the Rebooting Web of Trust community around: educational achievement claims (particularly using the Open Badges vocabulary); use of decentralized identifiers (DIDs) within web services where educational claims circulate; and integrating blockchain-reliant verification layers. We illustrate each of these cases with a set of example documents and describe user stories for Open Badges ecosystem software in the roles of Issuer, Host/Backpack, Displayer, and Verifier that need to be implemented in order to enable the capabilities described.","title":"Abstract"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#table-of-contents","text":"Open Badges are Verifiable Credentials Abstract Table of Contents Introduction Motivation Open Badges and Verifiable Credentials are compatible in purpose Similar data models Benefits of continued alignment to both communities The Open Badges Vocabulary The Verifiable Credentials Specification Implementation, present and future Implementation Options Option 1: A Verifiable Credential/Assertion claims that an entity holds an BadgeClass Option 2: An Assertion is a Verifiable Credential Claim Comparing Options 1 and 2 An Assertion Awarded to a Recipient Identified by a DID An Assertion Awarded by an Issuer identified with a DID Blockchain proof of existence with Blockcerts Implementing User Stories for Open Badges Tools Validator Validating Option 2 Validating Option 1 Issuer Host/Backpack Displayer Next Steps","title":"Table of Contents"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#introduction","text":"The Open Badges Specification is a vocabulary and set of protocols that describes credentials. The vocabulary can describe any achievement in terms of a common set of attributes and is most often used for educational or occupational credentials. At present in version 2.0, Open Badges defines two verification methods: HostedBadge (requiring resources hosted on HTTP in specific locations) and SignedBadge (using a JSON Web Signature, which references hosted Issuer Profile and CryptographicKey information). The Blockcerts Open Badges Draft Extension introduced a verification method based on those used by Verifiable Credentials for the specific use case of blockchain-anchored credentials. This paper expands that work and proposes a new option that can reside alongside existing Open Badges verification methods. This paper also explores new capabilities that the Open Badges ecosystem could gain by adopting some of the emerging technologies and conventions from the Verifiable Credentials specification and its communities of implementers. It will inform the authors' development of working prototypes to demonstrate the viability and utility of these methods for publishing and circulating Open Badges.","title":"Introduction"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#motivation","text":"In this section we provide the motivation for supporting Verifiable Credentials Specifications in Open Badges implementations. There are three key reasons for our work: these models are compatible in purpose; their data models are complementary; and there are benefits in alignment for the existing communities around both specifications.","title":"Motivation"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#open-badges-and-verifiable-credentials-are-compatible-in-purpose","text":"Both Open Badges (OB) and Verifiable Credentials (VC) are capable of expressing a cryptographically verifiable statement about the subject, issuer, evidence, and status of a credential. The Verifiable Credentials specification provides a lightweight structure for expressing a wide range of credentials \u2014 including driver's licenses or passports. A VC implementer chooses which schema/vocabulary to use, depending on the use case or domain. The strength of Verifiable Credentials is its flexibility across a wide variety of use cases. At the same time, there is not yet general agreement on schema and vocabulary sets to use with Verifiable Credentials that will allow them to serve their varied purposes. In comparison, Open Badges have been used in production deployments for nearly a decade. This has established fitness-of-purpose for real-world educational/occupational scenarios, and has led to a rich set of conventions and vocabularies. Successful deployments range from low to high stakes contexts, including informal recognition of a valuable contribution, completion of training, or completion of coursework or a university degrees. Building on this success, the Open Badges community has driven related standards, such as stackable credentials leading to a larger goal (\"Open Pathways\", which could be used to represent the path from individual university courses to a degree).","title":"Open Badges and Verifiable Credentials are compatible in purpose"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#similar-data-models","text":"There is significant cross-pollination in the development of Open Badges and Verifiable Credentials; accordingly, there is already some structural alignment: - Open Badge Endorsements employ the Verifiable Credentials structure. - Open Badges and Verifiable Credentials Issuer Profiles are aligned. - Open Badges and Verifiable Credentials both use JSON Linked Data (JSON-LD), allowing reuse among different contexts. They both allow the addition of terms beyond their own vocabulary, meaning components of each specification may be used in the other. Blockcerts/Open Badge signing and verification processes use the same methods as Verifiable Credentials to establish the authenticity and integrity of claims. This is because the Blockcerts extension draft for Open Badges 2.0 uses the JSON-LD signatures/verification method (the same used by Verifiable Credentials) to anchor an Open Badge to a blockchain.","title":"Similar data models"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#benefits-of-continued-alignment-to-both-communities","text":"Further alignment of Open Badges and Verifiable Credentials provides benefits to the communities and recipients: - Verifiable Credentials benefit from the rich expressiveness and vocabulary vetted by years of real-world Open Badges deployments and the ability to use these claims quickly across deployed services that already understand Open Badges. - Open Badges can participate in the upcoming Verifiable Credentials ecosystem, which benefits recipients by allowing their Open Badges to function as Verifiable Credentials (via alignment of structural and verification mechanisms). - Recipients may share their Open Badges just like any Verifiable Credential, taking advantage of potentially broad interoperability of these claims.","title":"Benefits of continued alignment to both communities"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#the-open-badges-vocabulary","text":"Understanding the alignment between these specifications is possible with a deeper examination of how Open Badges claims are described semantically. Open Badges defines three primary data classes, with one of each making up a valid badge awarded to a single recipient. The Open Badges Specification , currently in version 2.0, is published by the IMS Global Learning Consortium. The Issuer Profile describes the issuer entity, whether that is an individual, an organization, or something else, like an autonomous actor. The BadgeClass describes a particular achievement, the criteria that all recipients must pass in order to be awarded the badge. Each BadgeClass has an issuer, its creator. The Assertion describes an instance of the achievement as it applies to a single recipient. It identifies which BadgeClass is awarded, and describes the recipient of the award using a single string-type identifier (such as an email address). The Assertion does not currently identify its issuer directly, but only that the issuer of the BadgeClass may publish valid Assertions. The Open Badges validator(s) check to ensure the Assertion is either cryptographically signed with one of the Issuer's authorized keys or uses a hosted verification URI within the Issuer's allowed scope. Members of the Open Badges community have long talked about use cases for Assertions with a different issuer than the issuer of their BadgeClass, but this feature has not yet been implemented in the specification.","title":"The Open Badges Vocabulary"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#the-verifiable-credentials-specification","text":"Members of the W3C and Rebooting Web of Trust community have been developing the idea of Verifiable Credentials (formerly Verifiable Claims) for several years, and it has now been picked up as official work of the W3C Verifiable Credentials Working Group . The current editor's draft of the VC Data Model describes that a Credential is a set of one or more claims about a subject . All of the claims made by traditional credentials such as driver's licenses may be made by a digital Verifiable Credential, and many novel claims may also be made. VCs become verifiable through proofs , such as cryptographic signatures.","title":"The Verifiable Credentials Specification"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#implementation-present-and-future","text":"The Open Badges Specification defines everything needed to create an ecosystem where credentials circulate. The core vocabulary allows issuers to describe their profiles and achievements (available and awarded) in JSON-LD. The verification protocols for hosted and signed badges describe how to establish that a particular Assertion is the valid expression of a badge award published by an Issuer Profile, while the Baking Specification describes a method by which Assertion data may be embedded within a PNG or SVG image file, which is portable across many file systems and publishable on websites. There are dozens of implementers of Open Badges across the roles of Issuer, Host (commonly known as a \"backpack\"), and Displayer, including several platforms that are certified by IMS Global as compliant implementers of the 2.0 specification in these roles. Over 10 million credentials have been awarded as Open Badges. Going forward from 2.0, community members are interested in increased flexibility to describe credentials, better information about how specific badges and issuers are situated in networks of trust, and a growing ecosystem of adopters with good complementary integration with other specifications. The natural focus of the Open Badges Specification is as a vocabulary to describe defined achievements . There are advantages to be gained in consistent implementation of a common vocabulary for achievement credentials. Verifiable Credentials has a vibrant community of interested implementers, including several who are using draft versions of the specification in production systems. The Verifiable Credentials specification is a general purpose technology with a wide range of potential use cases. The specification's focus is not on a particular type of credential or specific set of terms and it does not have an inherent concept of a defined achievement like Open Badges does. The strength for Verifiable Credentials as a specification is to enable innovation in different types of proofs, claims, and methods of identifying credential subjects. Examples of these efforts include the use of blockchain pointers in proofs and Decentralized Identifiers (DIDs) for identifying subjects. There may be a clean fit between the focus of Open Badges as a controlled vocabulary for defined achievements and the innovation around proofs and subject identifiers occurring in the community of Verifiable Credentials implementers. This paper illustrates how these technologies may move closer to full compatibility by making careful choices at the integration surface. Effectively, the Open Badges ecosystem can implement the Verifiable Credentials data model as an option alongside its existing hosted and signed delivery/verification methods with minimal changes to specification and software in the Issuer, Host, and Displayer roles. Open Badges validation/verification software is most significantly affected, but the open source and shared nature of the official IMS Global Open Badges validator library means very few actual pieces of software must be updated in order to obtain wide-reaching availability of the techniques shown here across the Host and Displayer roles. Services in the Issuer Role have an easier and less critical adoption process, especially as long as existing verification methods remain in place as an option.","title":"Implementation, present and future"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#implementation-options","text":"We have identified two possible ways to connect the Open Badges and Verifiable Credentials specifications. We will circulate and compare the core options here and choose which to base our prototypes upon.","title":"Implementation Options"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#option-1-a-verifiable-credentialassertion-claims-that-an-entity-holds-an-badgeclass","text":"The Verifiable Credentials Data model has moved to enable some \"credential metadata\" options that parallel attributes of Open Badges Assertions, including evidence. The core purpose of an Assertion is to describe the instance of an award, an issuer's claim that a particular recipient has met the criteria defined in a BadgeClass and to present the evidence related to that. While Open Badges have a badge property that points to \"the BadgeClass definition that is presently asserted\" and a separate recipient property that serves to identify the recipient by some string-based identifier, such as an email or a DID, Verifiable Credentials have a claim that an issuer makes to describe attributes of a recipient. In the Verifiable Credentials context, the way to describe the Open Badges concept of \"earning an instance of a BadgeClass definition\" is to claim that \"a recipient, identified by an IRI or by an attribute of a different type of string-type identifier, holds or has met the criteria of a BadgeClass definition\". Here is an example of a Verifiable Credential/Assertion that a recipient identified by a HTTP profile URI holds a certain BadgeClass. In this and future examples, id is an alias in the Open Badges and Verifiable Credentials context of the linked data subject identifier keyword, canonically @id . This requires the introduction of a new holds property in the Open Badges vocabulary in order to make the claim shown here \"The entity identified as https://example.com/profiles/bob holds the BadgeClass 'Certificate of Accomplishment'\". { \"@context\": [\"https://w3id.org/credentials/v1\", \"https://w3id.org/openbadges/v2\"], \"id\": \"https://example.com/assertions/1001\", \"type\": [\"Credential\", \"Assertion\"], \"issuer\": \"https://example.com/profiles/alice\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": { \"id\": \"https://example.com/profiles/bob\", \"obi:holds\": { \"id\": \"https://example.com/badgeclasses/123\", \"type\": \"BadgeClass\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"A badge describing great accomplishments\", \"criteria\": { \"narrative\": \"Perform tasks of valor and wit.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"https://example.com/profiles/alice\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } } }, \"obi:evidence\": { \"id\": \"https://example.org/portfolios/25\", \"name\": \"Bob's Portfolio\", \"narrative\": \"Bob worked hard to develop a good portfolio\", \"genre\": \"ePortfolio\" }, \"sec:proof\": { \"@graph\": { \"type\": \"sec:RsaSignature2018\", \"created\": \"2018-03-07T19:22:15Z\", \"creator\": \"https://example.com/profiles/alice/keys/1\", \"sec:jws\": \"eyJhbGciOiJQUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..p9RDJqwzXyxfC69dMEz503_ZZ_af1e_hV931dPlIdrofC6p2y_dcjjqIDysReJy6W_fnN_dZGVoXqFAg2OD_SmbDi5dNMOZILot-zJdDJCxXWuwZtiCFlt29KfLmJs6me0bD5pU4RbknXDoyBhA8muMby8j1fUeBDo3Ienmzv5UlB3v0f0-w5l6-z_cswHB_UXIlWw4EzcsmLvHzjB7TI76QLwq3KeVPSB3U9aM3o2Ejkq6Ygh5XxUGkXiZUQ5ungQ9Psy_VicjZyOc19LoBPoiPxDHQodTrqCFNH2qCNhDc4lg2zE8S9KNlQhUUFatzkTN70s23fhWBMKz2a5DWgQ\" } } } Values for the claim may vary in two meaningful cases: The issuer wishes to take advantage of the light anti-correlation mechanism enabled by Open Badges hashed recipient identifiers for a recipient who is identified by an id string. The issuer wishes to identify the recipient by a profile identifier property other than id , such as email . Use of hashed identifier may or may not be used. In either case, the implementation option is to make an additional claim about the recipient. In addition to the above claim that \"a recipient holds or has met the criteria of a BadgeClass definition\", the issuer makes a claim that the recipient \"holds an identifier of a particular type\" using the Open Badges IdentityObject : { ... \"claim\": { \"holds\": { \"type\": \"BadgeClass\" }, \"recipient\": { \"type\": \"email\", \"hashed\": false, \"identity\": \"testrecipient@example.com\" } } } This example does not use any id at all for the credential's claim. This is nonstandard in the VC ecosystem if not disallowed. If an issuer desires to not omit id while still identifying the recipient via the IdentityObject, a one-time IRI may be generated, such as a randomly UUID, using the urn:uuid: scheme. An Open Badges verifier should assume that if a claim is found where obi:holds and recipient are claimed, the recipient IdentityObject takes precedence over the claim subject id or that both are valid ways to refer to the recipient.","title":"Option 1: A Verifiable Credential/Assertion claims that an entity holds an BadgeClass"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#option-2-an-assertion-is-a-verifiable-credential-claim","text":"The Verifiable Credentials Specification allows issuers to make a claim about a subject while remaining agnostic to the content of that claim. It is possible to use an Open Badges Assertion as the claim of a Verifiable Credential, signed with a Linked Data Signature. The ID for the key and its owner are modeled as an HTTP-resolvable document as in the examples on the jsonld-signatures library. { \"@context\": \"https://w3id.org/credentials/v1\", \"id\": \"https://some.university.edu/credentials/9732\", \"type\": [\"Credential\", \"OpenBadgeCredential\"], \"issuer\": \"https://example.com/i/alice\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": [{ \"@context\": \"https://w3id.org/openbadges/v2\", \"id\": \"urn:uuid:437fc6ff-bb3c-4987-a4b7-be8661ff6f21\", \"type\": \"Assertion\", \"recipient\": { \"type\": \"email\", \"identity\": \"testrecipient@example.com\", \"hashed\": false }, \"badge\": { \"type\": \"BadgeClass\", \"id\": \"urn:uuid:7aad3c57-3bfb-45ea-ae79-5a6023cc62e4\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"A badge describing great accomplishments\", \"criteria\": { \"narrative\": \"Perform tasks of valor and wit.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"https://example.com/profiles/alice\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } }, \"verification\": { \"type\": \"VerifiableClaim2018\" } }], \"sec:proof\": { \"@graph\": { \"type\": \"sec:RsaSignature2018\", \"created\": \"2018-03-07T19:22:15Z\", \"creator\": \"https://example.com/profiles/alice/keys/1\", \"sec:jws\": \"eyJhbGciOiJQUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..p9RDJqwzXyxfC69dMEz503_ZZ_af1e_hV931dPlIdrofC6p2y_dcjjqIDysReJy6W_fnN_dZGVoXqFAg2OD_SmbDi5dNMOZILot-zJdDJCxXWuwZtiCFlt29KfLmJs6me0bD5pU4RbknXDoyBhA8muMby8j1fUeBDo3Ienmzv5UlB3v0f0-w5l6-z_cswHB_UXIlWw4EzcsmLvHzjB7TI76QLwq3KeVPSB3U9aM3o2Ejkq6Ygh5XxUGkXiZUQ5ungQ9Psy_VicjZyOc19LoBPoiPxDHQodTrqCFNH2qCNhDc4lg2zE8S9KNlQhUUFatzkTN70s23fhWBMKz2a5DWgQ\" } } } The example above illustrates the following salient features: The claim ID is the ID of the Assertion, not of the Recipient. Semantically, that means that the Verifiable Credential is effectively claiming \"This Assertion exists with content...\" instead of the Option 1 model of introducing a \"holds\" property making the claim about a recipient instead. For example, \"The subject identified as id holds the Assertion with content\". The above approach is a simple option where we can preserve Open Badges' ability to \"hash\" the recipient identifier. When the recipient id has been hashed in this manner, the Verifiable Credential will likely not be usable in a wallet application intended for VCs. But because there will remain a strong market for Open Badges-specific tooling to serve recipients, this type of VC with a hashed subject identifier will likely remain useful as a light layer of personally identifiable information protection above a base VC. There is some duplication in the data package around the issuer. Future relaxation of the Open Badges Specification may become possible when the Assertion issuer is defined elsewhere in the \"envelope\", but the relative cost of duplicating this data to achieve parity with the current Open Badges Specification is low and will minimize the work needed in Open Badges tools to implement this delivery method.","title":"Option 2: An Assertion is a Verifiable Credential Claim"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#comparing-options-1-and-2","text":"Option 1 uses a recipient subject identifier in the same way as other claims, which is expected to increase compatibility with VC-native tooling. On the other hand, Option 2 requires significantly less work on the part of the Open Badges validator(s) to return a result graph that matches its existing format implemented based on Open Badges 2.0. Option 2, in which the claim is roughly the same as the existing Assertion class, is a valid Verifiable Credential. But because the claim subject is the Assertion itself instead of the recipient, this is not assumed to be a very strong integration path that would allow Open Badges Verifiable Credentials to be circulated among general purpose Verifiable Credential \"wallet\" tools. These tools expect claims to be made about their users, not about entities that have relationships to their users.","title":"Comparing Options 1 and 2"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#an-assertion-awarded-to-a-recipient-identified-by-a-did","text":"In the existing Open Badges ecosystem, recipients are identified by email address virtually all of the time. Open Badges 2.0 strengthened and clarified the ability to award to other types of identifiers, but Assertions far more often use email. An example implementation in an Assertion could identify a recipient like this: \"recipient\": { \"type\": \"email\", \"identity\": \"testrecipient@example.com\", \"hashed\": false } The Open Badges Specification allows for several string-type attributes of an entity to be used as an Assertion's Recipient Profile Identifier Property . Decentralized Identifiers (DIDs) are emerging as a new type of entity identifier that comes in an IRI string format. They have the ability to be deterministically \"resolved\" to a \"DID Document\". The resolved documents then describe the functionally useful aspects of its identity, such as the public keys and other authentication methods that can be used to connect a user in a web browser or a human sitting across the table from you to the DID identifier. As Open Badges can be awarded to other string-type identifiers, like @id (aliased as id in the Open Badges & Verifiable Credentials contexts). We can identify a recipient in an Option 2-style Assertion like: \"recipient\": { \"identity\": \"did:example:recipient_did\", \"type\": \"id\", \"hashed\": false } This is all that must be done to award an Assertion to a DID as recipient identifier. The verification that an assertion is awarded to the expected DID is already supported in the Open Badges Validator. The Specification can be clarified to add \"id\" to the list of identifier types \"considered serviceable\", but that will constitute a clarification rather than a change.","title":"An Assertion Awarded to a Recipient Identified by a DID"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#an-assertion-awarded-by-an-issuer-identified-with-a-did","text":"The primary use case for a Decentralized Identifier (DID) string is to resolve it to a DID Document , which describes the entity; specifically, it has attributes like public keys that allow for the authentication of messages sent by and under the auspices of the entity that controls the DID. Verifiable Credentials are one type of message that can be signed and delivered by a DID-identified entity, and Open Badges Verifiable Credentials are yet another type of Verifiable Credential that can be signed in the same way. With this model in place, viewers of Open Badges (through Verifiers, Backpacks/Hosts and Displayers) can resolve a DID to a DID Document, access the authorized public keys associated with that DID, determine which authorized key was identified as the creator of a Credential signature, and verify the authenticity of that signature. In the current Open Badges Specification, even when using the SignedBadge verification method, there are still at least two resources that must be hosted on HTTP(s) in order to be able to produce valid Open Badges Assertions: the Issuer Profile and the CryptographicKey (identified by the publicKey attribute of the Issuer Profile). Using a DID for the Issuer Profile id instead of an HTTP URI allows for open badge data to be entirely uncoupled from HTTP hosting dependencies. Here is an example DID for an issuer: did:example:issuer_did . The part of the DID between the first and second : characters identifies the resolver method . The part after the second : is the DID path . If a service has implemented the algorithm specified by the particular resolver method used, it can consistently retrieve a DID document identified by the DID. We now show an example DID document for the did:example:issuer_did issuer described above. An implementer of the example resolver method could have retrieved the DID document using the method specified by the method specified. For example, by fetching data from a specific blockchain and deriving the DID document from it. { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:example:issuer_did\", \"publicKey\": [{ \"id\": \"did:example:issuer_did#keys-1\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:issuer_did\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA2x3wWF6LY1pNxxTIUfPtHBGvPgcVvVDaoGpOjMk9+QfcXBVzcEnIlYenI8WfvoNSNUbblmDyn5sqWg9KyQpcoJ2IAoemjTcb+Skw9PaS2KAYjCYq9pmnvGhmmXJjk1xuT3gevG8K9XGJ2MmuqdKQ4yzfPhD5kHdLxV9Y9VY2rgZJzSNL83Oz596tcFbA1QB0p8wj7xQpLYRu5d1Mz+1qu1E8NM6HPgSBp54JHJF0yL3s39rGNbxmopwCq1Vw9E22ZnJpHQtc4nq4N3JksfVPeHhirC3eny0YS78Z6W7bGlVT+bf+T6r43Rq8kQ7N8hVLrGHc+NuHXm1JBIbpKwIDAQAB-----END PUBLIC KEY-----\\r\\n\" }], \"authentication\": [{ \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"did:example:issuer_did#keys-1\" }], \"service\": [{ \"type\": \"ExampleService\", \"serviceEndpoint\": \"https://example.com/endpoint/8377464\" }] } Within such a DID Document, one or more public keys can be identified. In the example above, a single RSA key is described. An Open Badges verifier that supports the required DID resolver method discovers keys that can be trusted to be under the control of the entity. The resolver then verifies that a particular Open Badges Verifiable Claim is signed by a key pair belonging to the entity. Here is an example of what such a signed Assertion looks like for this issuer: { \"@context\": \"https://w3id.org/credentials/v1\", \"id\": \"urn:uuid:01f0bb90-86ee-4469-9655-7ca6f4d591ae\", \"type\": [\"Credential\", \"OpenBadgeCredential\"], \"issuer\": \"did:example:issuer_did\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": [{ \"@context\": \"https://w3id.org/openbadges/v2\", \"id\": \"urn:uuid:437fc6ff-bb3c-4987-a4b7-be8661ff6f21\", \"type\": \"Assertion\", \"issuedOn\": \"2018-02-25T00:00:00+00:00\", \"recipient\": { \"type\": \"id\", \"identity\": \"did:example:recipient_did\", \"hashed\": false }, \"badge\": { \"type\": \"BadgeClass\", \"id\": \"urn:uuid:7aad3c57-3bfb-45ea-ae79-5a6023cc62e4\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"Lorem ipsum dolor sit amet, mei docendi concludaturque ad, cu nec partem graece. Est aperiam consetetur cu, expetenda moderatius neglegentur ei nam, suas dolor laudem eam an.\", \"criteria\": { \"narrative\": \"Nibh iriure ei nam, modo ridens neglegentur mel eu. At his cibo mucius.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"did:example:issuer_did\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } }, \"verification\": { \"type\": \"VerifiableClaim2018\" } }], \"sec:proof\": { \"@graph\": { \"type\": \"sec:RsaSignature2018\", \"created\": \"2018-03-07T22:26:57Z\", \"creator\": \"did:example:issuer_did#keys-1\", \"sec:jws\": \"eyJhbGciOiJQUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..GMPdCXn-qQcZHPoO6qHn6hBiysMNaZ7nSBx_e27LuDxJvRsCLbR1n7LGG7i8NVW1SVMwRjs8aJ3H2XXFphCZF_dGaueTsaehTzLQgh9n5imPgrQFsAKsRAKTJ_zpVL8JpsbPcrXbb-fkAcD52oDuYJg1uVr3MOhe4BzibDUKaFg5-cXZ-Gs8KcXrh_Ddqtd8CWw0zS3fRvI3SKbO6op6hNB1Jha4mfAn49Q0BRiSuCxbyPNy5MtX7FGoimvLhsluM7UAtPWHBi6iW8Nh57fk4uS5ZywHJSYS9-HPcvbDUGPHPHOnwq4qq7xc47yXveMmyo2VX4YSYe3LM-_9w1TnGg\" } } } The above example shows a signed message where the claim is the Assertion (Option 2), but the same type of signature could be applied to Option 1 and any other Verifiable Credential. The issuer id declared in the claim.badge.issuer is the DID that we expect the verifier will resolve to the did:example:issuer_did DID Document shown earlier in this section. When the verifier encounters this \"did:example\" IRI scheme, it will 1) resolve this DID to its corresponding DID Document, 2) verify the signature, and 3) verify that an authorized key created the signature. There is some repetition of data in the example above. The issuer id is included at both issuer and claim.badge.issuer . While this data can be normalized to only appear at issuer , leaving it in supports consistency within the Open Badges Specification \u2014 there are no breaking changes needed to add this new verification option. In addition, there are attributes of the Issuer Profile that are not assumed to be verifiable by DID resolver methods, such as the name of the issuer, which is used by the Open Badges ecosystem. A version of the claim using Option 1 looks very similar to non-DID-approach for the issuer, but the validator would be expected to resolve the issuer DID to the DID Document and to use that document to get the applicable signing keys for the issuer and confirm the Verifiable Credential was signed with one of those keys. { \"@context\": [\"https://w3id.org/credentials/v1\", \"https://w3id.org/openbadges/v2\"], \"id\": \"https://example.com/assertions/1001\", \"type\": [\"Credential\", \"Assertion\"], \"issuer\": \"did:example:issuer_did\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": { \"id\": \"did:example:recipient_did\", \"obi:holds\": { \"id\": \"urn:uuid:7aad3c57-3bfb-45ea-ae79-5a6023cc62e4\", \"type\": \"BadgeClass\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"A badge describing great accomplishments\", \"criteria\": { \"narrative\": \"Perform tasks of valor and wit.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"did:example:issuer_did\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } } }, \"obi:evidence\": { \"id\": \"https://example.org/portfolios/25\", \"name\": \"Bob's Portfolio\", \"narrative\": \"Bob worked hard to develop a good portfolio\", \"genre\": \"ePortfolio\" }, \"sec:proof\": { \"@graph\": { \"type\": \"sec:RsaSignature2018\", \"created\": \"2018-03-07T19:22:15Z\", \"creator\": \"did:example:issuer_did#keys-1\", \"sec:jws\": \"eyJhbGciOiJQUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..p9RDJqwzXyxfC69dMEz503_ZZ_af1e_hV931dPlIdrofC6p2y_dcjjqIDysReJy6W_fnN_dZGVoXqFAg2OD_SmbDi5dNMOZILot-zJdDJCxXWuwZtiCFlt29KfLmJs6me0bD5pU4RbknXDoyBhA8muMby8j1fUeBDo3Ienmzv5UlB3v0f0-w5l6-z_cswHB_UXIlWw4EzcsmLvHzjB7TI76QLwq3KeVPSB3U9aM3o2Ejkq6Ygh5XxUGkXiZUQ5ungQ9Psy_VicjZyOc19LoBPoiPxDHQodTrqCFNH2qCNhDc4lg2zE8S9KNlQhUUFatzkTN70s23fhWBMKz2a5DWgQ\" } } } An important capability enabled by using a DID instead of an HTTP URI as an Issuer id is that the issuer entity can authenticate into multiple applications that will then have equal ability to act on behalf of the issuer entity. There will be no implied primary/secondary distinction between these applications, because none of them will \"own\" the identifier the way DNS allows only one entity to authoritatively own an HTTP identifier. The DID Document doesn't have any ability to describe in a verifiable sense the issuer's name, URL or email, but these properties are desirable to display within badging systems as part of the Open Badges Profile class, so they are embedded in the claim in these examples. Open Badges ecosystem tools need to determine for themselves when and why they trust these values to be correctly associated with an issuer ID. We can authenticate the DID, but not these properties directly. However, they may be the claims of other Verifiable Credentials where the Issuer is the subject. If a consumer trusts one or more of these claims, they could trust this data wherever it is presented associated with the issuer. This is likely a case where an issuer would want to reference Endorsements (the Open Badges Vocabulary term for \"plain\" Verifiable Credentials) they have received within the Issuer Profile that gets embedded in claims like the above example.","title":"An Assertion Awarded by an Issuer identified with a DID"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#blockchain-proof-of-existence-with-blockcerts","text":"Blockchain-tethered issuance of Open Badges/Verifiable Credentials is enabled by the same JSON-LD signature and verification framework described here. The MerkleProof2017 LD signature suite allows verification of data anchored to a blockchain. This is the same signature suite used by the current version of Blockcerts, The above Option 1 example modified to use this MerkleProof2017 signature suite looks the following: { \"@context\": [\"https://w3id.org/credentials/v1\", \"https://w3id.org/openbadges/v2\", \"https://w3id.org/blockcerts/v2\"], \"id\": \"https://example.com/assertions/1001\", \"type\": [\"Credential\", \"Assertion\"], \"issuer\": \"did:example:issuer_did\", \"issued\": \"2018-02-28T14:58:57.461422+00:00\", \"claim\": { \"id\": \"did:example:recipient_did\", \"obi:holds\": { \"id\": \"urn:uuid:7aad3c57-3bfb-45ea-ae79-5a6023cc62e4\", \"type\": \"BadgeClass\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"A badge describing great accomplishments\", \"criteria\": { \"narrative\": \"Perform tasks of valor and wit.\" }, \"issuer\": { \"type\": \"Profile\", \"id\": \"did:example:issuer_did\", \"name\": \"Example Issuer\", \"url\": \"http://example.com\", \"email\": \"test@example.com\" } } }, \"obi:evidence\": { \"id\": \"https://example.org/portfolios/25\", \"name\": \"Bob's Portfolio\", \"narrative\": \"Bob worked hard to develop a good portfolio\", \"genre\": \"ePortfolio\" }, \"sec:proof\": { \"type\": \"MerkleProof2017\", \"targetHash\": \"637ec732fa4b7b56f4c15a6a12680519a17a9e9eade09f5b424a48eb0e6f5ad0\", \"merkleRoot\": \"f029b45bb1a7b1f0b970f6de35344b73cccd16177b4c037acbc2541c7fc27078\", \"anchors\": [ { \"sourceId\": \"d75b7a5bdb3d5244b753e6b84e987267cfa4ffa7a532a2ed49ad3848be1d82f8\", \"type\": \"BTCOpReturn\" }], \"proof\": [{ \"right\": \"11174e220fe74de907d1107e2a357e41434123f2948fc6b946fbfd7e3e3eecd1\" }] } } } This demonstrates a special form of proof, applying to the transaction containing the data. This proof technique is known as a \"Merkle proof\" \u2014 a cryptographic data structure used to show that the present credential belongs to a structure that is anchored to a blockchain transaction. This provides proof of existence through confidence in the timestamp of the Assertion, via the timestamp (if supported by the blockchain) associated with the block's addition to the ledger (according to the blockchain's consensus protocol and confirmation threshold). This also allows more than one credential to be part of a single blockchain transaction; batching of credentials is commonly performed for cost savings alongside natural logical groupings of credentials (e.g. \"Graduates of 2018\"). Informally, the Merkle proof, along with JSON-LD normalization, allows you to confirm that the local data matches an expected hash, and that hash combined with redacted data (and functions of redacted data), combine to a value that matches the value on the blockchain. More precisely, during verification, the hash of the present credential is calculated to ensure it matches targetHash . The proof section is how to merge this local credential with one-way hashes of other credentials (to avoid revealing the contents) into a tree structure, the base of which should equal merkleRoot . Lastly, the merkleRoot value is the expected value on the blockchain, which we can independently confirm. In the anchors part of the proof, the type and sourceId incidate which blockchain, transaction id, and transaction field to fetch; this value should equal the merkleRoot value. This example shows an issuer identified with a DID ( did:example:issuer_did ). This enables an improved authenticity check during the Blockcerts verification process, which will ensure the public key that signed the issuing transaction is authorized by the DID document associated with the issuer's DID. This format is proposed for Blockcerts v3, and has the advantage of unifying Blockcerts more tightly into Verifiable Credentials / Open Badges, as opposed to a separate Open Badge extension.","title":"Blockchain proof of existence with Blockcerts"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#implementing-user-stories-for-open-badges-tools","text":"The two different options outlined in this paper pose different distributions of cost and complexity on Open Badges Applications in the core roles of Validator, Issuer, Host, and Displayer. When choosing which option to pursue to implement the capabilities described above, understanding the implementation path for each option enables cost/benefit analysis.","title":"Implementing User Stories for Open Badges Tools"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#validator","text":"The Open Badges Validator is the main application that needs to be updated to support these features. The validator will need to be modified to detect that an input string is valid JSON-LD and of type Credential . The validator then queues a relevant procedure to intake the document that will be an alternative path to the existing procedures for intake based on a hosted URL input, image input, JWS string input, and existing JSON strings of Assertion, BadgeClass or Profile.","title":"Validator"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#validating-option-2","text":"Because internally, Option 2 looks entirely like an existing Assertion, the validator will not need to make many modifications to its internal expectations about how Open Badges objects are published. Resolving DIDs in this example occurs in neat parallel to resolving HTTP(s) identifiers into usable documents, but instead of the HTTP-based integrity verification tasks, DID-specific tasks will be queued, including: Resolve document from the DID. Verify DID Document signature (if signed) and requirement for it to be signed for particular contexts. Verify proofs for a few relevant Linked Data Signature Suites. Confirm badge object signing key is authorized to sign on behalf of DID-identified entity.","title":"Validating Option 2"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#validating-option-1","text":"Option 1 requires some changes to the Validator in addition to the DID resolution and signature verification changes. Update expectation for client that the Verifiable Credential style class can appear in the resultant data graph in place of the \"bare\" Assertion class. This structure is fundamentally the same as that already appears in the graph for Open Badges Endorsements, which are referenced from within the objects discovered while processing the verification input. Add procedure to queue up tests for the obi:holds claim in an VC that is found as part of processing. For convenience, the validator should be able to deliver a version of the VC translated into the 2.0 Assertion style to serve Backpacks and Displayers that cannot yet adapt to the Option 2 VC as Assertion model. This legacy version should be set as the \"validationSubject\" in the validation report.","title":"Validating Option 1"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#issuer","text":"The OpenBadgeCredential wrapper is an optional verification type for Open Badges. That means issuers don't need to implement it in order to continue to issue valid Open Badges. Once the validator is updated, issuers can publish badges using these options, with very little distinction between them. To use a DID as a recipient identifier, badges can be published with any method supported by the validator that uses the DID for recipient identification. It may be the subject identifier of a claim (as in Option 1) or it may be hashed in an IdentityObject (Option 1 or 2), for instance. In order to use a DID as an issuer identifier, the Issuer Service will need to create a new DID or gain access to an authorized signing key created by the DID Owner user (e.g. \"Alice\"), for which the validator will be able to confirm authorization. For example, Alice could use the features of her DID Resolver Method to add the Issuer Service's signing key to her DID Document. Some implementing Issuer services may register the DID themselves and act as its owner, instead of authenticating and gaining access to a DID created by a user. This would likely not allow the user of the Issuer System who created the Issuer Profile to independently act as the DID owner themselves outside of the system, but it is a potentially valid path to implementation. Extending the possibilities explored by example above, one emerging complementary technology is Linked Data Object Capabilities . Under a system that uses this, Alice will act as her own DID Owner and will grant the Issuer Service the capabilities to perform several relevant Open Badges actions on her behalf, such as Issuing. When the issuing system invokes these capabilities, it will reference the capability chain in its proof. Capabilities are safer than adding an external service's key to your DID Document as if it were your own, because Capabilities may be attenuated to only grant specific actions to the capability holder. To the validator, if the Validator supported Linked Data Capabilities, it would appear as if Alice issued a signed Verifiable Credential Open Badge herself, though the audit trail would be available to determine that a key held by the Issuer Service invoked an Issue capability in order to sign the valid proof. This topic is explored in more detail in a complementary paper about Open Badges Peer Claims . Another alternative would be for Alice to sign new Assertions interactively, using a private key held only on a trusted device. The issuer service could prepare the Assertions for signature and then obtain the signature from Alice directly.","title":"Issuer"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#hostbackpack","text":"Backpacks mostly rely on the verification information returned from a validator, so if the validator can be updated to present the Open Badges data in a predictable supported format, hosts don't need to specially support the Verifiable Credentials wrapper. Verifying that badges belong to users of the Host service is one of their most important capabilities, however, and the introduction of DIDs as recipient identifiers introduces significant new requirements around that, similar in scope to issuer support. There are some emerging capabilities being described for the Authentication of users identified by a DID who are interacting with a system via browser. These are in very early draft stages, so backpacks that wish to resolve and authenticate users as owners of DIDs will likely need to implement an interactive process. These features could be implemented in Host services or via external identity providers. Collaboration with implementers of DID-resolver and authentication services seems like a fruitful direction in order to achieve a good pace of adoption and even implementation support across an ecosystem for the various resolver methods and authentication methods that will begin to proliferate. If an external identity provider is trusted by an Open Badges Host, a connection could be made via OAuth2 or similar protocol that may already be built into a backpack product. DID owners may provide one or more methods of authentication in their DID document, and relying services may select an authentication method from those available. They would establish a communication channel with a user attempting to log in to send an authentication challenge to the user's selected authentication device. For example, if the user keeps their signing key(s) on a mobile phone, they could be directed to scan a QR Code to obtain authentication challenge data, which they would sign and submit, completing an authentication process on the device where they started the request, upon which, they could be sent back to the Host, authenticated as the owner of the DID. Once a Host has authenticated that a particular local user controls a specific DID, it may accept badges into that user's account that identify their recipient using that DID.","title":"Host/Backpack"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#displayer","text":"Displayers, like Hosts, rely on the return value of the validator to display trustworthy data, so if the validator returns essentially the same data structure, very few updates will be needed to support awards in VC format. Prototypes should negotiate a balance between adapting Displayers to handle the VC that makes a obi:holds claim and the validator returning a 2.0 Assertion style object in the graph that it synthesizes.","title":"Displayer"},{"location":"RWoT6/open-badges-are-verifiable-credentials/#next-steps","text":"An implementation of these capabilities will be best informed by prototypes of experimental branches of open source Open Badges software that fills the different ecosystem roles. Our primary goal with this proposal is to gain feedback from the Open Badges and Verifiable Credentials communities. We have planned implementations to prove out these concepts, including: - Open Badges Validator support for JSON-LD signature verification using Option 1 with Option 2 as a fallback. - Issuing Blockerts credentials that can be validated in the format described here. - Adding support for Blockcerts blockchain pointer in validator. - Collaborating with those in the Rebooting Web of Trust Community to integrate at least one method of DID Authentication into an Open Badges Host to connect reliably. The lessons learned from these prototypes will feed use cases to submit in the next stages of standardization of Open Badges and related technologies.","title":"Next Steps"},{"location":"RWoT6/sustainable-commons/","text":"Exploring Sustainable Technology Commons using Appreciative Inquiry Version 1.2: For Final Submission Rebooting Web of Trust 6 Santa Barbara, March 6-8, 2018 Wordcount: 2675 Lead Author Heather Vescent (heathervescent@gmail.com) Authors Kaliya \u201cIdentity Woman\u201d Young (kaliya@identitywoman.net) Adrian Gropper (Agropper@healthurl.com) Juan Caballero (juanxcaballero@gmail.com) Keywords : Commons, Technology Commons, Open Source, Technology Standards, Market Externalities, Sustainable Capitalism, Technology Infrastructure, Economics Introduction Technology Commons are critical for the development of technology that is used across platforms, companies, and industries. In the internet age, corporations structured around monetizing defensible intellectual property frequently utilize common technology to produce innovative products that work across shared infrastructure, including most of the core platforms and building blocks of what we know as the internet industry. These technology commons include the Internet and the standards that make it run. There are several standards bodies that support the development of these standards, primarily the IETF, W3C, and OASIS. The W3C defined web standards that could be used by everyone (including companies, individuals, nonprofits, governments) for the creation of the web. As one common saying summarized the thinking of the early 00s, \u201cNobody owns it, everyone can use it, and everyone can improve it.\u201d Technology commons come in a variety of flavors and have achieved varying levels of financial success. For-profit corporate activities have in few historical cases been set up with a financial feedback mechanism to support the commons upon which they depend and capitalize. Why do the commons and the technology sectors\u2019 available forms of capitalism act as incompatible as oil and water, even though they support each other\u2019s aims? When capitalist benefactors support the technology commons that they utilize, it creates a sustainable and thriving commons which enables and supports additional capitalistic technology innovation. Having worked on both sides of the equation, the authors of this piece propose a vocabulary to nourish these interactions between the two sides; identified characteristics of a sustainable technology commons; identified commons models and variations; applied Appreciative Inquiry principles to one commons model; and identified future research areas. What is a Technology Commons? We identified four characteristics of sustainable technology commons. These characteristics grow in an atmosphere of identity and autonomy, yet the pressures of interfacing with existing systems (data systems but also systems of intellectual property and/or social control and capital capture) make them feel like distant abstractions in 2018. Re-decentralization: this term names the reverse trend away from our current web ecosystem driven by privacy-poor, capital-rich big-data players and towards decentralized forms of the Web, ideally via self-sovereign technology that distributes data power and governance downward. It is the counter-trend to centralized web services successfully deployed by a few massively powerful corporations (e.g. Google, Amazon), and is often construed in cultural and ideological terms as a return towards the attitude of the early internet. Sustainable Capitalism: sustainable technologies foster sustainable forms of capitalism and intellectual property. These forms consider economic externalities and end-to-end infrastructural costs and include them in their calculations of market ROI. They protect the standards process from financial influence (regulatory capture), while leveraging market diversity/competition and production, without harming the commons. Infrastructure: elaborating on the last point, these interactions of capital and the common good would need to treat specifications and protocols as a public infrastructure, and thus consider all web users as sovereign stakeholders. By this analogy, internet and other information technology protocols are similar to roads and other essential infrastructure that is used by many entities and people. Human Dignity: protecting privacy and self-sovereignty are an information protocol\u2019s duty to human dignity, as many aspects of individual and family health, property rights, and political participation hinge upon privacy and control over one\u2019s own data in ways both culturally-specific and protected by international law. Much as economic externalities have been excluded from the economic calculus of private corporations operating on a broader public, so too have the rights of individuals fallen neatly outside the ethical consideration of technological projects scoped only by their direct litigative and regulatory obligations, typically referred to as compliance. To understand what works in the real world, we identified existing technology commons in which we participate firsthand and applied a form of Appreciative Inquiry. We have tried to apply terminology consistent with Christopher Allen\u2019s adaptation of Nobel Laureate Elinor Ostrom's \u201c8 principles of Commons Governance\u201d as spelled out in his succinct 2015 blog post, available here . Current Technology Commons Models Commons are many and vary in the models that they apply, how they are used, and who uses them. The authors identified multiple commons and commons-adjacent models of IT development and variations within those models used to build/design trust systems and protocols. Ad-hoc Technology R&D: Like-minded people organically create communities to explore new technology. These communities can start inside another community or due to the death of a community. The Internet Identity Workshop (IIW) and Rebooting Web of Trust (RWoT) are two examples of community-created technology commons. IIW in particular has been a spawning ground for new protocols, incubating ongoing work and the formation of associations for the general benefit of the broad digital identity space. The boundaries and technology ownership in these communities are often vague and there tends to be less corporate control of products created within the community. Participants tend to be early adopters exploring the bleeding edge of new technology. Participants may come from company technology divisions, however they are rarely expected by their employers to return significant economic benefit. We identified the following variations to this base model. Variations are not mutually exclusive: Benevolent Dictator: one or a small number of primary organizer(s) take(s) the main leadership role, primarily for altruistic, ideological, or other non-monetary motivation. Do-Ocracy: a peer to peer volunteerism model, where the people doing the work make the decisions, often in a very ad hoc or social/informal governance model. Blockchain and Fat Protocols: a new model that structures ownership and governance of a project around a central ledger. One key trait of this model is the potential to issue residual payments and governance to contributors over time, independent of traditional employment or stock ownership; theoretically, this could include sending residuals back to a commons or collective contributor post-factum. Examples: RWoT, IIW, Digital Life Collective, other public blockchains. Open Standards: Open standards are the development of approved standards that are open and available to the public. They can be thought of as public infrastructure. Regulators enforce regulations and depend on industries to write the standards to support the regulations. Regulatory capture can occur when the people writing the standards hold regulators hostage to use their standards. An example of regulatory capture can be seen in health records interoperability. Health records themselves are specific to the health vertical and need to be developed by healthcare industry incumbents. Patient and physician identity and authorization protocols however are not specific to the healthcare industry and could be left to others. This, however reduces the market power of the healthcare incumbents so they prefer to reinvent or \u201cenhance\u201d authentication and authorization standards to raise the barrier to entry. We believe involving multiple stakeholders in the standards process can help address this issue; however in the end only implementers, typically industry, can create useful standards. Layered (scope-specific) standards are more resistant to the corrosive influence of regulatory capture. This is a problem in healthcare and other verticals where commerce is not built on mostly open-source technologies. Another problem occurs when these standard groups ignore history and the experience of adjacent groups activities, mainly due to interpersonal power dynamics and differing objectives. Examples: Open-stand.org, Internet Engineering Task Force, HTML, IP, TCP. Regulatory capture example: HL7 mixing data model and authorization to lock out innovation based on non-healthcare standards like UMA (User-Managed Access). Government Funded R&D: The U.S. government has a history of investing in new technology standards to support a diverse and competitive marketplace. One goal of these investments is to support the development of interoperable technology and technology standards that otherwise would not be developed by competitive corners of the free market. Examples: DHS S&T Investments: Digital Bazaar/VeresOne, Evernym/Sovrin, Open Whisper Systems (Signal protocol), Tor. Open Source: It has been argued that open-source code bases in practice are often driven by and structured to maximize how large companies benefit from sharing one large code base for core infrastructure elements. Prominent examples include the Apache Web Server (which serves over 90% of the conventional web at time of press) and the Linux Operating System Almost all large web and tech firms significantly reduce their costs by sharing infrastructural code with each other while competing at the edges of the product. For example, companies like IBM sell their own Web Servers based on Apache. This dual-licensing model (partly proprietary, partly open) enabled by BSD-style licenses uses proprietary licensing fees to pay back into open-source development costs. A common pattern is that companies pay a developer\u2019s salary while they work full time on the open-source code base that has applications for its own proprietary business model. For projects with less obvious proprietary/monetizable applications, the open-source model has had more limited economic success. The question remains: how to compensate developers who are not working for a large company that leverages the open-source code base. Examples: Linux, Apache, Drupal, WordPress. Commons-Adjacent Business Models In addition to true commons models, we identified a variety of for-profit models that are \u201ccommons-adjacent\u201d. Corporate Non-Profit (e.g. SWIFT), Corporate Co-Op (variation): A top-down approach, this is a model where a non-profit representing a consortium of corporate interests is created to solve common technology and/or informational infrastructure problems for the benefit of member organizations. This closed process produces standards that may or may not be open when finalized. The jointly-developed technology can also be licensed or sold on a for- profit basis for the benefit of the consortium companies. While sharing infrastructure costs, these entities may find themselves starved of resources or limited in their ability to explore new technology solutions due to political goals of stakeholder companies (that generally drive resource allocation and development timelines) without much consideration for other stakeholders like end-users and regulatory bodies. Examples: Swift, CableLabs. Closed Technology: A company develops technology to be used by others but is restrictive as to how and who can use it. This is generally seen as a negative in the tech world but one of the most successful software platforms \u2014 the Apple App Store \u2014 utilizes this. This enables Apple to have complete control over what third party applications are allowed and distributed through their platform, retaining the power to revoke and remove that app from previous installations on their hardware. Because Apple owns the hardware and software they are able to retain this power while opening up the platform to developers who follow their rules. Example: Apple App Store Appreciative Inquiry on Ad-hoc Technology Models To understand the positive effects of existing technology commons, we applied Appreciative Inquiry Principles. We limited the scope of this paper to fit into RWoT time, focusing our area of inquiry on the ad-hoc technology R&D community model. Using RWoT, IIW, and Digital Life Collective as examples, we selected the positive aspects across these instances. While we worked from a limited set of examples, the authors have deep knowledge about them due to the longevity of the events and author participation. We use this starting point as a basis for our inquiry and invite others to build upon it. Appreciative Inquiry Principles We utilized the five principles for each example. The Constructivist Principle is the rule that we see the world as we are: our interior state, our mores, values, and experiences create the lens through which we view the world. The Poetic Principle says what we focus on grows. The Simultaneity Principle is the rule that when we ask questions we have already begun to change. We like to think of this as change in-flight. Change doesn\u2019t happen in a vacuum or only in a certain segment of a process. Change is happening all the time, at any stage of the process. How is your participation changing you right now? The Anticipatory Principle is the rule that what we believe, we conceive. The Positive Principle is where we identify and leverage strengths. We like to remember the successes we have already experienced as part of history. Appreciative Inquiry Model RWoT: workshop focused on co-developing technology papers IIW: unconference for User Centric Digital Identity DigiLife: community building technology with decentralized web values Constructivist Principle : we see the world as we are Focused on outcomes, developing ideas. A space for signals about digital identity to be voiced/named. Grassroots. Sustainable business model (with no exit). Create governance that assigns resources, investments, tracks reputation and credit, using ledgers instead of money itself. Poetic Principle: what we focus on grows Developing ideas. Deliverables: Specs, Papers, Prototype examples. Making the community sustainable. Exploring the edge of what is possible in digital identity with the technology available. Sustainable capitalism and IP. Growing resources / membership to fund and build the technology. Simultaneity Principle: the questions we ask cause us to change. How can we be more sustainable? What comes after federation? What is new? What is emerging? What new technology and ideas gives the community energy? What are our new models of capitalization? How can new models be found? Anticipatory Principle: what we believe, we conceive. By working together, we can do it better and each of us can individually benefit. Participants learn something new. It\u2019s always going to be around. Low barrier for entry/participation. Capitalism will not create technology we trust. We have to create it cooperatively. Positive Principle: identify and leverage strengths. Things are happening, people are working, we are moving the spec forward together. Huge community. Amplifies ideas developing on the edges. Has longevity and sustainability. IIW gives space to others for disparate agendas. Review and vet new technology. Build bridges across nascent technologies. The three examples are similar in that each group actively explores the bleeding edge of their chosen topic through an open community. However, the groups utilizes different governance models, have different community longevity, and focus on different outcomes. Areas for Further Research The initial RWOT topic paper proposed to apply AI to all commons models. However to keep our work in scope with the time available at RWOT, we applied AI to the ad hoc technology common models. We identify the following four areas for follow-up inquiry. Identify a feature set for technology rooted in a sustainable commons.\\ Once we understand the characteristics of a sustainable technology commons, we can draft a set of requirements or use cases that identity technology may be able to \u201csolve\u201d and sustain. Apply Appreciative Inquiry on more of the identified commons models (identified in Part 2). Apply Appreciative Inquiry on RWoT\u2019s Self-sovereign Identity architecture as it is currently being conceived and planned. At IIW in April 2018, a session exploring Self-sovereign Identity was held and a link to notes is in the Bibliography. Explore artificial intelligence as a technology method for the identification and measurement of market externalities. Bibliography The Internet\u2019s Three Virtues by Doc Searls and David Weinberger http://worldofends.com/#BM_8 Economic History of the Open Source movement https://www.amazon.com/Success-Open-Source-Steven-Weber/dp/0674018583 Protocol (Open Standards > Open Source) https://www.amazon.com/Protocol-Control-Exists-Decentralization-Leonardo/dp/0262572338/ref=sr_1_1?ie=UTF8&qid=1520380967&sr=8-1&keywords=protocol+control+decentralization Sustainable/anti-corporate business models https://www.amazon.com/Good-News-Change-Everyday-Helping/dp/155054926X/ref=sr_1_2?ie=UTF8&qid=1520381113&sr=8-2&keywords=good+news+for+a+change Do-ocracy (Anarchist co-ops) http://www.heathervescent.com/heathervescent/2007/04/doocracy.html http://www.heathervescent.com/heathervescent/2007/11/shes-geeky-sess.html http://www.heathervescent.com/heathervescent/2008/01/good-article-on.html On Govt Funded R&D: America by Design https://www.amazon.com/America-Design-Technology-Corporate-Capitalism/dp/0195026187/ Christopher Allen\u2019s blog post about Commons Governance Models http://www.lifewithalacrity.com/2015/11/a-revised-ostroms-design-principles-for-collective-governance-of-the-commons-.html Fat Protocols http://www.usv.com/blog/fat-protocols Ethereum Commons https://medium.com/@RhysLindmark/co-evolving-the-phase-shift-to-cryptocapitalism-by-founding-the-ethereum-commons-co-op-f4771e5f0c83 An Analysis of Self Sovereign Identity using Appreciative Inquiry, IIW April 2018 https://www.slideshare.net/heathervescent/selfsovereign-identity-an-analysis-using-appreciative-inquiry","title":"Sustainable commons"},{"location":"RWoT7/","text":"Rebooting the Web of Trust VII: Toronto (September 2018) This repository contains documents related to RWOT7, the seventh Rebooting the Web of Trust design workshop, which ran near Toronto, Canada, on September 26th to 28th, 2018. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community. Watch for our next event March 1st-3rd in Barcelona, Spain. Final Papers BTCR v0.1 Decisions (Text) Kim Hamilton Duffy, Christopher Allen, and Dan Pape The Bitcoin Reference (BTCR) DID method supports DIDs using the Bitcoin blockchain. This method has been under development through Rebooting Web of Trust events and hackathons over the past year. The BTCR method's reliance on the Bitcoin blockchain presents both advantages and design challenges. During RWOT7, the authors made a number of design and implementation decisions -- largely scope-cutting in nature -- in order to lock down a Minimum Viable Product (MVP) version, which we'll refer to as v0.1. This paper documents those decisions, which will apply to the upcoming v0.1 BTCR method specification and associated v0.1 BTCR reference implementation. A DID for Everything (Text) Shaun Conway, Andrew Hughes, Moses Ma, Jack Poole, Martin Riedel, Samuel M. Smith Ph.D., and Carsten St\u00f6cker The decentralized identifier (DID) is a new and open standard type of globally unique identifier that offers a model for lifetime-scope portable digital identity that does not depend on any centralized authority and that can never be taken away by third-parties. DIDs are supported by the W3C community and the Decentralized Identity Foundation (DIF). They are the \"atomic units\" of a new layer of decentralized identity infrastructure. However, DIDs can be extended from identifiers for people to any entity, thus identifying everything. We can use DIDs to help us identify and manage objects, machines, or agents through their digital twins; we can expand them to locations, to events, and even to pure data objects, which we refer to as decentralized autonomic data (DAD) items. The paper will present novel use-cases for DIDs and DADs and propose a new cryptographic data structure that is a self-contained blockchain of DADs. This enables the verification of the provenance of a given data flow. It builds on a prior paper and an associated reading. How to Convince Dad* of the Importance of Self-Sovereign Identity (Text) Shannon Appelcline, Kenneth Bok, Lucas Parker, Peter Scott, and Matthew Wong One of the major problems with bootstrapping self-sovereign identity is that it requires adoption by a large number of people. Pushing self-sovereign identity from the top-down is most likely to result in a technology that\u2019s not actually used, but instead encouraging the average person to demand self-sovereign identity from the bottom-up will result in the organic development of a vibrant, well-utilized decentralized web-of-trust ecosystem. This paper addresses that need by offering arguments to a variety of people who might be reluctant to use self-sovereign identity, uninterested in its possibilities, or oblivious to the dangers of centralization. By focusing on the needs of real people, we hope to also encourage developers, engineers, and software business owners to create the apps that will address their reluctance and fulfill their needs, making self-sovereign identity a reality. IPLD as a general pattern for DID documents and Verifiable Claims (Text) jonnycrunch, Anthony Ronning, Kim Duffy, Christian Lundkvist Since the emergence of the Decentralized Identifier (DID) specification at the Fall 2016 Rebooting the Web of Trust [1], numerous DID method specifications have appeared. Each DID method specification defines how to resolve a cryptographically-tied DID document given a method-specific identifier. In this paper, we describe a way to represent the DID document as a content-addressed Merkle Directed Acyclic Graph (DAG) using Interplanetary Linked Data (IPLD). This technique enables more cost-efficient, scaleable creation of DIDs and can be applied across different DID method specifications. Peer to Peer Degrees of Trust (Text) Harrison Stahl, Titus Capilnean, Peter Snyder, and Tyler Yasaka Aunthenticity is a challenge for any identity solution. In the physical world, at least in America, it is not difficult to change one's identity. In the digital world, there is the problem of bots. The botnet detection market is expected to be worth over one billion USD by 2023, in a landscape where most digital activity is still heavily centralized. These centralized digital solutions have the advantage of being able to track IP addresses, request phone verification, and present CAPTCHAs to users in order to authenticate them. If this problem is so difficult to solve in the centralized world, how much more challenging will it be in the decentralized world, where none of these techniques are available? In this paper, we explore the idea of using a web of trust as a tool to add authenticity to decentralized identifiers (DIDs). We define a framework for deriving relative trust degrees using a given trust metric: a \"trustworthiness\" score for a given identity from the perspective of another identity. It is our intent that this framework may be used as a starting point for an ongoing exploration of graph-based, decentralized trust. We believe this approach may ultimately be used as a foundation for decentralized reputation. Resource Integrity Proofs (Text) Ganesh Annan and Kim Hamilton Duffy Currently, the Web provides a simple yet powerful mechanism for the dissemination of information via links. Unfortunately, there is no generalized mechanism that enables verifying that a fetched resource has been delivered without unexpected manipulation. Would it be possible to create an extensible and multipurpose cryptographic link that provides discoverability, integrity, and scheme agility? This paper proposes a linking solution that decouples integrity information from link and resource syntaxes, enabling verification of any representation of a resource from any type of link. We call this approach Resource Integrity Proofs (RIPs). RIPs provide a succinct way to link to resources with cryptographically verifiable content integrity. RIPs can be combined with blockchain technology to create discoverable proofs of existence to off-chain resources. Use Cases and Proposed Solutions for Verifiable Offline Credentials (Text) Michael Lodder, Samantha Mathews Chase, and Wolf McNally In this paper we cover various scenarios where some or all parties have intermittent, unreliable, untrusted, insecure, or no network access, but require cryptographic verification (message protection and/or proofs). Furthermore, communications between the parties may be only via legacy voice channels. Applicable situations include marine, subterranean, remote expeditions, disaster areas, refugee camps, and high-security installations. This paper then recommends solutions for addressing offline deployments. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Here are the advanced readings to date: Addressing Global/Local Barriers to Adoption of Decentralized Identity Systems by Eric Brown Agent to Agent Communication Protocol Overview by Kyle Den Hartog Blockcerts -- Where we are and what's next by Kim Hamilton Duffy, Anthony Ronning, Lucas Parker, and Peter Scott Can Curation Markets Establish a Sustainable Technology Commons by Sam Chase CapAuth by Manu Sporny, Dave Longley, Chris Webber, and Ganesh Annan A Concept Diagram For RWOT Identity Terms by Andrew Hughes Cryptocurrency Wallets as a Form of Functional Identity by Mikerah Quintyne-Collins and Abdulwasay Mehar Decentralized Error Reporting by Jack Poole Decentralized Identities and eIDAS by Oliver Terbu Decentralized Identity: Hub Authentication & Message Encryption by Daniel Buchner DIDDoc Conventions for Interoperability by Stephen Curran & Olena Mitovska DIDs In DPKI by Greg Slepak DID Resolution Topics by Markus Sabadello Digital Identity for the Homeless by Matthew Wong, T. Tian & CG Chen Exploring Browser Web of Trust Use Cases by Peter Snyder and Ben Livshits Five Mental Models of Identity by Joe Andrieu Identity Hub Permissions / Authorization by Daniel Buchner IPLD as a general pattern for DID Documents by Christian Lundkvist Is a Decentralized Collective Identity Possible? by Heather Vescent Magenc Magnet URIs: Secure Object Permanence for the Web by Christopher Lemmer Webber Measuring Trust by Tyler Yasaka More Control for Identity Holders by Arturo Manzaneda and Ismenia Galvao Nobody REALLY Trusts the Blockchain by Dan Burnett, Shahan Khatchadourian, and Chaals Nevile Not-a-Bot: A Use Case for Decentralized Identity using Proximity Verification to generate a Web of Trust by Moses Ma & Claire Rumore The Political Economy of Naming by Kate Sills A Public Web of Trust of Public Identities by Ouri Poupko and Ehud Shapiro Resource Integrity Proofs by Ganesh Annan, Manu Sporny, Dave Longley, and David Lehn RWoT Tribal Knowledge: Cryptographic and Data Model Requirements by Manu Sporny, Dave Longley, and Chris Webber The Role of Standards in Accelerating Innovation by Michael B. Jones Scoped Presentation Request on Verifiable Credentials by Martin Riedel Secure Crypto-Wallet Introductions by Wolf McNally, Ryan Grant Standards for Agency and Decentralized Information Governance - Early Experience by Adrian Gropper, MD, Michael Chen, MD, and Lydia Fazzio, MD Towards Proof of Person by Peter Watts A Trustless Web-of-Trust by Ouri Poupko The United Humans by Bohdan Andriyiv Verifiable Displays by Kim Hamilton Duffy, Bohdan Andriyiv, and Lucas Parker Verifiable Offline Credentials by Michael Lodder What (and Who) Is In Your Wallet by Darrell O'Donnell Digital Identity for the Homeless by Matthew Wong, T. Tian & CG Chen Zero Trust Computing with DIDs and DADs by Samuel M. Smith Primers These primers overview major topics which are likely to be discussed at the design workshop. If you read nothing else, read these. (But really, read as much as you can!) DID Primer \u2014 Decentralized Identifiers ( extended version also available) Functional Identity Primer \u2014 A different way to look at identity Verifiable Credentials Primer \u2014 the project formerly known as Verifiable Claims DIDs In DPKI - how DIDs fit into Decentralized Public-key Infrastructure","title":"Rebooting the Web of Trust VII: Toronto (September 2018)"},{"location":"RWoT7/#rebooting-the-web-of-trust-vii-toronto-september-2018","text":"This repository contains documents related to RWOT7, the seventh Rebooting the Web of Trust design workshop, which ran near Toronto, Canada, on September 26th to 28th, 2018. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Please see the Web of Trust Info website for more information about our community. Watch for our next event March 1st-3rd in Barcelona, Spain.","title":"Rebooting the Web of Trust VII: Toronto (September 2018)"},{"location":"RWoT7/#final-papers","text":"","title":"Final Papers"},{"location":"RWoT7/#btcr-v01-decisions-text","text":"","title":"BTCR v0.1 Decisions (Text)"},{"location":"RWoT7/#kim-hamilton-duffy-christopher-allen-and-dan-pape","text":"The Bitcoin Reference (BTCR) DID method supports DIDs using the Bitcoin blockchain. This method has been under development through Rebooting Web of Trust events and hackathons over the past year. The BTCR method's reliance on the Bitcoin blockchain presents both advantages and design challenges. During RWOT7, the authors made a number of design and implementation decisions -- largely scope-cutting in nature -- in order to lock down a Minimum Viable Product (MVP) version, which we'll refer to as v0.1. This paper documents those decisions, which will apply to the upcoming v0.1 BTCR method specification and associated v0.1 BTCR reference implementation.","title":"Kim Hamilton Duffy, Christopher Allen, and Dan Pape"},{"location":"RWoT7/#a-did-for-everything-text","text":"","title":"A DID for Everything (Text)"},{"location":"RWoT7/#shaun-conway-andrew-hughes-moses-ma-jack-poole-martin-riedel-samuel-m-smith-phd-and-carsten-stocker","text":"The decentralized identifier (DID) is a new and open standard type of globally unique identifier that offers a model for lifetime-scope portable digital identity that does not depend on any centralized authority and that can never be taken away by third-parties. DIDs are supported by the W3C community and the Decentralized Identity Foundation (DIF). They are the \"atomic units\" of a new layer of decentralized identity infrastructure. However, DIDs can be extended from identifiers for people to any entity, thus identifying everything. We can use DIDs to help us identify and manage objects, machines, or agents through their digital twins; we can expand them to locations, to events, and even to pure data objects, which we refer to as decentralized autonomic data (DAD) items. The paper will present novel use-cases for DIDs and DADs and propose a new cryptographic data structure that is a self-contained blockchain of DADs. This enables the verification of the provenance of a given data flow. It builds on a prior paper and an associated reading.","title":"Shaun Conway, Andrew Hughes, Moses Ma, Jack Poole, Martin Riedel, Samuel M. Smith Ph.D., and Carsten St\u00f6cker"},{"location":"RWoT7/#how-to-convince-dad42-of-the-importance-of-self-sovereign-identity-text","text":"","title":"How to Convince Dad* of the Importance of Self-Sovereign Identity (Text)"},{"location":"RWoT7/#shannon-appelcline-kenneth-bok-lucas-parker-peter-scott-and-matthew-wong","text":"One of the major problems with bootstrapping self-sovereign identity is that it requires adoption by a large number of people. Pushing self-sovereign identity from the top-down is most likely to result in a technology that\u2019s not actually used, but instead encouraging the average person to demand self-sovereign identity from the bottom-up will result in the organic development of a vibrant, well-utilized decentralized web-of-trust ecosystem. This paper addresses that need by offering arguments to a variety of people who might be reluctant to use self-sovereign identity, uninterested in its possibilities, or oblivious to the dangers of centralization. By focusing on the needs of real people, we hope to also encourage developers, engineers, and software business owners to create the apps that will address their reluctance and fulfill their needs, making self-sovereign identity a reality.","title":"Shannon Appelcline, Kenneth Bok, Lucas Parker, Peter Scott, and Matthew Wong"},{"location":"RWoT7/#ipld-as-a-general-pattern-for-did-documents-and-verifiable-claims-text","text":"","title":"IPLD as a general pattern for DID documents and Verifiable Claims (Text)"},{"location":"RWoT7/#jonnycrunch-anthony-ronning-kim-duffy-christian-lundkvist","text":"Since the emergence of the Decentralized Identifier (DID) specification at the Fall 2016 Rebooting the Web of Trust [1], numerous DID method specifications have appeared. Each DID method specification defines how to resolve a cryptographically-tied DID document given a method-specific identifier. In this paper, we describe a way to represent the DID document as a content-addressed Merkle Directed Acyclic Graph (DAG) using Interplanetary Linked Data (IPLD). This technique enables more cost-efficient, scaleable creation of DIDs and can be applied across different DID method specifications.","title":"jonnycrunch, Anthony Ronning, Kim Duffy, Christian Lundkvist"},{"location":"RWoT7/#peer-to-peer-degrees-of-trust-text","text":"","title":"Peer to Peer Degrees of Trust (Text)"},{"location":"RWoT7/#harrison-stahl-titus-capilnean-peter-snyder-and-tyler-yasaka","text":"Aunthenticity is a challenge for any identity solution. In the physical world, at least in America, it is not difficult to change one's identity. In the digital world, there is the problem of bots. The botnet detection market is expected to be worth over one billion USD by 2023, in a landscape where most digital activity is still heavily centralized. These centralized digital solutions have the advantage of being able to track IP addresses, request phone verification, and present CAPTCHAs to users in order to authenticate them. If this problem is so difficult to solve in the centralized world, how much more challenging will it be in the decentralized world, where none of these techniques are available? In this paper, we explore the idea of using a web of trust as a tool to add authenticity to decentralized identifiers (DIDs). We define a framework for deriving relative trust degrees using a given trust metric: a \"trustworthiness\" score for a given identity from the perspective of another identity. It is our intent that this framework may be used as a starting point for an ongoing exploration of graph-based, decentralized trust. We believe this approach may ultimately be used as a foundation for decentralized reputation.","title":"Harrison Stahl, Titus Capilnean, Peter Snyder, and Tyler Yasaka"},{"location":"RWoT7/#resource-integrity-proofs-text","text":"","title":"Resource Integrity Proofs (Text)"},{"location":"RWoT7/#ganesh-annan-and-kim-hamilton-duffy","text":"Currently, the Web provides a simple yet powerful mechanism for the dissemination of information via links. Unfortunately, there is no generalized mechanism that enables verifying that a fetched resource has been delivered without unexpected manipulation. Would it be possible to create an extensible and multipurpose cryptographic link that provides discoverability, integrity, and scheme agility? This paper proposes a linking solution that decouples integrity information from link and resource syntaxes, enabling verification of any representation of a resource from any type of link. We call this approach Resource Integrity Proofs (RIPs). RIPs provide a succinct way to link to resources with cryptographically verifiable content integrity. RIPs can be combined with blockchain technology to create discoverable proofs of existence to off-chain resources.","title":"Ganesh Annan and Kim Hamilton Duffy"},{"location":"RWoT7/#use-cases-and-proposed-solutions-for-verifiable-offline-credentials-text","text":"","title":"Use Cases and Proposed Solutions for Verifiable Offline Credentials (Text)"},{"location":"RWoT7/#michael-lodder-samantha-mathews-chase-and-wolf-mcnally","text":"In this paper we cover various scenarios where some or all parties have intermittent, unreliable, untrusted, insecure, or no network access, but require cryptographic verification (message protection and/or proofs). Furthermore, communications between the parties may be only via legacy voice channels. Applicable situations include marine, subterranean, remote expeditions, disaster areas, refugee camps, and high-security installations. This paper then recommends solutions for addressing offline deployments.","title":"Michael Lodder, Samantha Mathews Chase, and Wolf McNally"},{"location":"RWoT7/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? Here are the advanced readings to date: Addressing Global/Local Barriers to Adoption of Decentralized Identity Systems by Eric Brown Agent to Agent Communication Protocol Overview by Kyle Den Hartog Blockcerts -- Where we are and what's next by Kim Hamilton Duffy, Anthony Ronning, Lucas Parker, and Peter Scott Can Curation Markets Establish a Sustainable Technology Commons by Sam Chase CapAuth by Manu Sporny, Dave Longley, Chris Webber, and Ganesh Annan A Concept Diagram For RWOT Identity Terms by Andrew Hughes Cryptocurrency Wallets as a Form of Functional Identity by Mikerah Quintyne-Collins and Abdulwasay Mehar Decentralized Error Reporting by Jack Poole Decentralized Identities and eIDAS by Oliver Terbu Decentralized Identity: Hub Authentication & Message Encryption by Daniel Buchner DIDDoc Conventions for Interoperability by Stephen Curran & Olena Mitovska DIDs In DPKI by Greg Slepak DID Resolution Topics by Markus Sabadello Digital Identity for the Homeless by Matthew Wong, T. Tian & CG Chen Exploring Browser Web of Trust Use Cases by Peter Snyder and Ben Livshits Five Mental Models of Identity by Joe Andrieu Identity Hub Permissions / Authorization by Daniel Buchner IPLD as a general pattern for DID Documents by Christian Lundkvist Is a Decentralized Collective Identity Possible? by Heather Vescent Magenc Magnet URIs: Secure Object Permanence for the Web by Christopher Lemmer Webber Measuring Trust by Tyler Yasaka More Control for Identity Holders by Arturo Manzaneda and Ismenia Galvao Nobody REALLY Trusts the Blockchain by Dan Burnett, Shahan Khatchadourian, and Chaals Nevile Not-a-Bot: A Use Case for Decentralized Identity using Proximity Verification to generate a Web of Trust by Moses Ma & Claire Rumore The Political Economy of Naming by Kate Sills A Public Web of Trust of Public Identities by Ouri Poupko and Ehud Shapiro Resource Integrity Proofs by Ganesh Annan, Manu Sporny, Dave Longley, and David Lehn RWoT Tribal Knowledge: Cryptographic and Data Model Requirements by Manu Sporny, Dave Longley, and Chris Webber The Role of Standards in Accelerating Innovation by Michael B. Jones Scoped Presentation Request on Verifiable Credentials by Martin Riedel Secure Crypto-Wallet Introductions by Wolf McNally, Ryan Grant Standards for Agency and Decentralized Information Governance - Early Experience by Adrian Gropper, MD, Michael Chen, MD, and Lydia Fazzio, MD Towards Proof of Person by Peter Watts A Trustless Web-of-Trust by Ouri Poupko The United Humans by Bohdan Andriyiv Verifiable Displays by Kim Hamilton Duffy, Bohdan Andriyiv, and Lucas Parker Verifiable Offline Credentials by Michael Lodder What (and Who) Is In Your Wallet by Darrell O'Donnell Digital Identity for the Homeless by Matthew Wong, T. Tian & CG Chen Zero Trust Computing with DIDs and DADs by Samuel M. Smith","title":"Topics &amp; Advance Readings"},{"location":"RWoT7/#primers","text":"These primers overview major topics which are likely to be discussed at the design workshop. If you read nothing else, read these. (But really, read as much as you can!) DID Primer \u2014 Decentralized Identifiers ( extended version also available) Functional Identity Primer \u2014 A different way to look at identity Verifiable Credentials Primer \u2014 the project formerly known as Verifiable Claims DIDs In DPKI - how DIDs fit into Decentralized Public-key Infrastructure","title":"Primers"},{"location":"RWoT7/A_DID_for_everything/","text":"A DID for Everything Attribution, Verification and Provenance for Entities and Data Items Presented by Shaun Conway , Andrew Hughes , Moses Ma , Jack Poole , Martin Riedel , Samuel M. Smith Ph.D. , and Carsten St\u00f6cker Submitted to the 7th Rebooting the Web of Trust Technical Workshop September 24-26, 2018, Toronto Keywords: decentralized identity, Internet of Things, autonomic data, verified claims, identity, blockchain, self-sovereign, fog computing, industry 4.0, digital twinning, data chain provenance, audit trails Abstract The decentralized identifier (DID) is a new and open standard type of globally unique identifier that offers a model for lifetime-scope portable digital identity that does not depend on any centralized authority and that can never be taken away by third-parties [[14]]. DIDs are supported by the W3C community [[14]] and the Decentralized Identity Foundation (DIF) [[16]]. They are the \"atomic units\" of a new layer of decentralized identity infrastructure. However, DIDs can be extended from identifiers for people to any entity, thus identifying everything . We can use DIDs to help us identify and manage objects, machines, or agents through their digital twins; we can expand them to locations, to events, and even to pure data objects, which we refer to as decentralized autonomic data (DAD) items [[1]][[3]]. The paper will present novel use-cases for DIDs and DADs and propose a new cryptographic data structure that is a self-contained blockchain of DADs. This enables the verification of the provenance of a given data flow. It builds on a prior paper [[1]] and an associated reading [[2]]. DIDs are only the base layer of decentralized identity infrastructure. The next higher layer (where most of the value is unlocked) is verifiable claims. This is the technical term for a digitally signed electronic data structure that conforms to the interoperability standards being developed by the W3C Verifiable Credentials Working Group [[15]]. When a DID and hence DADs of the resultant data are extended to machines and autonomic data, the provenance chain of the data flow can provide the basis for verifiable claims and attestations about the data flow as well as the basis for a reputation. Why This Matters Today, the Internet is probably best described as a network comprised of all interconnected entities, traditionally referring to human users and computers. When we add connected entities and devices in the so-called Internet of Things (IoT), the number of addressable elements is in the tens of billions, with an estimate of 75 bn connected IoT devices in 2025 [[4]]. Software services, such as algorithms and bots, further extend this universe of identifiable entities. The resulting combinatorics of possible connections between any given set of entities is an impossibly large number. Yet in today's user journeys or business environments, agents (whether human, machine, or software) increasingly need to communicate, access or transact with a diverse group of these interconnected objects to achieve their goals in both the digital and physical worlds. This requires a straightforward and ubiquitous method to address, verify, and connect these elements together. Definition of Entity : Something that has a distinct and independent existence either in the real or the digital world. Examples of an entity are: Living Organism Physical Object Locations or Events Machines and Devices in the Internet of Things (IoT) Digital Asset, Data Set, or Agent Human or object identities are stored in multiple centralised or federated systems such as government, ERP, IoT, or manufacturing systems. From the standpoint of cryptographic trust verification, each of these centralised authorities serves as its own root of trust. An entity trailing along a value chain is interacting with multiple systems. Consequently, a new actor in any given value chain has no method to independently verify credentials of a human or attributes of either a physical object or data item (provenance, audit trail). This results in the existence of complex validation, quality inspection, and paper trail processes, and enormous hidden trust overhead costs are added to all value chains and services. To be a truly global solution, easy to use and still safe from hacking and sovereign interference, such a scheme must include: preservation of privacy security from tampering reliable trust verification assurance of risk independence from any vendor-defined naming API one-to-one mappable onto each entity. Therefore, a universal addressing, trust-verification system and associated interoperable protocol must be utilised, empowering every form of entity. Why it Matters for People Today when entities want their identities to be confirmed they transfer information such as a birth certificate, physical address, or social security number to multiple third parties, who start to validate the same data in different contexts for KYC and authentication processes. The parties to which they sent that information retains it, meaning the data is out there in silos, creating risks in terms of data loss, privacy breaches, and use of inconsistent data and forcing companies that might not want to be in that position to store that information. It also enables businesses to harvest people's personal data for commercial purposes, which does not necessarily reflect the intentions of the individual people. This situation results in big problems for humans such as broken health care records. Patients will need a universally addressable healthcare record system that is controlled by the patient itself, that consistently stores all relevant verified health care data, and that is able to share this data with doctors that need to connect with it. To enable the doctors or algorithms, they need a data-flow provenance to verify the integrity, quality, or reputation of a healthcare record to decide on treatments and give the patient confidence about the proposed treatments. Why it Matters for Businesses Definition of a Digital Twin : A digital twin is a digital representation of either a real-world or digital entity. A digital twin exists over the life-cycle of an entity from planning, manufacturing, testing, birth, and operations to decommissioning and reuse. The more past and present data are related and analysed, the more knowledge can be deployed to drive significant improvements on an individual entity or system level. It is estimated that by 2022 the IoT powered by digital twins will save consumers and businesses worth $1 trillion a year in asset maintenance [[5]]. The notion of digital twinning for objects, machines, and agents is becoming relevant to an increasing number of human services and Industry 4.0 use cases. This is the result of the growth in digital services, connections, and data streams from the Internet of Things (IoT) devices that increasingly drive integration with machine learning algorithms, resulting in graph-type data chains for processing the IoT data streams. Today, digital twins are captured in siloed, proprietary IoT solutions by individual corporates that do not own the physical object over the life-cycle and even do not interact with parties using the object further down a value chain. Decentralized solutions are liberating the digital twins from silos and establishing more valuable and interoperable verifiable attributes about entities and the data chains they connect with. Why it Matters for Objects, Machines and Agents There is no widely adopted authentication or verification systems in place to provide the equivalent of KYC (know-your-customer) for non-human entities, that is: KYA (know-your-agent), KYB (know-your-bot), KYM (know-your-machine), or KYO (know-your-object). In a world when objects and machines are connected with datastreams and intelligent agents that perform transaction on behalf of the entity, the number of agent-to-agent transactions will outgrow the number of human transactions by many orders. An agent transacting with another party can independently verify the identifier of the this party, its attributes, and the provenance of the data sets that are involved in the transaction. Digital twins of 3D-printed objects for safety critical parts such as a turbine of an airplane provide an important example. For these parts, it is important to have an precise audit trail about the 3D printing process to prove that the object was manufactured in accordance to stringent specifications. The digital twin stores design, manufacturing, post processing, and quality-assurance data about the 3D-printed object. These data are coming from multiple systems resulting in a variety of data chains. With DIDs and DADs the integrity of the data chains can be verified. The verification of the datachains and the underlying data results in important proofs about the provenance of 3D printed object. Why it Matters for the World The diverse application of decentralized identifiers (DIDs) will have substantial influences in broader applications on a global scale. The seamless provenance of physical objects or data items through any value chain has major implications on the risk and value properties of the processed data. Within any dynamic process, participating entities have substantial interest in the authenticity and trustworthiness in any individual step. Data that is accumulated with an unforgeable audit trail that references decentralized identifying information (Person, Device or any other Entity) for any transformation step holds greater value then it would have without such properties. Managing the sustainability of the commons requires mechanisms to value natural capital and to account for the externalities that arise from human activities. This should attribute extractions from and contributions to the commons by organisations, organisms, machines and information. We need to identify these entities and must identify both positive and negative impacts these entities are having on the commons. Knowing what these impacts are enables us to count what matters and to put a value on what counts. The promise of a overarching prevalence through the broad use of DIDs also provides the key component for achieving the vision of a circular economy : a regenerative system in which resource input and waste, emission, and energy leakage are minimized by slowing, closing, and narrowing energy and material loops. This can be achieved through long-lasting design, maintenance, repair, reuse, remanufacturing, refurbishing, recycling, and upcycling. This contrasts with a linear economy, which is a self-destructing, catastrophic 'take, make, dispose' model of production. Comprehensive Solution Motivation The Decentralized Autonomic Data paper from the RWOT Spring 2018 introduced three new concepts related to DIDs [[1]]. Decentralized Autonomic Data (DADi) items Derived DID (dDID) Self-contained verification of data governance and integrity. This paper extends these concepts with a new one, that is, Data flows may be provenanced via a self-referential blockchain (list of signed DAD items where each subsequent DAD item includes the DID or dDID and associated signature of the previous item) Data Provenance The concepts above lay the groundwork for data-flow provenance that is the focus of this paper. In this context provenance is used in a general sense. By data-flow provenance we mean a mechanism for tracing data-item content and control through a processing system, including any transformations to the data item or its governance. This includes flows with multiple sources and sinks of data, independently and in combination. To restate, data-flow provenance means not just tracing control but also verifying the end-to-end integrity of every data flow, including any transformations (additions, deletions, modifications, and combinations). In this context provenance refers to the chain-of-custody of data items not ownership. Ownership is a more complex idea than mere control. Change of ownership requires the previous owner to relinquish ownership and the new owner to accept ownership. It reverts back if not accepted. Finally, ownership implies a restriction or limitation on rights so it may need to be interactive or involve a third party. In English we use the term \" custody \" to refer to having possession of something (under you control), but custody does not equal ownership. With physical objects one can have unique custody that could be equivalent to ownership because a physical object is a unique thing. Hence the expression \"possession is nine-tenths of the law\" [[17]]. Because data can be easily duplicated, however, it may not be possible to prove unique possession of data. There is no way to guarantee that there is not another copy of the data. So custody of data is not equivalent to restricted ownership. With data the strongest attestation of ownership is limited to merely rights to use the data. So transfer of ownership of data is a different concept; it's not transfer of the data itself but the transfer of rights to use the data. An important insight from the viewpoint of a distributed application is that an entity's influence on the application is solely based on the digital data flows that move between the entity and the other components of the distributed application. We choose to call those data flows the entity's projection onto the distributed application. If those projections consist of DADs and every interaction of internal components consists of DADs then we have a universal approach for implementing decentralized applications with total provenance of control and data within the application. A major motivation for establishing data-flow provenance is to support building and architecting decentralized processing systems that use a zero-trust or more correctly diffuse-trust perimeterless security model. The emerging decentralized processing paradigm for distributed applications where the sources and sinks of data may be controlled by multiple entities, i.e. decentralized governance, means that traditional perimeter security models are at best problematic. Indeed, even the newer perimeterless security model as originally formulated assumes singular governance of the network hosts and associated data [[7]][[8]]. We extend that model herein to include hosts and DADs with decentralized governance via DIDs. An earlier paper explored the architectural issues of building distributed computing infrastructure that applied the zero-trust security model but extended it to use distributed consensus for policy governance [[9]]. This type of architecture we called Zero-Trust-Computing or more correctly Diffuse-Trust-Computing . A simple way of explaining the zero-trust security model is the mantra, \"never trust, always verify\". The paradigm of Zero Trust Networking was first popularized in 2013 by a NIST report [[7]]. More recently the principles have received much broader attention including the book Zero Trust Networks [[8]]. The basic approach to Diffuse-Trust-Computing is to use a diffuse trust perimeterless security approach. As previously mentioned, some call this a trustless or zero-trust security model but that is a misnomer. There is still trust, it is just diffused in such a way that security is greatly enhanced [[9]]. In this paper we further extend that model to use DIDs and DADs to provide decentralized end-to-end data provenance. The caveats of conventional diffuse-trust perimeterless security are: The network is always hostile both internally and externally; locality is not trustworthy. By default, every network interaction or data flow must be authenticated and authorized using best practices cryptography. By default, inter-host communication must be end-to-end signed/encrypted, and data must be stored signed/encrypted using best practices cryptography; Data is signed/encrypted at motion and at rest. Policies for authentication and authorization must be dynamically modified based on behavior (reputation). Using end-to-end encryption and storage prevents exploits from anyone that merely has access to the network or the data-storage device. By authenticating and authorizing every network interaction or data flow, security becomes granular. A successful exploit of one interaction does not bleed into any other. Compromising one data flow does not compromise any other. Escalation opportunities are minimized. Many security exploits are discovered through repeated probes and experiments to find bugs, buffer overflows, or weaknesses in network protocols or software implementations. Dynamic policy modification that uses AI to first profile and detect anomalous behavior and then restrict the authorization of that user prevents discovery. This adds time as a defence. Extending the model to enhance the security of the policy management adds the following caveat: Policies must be governed by distributed consensus. Distributed consensus diffuses the trust for any policy decision to a group of hosts. In order to defeat the policy, an attacker must exploit some majority of the hosts. This makes exploits exponentially more difficult. Distributed consensus also allows for decentralized governance of the hosts. This paper extends these principles with one more: By default, each data flow including transformations must be end-to-end provenanced using decentralized identifiers (DIDs) and hence decentralized autonomic data items (DADis). This additional principle allows governance over the data using a decentralized trust model and a decentralized web of trust based on DIDs and DADis. This approach enables truly decentralized governance models for distributed applications. One can combine the third and sixth caveats from above, to a simple summary cavet: By default, data flows are end-to-end provenanced/signed/encrypted at motion and at rest using DIDs and DADis. This remainder of this section in the paper will explore salient issues in using DIDs and DADs to maintain provenance over each step in a data processing flow including transformations of the data to enable credible uses of the data for various applications while maintaining a zero or difuse-trust security model. Decentralized Autonomic Data Definition First introduced in more detail here [[1]], a decentralized autonomic data item (DADi) is associated with a decentralized identifier [[14]]. This paper does not provide a detailed definition of DIDs but does describe how DIDs are used by a DADi. The DID syntax specification is a modification of standard URL syntax per RFC-3986 [[19]]. As such, it benefits from familiarity, which is a boon to adoption. One of the features of a DID is that it is a self certifying identifier in that a DID includes either a public key or a fingerprint of a public key from a cryptographic public/private key pair. Thereby a signature created with the private key can be verified using the public key provided by the DID. The inclusion of the public part of a cryptographic key pair in the DID gives the DID other desirable properties. These include universal uniqueness and pseuodnynmity. Another way to describe a DID is that it is a cryptonym: a cryptographically derived pseudonym. In the DAD acronym, decentralized means that the governance of the data may not reside with a single party. The term autonomic means self-managing or self-regulating. In the context of data, we crystalize the meaning of self-managing to include cryptographic techniques for maintaining data provenance that make the data self-identifying, self-certifying, and self-securing. Implied thereby is the use of cryptographic keys and signatures to provide a root of trust for data integrity and to maintain that trust over change-in-custody and transformation of that data, i.e. provenance . The motivating use of DAD is to provide provenance for streaming data that is generated and processed in a distributed manner with decentralized governance. Streaming data are typically measurements that are collected and aggregated to form higher level constructs. Applications include analytics and instrumentation of distributed web or internet-of-things (IoT) applications as well as portable reputation systems. A DAD seeks to maintain a provenance chain for data undergoing various processing stages that follows diffuse trust security principles including signed at rest and in motion. Streaming data applications may impose significant performance demands on the processing of the associated data. Associated with a DID is a DID Document (DDO) [[14]]. The DDO provides meta-data about the DID that can be used to manage the DID as well as discover services affiliated with the DID. Typically the DDO is provided by a DID resolution service. The DID/DDO model is not a good match for streaming data especially if a new DID/DDO pair would need to be created for each new DAD item. But a DID/DDO is a good match when used as the root or master identifier from which an identifier for the DAD is derived. This derived identifier is called a derived-DID or dDID. Thus only one DID/DDO paring is required to manage a large number of DADs where each DAD may have a unique dDID. The syntax for a dDID is identical for a DID. The difference is that only one DDO with meta-data is needed for the root DID used to create the dDIDs in the DADs. Each DAD item carries any additional DAD-specific meta-data, thus making them self-contained (autonomic). DID Syntax A DID or dDID has the following required syntax: did:method:idstring The method is some short string that namespaces the DID and provides for unique behavior in the associated method specification. In this paper we will use the method dad . The idstring must be universally unique. The idstring can have multiple colon \":\" separated parts, thus allowing for namespacing. In this document the first part of the idstring is linked to the public member of a cryptographic key pair that is defined by the method. We will use a 44-character Base64 URL-File safe encoding as per RFC-4648 [[18]], with one trailing pad byte of the 32-byte public verification key for an EdDSA (Ed25519) signing key pair. Unless otherwise specified Base64 in this document refers to the URL-File safe version of Base64. The URL-File safe version of Base64 encoding replaces plus \"+\" with minus \"-\" and slash \"/\" with underscore \" \". A DID may have optional parts including a path, query, or fragment. These use the same syntax as a URL, that is, the path is delimited with slashes, / , the query with a question mark, ? , and the fragment with a pound sign, #_. When the path part is provided then the query applies to the resource referenced by the path and the fragment refers to an element in the document referenced by the path. An example follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=/mom?who=me#blue In contrast, when the path part is missing but either the query or fragment part is provided then the query and/or fragment parts have special meaning. A query without a path means the the query is an operation on either the DID itself or the DID document (DDO). Likewise when a fragment is provided then the fragment is referencing an element of the DDO. An example of a DID without a path but with a query follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?who=me As will be described later, a query part on a DID expression without a path part will enable the generation of dDIDs (derived DIDs). DIDs form a new class of identifiers that can be used to replace the identifiers commonly used in distributed applications. DIDs can replace UUIDs, URLs, and Public Keys because a DID combines the desirable features of all three identifier classes. UUID: A Universally Unique Identifier per RFC 4122 has types 1-5. These are 16-byte collision-resistant decentralized identifiers generated with a pseudo-random-number generator and optional name-spacing data. A UUID enables distributed applications to create unique identifiers without central authority. Prefixed name-spacing allows for sorting and searching properties such as time order, lexical order, nesting etc. URI: A Uniform Resource Identifier (URI), Uniform Resource Locator( URL), Uniform Resource Name (URN) per RFC 3986 is of the form scheme:[//[user[:password]@]host[:port]][/path][?query][#fragment]. This provides not just an identifier but namespacing authority and a mini-language for performing operations on the identifier and associated resources. Decentralized Self-Certifying Identifier: A Self-Certifying Identifier contains the fingerprint of a public member of cryptographic public/private key pair. A decentralized Self-Certifying Identifier contains the fingerprint of a public/private key pair that is generated by the user not a central registry. A Hierarchical Deterministic Self-Certifying Identifier of the form selfcertroot:/path/to/related/data provides a way to reproduce private keys without having to store them. Tupleizable Identifier. A tupleizable identifier of the form (channel, host, process, data) tenables a routing overlay on top of IP that can be provenanced. Indeed any computing infrastructure that is now using UUIDs and URLs could beneficially replace them with DIDs. Minimal DAD A minimal DAD (decentralized autonomic data) item is a data item that contains a DID or dDID that helps uniquely identify that data item or affiliated data stream. In this paper JSON is used to represent serialized DAD items but other formats could be used instead. To ensure non-repudiable data integrity (i.e. that the data has not been tampered with and can be assigned to a given signing entity) a signature is appended to the DAD item that is verifiable as being generated by the private key associated with the public key in the id field value. This signature verifies that the DAD item was created by the holder of the associated private key. The DAD item is thereby both self-identifying and self-certifying because the identifier value given by the id field is included in the signed data and is verifiable against the private key associated with the public key obtained from the associated DID in the id field. The signature is separated from the JSON serialization with characters that may not appear in the JSON. An example DAD with a payload follows: { \"id\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"data\": { \"name\": \"John Smith\", \"nation\": \"USA\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== As previously mentioned, the motivating use case of DADs is to support distributed streaming data applications. These could entail the creation of a large number of DIDs thus simplifying the reproduction of the associated public/private key pairs is an important consideration. One way to accomplish this is with a deterministic procedure for generating new public/private keys pairs where the private keys may be reproduced securely from some public information without having to be stored. A hierarchically deterministic (HD) key-generation algorithm does this by using a master or root private key and then generating new key pairs using a deterministic key-derivation algorithm. A derived key is expressed as a branch in a tree of parent/child keys. Each public key includes the path to its location in the tree. The private key for a given public key in the tree can be securely regenerated using the root private key and the key path, also called a chain code. Only one private key, the root, needs to be stored. The query part of the DID syntax may be used to represent an HD chain code for an HD key that is derived from a root DID. This provides an economical way to specify derived DIDs (dDIDs) that are used to identify DADS. An example follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2 The expression above discloses the root public DID as well as the key derivation path via the query part. For the sake of brevity this will be call an extended DID. The actual derived dDID is created by applying the HD algorithm with the result: did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE= Thus a database of dDIDs could be indexed by dDID expressions with each value being the extended DID. Looking up the extended DID allows the holder to recreate on the fly the associated private key for the dDID without ever having to store the private key. An entry in the database might look like the following: { \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2\", ... } The namespacing of the DID idstring also provides information that could be used to help formulate an HD path to generate a dDID. The following example shows two different dDIDs using the same public key and the same chain code but with a different extended idstring. did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:blue?chain=0/1 did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:red?chain=0/1 Some refinements to this approach may be useful. One is the granularity of dDID allocation. A unique dDID could be used for each unique DAD or a unique dDID could be used for each unique destination party that is receiving a data stream. In this case each DAD would need an additional identifier to disambiguate each DAD sent to the same party. This can be provided with an additional field or by using the DID path part to provide a sequence number. This is shown in the following example: did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057 The associated DAD is as follows: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== Change Detection Stale DAD items must often be detectable to prevent replay attacks. A later re-transmission of an old copy of the DAD item must not supercede a newer copy. Using a sequence number or some other identifier could provide change detection. Another way to provide change detection is for the DAD item to include a changed field whose value is monotonically increasing and changes every time the data is changed. The source of the data can enforce that the changed field value is monotonically increasing. Typical approaches include a monotonically increasing date-time stamp or sequence number. Any older data items resent or replayed would have older date-time stamps or lower sequence numbers and would thus be detectable as stale. Below is an example of an non-trivial data item that has a changed field for change detection. { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== Change detection prevents replay attacks in the following manner. A second party receives DAD updates that are each signed by the associated private key. Each update has a monotonically increasing changed field. The source signer controls the contents of the data wrapped by the signature. Therefore the signer controls any changed field. A consistent signer will use a monotonically increasing changed value whenever the data wrapped by the signature is changed. Thus a malicious third party cannot replay earlier instances of the DAD wrapped by a valid signature to the original second party because the second party knows to discard any receptions that have older changed fields than the latest one they have already received. On the Fly dDIDs in DADs One important use case for dDIDs in DADs is to identify data that is received from a source that is not providing identifying information with the data. The receiver then creates an associated DID and dDIDs to identify the data. At some later point the receiver may be able to link this data with some other identifying information or the source may \"claim\" this data by supplying identifying information. In this case the dDIDs are private to the receiver but can later be used to credibly provenance the internal use of the data. This may be extremely beneficial when shared amongst the entities in the processing chain as a way to manage the entailed proliferation of keys that may all be claimed later as a hierarchical group. The DIDs and associated derivation operations for dDIDS may be shared amongst a group of more-or-less trusted entities that are involved in the processing chain. Data-uniqe dDIDs Every DAD MUST have a unique DID. A database of DADs could be addressable by either their DID or their signature as for content-addressability. It might happen that an entity accidentally uses a public private key pair to create a dDID and use it two or more times. One way to ensure uniqueness is to use a random number as UUID or a timestamp and concatenate it as an extension in the DID path. did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/UUID Another way would be to use the signature in a content-addressable database to resolve the DAD. Public Derivation Another important use case for dDIDS in DADS is to avoid storing even the dDID with its derivation chain. This may be an issue when a client wishes to communicate with a potentially very large number of public services. Each public service would be a new pairing with a unique dDID. If the derivation algorithm for an HD-Key dDID could use the public key or public DID of the public service to generate the dDID then the client need not store the actual dDID but can recover the dDID by using the public DID of the server to re-derive the associated dDID. This can be done by creating a hash of the root DID private key and the remote server public DID to create the seed used to generate the dDID for the DAD. This also means that the dDIDs or chain codes do not have to be included in the keys preserved by a key-recovery system. Chaining DADs The provenance of data in a data flow through a data processing system with transformation can be established by forming a literal block chain of the data. When using DAD items to represent the data, the chain of DADs can be represented simply in a self-contained manner. At each step in the data flow of the originating DAD, where the contained data is transformed in any way, a new DAD is generated by the controlling entity of the transformation. This entity assigns a new DID (or dDID) to this DAD. The new DAD payload includes the DID of the DAD prior to transformation as well as the signature of the prior DAD. This links the new DAD to the prior DAD. The signature of the prior DAD provides both a hash that establishes the content integrity of the prior DAD as well a non-repudiation of the controller of the embedded prior DID. All the DADs in the data flow need to be stored some place indexed by their DIDs. Given this storage, any single DAD can then be used to recall the string of prior DADs back to the originating DAD or DADs. A special case is when an entity merely wishes to establish custody of data without changing or transforming it. The simplest way to to this that the entity adds a copy of the DAD as a link in the DAD chain without changing the underlying data. This forms an assertion that they control that link. If they do not transform the data then merely signing is enough to assert control over the link or equivalenty custody of the the data for that link of the chain. A one to many data stream is just a branch or fork in the chain into multiple chains. An example of an originating DAD and transformed DAD follows: Originating: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/alpha/10057\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } }\\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== Transformed: { \"id\": \"did:dad:AbC7fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/beta/10057\", \"changed\" : \"2000-01-01T00:00:02+00:00\", \"data\": { \"temp\": 50, \"humid\": 87, \"time\": \"12:15:37\" } \"prior\", { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/alpha/10057\", \"sig\": u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== }\\r\\n\\r\\n wbcj9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== This approach is then repeated until the data flow reaches the final consumer or sink of the data as shown in the following diagram. In the case where multiple data flows are aggregated or combined in some way such as a moving average or some type of statistical operation then the resultant DAD payload would include a list of the contributing or combined prior DADs. An example follows: Combined: { \"id\": \"did:dad:AbC7fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/gamma/10057\", \"changed\" : \"2000-01-01T00:00:03+00:00\", \"data\": { \"Avg temp\": 55, \"time\": \"12:15:39\" } \"priors\", [ { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/alpha/10057\", \"sig\": u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== }, { \"id\": \"did:dad:WA27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/beta/10058\", \"sig\": j78j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== }, ] }\\r\\n\\r\\n dy3j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== This can be extended to any arbitrarily complicated tree of processing transformations as shown in the following diagram. Because each DAD embeds a DID and is signed by the associated private key belonging to the DID, provenance of both the controller of the transformation step and the integrity of the associated data can be determined. Because each subsequent DAD in a processing flow or DAD chain contains the DID and signature of the prior DAD or DADs, full traceability can be established back to the originating DAD or DADs, preserving both data integrity and proof of control. This then is a critically enabling capability for decentralized computing infrastructure. A few open source projects are providing support of the key management needed for building applications with DIDs and DADs. Notable are the Didery Service [[10]] with web [[11]] and command line [[12]] clients and the SeedQuest [[13]] key-recovery application. Use Cases General When working with people, the proposed system could use proximity verification, implemented with software toolkits such as Google Nearby and p2pkit, to bind virtual identities to real people. This could be useful in many situations. For example, if two people met over Craigslist to sell a used bicycle, then the system could verify that the two parties actually met and transacted. If two people found each other through a dating site, and followed up with a face to face meeting, then the system could verify that these people were actually in the stated city and not catfishers from Ukraine or Nigeria. Similarly, transactional events could build trust in a similar manner for machines. For example, a certified mechanic performing a tune up on a car with a digital twin can also provide an attestation for the DID for that automobile. A complete history of the automobile's maintenance would add value for its owner. It should be noted that a single instance of meeting is not as trustable as an entire history of meeting many people. For a state actor generating a legend for a sockpuppet, this would entail an unattainable level of work to prove personhood. For a regular human being, it's relatively effortless to use the system in an organic and unobtrusive manner. Furthermore, these histories of meetings and verifications \u2014 using location data to prove it was not in Nigeria, and time data to prove it wasn't in the middle of the night \u2014 would be aggregated to increase the trustability of the personhood assessment. Every resource that is reference by a URL could instead be referenced by a DID or dDID. IoT: Data Stream: DID for Data Stream / DID for individual packaging / DID for Data Transformation (Signature of on old data generator + Signature of transformer) \u2192 Storing Transformations in a \"Blockchain\" DID for API Endpoints: Discovery / Authentication / Transformation. DID as UUID replacement: Unique Identifier + Properties of Self-Certification DID as Resource Identifier Replacements: Derived DIDs Devices: Pacemaker Software Oracles: Stock prices from an exchange Chatbots: Automated issue-management system Robotic Controllers: Car windscreen-wiper motors Locations: The place where two people met Software applications: Online gaming platform Sensors: Thermostat measuring the ambient temperature in a home Messaging: Peer-to-peer chat application Machines: Wind turbine generating clean energy Digital Media: Video recording of a news report Store of value: Bank account Software containers Microprocessors: Computational cloud server Validator nodes: A blockchain network Databases: Personal Healthcare Record Events [Future] Biological molecule: CRISPR gene therapy An array or matrix of other DIDs Use Case: Medical Imaging Workflow Image processing is operational in many scientific and engineering disciplines. These include: Image processing methods are used to count, measure, inspect, and analyse objects, coded information, and/or visual patterns. X-ray and ultrasonic devices deliver medical images with image processing that the doctor or medical algorithm can interpret more easily. X-ray machines in security zones automatically inspect luggage and clothing for dangerous objects. Visual image processing for quality assurance of objects in manufacturing systems. Radar and visual images for earth-observation applications. Image processing can include multiple data sources, parties, algorithms, and processing steps. An image processing end-user needs to be able to validate trustability and accuracy of an image data chain output data. This requirements becomes of critical importance when the output data is used in safety or security-relevant use cases or to make economic decision with significant commercial values involved. The cryptographic DAD data structures enable a user to validate the provenance of an image processing data chain including the authenticity and integrity of the input data and the provenance of the processing algorithms. We recommend the creation of DAD verifiable data chains for image processing that provide a DID for every image and data processing output data set. These data chains are then integrated with DID registries and/or digital twins that provide information about organisations, imaging devices, external data sources, and software algorithms involved in the data chain. This approach is of particular value when validation or benchmarking data are available about the image devices and the algorithms that are processing the images. In combination with a reputation or validation system any user can calculate trustability and accuracy metrics about the output data. In context of the medical image processing use case, the digital twins of the imaging device include verifiable claims issued by the OEM about the authenticity, accuracy, and calibration of the camera as well as benchmarking information about the accuracy of the machine learning algorithms. The following diagram provides a notional example of a DAD-chain for image processing data provenance. Use Case: Proof of Personhood The root use case is to help us identify and manage our identity interactions with other people. But to do so more effectively, we need to know that an identifier is accurately associated with an actual person. The fraudulent misuse of identity has a significant negative impact on society. At an individual level, fraudulent users could cheat others in commercial transaction, become catfishers on dating sites, or worse. At a societal level, the fraudulent misuse of identity through bots and sockpuppets have caused havoc in elections and through the manipulation of public opinion through weaponized propaganda. The highest purpose of the blockchain is as a kind of \"truth machine\". For decentralized identity to succeed, and not be co-opted as just another way to empower sockpuppets and botnets, there needs to be an equivalent to proof-of-work, a mechanism that binds physical entities to virtual identities in a way that enables accountability while preserving anonymity. This is now being referred to as \"proof of personhood\". One use case would be to use proximity verification, implemented with software toolkits such as Google Nearby and p2pkit, to bind virtual identities to real people, in a way that preserves privacy, non-correlation, zero-knowledge proofs, and pseudonymous operations. We're currently building a technology called Not_a_Bot, which provides proof of personhood through a variety of techniques. One technique is to verify that the user has actually met another actual person, in physical space\u2026 and is not a catfisher, not a scammer, and \"not a bot\". It should be noted that a single instance of meeting is not as trustable as an entire history of meeting many people. For a state actor generating a legend for a sockpuppet, this would entail an unattainable level of work to prove personhood. For a regular human being, it's relatively effortless to use the system in an organic and unobtrusive manner. Once a root personhood verification could be insured, then trustable pseudonyms could be generated. Adding this verification to DIDs would provide trust in a trustless environment, as the DID could then provide identity and credentialing services in environments that support, or even require, pseudonymity. Decentralized Fog Computing Infrastructure The coming tidal wave of digital data due to the proliferation of digital devices will require an exponential increase in computing capacity for data integration and analysis. Currently data integration and analysis is handled predominantly in the cloud. This is not efficient as it requires transport of data from the edge of the internet where it is created to remote data centers that are the cloud for processing and then transport back to the edge of the results of processing. In many cases the data processing can be performed in a hierarchical-tree-like bottom-up fashion which is more cost effectively done in the edge close to the data. Likewise new applications benefit from low latency processing of data near the sources and sinks of the data. These applications also benefit from processing the data in the edge. Currently there is little capacity to perform processing in the edge. Consequently an opportunity exists to build a public decentralized edge computing infrastructure. Edge computing infrastructure is called the fog . A public decentralized fog computing infrastructure provides an opportunity to scale capacity using two sided network effects where a large number of entities can cooperatively participate in the two-sided network both as consumers and as importantly producers of compute. This is in contrast to the highly centralized nature of cloud computing where a handful of producers control most of the public computing infrastructure. This stifles innovation and raises costs. A public, decentralized, fog computing infrastructure could enable the monetization of pre-existing spare capacity in the form of bandwidth, space, energy, and compute devices. This would provide lower costs and incentivize innovation. Future Direction Data flows can be provenanced by verifying the end-to-end integrity of data with DIDs. By enabling DIDs to sign claims about other DIDs, the fidelity of these data flows can be increased further. There are several ideas that are good candidates for future exploration. These are as follows: Examine how a DID can utilize verifiable credentials to prove verified aspects of their identity when signing claims about other DIDs. Such a use case could enable verified inspectors to sign a claim with their DID that they have serviced an IoT sensor and certify what software and hardware upgrades the sensor is using. Examine how verifiable claims and credentials can be issued for location verification. Examine the social/network interactions between DIDs that sign attestations about other DIDs. Examine how pairwise-unique DIDs and zero-knowledge proofs could empower users to make contentious counterfactual claims in a privacy respecting manner. Examine how the system could help to drive non-correlation functionality. Open discussion on other issues such as cognitive models, optimization and AI models, and the potential use of tokenization to drive behavioral economics. Examine how attestations or other types of claims on a DID can build an attribution graph that increases the value of the credentials associated with a DID. Conclusion Imagine a world where this proposed technology has been deployed and globally adopted. Let us paint a picture for how this might be achieved. Imagine that this approach becomes part of a decentralized identity solution for every entity, driven by a robust and active developer community. The vision is to generate technologies that would be integrated into applications that are used in IoT, e-commerce, social interaction, banking, healthcare, and so on. Now imagine that mobile telephony companies agree to embed the technology into the operating systems for all smartphones, and the dominant social network providers agree to use DIDs and DADs and proofs about the entities controlling these DIDs and DADs in their algorithms for determining which content to propel. This would mean the end of phishing. The end of fake news. This is the beginning of new era for society, built on an interconnecting web of trust: a world in which we know what impacts we are having. The emergent property of this new data fabric is Knowing. This is greatly needed as trust in media is at an all-time low, and centralized, algorithmic distribution have created a perfect storm for the rise of misinformation, disinformation, and fake news. This is driving polarization while simultaneously undermining public trust in institutions. However, realistically, most of society's greatest challenges have no silver-bullet solution. Consider the problem of using sock puppets to weaponize propaganda. Proximity verification is one component of a multi-pronged solution that might help mitigate the problem. Consider that certain highly problematic diseases can be treated with drug combinations consisting of antiretroviral compounds mixed with transcriptase inhibitors and steroids. The combinations are called \"cocktails,\" and they're so effective that they're called the \"Lazarus Effect,\" named for the biblical figure who was raised from the dead. Cocktails can turn an HIV death sentence into a manageable chronic condition. Just as complex and evolving health challenges must be addressed with complex and evolving multi-pronged solutions, the complex challenges of online identity require a comprehensive and systematic approach using multi-pronged solutions that synergistically combine to enable disruption, change and transformation at multiple levels. This paper aims help to determine what other solutions would need to be integrated, to create a \"cocktail prescription\" to address this problem. Automating the detection of misinformation is only half the problem. Preventing the weaponization of that propaganda is the other half, and this proposed technique could help provide at least part of a comprehensive cocktail prescription to address the issue of fake news The Internet's current capacity to support democratic societies in making well-informed decisions is being subverted by globally networked state actors. However, there are additional benefits for this technology in computing, social networking, connected governmental services, and e-commerce \u2014 where the use of sockpuppets is more of an aggravation than a grave danger. For example, in terms of government service, we envision a system where elected officials could verify how many people they actually meet and how much time was spent with them, to back up claims of being a \"man of the people\". For fully transparent politics, this system should could provide the electorate with an accurate sense of whether a politician has actually met with leaders of social movements or is spending the majority of time with donors, lobbyists, and political action committees. Underlying the benefits of decentralized identity outlined above is the need for open interoperable standards to ensure the reputable provenance of the associated data flows between decentralized entities. This paper describes a novel concept for provenancing data flows using DADis (Decentralized Autonomic Data items) that are built upon the emerging DID standard. This approach uses and extends the advanced diffuse-trust or zero-trust computing paradigm that is needed to operate securely in a world of decentralized data. Authors In alphabetical order. Shaun Conway shaun@ixo.world Andrew Hughes andrewhughes3000@gmail.com Moses Ma moses.ma@futurelabconsulting.com Jack Poole jack.w.poole@gmail.com Martin Riedel martin@civic.com Samuel M. Smith Ph.D. sam@samuelsmith.org Carsten St\u00f6cker carsten.stoecker@interlinked.ai References 1 . https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2018/blob/master/final-documents/DecentralizedAutonomicData.pdf 2 . https://github.com/WebOfTrustInfo/rwot7/blob/master/topics-and-advance-readings/ZeroTrustComputingWithDidsAndDads.md 3 . https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2017/blob/master/topics-and-advance-readings/did-primer.md 4 . https://www.statista.com/statistics/471264/iot-number-of-connected-devices-worldwide/ 5 . https://www.gartner.com/smarterwithgartner/gartner-predicts-a-virtual-world-of-exponential-change/ 6 . Redemocratizing Permissionless Cryptocurrencies, by Maria Borge, Eleftherios Kokoris-Kogias, Philipp Jovanovic, Linus Gasser, Nicolas Gailly, Bryan Ford. 2017 IEEE European Symposium on Security and Privacy Workshops EuroS&PW, April 2017 7 . https://www.nist.gov/sites/default/files/documents/2017/06/05/040813_forrester_research.pdf 8 . https://www.amazon.com/Zero-Trust-Networks-Building-Untrusted/dp/1491962194 9 . https://github.com/SmithSamuelM/Papers/blob/master/whitepapers/ManyCubed.pdf 10 . https://github.com/reputage/didery 11 . https://github.com/reputage/didery.js 12 . https://github.com/reputage/didery.py 13 . https://github.com/reputage/seedQuest 14 . https://w3c-ccg.github.io/did-spec/ 15 . https://www.w3.org/2017/vc/WG/ 16 . https://identity.foundation 17 . https://en.wikipedia.org/wiki/Possession_is_nine-tenths_of_the_law 18 . https://www.ietf.org/rfc/rfc3986.txt","title":"A DID for Everything"},{"location":"RWoT7/A_DID_for_everything/#a-did-for-everything","text":"Attribution, Verification and Provenance for Entities and Data Items Presented by Shaun Conway , Andrew Hughes , Moses Ma , Jack Poole , Martin Riedel , Samuel M. Smith Ph.D. , and Carsten St\u00f6cker Submitted to the 7th Rebooting the Web of Trust Technical Workshop September 24-26, 2018, Toronto Keywords: decentralized identity, Internet of Things, autonomic data, verified claims, identity, blockchain, self-sovereign, fog computing, industry 4.0, digital twinning, data chain provenance, audit trails","title":"A DID for Everything"},{"location":"RWoT7/A_DID_for_everything/#abstract","text":"The decentralized identifier (DID) is a new and open standard type of globally unique identifier that offers a model for lifetime-scope portable digital identity that does not depend on any centralized authority and that can never be taken away by third-parties [[14]]. DIDs are supported by the W3C community [[14]] and the Decentralized Identity Foundation (DIF) [[16]]. They are the \"atomic units\" of a new layer of decentralized identity infrastructure. However, DIDs can be extended from identifiers for people to any entity, thus identifying everything . We can use DIDs to help us identify and manage objects, machines, or agents through their digital twins; we can expand them to locations, to events, and even to pure data objects, which we refer to as decentralized autonomic data (DAD) items [[1]][[3]]. The paper will present novel use-cases for DIDs and DADs and propose a new cryptographic data structure that is a self-contained blockchain of DADs. This enables the verification of the provenance of a given data flow. It builds on a prior paper [[1]] and an associated reading [[2]]. DIDs are only the base layer of decentralized identity infrastructure. The next higher layer (where most of the value is unlocked) is verifiable claims. This is the technical term for a digitally signed electronic data structure that conforms to the interoperability standards being developed by the W3C Verifiable Credentials Working Group [[15]]. When a DID and hence DADs of the resultant data are extended to machines and autonomic data, the provenance chain of the data flow can provide the basis for verifiable claims and attestations about the data flow as well as the basis for a reputation.","title":"Abstract"},{"location":"RWoT7/A_DID_for_everything/#why-this-matters","text":"Today, the Internet is probably best described as a network comprised of all interconnected entities, traditionally referring to human users and computers. When we add connected entities and devices in the so-called Internet of Things (IoT), the number of addressable elements is in the tens of billions, with an estimate of 75 bn connected IoT devices in 2025 [[4]]. Software services, such as algorithms and bots, further extend this universe of identifiable entities. The resulting combinatorics of possible connections between any given set of entities is an impossibly large number. Yet in today's user journeys or business environments, agents (whether human, machine, or software) increasingly need to communicate, access or transact with a diverse group of these interconnected objects to achieve their goals in both the digital and physical worlds. This requires a straightforward and ubiquitous method to address, verify, and connect these elements together. Definition of Entity : Something that has a distinct and independent existence either in the real or the digital world. Examples of an entity are: Living Organism Physical Object Locations or Events Machines and Devices in the Internet of Things (IoT) Digital Asset, Data Set, or Agent Human or object identities are stored in multiple centralised or federated systems such as government, ERP, IoT, or manufacturing systems. From the standpoint of cryptographic trust verification, each of these centralised authorities serves as its own root of trust. An entity trailing along a value chain is interacting with multiple systems. Consequently, a new actor in any given value chain has no method to independently verify credentials of a human or attributes of either a physical object or data item (provenance, audit trail). This results in the existence of complex validation, quality inspection, and paper trail processes, and enormous hidden trust overhead costs are added to all value chains and services. To be a truly global solution, easy to use and still safe from hacking and sovereign interference, such a scheme must include: preservation of privacy security from tampering reliable trust verification assurance of risk independence from any vendor-defined naming API one-to-one mappable onto each entity. Therefore, a universal addressing, trust-verification system and associated interoperable protocol must be utilised, empowering every form of entity.","title":"Why This Matters"},{"location":"RWoT7/A_DID_for_everything/#why-it-matters-for-people","text":"Today when entities want their identities to be confirmed they transfer information such as a birth certificate, physical address, or social security number to multiple third parties, who start to validate the same data in different contexts for KYC and authentication processes. The parties to which they sent that information retains it, meaning the data is out there in silos, creating risks in terms of data loss, privacy breaches, and use of inconsistent data and forcing companies that might not want to be in that position to store that information. It also enables businesses to harvest people's personal data for commercial purposes, which does not necessarily reflect the intentions of the individual people. This situation results in big problems for humans such as broken health care records. Patients will need a universally addressable healthcare record system that is controlled by the patient itself, that consistently stores all relevant verified health care data, and that is able to share this data with doctors that need to connect with it. To enable the doctors or algorithms, they need a data-flow provenance to verify the integrity, quality, or reputation of a healthcare record to decide on treatments and give the patient confidence about the proposed treatments.","title":"Why it Matters for People"},{"location":"RWoT7/A_DID_for_everything/#why-it-matters-for-businesses","text":"Definition of a Digital Twin : A digital twin is a digital representation of either a real-world or digital entity. A digital twin exists over the life-cycle of an entity from planning, manufacturing, testing, birth, and operations to decommissioning and reuse. The more past and present data are related and analysed, the more knowledge can be deployed to drive significant improvements on an individual entity or system level. It is estimated that by 2022 the IoT powered by digital twins will save consumers and businesses worth $1 trillion a year in asset maintenance [[5]]. The notion of digital twinning for objects, machines, and agents is becoming relevant to an increasing number of human services and Industry 4.0 use cases. This is the result of the growth in digital services, connections, and data streams from the Internet of Things (IoT) devices that increasingly drive integration with machine learning algorithms, resulting in graph-type data chains for processing the IoT data streams. Today, digital twins are captured in siloed, proprietary IoT solutions by individual corporates that do not own the physical object over the life-cycle and even do not interact with parties using the object further down a value chain. Decentralized solutions are liberating the digital twins from silos and establishing more valuable and interoperable verifiable attributes about entities and the data chains they connect with.","title":"Why it Matters for Businesses"},{"location":"RWoT7/A_DID_for_everything/#why-it-matters-for-objects-machines-and-agents","text":"There is no widely adopted authentication or verification systems in place to provide the equivalent of KYC (know-your-customer) for non-human entities, that is: KYA (know-your-agent), KYB (know-your-bot), KYM (know-your-machine), or KYO (know-your-object). In a world when objects and machines are connected with datastreams and intelligent agents that perform transaction on behalf of the entity, the number of agent-to-agent transactions will outgrow the number of human transactions by many orders. An agent transacting with another party can independently verify the identifier of the this party, its attributes, and the provenance of the data sets that are involved in the transaction. Digital twins of 3D-printed objects for safety critical parts such as a turbine of an airplane provide an important example. For these parts, it is important to have an precise audit trail about the 3D printing process to prove that the object was manufactured in accordance to stringent specifications. The digital twin stores design, manufacturing, post processing, and quality-assurance data about the 3D-printed object. These data are coming from multiple systems resulting in a variety of data chains. With DIDs and DADs the integrity of the data chains can be verified. The verification of the datachains and the underlying data results in important proofs about the provenance of 3D printed object.","title":"Why it Matters for Objects, Machines and Agents"},{"location":"RWoT7/A_DID_for_everything/#why-it-matters-for-the-world","text":"The diverse application of decentralized identifiers (DIDs) will have substantial influences in broader applications on a global scale. The seamless provenance of physical objects or data items through any value chain has major implications on the risk and value properties of the processed data. Within any dynamic process, participating entities have substantial interest in the authenticity and trustworthiness in any individual step. Data that is accumulated with an unforgeable audit trail that references decentralized identifying information (Person, Device or any other Entity) for any transformation step holds greater value then it would have without such properties. Managing the sustainability of the commons requires mechanisms to value natural capital and to account for the externalities that arise from human activities. This should attribute extractions from and contributions to the commons by organisations, organisms, machines and information. We need to identify these entities and must identify both positive and negative impacts these entities are having on the commons. Knowing what these impacts are enables us to count what matters and to put a value on what counts. The promise of a overarching prevalence through the broad use of DIDs also provides the key component for achieving the vision of a circular economy : a regenerative system in which resource input and waste, emission, and energy leakage are minimized by slowing, closing, and narrowing energy and material loops. This can be achieved through long-lasting design, maintenance, repair, reuse, remanufacturing, refurbishing, recycling, and upcycling. This contrasts with a linear economy, which is a self-destructing, catastrophic 'take, make, dispose' model of production.","title":"Why it Matters for the World"},{"location":"RWoT7/A_DID_for_everything/#comprehensive-solution","text":"","title":"Comprehensive Solution"},{"location":"RWoT7/A_DID_for_everything/#motivation","text":"The Decentralized Autonomic Data paper from the RWOT Spring 2018 introduced three new concepts related to DIDs [[1]]. Decentralized Autonomic Data (DADi) items Derived DID (dDID) Self-contained verification of data governance and integrity. This paper extends these concepts with a new one, that is, Data flows may be provenanced via a self-referential blockchain (list of signed DAD items where each subsequent DAD item includes the DID or dDID and associated signature of the previous item)","title":"Motivation"},{"location":"RWoT7/A_DID_for_everything/#data-provenance","text":"The concepts above lay the groundwork for data-flow provenance that is the focus of this paper. In this context provenance is used in a general sense. By data-flow provenance we mean a mechanism for tracing data-item content and control through a processing system, including any transformations to the data item or its governance. This includes flows with multiple sources and sinks of data, independently and in combination. To restate, data-flow provenance means not just tracing control but also verifying the end-to-end integrity of every data flow, including any transformations (additions, deletions, modifications, and combinations). In this context provenance refers to the chain-of-custody of data items not ownership. Ownership is a more complex idea than mere control. Change of ownership requires the previous owner to relinquish ownership and the new owner to accept ownership. It reverts back if not accepted. Finally, ownership implies a restriction or limitation on rights so it may need to be interactive or involve a third party. In English we use the term \" custody \" to refer to having possession of something (under you control), but custody does not equal ownership. With physical objects one can have unique custody that could be equivalent to ownership because a physical object is a unique thing. Hence the expression \"possession is nine-tenths of the law\" [[17]]. Because data can be easily duplicated, however, it may not be possible to prove unique possession of data. There is no way to guarantee that there is not another copy of the data. So custody of data is not equivalent to restricted ownership. With data the strongest attestation of ownership is limited to merely rights to use the data. So transfer of ownership of data is a different concept; it's not transfer of the data itself but the transfer of rights to use the data. An important insight from the viewpoint of a distributed application is that an entity's influence on the application is solely based on the digital data flows that move between the entity and the other components of the distributed application. We choose to call those data flows the entity's projection onto the distributed application. If those projections consist of DADs and every interaction of internal components consists of DADs then we have a universal approach for implementing decentralized applications with total provenance of control and data within the application. A major motivation for establishing data-flow provenance is to support building and architecting decentralized processing systems that use a zero-trust or more correctly diffuse-trust perimeterless security model. The emerging decentralized processing paradigm for distributed applications where the sources and sinks of data may be controlled by multiple entities, i.e. decentralized governance, means that traditional perimeter security models are at best problematic. Indeed, even the newer perimeterless security model as originally formulated assumes singular governance of the network hosts and associated data [[7]][[8]]. We extend that model herein to include hosts and DADs with decentralized governance via DIDs. An earlier paper explored the architectural issues of building distributed computing infrastructure that applied the zero-trust security model but extended it to use distributed consensus for policy governance [[9]]. This type of architecture we called Zero-Trust-Computing or more correctly Diffuse-Trust-Computing . A simple way of explaining the zero-trust security model is the mantra, \"never trust, always verify\". The paradigm of Zero Trust Networking was first popularized in 2013 by a NIST report [[7]]. More recently the principles have received much broader attention including the book Zero Trust Networks [[8]]. The basic approach to Diffuse-Trust-Computing is to use a diffuse trust perimeterless security approach. As previously mentioned, some call this a trustless or zero-trust security model but that is a misnomer. There is still trust, it is just diffused in such a way that security is greatly enhanced [[9]]. In this paper we further extend that model to use DIDs and DADs to provide decentralized end-to-end data provenance. The caveats of conventional diffuse-trust perimeterless security are: The network is always hostile both internally and externally; locality is not trustworthy. By default, every network interaction or data flow must be authenticated and authorized using best practices cryptography. By default, inter-host communication must be end-to-end signed/encrypted, and data must be stored signed/encrypted using best practices cryptography; Data is signed/encrypted at motion and at rest. Policies for authentication and authorization must be dynamically modified based on behavior (reputation). Using end-to-end encryption and storage prevents exploits from anyone that merely has access to the network or the data-storage device. By authenticating and authorizing every network interaction or data flow, security becomes granular. A successful exploit of one interaction does not bleed into any other. Compromising one data flow does not compromise any other. Escalation opportunities are minimized. Many security exploits are discovered through repeated probes and experiments to find bugs, buffer overflows, or weaknesses in network protocols or software implementations. Dynamic policy modification that uses AI to first profile and detect anomalous behavior and then restrict the authorization of that user prevents discovery. This adds time as a defence. Extending the model to enhance the security of the policy management adds the following caveat: Policies must be governed by distributed consensus. Distributed consensus diffuses the trust for any policy decision to a group of hosts. In order to defeat the policy, an attacker must exploit some majority of the hosts. This makes exploits exponentially more difficult. Distributed consensus also allows for decentralized governance of the hosts. This paper extends these principles with one more: By default, each data flow including transformations must be end-to-end provenanced using decentralized identifiers (DIDs) and hence decentralized autonomic data items (DADis). This additional principle allows governance over the data using a decentralized trust model and a decentralized web of trust based on DIDs and DADis. This approach enables truly decentralized governance models for distributed applications. One can combine the third and sixth caveats from above, to a simple summary cavet: By default, data flows are end-to-end provenanced/signed/encrypted at motion and at rest using DIDs and DADis. This remainder of this section in the paper will explore salient issues in using DIDs and DADs to maintain provenance over each step in a data processing flow including transformations of the data to enable credible uses of the data for various applications while maintaining a zero or difuse-trust security model.","title":"Data Provenance"},{"location":"RWoT7/A_DID_for_everything/#decentralized-autonomic-data","text":"","title":"Decentralized Autonomic Data"},{"location":"RWoT7/A_DID_for_everything/#definition","text":"First introduced in more detail here [[1]], a decentralized autonomic data item (DADi) is associated with a decentralized identifier [[14]]. This paper does not provide a detailed definition of DIDs but does describe how DIDs are used by a DADi. The DID syntax specification is a modification of standard URL syntax per RFC-3986 [[19]]. As such, it benefits from familiarity, which is a boon to adoption. One of the features of a DID is that it is a self certifying identifier in that a DID includes either a public key or a fingerprint of a public key from a cryptographic public/private key pair. Thereby a signature created with the private key can be verified using the public key provided by the DID. The inclusion of the public part of a cryptographic key pair in the DID gives the DID other desirable properties. These include universal uniqueness and pseuodnynmity. Another way to describe a DID is that it is a cryptonym: a cryptographically derived pseudonym. In the DAD acronym, decentralized means that the governance of the data may not reside with a single party. The term autonomic means self-managing or self-regulating. In the context of data, we crystalize the meaning of self-managing to include cryptographic techniques for maintaining data provenance that make the data self-identifying, self-certifying, and self-securing. Implied thereby is the use of cryptographic keys and signatures to provide a root of trust for data integrity and to maintain that trust over change-in-custody and transformation of that data, i.e. provenance . The motivating use of DAD is to provide provenance for streaming data that is generated and processed in a distributed manner with decentralized governance. Streaming data are typically measurements that are collected and aggregated to form higher level constructs. Applications include analytics and instrumentation of distributed web or internet-of-things (IoT) applications as well as portable reputation systems. A DAD seeks to maintain a provenance chain for data undergoing various processing stages that follows diffuse trust security principles including signed at rest and in motion. Streaming data applications may impose significant performance demands on the processing of the associated data. Associated with a DID is a DID Document (DDO) [[14]]. The DDO provides meta-data about the DID that can be used to manage the DID as well as discover services affiliated with the DID. Typically the DDO is provided by a DID resolution service. The DID/DDO model is not a good match for streaming data especially if a new DID/DDO pair would need to be created for each new DAD item. But a DID/DDO is a good match when used as the root or master identifier from which an identifier for the DAD is derived. This derived identifier is called a derived-DID or dDID. Thus only one DID/DDO paring is required to manage a large number of DADs where each DAD may have a unique dDID. The syntax for a dDID is identical for a DID. The difference is that only one DDO with meta-data is needed for the root DID used to create the dDIDs in the DADs. Each DAD item carries any additional DAD-specific meta-data, thus making them self-contained (autonomic).","title":"Definition"},{"location":"RWoT7/A_DID_for_everything/#did-syntax","text":"A DID or dDID has the following required syntax: did:method:idstring The method is some short string that namespaces the DID and provides for unique behavior in the associated method specification. In this paper we will use the method dad . The idstring must be universally unique. The idstring can have multiple colon \":\" separated parts, thus allowing for namespacing. In this document the first part of the idstring is linked to the public member of a cryptographic key pair that is defined by the method. We will use a 44-character Base64 URL-File safe encoding as per RFC-4648 [[18]], with one trailing pad byte of the 32-byte public verification key for an EdDSA (Ed25519) signing key pair. Unless otherwise specified Base64 in this document refers to the URL-File safe version of Base64. The URL-File safe version of Base64 encoding replaces plus \"+\" with minus \"-\" and slash \"/\" with underscore \" \". A DID may have optional parts including a path, query, or fragment. These use the same syntax as a URL, that is, the path is delimited with slashes, / , the query with a question mark, ? , and the fragment with a pound sign, #_. When the path part is provided then the query applies to the resource referenced by the path and the fragment refers to an element in the document referenced by the path. An example follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=/mom?who=me#blue In contrast, when the path part is missing but either the query or fragment part is provided then the query and/or fragment parts have special meaning. A query without a path means the the query is an operation on either the DID itself or the DID document (DDO). Likewise when a fragment is provided then the fragment is referencing an element of the DDO. An example of a DID without a path but with a query follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?who=me As will be described later, a query part on a DID expression without a path part will enable the generation of dDIDs (derived DIDs). DIDs form a new class of identifiers that can be used to replace the identifiers commonly used in distributed applications. DIDs can replace UUIDs, URLs, and Public Keys because a DID combines the desirable features of all three identifier classes. UUID: A Universally Unique Identifier per RFC 4122 has types 1-5. These are 16-byte collision-resistant decentralized identifiers generated with a pseudo-random-number generator and optional name-spacing data. A UUID enables distributed applications to create unique identifiers without central authority. Prefixed name-spacing allows for sorting and searching properties such as time order, lexical order, nesting etc. URI: A Uniform Resource Identifier (URI), Uniform Resource Locator( URL), Uniform Resource Name (URN) per RFC 3986 is of the form scheme:[//[user[:password]@]host[:port]][/path][?query][#fragment]. This provides not just an identifier but namespacing authority and a mini-language for performing operations on the identifier and associated resources. Decentralized Self-Certifying Identifier: A Self-Certifying Identifier contains the fingerprint of a public member of cryptographic public/private key pair. A decentralized Self-Certifying Identifier contains the fingerprint of a public/private key pair that is generated by the user not a central registry. A Hierarchical Deterministic Self-Certifying Identifier of the form selfcertroot:/path/to/related/data provides a way to reproduce private keys without having to store them. Tupleizable Identifier. A tupleizable identifier of the form (channel, host, process, data) tenables a routing overlay on top of IP that can be provenanced. Indeed any computing infrastructure that is now using UUIDs and URLs could beneficially replace them with DIDs.","title":"DID Syntax"},{"location":"RWoT7/A_DID_for_everything/#minimal-dad","text":"A minimal DAD (decentralized autonomic data) item is a data item that contains a DID or dDID that helps uniquely identify that data item or affiliated data stream. In this paper JSON is used to represent serialized DAD items but other formats could be used instead. To ensure non-repudiable data integrity (i.e. that the data has not been tampered with and can be assigned to a given signing entity) a signature is appended to the DAD item that is verifiable as being generated by the private key associated with the public key in the id field value. This signature verifies that the DAD item was created by the holder of the associated private key. The DAD item is thereby both self-identifying and self-certifying because the identifier value given by the id field is included in the signed data and is verifiable against the private key associated with the public key obtained from the associated DID in the id field. The signature is separated from the JSON serialization with characters that may not appear in the JSON. An example DAD with a payload follows: { \"id\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=\", \"data\": { \"name\": \"John Smith\", \"nation\": \"USA\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== As previously mentioned, the motivating use case of DADs is to support distributed streaming data applications. These could entail the creation of a large number of DIDs thus simplifying the reproduction of the associated public/private key pairs is an important consideration. One way to accomplish this is with a deterministic procedure for generating new public/private keys pairs where the private keys may be reproduced securely from some public information without having to be stored. A hierarchically deterministic (HD) key-generation algorithm does this by using a master or root private key and then generating new key pairs using a deterministic key-derivation algorithm. A derived key is expressed as a branch in a tree of parent/child keys. Each public key includes the path to its location in the tree. The private key for a given public key in the tree can be securely regenerated using the root private key and the key path, also called a chain code. Only one private key, the root, needs to be stored. The query part of the DID syntax may be used to represent an HD chain code for an HD key that is derived from a root DID. This provides an economical way to specify derived DIDs (dDIDs) that are used to identify DADS. An example follows: did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2 The expression above discloses the root public DID as well as the key derivation path via the query part. For the sake of brevity this will be call an extended DID. The actual derived dDID is created by applying the HD algorithm with the result: did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE= Thus a database of dDIDs could be indexed by dDID expressions with each value being the extended DID. Looking up the extended DID allows the holder to recreate on the fly the associated private key for the dDID without ever having to store the private key. An entry in the database might look like the following: { \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=\": \"did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=?chain=0\\1\\2\", ... } The namespacing of the DID idstring also provides information that could be used to help formulate an HD path to generate a dDID. The following example shows two different dDIDs using the same public key and the same chain code but with a different extended idstring. did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:blue?chain=0/1 did:dad:Xq5YqaL6L48pf0fu7IUhL0JRaU2_RxFP0AL43wYn148=:red?chain=0/1 Some refinements to this approach may be useful. One is the granularity of dDID allocation. A unique dDID could be used for each unique DAD or a unique dDID could be used for each unique destination party that is receiving a data stream. In this case each DAD would need an additional identifier to disambiguate each DAD sent to the same party. This can be provided with an additional field or by using the DID path part to provide a sequence number. This is shown in the following example: did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057 The associated DAD is as follows: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ==","title":"Minimal DAD"},{"location":"RWoT7/A_DID_for_everything/#change-detection","text":"Stale DAD items must often be detectable to prevent replay attacks. A later re-transmission of an old copy of the DAD item must not supercede a newer copy. Using a sequence number or some other identifier could provide change detection. Another way to provide change detection is for the DAD item to include a changed field whose value is monotonically increasing and changes every time the data is changed. The source of the data can enforce that the changed field value is monotonically increasing. Typical approaches include a monotonically increasing date-time stamp or sequence number. Any older data items resent or replayed would have older date-time stamps or lower sequence numbers and would thus be detectable as stale. Below is an example of an non-trivial data item that has a changed field for change detection. { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/10057\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } } \\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== Change detection prevents replay attacks in the following manner. A second party receives DAD updates that are each signed by the associated private key. Each update has a monotonically increasing changed field. The source signer controls the contents of the data wrapped by the signature. Therefore the signer controls any changed field. A consistent signer will use a monotonically increasing changed value whenever the data wrapped by the signature is changed. Thus a malicious third party cannot replay earlier instances of the DAD wrapped by a valid signature to the original second party because the second party knows to discard any receptions that have older changed fields than the latest one they have already received.","title":"Change Detection"},{"location":"RWoT7/A_DID_for_everything/#on-the-fly-ddids-in-dads","text":"One important use case for dDIDs in DADs is to identify data that is received from a source that is not providing identifying information with the data. The receiver then creates an associated DID and dDIDs to identify the data. At some later point the receiver may be able to link this data with some other identifying information or the source may \"claim\" this data by supplying identifying information. In this case the dDIDs are private to the receiver but can later be used to credibly provenance the internal use of the data. This may be extremely beneficial when shared amongst the entities in the processing chain as a way to manage the entailed proliferation of keys that may all be claimed later as a hierarchical group. The DIDs and associated derivation operations for dDIDS may be shared amongst a group of more-or-less trusted entities that are involved in the processing chain.","title":"On the Fly dDIDs in DADs"},{"location":"RWoT7/A_DID_for_everything/#data-uniqe-ddids","text":"Every DAD MUST have a unique DID. A database of DADs could be addressable by either their DID or their signature as for content-addressability. It might happen that an entity accidentally uses a public private key pair to create a dDID and use it two or more times. One way to ensure uniqueness is to use a random number as UUID or a timestamp and concatenate it as an extension in the DID path. did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/UUID Another way would be to use the signature in a content-addressable database to resolve the DAD.","title":"Data-uniqe dDIDs"},{"location":"RWoT7/A_DID_for_everything/#public-derivation","text":"Another important use case for dDIDS in DADS is to avoid storing even the dDID with its derivation chain. This may be an issue when a client wishes to communicate with a potentially very large number of public services. Each public service would be a new pairing with a unique dDID. If the derivation algorithm for an HD-Key dDID could use the public key or public DID of the public service to generate the dDID then the client need not store the actual dDID but can recover the dDID by using the public DID of the server to re-derive the associated dDID. This can be done by creating a hash of the root DID private key and the remote server public DID to create the seed used to generate the dDID for the DAD. This also means that the dDIDs or chain codes do not have to be included in the keys preserved by a key-recovery system.","title":"Public Derivation"},{"location":"RWoT7/A_DID_for_everything/#chaining-dads","text":"The provenance of data in a data flow through a data processing system with transformation can be established by forming a literal block chain of the data. When using DAD items to represent the data, the chain of DADs can be represented simply in a self-contained manner. At each step in the data flow of the originating DAD, where the contained data is transformed in any way, a new DAD is generated by the controlling entity of the transformation. This entity assigns a new DID (or dDID) to this DAD. The new DAD payload includes the DID of the DAD prior to transformation as well as the signature of the prior DAD. This links the new DAD to the prior DAD. The signature of the prior DAD provides both a hash that establishes the content integrity of the prior DAD as well a non-repudiation of the controller of the embedded prior DID. All the DADs in the data flow need to be stored some place indexed by their DIDs. Given this storage, any single DAD can then be used to recall the string of prior DADs back to the originating DAD or DADs. A special case is when an entity merely wishes to establish custody of data without changing or transforming it. The simplest way to to this that the entity adds a copy of the DAD as a link in the DAD chain without changing the underlying data. This forms an assertion that they control that link. If they do not transform the data then merely signing is enough to assert control over the link or equivalenty custody of the the data for that link of the chain. A one to many data stream is just a branch or fork in the chain into multiple chains. An example of an originating DAD and transformed DAD follows: Originating: { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/alpha/10057\", \"changed\" : \"2000-01-01T00:00:00+00:00\", \"data\": { \"temp\": 50, \"time\": \"12:15:35\" } }\\r\\n\\r\\n u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== Transformed: { \"id\": \"did:dad:AbC7fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/beta/10057\", \"changed\" : \"2000-01-01T00:00:02+00:00\", \"data\": { \"temp\": 50, \"humid\": 87, \"time\": \"12:15:37\" } \"prior\", { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/alpha/10057\", \"sig\": u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== }\\r\\n\\r\\n wbcj9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== This approach is then repeated until the data flow reaches the final consumer or sink of the data as shown in the following diagram. In the case where multiple data flows are aggregated or combined in some way such as a moving average or some type of statistical operation then the resultant DAD payload would include a list of the contributing or combined prior DADs. An example follows: Combined: { \"id\": \"did:dad:AbC7fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/gamma/10057\", \"changed\" : \"2000-01-01T00:00:03+00:00\", \"data\": { \"Avg temp\": 55, \"time\": \"12:15:39\" } \"priors\", [ { \"id\": \"did:dad:Qt27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/alpha/10057\", \"sig\": u72j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== }, { \"id\": \"did:dad:WA27fThWoNZsa88VrTkep6H-4HA8tr54sHON1vWl6FE=/beta/10058\", \"sig\": j78j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== }, ] }\\r\\n\\r\\n dy3j9aKHgz99f0K8pSkMnyqwvEr_3rpS_z2034L99sTWrMIIJGQPbVuIJ1cupo6cfIf_KCB5ecVRYoFRzAPnAQ== This can be extended to any arbitrarily complicated tree of processing transformations as shown in the following diagram. Because each DAD embeds a DID and is signed by the associated private key belonging to the DID, provenance of both the controller of the transformation step and the integrity of the associated data can be determined. Because each subsequent DAD in a processing flow or DAD chain contains the DID and signature of the prior DAD or DADs, full traceability can be established back to the originating DAD or DADs, preserving both data integrity and proof of control. This then is a critically enabling capability for decentralized computing infrastructure. A few open source projects are providing support of the key management needed for building applications with DIDs and DADs. Notable are the Didery Service [[10]] with web [[11]] and command line [[12]] clients and the SeedQuest [[13]] key-recovery application.","title":"Chaining DADs"},{"location":"RWoT7/A_DID_for_everything/#use-cases","text":"","title":"Use Cases"},{"location":"RWoT7/A_DID_for_everything/#general","text":"When working with people, the proposed system could use proximity verification, implemented with software toolkits such as Google Nearby and p2pkit, to bind virtual identities to real people. This could be useful in many situations. For example, if two people met over Craigslist to sell a used bicycle, then the system could verify that the two parties actually met and transacted. If two people found each other through a dating site, and followed up with a face to face meeting, then the system could verify that these people were actually in the stated city and not catfishers from Ukraine or Nigeria. Similarly, transactional events could build trust in a similar manner for machines. For example, a certified mechanic performing a tune up on a car with a digital twin can also provide an attestation for the DID for that automobile. A complete history of the automobile's maintenance would add value for its owner. It should be noted that a single instance of meeting is not as trustable as an entire history of meeting many people. For a state actor generating a legend for a sockpuppet, this would entail an unattainable level of work to prove personhood. For a regular human being, it's relatively effortless to use the system in an organic and unobtrusive manner. Furthermore, these histories of meetings and verifications \u2014 using location data to prove it was not in Nigeria, and time data to prove it wasn't in the middle of the night \u2014 would be aggregated to increase the trustability of the personhood assessment. Every resource that is reference by a URL could instead be referenced by a DID or dDID. IoT: Data Stream: DID for Data Stream / DID for individual packaging / DID for Data Transformation (Signature of on old data generator + Signature of transformer) \u2192 Storing Transformations in a \"Blockchain\" DID for API Endpoints: Discovery / Authentication / Transformation. DID as UUID replacement: Unique Identifier + Properties of Self-Certification DID as Resource Identifier Replacements: Derived DIDs Devices: Pacemaker Software Oracles: Stock prices from an exchange Chatbots: Automated issue-management system Robotic Controllers: Car windscreen-wiper motors Locations: The place where two people met Software applications: Online gaming platform Sensors: Thermostat measuring the ambient temperature in a home Messaging: Peer-to-peer chat application Machines: Wind turbine generating clean energy Digital Media: Video recording of a news report Store of value: Bank account Software containers Microprocessors: Computational cloud server Validator nodes: A blockchain network Databases: Personal Healthcare Record Events [Future] Biological molecule: CRISPR gene therapy An array or matrix of other DIDs","title":"General"},{"location":"RWoT7/A_DID_for_everything/#use-case-medical-imaging-workflow","text":"Image processing is operational in many scientific and engineering disciplines. These include: Image processing methods are used to count, measure, inspect, and analyse objects, coded information, and/or visual patterns. X-ray and ultrasonic devices deliver medical images with image processing that the doctor or medical algorithm can interpret more easily. X-ray machines in security zones automatically inspect luggage and clothing for dangerous objects. Visual image processing for quality assurance of objects in manufacturing systems. Radar and visual images for earth-observation applications. Image processing can include multiple data sources, parties, algorithms, and processing steps. An image processing end-user needs to be able to validate trustability and accuracy of an image data chain output data. This requirements becomes of critical importance when the output data is used in safety or security-relevant use cases or to make economic decision with significant commercial values involved. The cryptographic DAD data structures enable a user to validate the provenance of an image processing data chain including the authenticity and integrity of the input data and the provenance of the processing algorithms. We recommend the creation of DAD verifiable data chains for image processing that provide a DID for every image and data processing output data set. These data chains are then integrated with DID registries and/or digital twins that provide information about organisations, imaging devices, external data sources, and software algorithms involved in the data chain. This approach is of particular value when validation or benchmarking data are available about the image devices and the algorithms that are processing the images. In combination with a reputation or validation system any user can calculate trustability and accuracy metrics about the output data. In context of the medical image processing use case, the digital twins of the imaging device include verifiable claims issued by the OEM about the authenticity, accuracy, and calibration of the camera as well as benchmarking information about the accuracy of the machine learning algorithms. The following diagram provides a notional example of a DAD-chain for image processing data provenance.","title":"Use Case: Medical Imaging Workflow"},{"location":"RWoT7/A_DID_for_everything/#use-case-proof-of-personhood","text":"The root use case is to help us identify and manage our identity interactions with other people. But to do so more effectively, we need to know that an identifier is accurately associated with an actual person. The fraudulent misuse of identity has a significant negative impact on society. At an individual level, fraudulent users could cheat others in commercial transaction, become catfishers on dating sites, or worse. At a societal level, the fraudulent misuse of identity through bots and sockpuppets have caused havoc in elections and through the manipulation of public opinion through weaponized propaganda. The highest purpose of the blockchain is as a kind of \"truth machine\". For decentralized identity to succeed, and not be co-opted as just another way to empower sockpuppets and botnets, there needs to be an equivalent to proof-of-work, a mechanism that binds physical entities to virtual identities in a way that enables accountability while preserving anonymity. This is now being referred to as \"proof of personhood\". One use case would be to use proximity verification, implemented with software toolkits such as Google Nearby and p2pkit, to bind virtual identities to real people, in a way that preserves privacy, non-correlation, zero-knowledge proofs, and pseudonymous operations. We're currently building a technology called Not_a_Bot, which provides proof of personhood through a variety of techniques. One technique is to verify that the user has actually met another actual person, in physical space\u2026 and is not a catfisher, not a scammer, and \"not a bot\". It should be noted that a single instance of meeting is not as trustable as an entire history of meeting many people. For a state actor generating a legend for a sockpuppet, this would entail an unattainable level of work to prove personhood. For a regular human being, it's relatively effortless to use the system in an organic and unobtrusive manner. Once a root personhood verification could be insured, then trustable pseudonyms could be generated. Adding this verification to DIDs would provide trust in a trustless environment, as the DID could then provide identity and credentialing services in environments that support, or even require, pseudonymity.","title":"Use Case: Proof of Personhood"},{"location":"RWoT7/A_DID_for_everything/#decentralized-fog-computing-infrastructure","text":"The coming tidal wave of digital data due to the proliferation of digital devices will require an exponential increase in computing capacity for data integration and analysis. Currently data integration and analysis is handled predominantly in the cloud. This is not efficient as it requires transport of data from the edge of the internet where it is created to remote data centers that are the cloud for processing and then transport back to the edge of the results of processing. In many cases the data processing can be performed in a hierarchical-tree-like bottom-up fashion which is more cost effectively done in the edge close to the data. Likewise new applications benefit from low latency processing of data near the sources and sinks of the data. These applications also benefit from processing the data in the edge. Currently there is little capacity to perform processing in the edge. Consequently an opportunity exists to build a public decentralized edge computing infrastructure. Edge computing infrastructure is called the fog . A public decentralized fog computing infrastructure provides an opportunity to scale capacity using two sided network effects where a large number of entities can cooperatively participate in the two-sided network both as consumers and as importantly producers of compute. This is in contrast to the highly centralized nature of cloud computing where a handful of producers control most of the public computing infrastructure. This stifles innovation and raises costs. A public, decentralized, fog computing infrastructure could enable the monetization of pre-existing spare capacity in the form of bandwidth, space, energy, and compute devices. This would provide lower costs and incentivize innovation.","title":"Decentralized Fog Computing Infrastructure"},{"location":"RWoT7/A_DID_for_everything/#future-direction","text":"Data flows can be provenanced by verifying the end-to-end integrity of data with DIDs. By enabling DIDs to sign claims about other DIDs, the fidelity of these data flows can be increased further. There are several ideas that are good candidates for future exploration. These are as follows: Examine how a DID can utilize verifiable credentials to prove verified aspects of their identity when signing claims about other DIDs. Such a use case could enable verified inspectors to sign a claim with their DID that they have serviced an IoT sensor and certify what software and hardware upgrades the sensor is using. Examine how verifiable claims and credentials can be issued for location verification. Examine the social/network interactions between DIDs that sign attestations about other DIDs. Examine how pairwise-unique DIDs and zero-knowledge proofs could empower users to make contentious counterfactual claims in a privacy respecting manner. Examine how the system could help to drive non-correlation functionality. Open discussion on other issues such as cognitive models, optimization and AI models, and the potential use of tokenization to drive behavioral economics. Examine how attestations or other types of claims on a DID can build an attribution graph that increases the value of the credentials associated with a DID.","title":"Future Direction"},{"location":"RWoT7/A_DID_for_everything/#conclusion","text":"Imagine a world where this proposed technology has been deployed and globally adopted. Let us paint a picture for how this might be achieved. Imagine that this approach becomes part of a decentralized identity solution for every entity, driven by a robust and active developer community. The vision is to generate technologies that would be integrated into applications that are used in IoT, e-commerce, social interaction, banking, healthcare, and so on. Now imagine that mobile telephony companies agree to embed the technology into the operating systems for all smartphones, and the dominant social network providers agree to use DIDs and DADs and proofs about the entities controlling these DIDs and DADs in their algorithms for determining which content to propel. This would mean the end of phishing. The end of fake news. This is the beginning of new era for society, built on an interconnecting web of trust: a world in which we know what impacts we are having. The emergent property of this new data fabric is Knowing. This is greatly needed as trust in media is at an all-time low, and centralized, algorithmic distribution have created a perfect storm for the rise of misinformation, disinformation, and fake news. This is driving polarization while simultaneously undermining public trust in institutions. However, realistically, most of society's greatest challenges have no silver-bullet solution. Consider the problem of using sock puppets to weaponize propaganda. Proximity verification is one component of a multi-pronged solution that might help mitigate the problem. Consider that certain highly problematic diseases can be treated with drug combinations consisting of antiretroviral compounds mixed with transcriptase inhibitors and steroids. The combinations are called \"cocktails,\" and they're so effective that they're called the \"Lazarus Effect,\" named for the biblical figure who was raised from the dead. Cocktails can turn an HIV death sentence into a manageable chronic condition. Just as complex and evolving health challenges must be addressed with complex and evolving multi-pronged solutions, the complex challenges of online identity require a comprehensive and systematic approach using multi-pronged solutions that synergistically combine to enable disruption, change and transformation at multiple levels. This paper aims help to determine what other solutions would need to be integrated, to create a \"cocktail prescription\" to address this problem. Automating the detection of misinformation is only half the problem. Preventing the weaponization of that propaganda is the other half, and this proposed technique could help provide at least part of a comprehensive cocktail prescription to address the issue of fake news The Internet's current capacity to support democratic societies in making well-informed decisions is being subverted by globally networked state actors. However, there are additional benefits for this technology in computing, social networking, connected governmental services, and e-commerce \u2014 where the use of sockpuppets is more of an aggravation than a grave danger. For example, in terms of government service, we envision a system where elected officials could verify how many people they actually meet and how much time was spent with them, to back up claims of being a \"man of the people\". For fully transparent politics, this system should could provide the electorate with an accurate sense of whether a politician has actually met with leaders of social movements or is spending the majority of time with donors, lobbyists, and political action committees. Underlying the benefits of decentralized identity outlined above is the need for open interoperable standards to ensure the reputable provenance of the associated data flows between decentralized entities. This paper describes a novel concept for provenancing data flows using DADis (Decentralized Autonomic Data items) that are built upon the emerging DID standard. This approach uses and extends the advanced diffuse-trust or zero-trust computing paradigm that is needed to operate securely in a world of decentralized data.","title":"Conclusion"},{"location":"RWoT7/A_DID_for_everything/#authors","text":"In alphabetical order. Shaun Conway shaun@ixo.world Andrew Hughes andrewhughes3000@gmail.com Moses Ma moses.ma@futurelabconsulting.com Jack Poole jack.w.poole@gmail.com Martin Riedel martin@civic.com Samuel M. Smith Ph.D. sam@samuelsmith.org Carsten St\u00f6cker carsten.stoecker@interlinked.ai","title":"Authors"},{"location":"RWoT7/A_DID_for_everything/#references","text":"1 . https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2018/blob/master/final-documents/DecentralizedAutonomicData.pdf 2 . https://github.com/WebOfTrustInfo/rwot7/blob/master/topics-and-advance-readings/ZeroTrustComputingWithDidsAndDads.md 3 . https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2017/blob/master/topics-and-advance-readings/did-primer.md 4 . https://www.statista.com/statistics/471264/iot-number-of-connected-devices-worldwide/ 5 . https://www.gartner.com/smarterwithgartner/gartner-predicts-a-virtual-world-of-exponential-change/ 6 . Redemocratizing Permissionless Cryptocurrencies, by Maria Borge, Eleftherios Kokoris-Kogias, Philipp Jovanovic, Linus Gasser, Nicolas Gailly, Bryan Ford. 2017 IEEE European Symposium on Security and Privacy Workshops EuroS&PW, April 2017 7 . https://www.nist.gov/sites/default/files/documents/2017/06/05/040813_forrester_research.pdf 8 . https://www.amazon.com/Zero-Trust-Networks-Building-Untrusted/dp/1491962194 9 . https://github.com/SmithSamuelM/Papers/blob/master/whitepapers/ManyCubed.pdf 10 . https://github.com/reputage/didery 11 . https://github.com/reputage/didery.js 12 . https://github.com/reputage/didery.py 13 . https://github.com/reputage/seedQuest 14 . https://w3c-ccg.github.io/did-spec/ 15 . https://www.w3.org/2017/vc/WG/ 16 . https://identity.foundation 17 . https://en.wikipedia.org/wiki/Possession_is_nine-tenths_of_the_law 18 . https://www.ietf.org/rfc/rfc3986.txt","title":"References"},{"location":"RWoT7/btcr_0_1/","text":"BTCR v0.1 Decisions Authors: Kim Hamilton Duffy, Christopher Allen, Dan Pape Contributors: Ryan Grant, Anthony Ronning, Ganesh Annan, Wolf McNally Abstract The Bitcoin Reference (BTCR) DID method supports DIDs using the Bitcoin blockchain. This method has been under development through Rebooting Web of Trust events and hackathons over the past year. The BTCR method's reliance on the Bitcoin blockchain presents both advantages and design challenges. During RWOT7, the authors made a number of design and implementation decisions -- largely scope-cutting in nature -- in order to lock down a Minimum Viable Product (MVP) version, which we'll refer to as v0.1. This paper documents those decisions, which will apply to the upcoming v0.1 BTCR method specification and associated v0.1 BTCR reference implementation. Overview The design decisions include: - What's in and out of scope - BTCR semantics - Wallet MVP requirements and functionality - Credential schema and content Scope Clarifications To reduce design and implementation complexity, we decided on some reductions for a 0.1 release of BTCR. A variety of factors influenced these decisions, including lack of library support for our MVP scenarios and our desire for simplicity. Later versions of BTCR will address more advanced scenarios, including cost reduction, improved library security, and, most significantly, support for the mainnet chain. 1. Assume P2PKH scripts Our initial BTCR prototypes relied on Pay-to-Public-Key-Hash (P2PKH) scripts. We explored other script types, but these introduced complications for extracting the signing public key directly from the transaction, which we require for the final DID document. For this reason, we decided that only P2PKH scripts are supported, and other scripts types -- for example P2SH and Segwit -- are out of scope. Because of the size and cost implications, we will revisit this decision in later versions. 2. Only an HTTP URL in OP_RETURN (Note: no IPFS support) Background/Context A BTCR transaction allows an optional OP_RETURN field pointing to a \"continuation\" DID document, which is a DID document containing additional key material and capabilities to be merged into the final BTCR DID Document (generated by the resolver). The storage type of the continuation document introduced a fork during DID document resolution: if the link in the transaction pointed to mutable storage, the document could be updated after the transaction with the known DID (which isn't known until after the tx has been confirmed). However, if the link was a cryptographic hash link, then the document could not be changed without invalidating the hash. Our original documentation described two paths to address these scenarios: - If the document is in an immutable store, we consider the transaction signature an implicit signature on the immutable content. - Otherwise, require a signature on the continuation DID document. However, this introduces a requirement for the resolvers to be aware of different link types (which are content hashes or not), which we've tracked here . v0.1 Scope Reduction: no special path for immutable continuation DID documents The BTCR team strongly prefers use of immutable storage for its simplicity; however, we considered the burden of ensuring availability of IPFS objects prohibitive for evaluting BTCR DIDs in end-to-end scenarios. This would require the user to run an IPFS node (or have their objects pinned on one), and this isn't feasible on a mobile device. For example, on an iPhone (our MVP target device) the IPFS lib on iPhone has to be running all the time, and can\u2019t be running in the background. For these reasons, we decided to wait for greater support for IPFS on iPhone or to use something like filecoin. In v0.1 we will assume that, if the OP_RETURN is present, it points to an HTTP URL. We also assume that the target content could have been altered. This allows us to cut scope and address the case of mutable storage only. 3. Txrefs and Txref-ext The finalization of the txref spec (BIP-0136) is currently still in progress, but we have decided that BTCR DIDs will use the extended form of the txref, which encodes the TXO index within the transaction, as well as the block height and TX index of the transaction itself. See here for details. 4. Continuation DID Document in github (if the tx has an OP_RETURN) Extending on the previous decision, we've decided in v0.1 to store continuation DID Documents in github. The continuation DID Document must be updated after tx confirmation to explicity list fields we formerly derived as part of the implicit DID document. Specifically, after tx confirmation, the user must specify: 1. the resulting DID in relevant fields such as id and creator . Note this must be done after confirmation because the DID will be known only after the BTCR tx is confirmed. 2. a signature on the updated DID Document from the tx signing key The decision to store it in github specifically reduces the target audience, since it requires a github account. However, we were already assuming a dev audience for v0.1. One advantage is of this restriction is the ability to sign DID Document updates with a PGP key (via github signed commits). The flow would look like this: Create BTCR DID Create the full BTCR DID document in github, including the PGP key in this document Note the DID document itself is signed with BTCR transaction signing key, as usual Commit to github using PGP key from (2). 5. Testnet only Perhaps most notably, we've decided to only support testnet (not mainnet) as we work through the initial reference implementations and obtain feedback from test usage. Semantics A BTCR DID document relies partially on transaction structure, partially on the continuation DID document. We clarified the semantics of scenarios that were previously undefined. 1. Transaction input/output semantics a. TX Input addresses In v0.1, the keypair corresponding to the first TX input has the following properties: - it is the only key that can be used to verify control of the DID and continuation DID document - it must be used to sign the updated continuation DID document (after TX confirmation) If an OP_RETURN doesn't exist, the keypair corresponding to the first TX input is granted the following additional capabilities (by the resolver): - DID auth - Sign/verify Verifiable Credentials If the OP_RETURN exists, the continuation DID document obviates the need for additional implicit capabilities; we assume each capability is listed explicitly in the continuation DID document. b. TX Output address (1st monetary output) The output address is used for \"following the tip\". If spent, the BTCR DID has either been rotated or revoked. Resolvers must follow the tip to find the latest transaction. Determining whether the DID has been rotated or revoked is described in the next section. 2. No OP_RETURN after a TX means revoked A BTCR DID is considered revoked if: - The latest transaction has no OP_RETURN, AND - There is more than one transaction in the BTCR DID \"chain\" The first factor is important because a missing OP_RETURN is considered valid in the very first TX in the chain. However, all subsequent TXs in the chain must have OP_RETURNs, or else it is considered revoked. This behavior must be enforced by BTCR resolvers. Wallet requirements and functionality We will release a v0.1 BTCR wallet as an iPhone app. We wanted to include sufficient features to demonstrate core BTCR features (as described in the rest of this paper). While defining requirements, we included some usability features and also realized library limitations affecting our implementation choices. 1. Import previous Bitcoin transactions The v0.1 BTCR wallet implementation will support importing existing private keys corresponding to unspent outputs from previous transactions. This allows users to generate a valid BTCR DID from an existing transaction, with no additional cost. A consequence is that a transaction doesn't need to be created or broadcast (from within the ID wallet) to instantiate this initial BTCR DID. The BTCR wallet only needs to create transactions for any subsequent update/revoke operations. In this case, the BTCR DID will have no continuation DID Document. 2. Updates must use non-financial HD derivation paths We want to allow users to share mnemonic seeds across their ID wallet and their Bitcoin wallets. To achieve this, we must make sure addresses backing valid DIDs are not accidentally spent, which would result in a BTCR DID revocation. We will achieve this by choosing our own convention about the derivation path. 3. Tip following Many current BTCR prototypes (such as the BTCR playground) use Bitcoin APIs to look up transactions. This introduces many concerns from security to availability (rate-limiting). We want our MVP deployment to require minimal resource overhead. To achieve this, our preference is to follow the best practice described in BIP 157/158 (aka Neutrino), which helps preserve the privacy of your DID-related addresses (over SPV). Note that Neutrino is not as efficient as SPV. SPV monitors only the addresses it cares about. In contrast, Neutrino introduces noise in attempt to hide the specific addresses it actually cares about. The problem we encountered is that library support for Neutrino is not readily available. As a fallback, we will use REST services backed by a bitcoin node (initially maintained by the BTCR team). 4. Pre-revocation The v0.1 BTCR wallet may support pre-revocation in cases of emergency. This works by pre-signing a transaction spending the tip, which can be stored away and broadcast later to revoke the DID even without the key material. Credential schema and content This section applies to credential schema and content, which is relevant when demonstrating DID use cases. Consistent with the previous decisions, we urge people to use common schemas (for ease of interop) and not to issue high-stakes claims (for privacy/security). 1. Use JSON-LD 1.1 javascript lib, because 0.1 doesn't need high-stakes verifiers As many of the decisions above imply, BTCR v0.1 is not recommended to be used for anything other than experimentation. In a future production-ready version, we will use more secure libraries (and generally avoid reliance on javascript, as we've done for prototyping). This implies that where JSON-LD libraries are required in BTCR v0.1, we may use the JSON-LD 1.1 javascript library. In future versions, we will require library support in a different language. 2. Restrict to schema.org schemas, like person. We'll restrict our prototypes to schema.org schemas as opposed to custom schemas. We've developed a suite of test cases that can be used. 3. Privacy: stick to pseudoanonymous claim content Users should not include any PII in claims used for BTCR v0.1 prototypes 4. Restrict to simple VCs one wishes to share Similar to above, only include claim content that you want others to see. This phase is not equipped to handle high-stakes scenarios. Example { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l\", \"publicKey\": [ { \"id\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l#keys-1\", \"owner\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l\", \"type\": \"EdDsaSAPublicKeySecp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" }, { \"id\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l#keys-2\", \"type\": \"RsaVerificationKey2018\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\", \"owner\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l\" } ], \"authentication\": [ { \"type\": \"EdDsaSAPublicKeySecp256k1Authentication\", \"publicKey\": \"#keys-1\" }, { \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"#keys-2\" } ], \"service\": [ { \"type\": \"BTCREndpoint\", \"serviceEndpoint\": \"https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld\" } ], \"SatoshiAuditTrail\": [ { \"chain\": \"testnet\", \"blockhash\": \"0000000000000722ded9d85d67e145ba41c53ef2e8680f75540a08b885febba5\", \"blockindex\": 2, \"outputindex\": 1, \"blocktime\": \"2017-09-23T17:27:56.682Z\", \"time\": 1499501000, \"timereceived\": \"2017-09-23T17:27:56.682Z\", \"burn-fee\": -0.05 } ], \"claims\": [ ] }","title":"BTCR v0.1 Decisions"},{"location":"RWoT7/btcr_0_1/#btcr-v01-decisions","text":"Authors: Kim Hamilton Duffy, Christopher Allen, Dan Pape Contributors: Ryan Grant, Anthony Ronning, Ganesh Annan, Wolf McNally","title":"BTCR v0.1 Decisions"},{"location":"RWoT7/btcr_0_1/#abstract","text":"The Bitcoin Reference (BTCR) DID method supports DIDs using the Bitcoin blockchain. This method has been under development through Rebooting Web of Trust events and hackathons over the past year. The BTCR method's reliance on the Bitcoin blockchain presents both advantages and design challenges. During RWOT7, the authors made a number of design and implementation decisions -- largely scope-cutting in nature -- in order to lock down a Minimum Viable Product (MVP) version, which we'll refer to as v0.1. This paper documents those decisions, which will apply to the upcoming v0.1 BTCR method specification and associated v0.1 BTCR reference implementation.","title":"Abstract"},{"location":"RWoT7/btcr_0_1/#overview","text":"The design decisions include: - What's in and out of scope - BTCR semantics - Wallet MVP requirements and functionality - Credential schema and content","title":"Overview"},{"location":"RWoT7/btcr_0_1/#scope-clarifications","text":"To reduce design and implementation complexity, we decided on some reductions for a 0.1 release of BTCR. A variety of factors influenced these decisions, including lack of library support for our MVP scenarios and our desire for simplicity. Later versions of BTCR will address more advanced scenarios, including cost reduction, improved library security, and, most significantly, support for the mainnet chain.","title":"Scope Clarifications"},{"location":"RWoT7/btcr_0_1/#1-assume-p2pkh-scripts","text":"Our initial BTCR prototypes relied on Pay-to-Public-Key-Hash (P2PKH) scripts. We explored other script types, but these introduced complications for extracting the signing public key directly from the transaction, which we require for the final DID document. For this reason, we decided that only P2PKH scripts are supported, and other scripts types -- for example P2SH and Segwit -- are out of scope. Because of the size and cost implications, we will revisit this decision in later versions.","title":"1. Assume P2PKH scripts"},{"location":"RWoT7/btcr_0_1/#2-only-an-http-url-in-op_return-note-no-ipfs-support","text":"","title":"2. Only an HTTP URL in OP_RETURN (Note: no IPFS support)"},{"location":"RWoT7/btcr_0_1/#backgroundcontext","text":"A BTCR transaction allows an optional OP_RETURN field pointing to a \"continuation\" DID document, which is a DID document containing additional key material and capabilities to be merged into the final BTCR DID Document (generated by the resolver). The storage type of the continuation document introduced a fork during DID document resolution: if the link in the transaction pointed to mutable storage, the document could be updated after the transaction with the known DID (which isn't known until after the tx has been confirmed). However, if the link was a cryptographic hash link, then the document could not be changed without invalidating the hash. Our original documentation described two paths to address these scenarios: - If the document is in an immutable store, we consider the transaction signature an implicit signature on the immutable content. - Otherwise, require a signature on the continuation DID document. However, this introduces a requirement for the resolvers to be aware of different link types (which are content hashes or not), which we've tracked here .","title":"Background/Context"},{"location":"RWoT7/btcr_0_1/#v01-scope-reduction-no-special-path-for-immutable-continuation-did-documents","text":"The BTCR team strongly prefers use of immutable storage for its simplicity; however, we considered the burden of ensuring availability of IPFS objects prohibitive for evaluting BTCR DIDs in end-to-end scenarios. This would require the user to run an IPFS node (or have their objects pinned on one), and this isn't feasible on a mobile device. For example, on an iPhone (our MVP target device) the IPFS lib on iPhone has to be running all the time, and can\u2019t be running in the background. For these reasons, we decided to wait for greater support for IPFS on iPhone or to use something like filecoin. In v0.1 we will assume that, if the OP_RETURN is present, it points to an HTTP URL. We also assume that the target content could have been altered. This allows us to cut scope and address the case of mutable storage only.","title":"v0.1 Scope Reduction: no special path for immutable continuation DID documents"},{"location":"RWoT7/btcr_0_1/#3-txrefs-and-txref-ext","text":"The finalization of the txref spec (BIP-0136) is currently still in progress, but we have decided that BTCR DIDs will use the extended form of the txref, which encodes the TXO index within the transaction, as well as the block height and TX index of the transaction itself. See here for details.","title":"3. Txrefs and Txref-ext"},{"location":"RWoT7/btcr_0_1/#4-continuation-did-document-in-github-if-the-tx-has-an-op_return","text":"Extending on the previous decision, we've decided in v0.1 to store continuation DID Documents in github. The continuation DID Document must be updated after tx confirmation to explicity list fields we formerly derived as part of the implicit DID document. Specifically, after tx confirmation, the user must specify: 1. the resulting DID in relevant fields such as id and creator . Note this must be done after confirmation because the DID will be known only after the BTCR tx is confirmed. 2. a signature on the updated DID Document from the tx signing key The decision to store it in github specifically reduces the target audience, since it requires a github account. However, we were already assuming a dev audience for v0.1. One advantage is of this restriction is the ability to sign DID Document updates with a PGP key (via github signed commits). The flow would look like this: Create BTCR DID Create the full BTCR DID document in github, including the PGP key in this document Note the DID document itself is signed with BTCR transaction signing key, as usual Commit to github using PGP key from (2).","title":"4. Continuation DID Document in github (if the tx has an OP_RETURN)"},{"location":"RWoT7/btcr_0_1/#5-testnet-only","text":"Perhaps most notably, we've decided to only support testnet (not mainnet) as we work through the initial reference implementations and obtain feedback from test usage.","title":"5. Testnet only"},{"location":"RWoT7/btcr_0_1/#semantics","text":"A BTCR DID document relies partially on transaction structure, partially on the continuation DID document. We clarified the semantics of scenarios that were previously undefined.","title":"Semantics"},{"location":"RWoT7/btcr_0_1/#1-transaction-inputoutput-semantics","text":"","title":"1. Transaction input/output semantics"},{"location":"RWoT7/btcr_0_1/#a-tx-input-addresses","text":"In v0.1, the keypair corresponding to the first TX input has the following properties: - it is the only key that can be used to verify control of the DID and continuation DID document - it must be used to sign the updated continuation DID document (after TX confirmation) If an OP_RETURN doesn't exist, the keypair corresponding to the first TX input is granted the following additional capabilities (by the resolver): - DID auth - Sign/verify Verifiable Credentials If the OP_RETURN exists, the continuation DID document obviates the need for additional implicit capabilities; we assume each capability is listed explicitly in the continuation DID document.","title":"a. TX Input addresses"},{"location":"RWoT7/btcr_0_1/#b-tx-output-address-1st-monetary-output","text":"The output address is used for \"following the tip\". If spent, the BTCR DID has either been rotated or revoked. Resolvers must follow the tip to find the latest transaction. Determining whether the DID has been rotated or revoked is described in the next section.","title":"b. TX Output address (1st monetary output)"},{"location":"RWoT7/btcr_0_1/#2-no-op_return-after-a-tx-means-revoked","text":"A BTCR DID is considered revoked if: - The latest transaction has no OP_RETURN, AND - There is more than one transaction in the BTCR DID \"chain\" The first factor is important because a missing OP_RETURN is considered valid in the very first TX in the chain. However, all subsequent TXs in the chain must have OP_RETURNs, or else it is considered revoked. This behavior must be enforced by BTCR resolvers.","title":"2. No OP_RETURN after a TX means revoked"},{"location":"RWoT7/btcr_0_1/#wallet-requirements-and-functionality","text":"We will release a v0.1 BTCR wallet as an iPhone app. We wanted to include sufficient features to demonstrate core BTCR features (as described in the rest of this paper). While defining requirements, we included some usability features and also realized library limitations affecting our implementation choices.","title":"Wallet requirements and functionality"},{"location":"RWoT7/btcr_0_1/#1-import-previous-bitcoin-transactions","text":"The v0.1 BTCR wallet implementation will support importing existing private keys corresponding to unspent outputs from previous transactions. This allows users to generate a valid BTCR DID from an existing transaction, with no additional cost. A consequence is that a transaction doesn't need to be created or broadcast (from within the ID wallet) to instantiate this initial BTCR DID. The BTCR wallet only needs to create transactions for any subsequent update/revoke operations. In this case, the BTCR DID will have no continuation DID Document.","title":"1. Import previous Bitcoin transactions"},{"location":"RWoT7/btcr_0_1/#2-updates-must-use-non-financial-hd-derivation-paths","text":"We want to allow users to share mnemonic seeds across their ID wallet and their Bitcoin wallets. To achieve this, we must make sure addresses backing valid DIDs are not accidentally spent, which would result in a BTCR DID revocation. We will achieve this by choosing our own convention about the derivation path.","title":"2. Updates must use non-financial HD derivation paths"},{"location":"RWoT7/btcr_0_1/#3-tip-following","text":"Many current BTCR prototypes (such as the BTCR playground) use Bitcoin APIs to look up transactions. This introduces many concerns from security to availability (rate-limiting). We want our MVP deployment to require minimal resource overhead. To achieve this, our preference is to follow the best practice described in BIP 157/158 (aka Neutrino), which helps preserve the privacy of your DID-related addresses (over SPV). Note that Neutrino is not as efficient as SPV. SPV monitors only the addresses it cares about. In contrast, Neutrino introduces noise in attempt to hide the specific addresses it actually cares about. The problem we encountered is that library support for Neutrino is not readily available. As a fallback, we will use REST services backed by a bitcoin node (initially maintained by the BTCR team).","title":"3. Tip following"},{"location":"RWoT7/btcr_0_1/#4-pre-revocation","text":"The v0.1 BTCR wallet may support pre-revocation in cases of emergency. This works by pre-signing a transaction spending the tip, which can be stored away and broadcast later to revoke the DID even without the key material.","title":"4. Pre-revocation"},{"location":"RWoT7/btcr_0_1/#credential-schema-and-content","text":"This section applies to credential schema and content, which is relevant when demonstrating DID use cases. Consistent with the previous decisions, we urge people to use common schemas (for ease of interop) and not to issue high-stakes claims (for privacy/security).","title":"Credential schema and content"},{"location":"RWoT7/btcr_0_1/#1-use-json-ld-11-javascript-lib-because-01-doesnt-need-high-stakes-verifiers","text":"As many of the decisions above imply, BTCR v0.1 is not recommended to be used for anything other than experimentation. In a future production-ready version, we will use more secure libraries (and generally avoid reliance on javascript, as we've done for prototyping). This implies that where JSON-LD libraries are required in BTCR v0.1, we may use the JSON-LD 1.1 javascript library. In future versions, we will require library support in a different language.","title":"1. Use JSON-LD 1.1 javascript lib, because 0.1 doesn't need high-stakes verifiers"},{"location":"RWoT7/btcr_0_1/#2-restrict-to-schemaorg-schemas-like-person","text":"We'll restrict our prototypes to schema.org schemas as opposed to custom schemas. We've developed a suite of test cases that can be used.","title":"2. Restrict to schema.org schemas, like person."},{"location":"RWoT7/btcr_0_1/#3-privacy-stick-to-pseudoanonymous-claim-content","text":"Users should not include any PII in claims used for BTCR v0.1 prototypes","title":"3. Privacy: stick to pseudoanonymous claim content"},{"location":"RWoT7/btcr_0_1/#4-restrict-to-simple-vcs-one-wishes-to-share","text":"Similar to above, only include claim content that you want others to see. This phase is not equipped to handle high-stakes scenarios.","title":"4. Restrict to simple VCs one wishes to share"},{"location":"RWoT7/btcr_0_1/#example","text":"{ \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l\", \"publicKey\": [ { \"id\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l#keys-1\", \"owner\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l\", \"type\": \"EdDsaSAPublicKeySecp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" }, { \"id\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l#keys-2\", \"type\": \"RsaVerificationKey2018\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\", \"owner\": \"did:btcr:8kyt-fzzq-qpqq-ljsc-5l\" } ], \"authentication\": [ { \"type\": \"EdDsaSAPublicKeySecp256k1Authentication\", \"publicKey\": \"#keys-1\" }, { \"type\": \"RsaSignatureAuthentication2018\", \"publicKey\": \"#keys-2\" } ], \"service\": [ { \"type\": \"BTCREndpoint\", \"serviceEndpoint\": \"https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld\" } ], \"SatoshiAuditTrail\": [ { \"chain\": \"testnet\", \"blockhash\": \"0000000000000722ded9d85d67e145ba41c53ef2e8680f75540a08b885febba5\", \"blockindex\": 2, \"outputindex\": 1, \"blocktime\": \"2017-09-23T17:27:56.682Z\", \"time\": 1499501000, \"timereceived\": \"2017-09-23T17:27:56.682Z\", \"burn-fee\": -0.05 } ], \"claims\": [ ] }","title":"Example"},{"location":"RWoT7/convincing-dad/","text":"How to Convince Dad* of the Importance of Self-Sovereign Identity * and your sister and your daughter and your best friend and your nephew by Shannon Appelcline, Kenneth Bok, Lucas Parker, Peter Scott, and Matthew Wong Abstract One of the major problems with bootstrapping self-sovereign identity is that it requires adoption by a large number of people. Pushing self-sovereign identity from the top-down is most likely to result in a technology that\u2019s not actually used, but instead encouraging the average person to demand self-sovereign identity from the bottom-up will result in the organic development of a vibrant, well-utilized decentralized web-of-trust ecosystem. This paper addresses that need by offering arguments to a variety of people who might be reluctant to use self-sovereign identity, uninterested in its possibilities, or oblivious to the dangers of centralization. By focusing on the needs of real people, we hope to also encourage developers, engineers, and software business owners to create the apps that will address their reluctance and fulfill their needs, making self-sovereign identity a reality. Introduction \"Cogito ergo sum \u2014 I think, therefore I am.\" \\~Ren\u00e9 Descartes \"Identity is a uniquely human concept; however modern society view this concept of identity as state-issued credentials such as driver\u2019s license and social security cards, which suggests a person can lose his very identity if a state revokes his credentials or even if he just crosses state borders. I think, but I am not .\" \\~ Christopher Allen The possibility of losing your identity is a serious problem in the digital world. Vloggers could lose their identities if YouTube closes their accounts, while common internet citizens like you and me could lose a big part of our life if Facebook revokes our credentials. As digital accounts become a major part of our identities, we need a paradigm that allows us to bring identity back under our control. Self-sovereign identity seeks to be that new model, creating a paradigm shift in an increasingly data-governed world. It puts the individual in control of her identity and prioritizes her privacy. It does so by acting as the root anchor for an individual\u2019s data stream, permitting an individual to manage, store, and control her own data and life. It could become a ubiquitous technology that affects the lives of billions on a daily basis. This runs counter to the traditional model for online identity, designed from the perspective of the corporation and government, with the needs of the individual being secondary. In the age of \"Surveillance Capitalism\", personal data is typically abused by large corporations in the never-ending quest for profit, disregarding user privacy and inadequately safeguarding user data. The Equifax hack in late 2017 and the Cambridge Analytica scandal highlight the risks of large, centralized databases of personal information (i.e. honeypots of data), which present high-value targets for hackers. But how do we convince the average person to move from the old, centralized model to the new, self-sovereign model? We think that this requires proactively fulfilling their needs. To highlight these needs, we\u2019ve created case studies for five people who could be served by self-sovereign identity, if they were only aware of its possibilities. They are: Your dad, who is preparing for retirement; Your sister, who is a world traveller; Your daughter, who is a reckless social media user; Your best friend, who is a content creator; and Your nephew, who runs a convenience store. These five people represent a spectrum of use-cases and applications, meant to portray some of the monumental possibilities of self-sovereign identity in the not-so-distant future. They are concentrated in the developed world, because we believe that is where adoption of self-sovereign identity will begin. Some of these use cases are possible now, while others will require a more fully fleshed-out web-of-trust ecosystem. Your Dad is Preparing for Retirement Dad is getting ready to retire, which has him thinking more about his financial security. So much is online now! He has to use his laptop computer to pay some of his bills from his bank account, and he looks at his retirement accounts through his browser too. He has a few different logins and passwords, because the different institutions have different requirements. He keeps them written down on a yellow post-it that he hides in his desk drawer. Recently, Dad has become paranoid about having his money stolen because his best friend got phished by someone claiming that their Microsoft Windows installation needed updating, which let the hackers install a keylogger on their computer and steal some money. Though Dad finally traded in his flip phone for a smartphone last year, he doesn\u2019t use it for anything but reading news stories and text messages. \u201cI hate having to log into so many financial accounts.\u201d The Problem. Dad gets annoyed at all these confusing accounts and logins and passwords. He\u2019d like to have a single account that accesses all of his financial services and he\u2019d prefer not to need a login and password for it at all. The Solution. Using his self-sovereign identity, Dad can federate logins to various financial services. He accesses it using biometrics: he just looks at his smartphone\u2019s camera, it scans his face, and then the smartphone verifies his identity to his laptop. This gives him access to all of his federated financial accounts without needing to type logins or passwords. An app on his laptop consolidates all of the information from the accounts and allows him to view his finances, write checks, and free up retirement money. \u201cI\u2019m afraid someone will steal my money through some sort of fake login.\u201d The Problem. Dad is especially vulnerable to phishing attacks, in which someone obtains access to his financial account by pretending to represent the financial institution. He lives in constant fear of losing his nest egg: he is concerned that it is impossible to tell the difference between a legitimate representative and a scammer. The Solution. Because Dad logs in using his smartphone and face recognition, there is no way for him to accidentally log in to a fake portal. The application will take him to the correct site. \u201cIf I get my identity stolen, I\u2019m screwed.\u201d The Problem. Dad has heard a lot about the ravages of identity theft: attackers stole names, phone numbers, and addresses from Home Depot and stole reams of identity information from Equifax. Dad is afraid that the thieves could use this information to steal his money at his financial institutions or to take out credit or loans in his name. The Solution. This sort of breach would be less likely in a world of self-sovereign identity because users can safeguard their information under their self-sovereign identity, preventing it from entering large honeypots of personally identifiable information (PII). But, even if a breach were to expose Dad\u2019s information, it wouldn\u2019t affect his access to financial institutions: the PII wouldn\u2019t give access to his accounts without validation from his smartphone; if someone tried to take out a new loan using his PII, the bank or credit bureau would reach out to Dad for verification. Thus, in the world of self-sovereign identity, PII is less valuable and its use is more tightly under Dad\u2019s control. Your Sister is a World Traveller Sis works with M\u00e9decins Sans Fronti\u00e8res offering humanitarian assistance. This brings her to dozens of countries, some of which have non-western values. Sometimes she\u2019s been targeted by the authorities, which has put her in jeopardy. She also has occasionally needed to seek treatment in these countries due to the problems that a varying diet causes for her diabetes. \u201cI\u2019m afraid of losing my passport.\u201d The Problem. Sis was once forced to leave her passport behind when she had to flee a city: she was afraid to return to her lodgings due to the local authority\u2019s disagreement with MSF . This terrified her, because a passport is expected to be in a traveler\u2019s possession when they are traveling in a foreign country. This left her unable to easily leave the country, which she now felt was hostile to her presence. Replacing her passport was both time consuming and logistically challenging. She is horrified by the idea of repeating this experience. The Solution. Sis has a digital passport stored on a hardware device that acts as a data store. She keeps it attached to her keychain. She can unlock and transmit her passport data using her thumbprint. If she loses her physical passport, she has this backup. Authorities in some countries may accept it themselves, but otherwise, she can use it anywhere to establish her identity to the local embassy by logging into her government\u2019s online platform. \u201cI don\u2019t want the police looking at my passport if they stop me.\u201d The Problem. Due to her country of origin, Sis experienced harassment from the police when they were reviewing her passport. She also endured extensive interviews because of the countries she has visited. The Solution. When Sis enters a country, the border agents issue digital documents (a verifiable credential) approving her legal status in the country for a certain span of time. She adds this to her self-sovereign identity\u2019s data store. When she is stopped by the police, she uses her hardware device to selectively disclose only her name and the credential issued by the border agents. The police now know who she is and what her status is; they don\u2019t need to know about her country of origin or past travels. The border agents have already verified that information, so she doesn\u2019t need to give it to the municipal authorities. \u201cI worry that I don\u2019t have all of my health records with me.\u201d The Problem. Sis has type 1 diabetes and has to ensure her sugar levels remain stable. If she were to have a medical emergency while traveling, she needs the doctors to know the details of her condition, including her current medications and allergies, without having to carry a sheaf of documents. The Solution. Sis keeps her medical records on her hardware device. Using her self-sovereign identity, she can protect the information or share it when necessary. In addition, the encoded, digital nature of her medical records makes it easy to translate into different languages. When she visits the local doctor, it is presented in the local language, courtesy of the AI-powered translation program. Your Daughter is a Reckless Social Media User Daughter has spent her life publishing pictures, tweeting, and posting on social media services such as Facebook and Snapchat. Now that she is going for job interviews, she realizes that all of her photos of holidays and parties and all of her tweets are a bit more public than she would like. \u201cI don\u2019t want to show my employer everything.\u201d The Problem. Daughter is going for a job interview soon and has heard rumors of employers making hiring decisions based on social media profiles. She is concerned over which of her images and tweets will turn up if they access her social media profiles. The Solution. With self-sovereign identity, Daughter has granular control over who sees what in her social media feeds. When she gives access to potential employers, she gives access to feeds that selectively disclose specific tags. \u201cI\u2019ve put something online that I can\u2019t take back.\u201d The Problem. Daughter\u2019s photo blog features a picture of her in front of a popular storefront very near her home. This photo went viral over the weekend, which brought a huge amount of attention to her online presence. Shortly afterward, she got a creepy email from someone mentioning the store she was at and the city it\u2019s in; she\u2019s afraid she accidentally revealed too much about herself. The Solution. Daughter\u2019s self-sovereign identity is on her mobile phone, and it signs every picture she takes and stores that signature as part of the image file. This allows her to prove ownership of the photo. After she realizes the picture is problematic, Daughter\u2019s robo-attorney sends out a signed takedown request to the social media networks and search engines; most sites will remove the photo since she can prove her ownership of the photo. \u201cI\u2019m afraid of being doxxed.\u201d The Problem. In recent years, Daughter has been vocal on the issue of presentation of female characters in video games. She just heard about a few friends getting doxxed because of similar comments online. She is afraid that she might be a target of harassment. The Solution. Daughter\u2019s social media platform incorporates many novel features that evolved out of the self-sovereign ecosystem. This enables her to selectively disclose her PII only to people who have received attestations from a select group of friends that she defines. She immediately activates this feature to better control access to her personal information. Your Best Friend is a Content Creator Best Friend is working in a non-Western country. He\u2019s an entrepreneur who regularly vlogs about the local startup scene. Sometimes, his comments are opinionated, which has caused tension with these companies and the local government that supports them. His vlogs are posted to the local social media networks, which are also controlled by that government, but his followers are worldwide, including both local and Western business leaders. \u201cI don\u2019t want my followers to lose access to my published work.\u201d The Problem. The local social media network has a reputation for following the whims of the local government. Sometimes they arbitrarily delete posts that they don\u2019t agree with. Best Friend is concerned that when this happens, his followers will see references and citations to his work, but not be able to access the original posts. The Solution. Best Friend uses his self-sovereign identity to publish all of his videos. They\u2019re posted to his data store, a decentralized file storage and delivery system that is accessible through his self-sovereign identity and provably attributable to him. Hyperlinked citations and references connect to this original data source. Even if the content on the local social network is removed, anyone looking at a citation or reference can still find the original content and know that he wrote it \u2014 even though it\u2019s no longer connected to his social media presence. \u201cI\u2019m afraid of losing my followers\u201d The Problem. If Best Friend is sufficiently troublesome, the local government might delete his social media account entirely. This could cost him all the connections and relationships that he\u2019s built on the social media network. The Solution. Best Friend created his social media account using his self-sovereign identity, fundamentally linking them. His western followers tended to do the same, while his local followers only have localized accounts. If his local social media account is deleted, he automatically maintains links to everyone with a self-sovereign identity. Unfortunately, he loses access to all of the followers with local accounts, but his self-sovereign identity\u2019s decentralized identifier (DID) was available through his social media account and remains available through any citations and references. Any local follower could choose to link to his self-sovereign identity with this information. \u201cI don\u2019t want to get arrested for what I write.\u201d The Problem. The local government has decided Best Friend is troublesome and actively targets him for a smear and disinformation campaign, designed to get him arrested. They do so by publishing portions of his material that are taken out of context with a negative spin. The Solution. All of Best Friend\u2019s videos are time-stamped and then signed with his self-sovereign identity. He can refute the out-of-context statement by showing the full video, proving that it is his, and demonstrating its origin. The more reputable news agencies only use signed videos of this sort; though the propaganda network continues using the out-of-context video, no one else replicates it. Your Nephew Runs a Convenience Store Nephew has decided to open his first business! However, he wants to do more than just run a simple convenience store: he wants to run a convenience store that\u2019s sustainable. He\u2019s done a good job of getting it off the ground, but he\u2019s a one-man operation, and sometimes details slip through the cracks. He is also thinking about serial entrepreneurship, so he doesn\u2019t plan for this to be his last venture. \u201cI don\u2019t want to accidentally sell products that aren\u2019t sustainable if I say they are.\u201d The Problem. Nephew is concerned with the claims that various products make about their sustainability and provenance. There have been many recent articles about false claims, and his customers are asking questions that he can\u2019t answer. For his brand to stand out, he wants a degree of confidence about the products he\u2019s selling. The Solution. Independent inspectors create verifiable credentials for the authenticity of the sustainable products that Nephew sells in his store. Nephew authenticates these credentials and attests their veracity with his self-sovereign identity. So long as his customers trust him, they no longer need to verify the authenticity of their goods themselves, though they may if they wish. *\u201cI want to be able to get the best value for my business when I sell.\u201d * The Problem. Nephew knows that he could open the books of his business to show its profitability and lay out its money flow to potential purchasers. But this information isn\u2019t verifiable! The Solution. Nephew, with the aid of self-sovereign identity, can provide a full-spectrum, well-defined, verifiable suite of business analytics. Every purchase order, credit card sale, inventory item, and bill payment is signed with the business\u2019 decentralized identifier (DID), a component of his self-sovereign identity. This provability increases the value of his business. \u201cI forget to order things sometimes.\u201d The Problem. Because of his lack of staff, Nephew worries about losing track of inventory, which can result in delays of supply that have a negative impact on revenue. The Solution. Nephew has granted permissions to his smart refrigerator to use the business\u2019 DID to initiate and sign purchase orders on his behalf. Because the refrigerator has sensors that track inventory, supplies are reordered automatically. The fridge issues a purchase order signed with the business\u2019 DID and the refrigerator\u2019s DID, verifying both the authenticity and the origin of the order. This in turn gives his supplier a sense of security that the orders are valid. Milk arrives a day later and is loaded into the drinks cooler. Conclusion These use cases may be just what you need to convince your dad (or sister or daughter or best friend or nephew) of the importance of self-sovereign identity. You can use them as a library of arguments to bring around people who are interested, respectively, in finance, government, privacy, content, or business. And, these possibilities are just the tip of the iceberg. Because the adoption of self-sovereign identity is likely to begin in the developed world, the use cases in this paper focused on our own privileged family members. However, self-sovereign identity might be even more useful for marginalized people, who could gain protections that the developed world takes for granted. Refugees could be guaranteed identity when they flee their home state; peoples living in autocratic societies could enjoy new protections; and disadvantaged people could see their playing fields leveled. Self-sovereign identity could radically change the structure of the social contract. The catch is that we\u2019re not there yet. Though some of the arguments in this paper refer to technologies that are already being specified and implemented, others remain mere possibilities. In order to convince dad (and sis and the rest) fully, we need to be able to point not just to our dreams of self-sovereign identity, but also to a concrete reality. That\u2019s where you come in. If you\u2019re a developer, an engineer, or a software business owner, you can help realize these ideas. This article systematically approaches the problems that might be faced by people using the internet for five broad classes of works, imagines the very real problems they currently experience, and suggests how self-sovereign identity could offer solutions. The following table summarizes many of these possibilities: Actor Category Needs Dad Finance 1. Self-sovereign federated logins 2. Biometric logins 3. Validated identity Sister Government 1. Digital passport 2. Selective identity disclosure 3. Digital health records Daughter Privacy 1. Selective data disclosure 2. Data revocation 3. Automated identity disclosure Best Friend Content 1. Self-sovereign content 2. Self-sovereign relationships 3. Validated content Nephew Business 1. Verifiable credentials 2. Validated finances 3. Delegated identity Offering concrete solutions for these use cases could drive adoption of self-sovereign identity. We thus encourage software developers to consider these needs and see if they can make the solutions that fulfill them a reality. Of course, any new identity system must be built with care, as it could be misused if it\u2019s poorly developed. We don\u2019t want to increase the attack surfaces on identity, and we definitely don\u2019t want to create new honeypots. Keeping the focus on self-sovereign identity should put the Internet\u2019s next identity system on the right path. We own our identities . References Allen, Christopher. \u201cThe Path to Self-Sovereign Identity.\u201d Life with Alacrity . April 25, 2016. http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html. Andrieu, Joe, Frederic Engel, Adam Lake, Moses Ma, Olivier Maas, and Mark Van Der Waal. \u201cRe-Imagining What Users Really Want.\u201d Rebooting the Web of Trust. September 27, 2017. https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2017/blob/master/final-documents/what-users-really-want.pdf . Berners-Lee, Tim. \"A Public Identity.\" A Public Identity - Design Issues. January 19, 2018.. https://www.w3.org/DesignIssues/PublicIdentity.html. Casey, Michael. \"Blockchain Technology: Redefining Trust for a Global, Digital Economy.\" Medium. June 14, 2016. https://medium.com/mit-media-lab-digital-currency-initiative/blockchain-technology-redefining-trust-for-a-global-digital-economy-1dc869593308. Broudy, Alex. \"How Blockchains and Decentralized ID Solutions Flip the Switch on Privacy.\" CryptoDigest. June 12, 2018. https://cryptodigestnews.com/how-blockchains-and-decentralized-id-solutions-flip-the-switch-on-privacy-63e21e060670. Pettey, Christy. \"The Beginner's Guide to Decentralized Identity.\" Smarter With Gartner. June 28, 2018. . https://www.gartner.com/smarterwithgartner/the-beginners-guide-to-decentralized-identity/. Rosenberg, Matthew, Nicholas Confessore, and Carole Cadwalladr. \"How Trump Consultants Exploited the Facebook Data of Millions.\" The New York Times. March 17, 2018. https://www.nytimes.com/2018/03/17/us/politics/cambridge-analytica-trump-campaign.html. Smolenski, Natalie. \"The Evolution of Trust in a Digital Economy.\" Scientific American. Accessed January 2018.https://www.scientificamerican.com/article/the-evolution-of-trust-in-a-digital-economy/. Stempel, Jonathan. \"Home Depot Settles Consumer Lawsuit over Big 2014 Data Breach.\" Reuters . March 08, 2016. https://www.reuters.com/article/us-home-depot-breach-settlement/home-depot-settles-consumer-lawsuit-over-big-2014-data-breach-idUSKCN0WA24Z. Swamynathan, Yashaswini. \"Equifax Reveals Hack That Likely Exposed Data of 143 Million Customers.\" Reuters . September 08, 2017. https://www.reuters.com/article/us-equifax-cyber/equifax-reveals-hack-that-likely-exposed-data-of-143-million-customers-idUSKCN1BI2VK . Wolff, Josephine. \"How a 2011 Hack You've Never Heard of Changed the Internet's Infrastructure.\" Slate Magazine. December 21, 2016. https://slate.com/technology/2016/12/how-the-2011-hack-of-diginotar-changed-the-internets-infrastructure.html","title":"Convincing dad"},{"location":"RWoT7/convincing-dad/#how-to-convince-dad42-of-the-importance-of-self-sovereign-identity","text":"","title":"How to Convince Dad* of the Importance of Self-Sovereign Identity"},{"location":"RWoT7/convincing-dad/#42-and-your-sister-and-your-daughter-and-your-best-friend-and-your-nephew","text":"","title":"* and your sister and your daughter and your best friend and your nephew"},{"location":"RWoT7/convincing-dad/#by-shannon-appelcline-kenneth-bok-lucas-parker-peter-scott-and-matthew-wong","text":"","title":"by Shannon Appelcline, Kenneth Bok, Lucas Parker, Peter Scott, and Matthew Wong"},{"location":"RWoT7/convincing-dad/#abstract","text":"One of the major problems with bootstrapping self-sovereign identity is that it requires adoption by a large number of people. Pushing self-sovereign identity from the top-down is most likely to result in a technology that\u2019s not actually used, but instead encouraging the average person to demand self-sovereign identity from the bottom-up will result in the organic development of a vibrant, well-utilized decentralized web-of-trust ecosystem. This paper addresses that need by offering arguments to a variety of people who might be reluctant to use self-sovereign identity, uninterested in its possibilities, or oblivious to the dangers of centralization. By focusing on the needs of real people, we hope to also encourage developers, engineers, and software business owners to create the apps that will address their reluctance and fulfill their needs, making self-sovereign identity a reality.","title":"Abstract"},{"location":"RWoT7/convincing-dad/#introduction","text":"\"Cogito ergo sum \u2014 I think, therefore I am.\" \\~Ren\u00e9 Descartes \"Identity is a uniquely human concept; however modern society view this concept of identity as state-issued credentials such as driver\u2019s license and social security cards, which suggests a person can lose his very identity if a state revokes his credentials or even if he just crosses state borders. I think, but I am not .\" \\~ Christopher Allen The possibility of losing your identity is a serious problem in the digital world. Vloggers could lose their identities if YouTube closes their accounts, while common internet citizens like you and me could lose a big part of our life if Facebook revokes our credentials. As digital accounts become a major part of our identities, we need a paradigm that allows us to bring identity back under our control. Self-sovereign identity seeks to be that new model, creating a paradigm shift in an increasingly data-governed world. It puts the individual in control of her identity and prioritizes her privacy. It does so by acting as the root anchor for an individual\u2019s data stream, permitting an individual to manage, store, and control her own data and life. It could become a ubiquitous technology that affects the lives of billions on a daily basis. This runs counter to the traditional model for online identity, designed from the perspective of the corporation and government, with the needs of the individual being secondary. In the age of \"Surveillance Capitalism\", personal data is typically abused by large corporations in the never-ending quest for profit, disregarding user privacy and inadequately safeguarding user data. The Equifax hack in late 2017 and the Cambridge Analytica scandal highlight the risks of large, centralized databases of personal information (i.e. honeypots of data), which present high-value targets for hackers. But how do we convince the average person to move from the old, centralized model to the new, self-sovereign model? We think that this requires proactively fulfilling their needs. To highlight these needs, we\u2019ve created case studies for five people who could be served by self-sovereign identity, if they were only aware of its possibilities. They are: Your dad, who is preparing for retirement; Your sister, who is a world traveller; Your daughter, who is a reckless social media user; Your best friend, who is a content creator; and Your nephew, who runs a convenience store. These five people represent a spectrum of use-cases and applications, meant to portray some of the monumental possibilities of self-sovereign identity in the not-so-distant future. They are concentrated in the developed world, because we believe that is where adoption of self-sovereign identity will begin. Some of these use cases are possible now, while others will require a more fully fleshed-out web-of-trust ecosystem.","title":"Introduction"},{"location":"RWoT7/convincing-dad/#your-dad-is-preparing-for-retirement","text":"Dad is getting ready to retire, which has him thinking more about his financial security. So much is online now! He has to use his laptop computer to pay some of his bills from his bank account, and he looks at his retirement accounts through his browser too. He has a few different logins and passwords, because the different institutions have different requirements. He keeps them written down on a yellow post-it that he hides in his desk drawer. Recently, Dad has become paranoid about having his money stolen because his best friend got phished by someone claiming that their Microsoft Windows installation needed updating, which let the hackers install a keylogger on their computer and steal some money. Though Dad finally traded in his flip phone for a smartphone last year, he doesn\u2019t use it for anything but reading news stories and text messages. \u201cI hate having to log into so many financial accounts.\u201d The Problem. Dad gets annoyed at all these confusing accounts and logins and passwords. He\u2019d like to have a single account that accesses all of his financial services and he\u2019d prefer not to need a login and password for it at all. The Solution. Using his self-sovereign identity, Dad can federate logins to various financial services. He accesses it using biometrics: he just looks at his smartphone\u2019s camera, it scans his face, and then the smartphone verifies his identity to his laptop. This gives him access to all of his federated financial accounts without needing to type logins or passwords. An app on his laptop consolidates all of the information from the accounts and allows him to view his finances, write checks, and free up retirement money. \u201cI\u2019m afraid someone will steal my money through some sort of fake login.\u201d The Problem. Dad is especially vulnerable to phishing attacks, in which someone obtains access to his financial account by pretending to represent the financial institution. He lives in constant fear of losing his nest egg: he is concerned that it is impossible to tell the difference between a legitimate representative and a scammer. The Solution. Because Dad logs in using his smartphone and face recognition, there is no way for him to accidentally log in to a fake portal. The application will take him to the correct site. \u201cIf I get my identity stolen, I\u2019m screwed.\u201d The Problem. Dad has heard a lot about the ravages of identity theft: attackers stole names, phone numbers, and addresses from Home Depot and stole reams of identity information from Equifax. Dad is afraid that the thieves could use this information to steal his money at his financial institutions or to take out credit or loans in his name. The Solution. This sort of breach would be less likely in a world of self-sovereign identity because users can safeguard their information under their self-sovereign identity, preventing it from entering large honeypots of personally identifiable information (PII). But, even if a breach were to expose Dad\u2019s information, it wouldn\u2019t affect his access to financial institutions: the PII wouldn\u2019t give access to his accounts without validation from his smartphone; if someone tried to take out a new loan using his PII, the bank or credit bureau would reach out to Dad for verification. Thus, in the world of self-sovereign identity, PII is less valuable and its use is more tightly under Dad\u2019s control.","title":"Your Dad is Preparing for Retirement"},{"location":"RWoT7/convincing-dad/#your-sister-is-a-world-traveller","text":"Sis works with M\u00e9decins Sans Fronti\u00e8res offering humanitarian assistance. This brings her to dozens of countries, some of which have non-western values. Sometimes she\u2019s been targeted by the authorities, which has put her in jeopardy. She also has occasionally needed to seek treatment in these countries due to the problems that a varying diet causes for her diabetes. \u201cI\u2019m afraid of losing my passport.\u201d The Problem. Sis was once forced to leave her passport behind when she had to flee a city: she was afraid to return to her lodgings due to the local authority\u2019s disagreement with MSF . This terrified her, because a passport is expected to be in a traveler\u2019s possession when they are traveling in a foreign country. This left her unable to easily leave the country, which she now felt was hostile to her presence. Replacing her passport was both time consuming and logistically challenging. She is horrified by the idea of repeating this experience. The Solution. Sis has a digital passport stored on a hardware device that acts as a data store. She keeps it attached to her keychain. She can unlock and transmit her passport data using her thumbprint. If she loses her physical passport, she has this backup. Authorities in some countries may accept it themselves, but otherwise, she can use it anywhere to establish her identity to the local embassy by logging into her government\u2019s online platform. \u201cI don\u2019t want the police looking at my passport if they stop me.\u201d The Problem. Due to her country of origin, Sis experienced harassment from the police when they were reviewing her passport. She also endured extensive interviews because of the countries she has visited. The Solution. When Sis enters a country, the border agents issue digital documents (a verifiable credential) approving her legal status in the country for a certain span of time. She adds this to her self-sovereign identity\u2019s data store. When she is stopped by the police, she uses her hardware device to selectively disclose only her name and the credential issued by the border agents. The police now know who she is and what her status is; they don\u2019t need to know about her country of origin or past travels. The border agents have already verified that information, so she doesn\u2019t need to give it to the municipal authorities. \u201cI worry that I don\u2019t have all of my health records with me.\u201d The Problem. Sis has type 1 diabetes and has to ensure her sugar levels remain stable. If she were to have a medical emergency while traveling, she needs the doctors to know the details of her condition, including her current medications and allergies, without having to carry a sheaf of documents. The Solution. Sis keeps her medical records on her hardware device. Using her self-sovereign identity, she can protect the information or share it when necessary. In addition, the encoded, digital nature of her medical records makes it easy to translate into different languages. When she visits the local doctor, it is presented in the local language, courtesy of the AI-powered translation program.","title":"Your Sister is a World Traveller"},{"location":"RWoT7/convincing-dad/#your-daughter-is-a-reckless-social-media-user","text":"Daughter has spent her life publishing pictures, tweeting, and posting on social media services such as Facebook and Snapchat. Now that she is going for job interviews, she realizes that all of her photos of holidays and parties and all of her tweets are a bit more public than she would like. \u201cI don\u2019t want to show my employer everything.\u201d The Problem. Daughter is going for a job interview soon and has heard rumors of employers making hiring decisions based on social media profiles. She is concerned over which of her images and tweets will turn up if they access her social media profiles. The Solution. With self-sovereign identity, Daughter has granular control over who sees what in her social media feeds. When she gives access to potential employers, she gives access to feeds that selectively disclose specific tags. \u201cI\u2019ve put something online that I can\u2019t take back.\u201d The Problem. Daughter\u2019s photo blog features a picture of her in front of a popular storefront very near her home. This photo went viral over the weekend, which brought a huge amount of attention to her online presence. Shortly afterward, she got a creepy email from someone mentioning the store she was at and the city it\u2019s in; she\u2019s afraid she accidentally revealed too much about herself. The Solution. Daughter\u2019s self-sovereign identity is on her mobile phone, and it signs every picture she takes and stores that signature as part of the image file. This allows her to prove ownership of the photo. After she realizes the picture is problematic, Daughter\u2019s robo-attorney sends out a signed takedown request to the social media networks and search engines; most sites will remove the photo since she can prove her ownership of the photo. \u201cI\u2019m afraid of being doxxed.\u201d The Problem. In recent years, Daughter has been vocal on the issue of presentation of female characters in video games. She just heard about a few friends getting doxxed because of similar comments online. She is afraid that she might be a target of harassment. The Solution. Daughter\u2019s social media platform incorporates many novel features that evolved out of the self-sovereign ecosystem. This enables her to selectively disclose her PII only to people who have received attestations from a select group of friends that she defines. She immediately activates this feature to better control access to her personal information.","title":"Your Daughter is a Reckless Social Media User"},{"location":"RWoT7/convincing-dad/#your-best-friend-is-a-content-creator","text":"Best Friend is working in a non-Western country. He\u2019s an entrepreneur who regularly vlogs about the local startup scene. Sometimes, his comments are opinionated, which has caused tension with these companies and the local government that supports them. His vlogs are posted to the local social media networks, which are also controlled by that government, but his followers are worldwide, including both local and Western business leaders. \u201cI don\u2019t want my followers to lose access to my published work.\u201d The Problem. The local social media network has a reputation for following the whims of the local government. Sometimes they arbitrarily delete posts that they don\u2019t agree with. Best Friend is concerned that when this happens, his followers will see references and citations to his work, but not be able to access the original posts. The Solution. Best Friend uses his self-sovereign identity to publish all of his videos. They\u2019re posted to his data store, a decentralized file storage and delivery system that is accessible through his self-sovereign identity and provably attributable to him. Hyperlinked citations and references connect to this original data source. Even if the content on the local social network is removed, anyone looking at a citation or reference can still find the original content and know that he wrote it \u2014 even though it\u2019s no longer connected to his social media presence. \u201cI\u2019m afraid of losing my followers\u201d The Problem. If Best Friend is sufficiently troublesome, the local government might delete his social media account entirely. This could cost him all the connections and relationships that he\u2019s built on the social media network. The Solution. Best Friend created his social media account using his self-sovereign identity, fundamentally linking them. His western followers tended to do the same, while his local followers only have localized accounts. If his local social media account is deleted, he automatically maintains links to everyone with a self-sovereign identity. Unfortunately, he loses access to all of the followers with local accounts, but his self-sovereign identity\u2019s decentralized identifier (DID) was available through his social media account and remains available through any citations and references. Any local follower could choose to link to his self-sovereign identity with this information. \u201cI don\u2019t want to get arrested for what I write.\u201d The Problem. The local government has decided Best Friend is troublesome and actively targets him for a smear and disinformation campaign, designed to get him arrested. They do so by publishing portions of his material that are taken out of context with a negative spin. The Solution. All of Best Friend\u2019s videos are time-stamped and then signed with his self-sovereign identity. He can refute the out-of-context statement by showing the full video, proving that it is his, and demonstrating its origin. The more reputable news agencies only use signed videos of this sort; though the propaganda network continues using the out-of-context video, no one else replicates it.","title":"Your Best Friend is a Content Creator"},{"location":"RWoT7/convincing-dad/#your-nephew-runs-a-convenience-store","text":"Nephew has decided to open his first business! However, he wants to do more than just run a simple convenience store: he wants to run a convenience store that\u2019s sustainable. He\u2019s done a good job of getting it off the ground, but he\u2019s a one-man operation, and sometimes details slip through the cracks. He is also thinking about serial entrepreneurship, so he doesn\u2019t plan for this to be his last venture. \u201cI don\u2019t want to accidentally sell products that aren\u2019t sustainable if I say they are.\u201d The Problem. Nephew is concerned with the claims that various products make about their sustainability and provenance. There have been many recent articles about false claims, and his customers are asking questions that he can\u2019t answer. For his brand to stand out, he wants a degree of confidence about the products he\u2019s selling. The Solution. Independent inspectors create verifiable credentials for the authenticity of the sustainable products that Nephew sells in his store. Nephew authenticates these credentials and attests their veracity with his self-sovereign identity. So long as his customers trust him, they no longer need to verify the authenticity of their goods themselves, though they may if they wish. *\u201cI want to be able to get the best value for my business when I sell.\u201d * The Problem. Nephew knows that he could open the books of his business to show its profitability and lay out its money flow to potential purchasers. But this information isn\u2019t verifiable! The Solution. Nephew, with the aid of self-sovereign identity, can provide a full-spectrum, well-defined, verifiable suite of business analytics. Every purchase order, credit card sale, inventory item, and bill payment is signed with the business\u2019 decentralized identifier (DID), a component of his self-sovereign identity. This provability increases the value of his business. \u201cI forget to order things sometimes.\u201d The Problem. Because of his lack of staff, Nephew worries about losing track of inventory, which can result in delays of supply that have a negative impact on revenue. The Solution. Nephew has granted permissions to his smart refrigerator to use the business\u2019 DID to initiate and sign purchase orders on his behalf. Because the refrigerator has sensors that track inventory, supplies are reordered automatically. The fridge issues a purchase order signed with the business\u2019 DID and the refrigerator\u2019s DID, verifying both the authenticity and the origin of the order. This in turn gives his supplier a sense of security that the orders are valid. Milk arrives a day later and is loaded into the drinks cooler.","title":"Your Nephew Runs a Convenience Store"},{"location":"RWoT7/convincing-dad/#conclusion","text":"These use cases may be just what you need to convince your dad (or sister or daughter or best friend or nephew) of the importance of self-sovereign identity. You can use them as a library of arguments to bring around people who are interested, respectively, in finance, government, privacy, content, or business. And, these possibilities are just the tip of the iceberg. Because the adoption of self-sovereign identity is likely to begin in the developed world, the use cases in this paper focused on our own privileged family members. However, self-sovereign identity might be even more useful for marginalized people, who could gain protections that the developed world takes for granted. Refugees could be guaranteed identity when they flee their home state; peoples living in autocratic societies could enjoy new protections; and disadvantaged people could see their playing fields leveled. Self-sovereign identity could radically change the structure of the social contract. The catch is that we\u2019re not there yet. Though some of the arguments in this paper refer to technologies that are already being specified and implemented, others remain mere possibilities. In order to convince dad (and sis and the rest) fully, we need to be able to point not just to our dreams of self-sovereign identity, but also to a concrete reality. That\u2019s where you come in. If you\u2019re a developer, an engineer, or a software business owner, you can help realize these ideas. This article systematically approaches the problems that might be faced by people using the internet for five broad classes of works, imagines the very real problems they currently experience, and suggests how self-sovereign identity could offer solutions. The following table summarizes many of these possibilities: Actor Category Needs Dad Finance 1. Self-sovereign federated logins 2. Biometric logins 3. Validated identity Sister Government 1. Digital passport 2. Selective identity disclosure 3. Digital health records Daughter Privacy 1. Selective data disclosure 2. Data revocation 3. Automated identity disclosure Best Friend Content 1. Self-sovereign content 2. Self-sovereign relationships 3. Validated content Nephew Business 1. Verifiable credentials 2. Validated finances 3. Delegated identity Offering concrete solutions for these use cases could drive adoption of self-sovereign identity. We thus encourage software developers to consider these needs and see if they can make the solutions that fulfill them a reality. Of course, any new identity system must be built with care, as it could be misused if it\u2019s poorly developed. We don\u2019t want to increase the attack surfaces on identity, and we definitely don\u2019t want to create new honeypots. Keeping the focus on self-sovereign identity should put the Internet\u2019s next identity system on the right path. We own our identities .","title":"Conclusion"},{"location":"RWoT7/convincing-dad/#references","text":"Allen, Christopher. \u201cThe Path to Self-Sovereign Identity.\u201d Life with Alacrity . April 25, 2016. http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html. Andrieu, Joe, Frederic Engel, Adam Lake, Moses Ma, Olivier Maas, and Mark Van Der Waal. \u201cRe-Imagining What Users Really Want.\u201d Rebooting the Web of Trust. September 27, 2017. https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2017/blob/master/final-documents/what-users-really-want.pdf . Berners-Lee, Tim. \"A Public Identity.\" A Public Identity - Design Issues. January 19, 2018.. https://www.w3.org/DesignIssues/PublicIdentity.html. Casey, Michael. \"Blockchain Technology: Redefining Trust for a Global, Digital Economy.\" Medium. June 14, 2016. https://medium.com/mit-media-lab-digital-currency-initiative/blockchain-technology-redefining-trust-for-a-global-digital-economy-1dc869593308. Broudy, Alex. \"How Blockchains and Decentralized ID Solutions Flip the Switch on Privacy.\" CryptoDigest. June 12, 2018. https://cryptodigestnews.com/how-blockchains-and-decentralized-id-solutions-flip-the-switch-on-privacy-63e21e060670. Pettey, Christy. \"The Beginner's Guide to Decentralized Identity.\" Smarter With Gartner. June 28, 2018. . https://www.gartner.com/smarterwithgartner/the-beginners-guide-to-decentralized-identity/. Rosenberg, Matthew, Nicholas Confessore, and Carole Cadwalladr. \"How Trump Consultants Exploited the Facebook Data of Millions.\" The New York Times. March 17, 2018. https://www.nytimes.com/2018/03/17/us/politics/cambridge-analytica-trump-campaign.html. Smolenski, Natalie. \"The Evolution of Trust in a Digital Economy.\" Scientific American. Accessed January 2018.https://www.scientificamerican.com/article/the-evolution-of-trust-in-a-digital-economy/. Stempel, Jonathan. \"Home Depot Settles Consumer Lawsuit over Big 2014 Data Breach.\" Reuters . March 08, 2016. https://www.reuters.com/article/us-home-depot-breach-settlement/home-depot-settles-consumer-lawsuit-over-big-2014-data-breach-idUSKCN0WA24Z. Swamynathan, Yashaswini. \"Equifax Reveals Hack That Likely Exposed Data of 143 Million Customers.\" Reuters . September 08, 2017. https://www.reuters.com/article/us-equifax-cyber/equifax-reveals-hack-that-likely-exposed-data-of-143-million-customers-idUSKCN1BI2VK . Wolff, Josephine. \"How a 2011 Hack You've Never Heard of Changed the Internet's Infrastructure.\" Slate Magazine. December 21, 2016. https://slate.com/technology/2016/12/how-the-2011-hack-of-diginotar-changed-the-internets-infrastructure.html","title":"References"},{"location":"RWoT7/ipld-did/","text":"IPLD as a general pattern for DID documents and Verifiable Claims By jonnycrunch , Anthony Ronning , Kim Duffy , Christian Lundkvist Rebooting the Web of Trust, Fall 2018 Abstract Since the emergence of the Decentralized Identifier (DID) specification at the Fall 2016 Rebooting the Web of Trust [1] , numerous DID method specifications have appeared. Each DID method specification defines how to resolve a cryptographically-tied DID document given a method-specific identifier. In this paper, we describe a way to represent the DID document as a content-addressed Merkle Directed Acyclic Graph (DAG) using Interplanetary Linked Data (IPLD) . This technique enables more cost-efficient, scaleable creation of DIDs and can be applied across different DID method specifications. Why IPLD Content addressing through hashes has become a widely-used means of connecting data in distributed systems. IPLD is a way of representing hash-linked data to be used in content-addressed data retrieval systems such as IPFS . Other content that can be resolved using IPLD includes Git repositories and blockchains such as Bitcoin, Ethereum, and ZCash. IPLD enables creation of decentralized data-structures that are universally addressable, facilitating resolving content accross different protocols. It achieves this through an interoperable data model that represents various protocol formats. IPLD relies on Content Identifiers (CIDs) for content addressing. CIDs are a self-describing, flexible, and interoperable way of expressing cryptographic hashes. It uses several multiformats to achieve flexible self-description, namely multihash for hashes, multicodec for data content types, and multibase to represent the base encoding of the CID itself. This interoperability makes IPLD a valuable structure for the DID document that can be used across a variety of DID methods or distributed ledgers and ensures cryptographic validity of the DID document. In this paper, we demostrate how IPLD could be used as a general pattern for representing the DID document for numerous DID methods. IPLD IPLD uses an abstract model for linking data via cryptographic hashes, which enables link traversal to the referenced data via path \"/\" notation. This path notation has its roots in the Linux Filesystem Hierarchy Standard . A link in IPLD is represented in JSON as a \"link object\" and uses the path syntax \"/\" as the key to the object that is followed by the CID of the link. For example: { \"/\" : \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" } This syntax for representing linked data can be expanded and used for other JSON structures. For instance, building on this example we can leverage JSON-LD semantics by inserting a @context to our IPLD object and apply the above as a link that resolves to the JSON-LD document that describes the attributes for our IPLD object. { \"@context\" : { \"/\" : \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" } } JSON-LD is a syntax to serialize Linked Data in JSON and provide semantics without the overhead of a large rdf model [2] . Since both IPLD and JSON-LD are 100% compatible with JSON, the large number of JSON parsers and libraries are already available. However, when using JSON-LD in a browser, it is impossible to discover the Base IRI after an http redirect ( see #316 ), and the content of the @context can potentially change over time [3] . Finally, since a URI depends upon the security of DNS, DNS spoofing/DNS poisoning could offer a simple attack vector. Essentially, without much effort, an attacker can adjust the cache of a DNS server and begin pointing traffic from 'schema.org' (or any other desired host) to anywhere else on the internet or local LAN. Given the critical nature of the JSON-LD @context resource, the attacker can make a fraudulent signature pass as being valid. Using IPLD, we can use the entire JSON data model and we can layer any JSON-LD on top of IPLD [4] . This will enable cryptographic guarantees to the authenticity of the JSON-LD schema and mitigate such an attack. One additional point to emphasize is that the content loaded into IPLD is serialized using Concise Binary Object Representation (CBOR) , allowing for deterministic representation and retrieval. Content stored on IPLD is cryptographically retrievable regardless of the domain. The content can be retrieved via numerous http gateways including: ipfs.io , ipfs.infura.io , and cloudflare-ipfs.com , as well as a locally hosted gateway or via the command-line ipfs application. Retrieval of IPLD content represented as dag-cbor from ipfs.io gateway and validating the content by reloading it into ipfs via command-line > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq | ipfs dag put outputs: zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq , thus validating the hash of the cid Retrieval of IPLD content represented as dag-cbor from ipfs command-line > ipfs dag get zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq outputs the original JSON-LD schema link . DID Document Creation and Updates using IPLD At initialization of the DID document, the public/private keys pairs associated with the DID method specific identifier may or may not also have been generated. Additionally, the public/private key pair may or may not be the same as the master publishing key associated with the DID. In the example of the IPID DID method , the DID method specific identifier is the self-describing cryptographic multihash of the public key of an IPFS node and is different than the public/private key pair represented in the DID document. At present, RSA and Ed25519 elliptic curves are supported and there are plans to support secp256k1. Other DID methods that use IPLD to store the DID document may use alternative approaches to associate the DID document with the method specific identifier. For instance, in the muPort DID method, an Ethereum smart contract is used to point to the hash of the latest version of the DID document, and the document can be retrieved using the content-addressed IPFS system. The Sidetree DID method uses a scalable Merkle data structure that aggregates multiple DID document updates in a Merkle Tree and publishes the root hash in a blockchain such as Bitcoin or Ethereum. A sidetree node can use these data structures to aggregate the updates into a complete DID document Since the DID document needs to contain information about where to discover the latest version of the DID document (which is a requirement of DID resolution), in IPID the id field corresponds to the DID method specific identifier (i.e. did:ipid: ), but other DID methods specs can implement this in their own ways. While the exact update details are up to the DID method, Example 1 shows how the resulting structure might appear using IPID, which publishes the hash digest of the DID document to IPNS. Intrinsic to the integrity of any blockchain-based application is the existence of a previous field. In a blockchain data structure each block contains an ordered list of cyptographic transactions. A cryptographic reference to hash of the previous block is stored in the block header. Similarly, in this lightweight protocol based on IPLD, we have made use of the previous field with a cyptographic reference to the previous version of the DID document. This faciliatates traversal of the entire history of the DID document. One drawback to this approach is that this is a retrospective perspective. This highlights the importance of the id field, which allows an agent to follow the tip of the chain and at any point cryptographically resolve the latest version of the document. In IPID , associating the DID document with a DID is accomplished by cryptographically publishing the CID to the IPNS public key associated with the identity owner (DID method specific identifier). Any updates to the DID document are saved to IPLD and the resulting hash is published to IPNS cryptographically associating the new CID with the DID (for IPID this is the multihash of the public key). IPID uses a PubSub model for realtime updates to the DID. So far, on its own, this approach is not considered a comprehensive standalone blockchain solution. Most importantly: this does not faciliatate consensus of the document across peers, and timestamps are self attesting. The phrase \"microledger\" is often used to describe this approach. To overcome this shortcoming, the CID of the DID document can be anchored in a proof of existence smart contract (e.g. Truffle) created by the identity owner. Alternatively, cryptographic timestamping protocols such as openTimestamps (free) or Chainpoint could be utilized. Examples Proof of Existence Smart Contract to anchor CID of a DID document pragma solidity ^0.4.23; contract ProofOfExistence { event ProofCreated( bytes indexed cid, bytes did ); struct Proof { bytes did; uint block; } address public owner; mapping (bytes => Proof) DIDbyCID; modifier onlyOwner() { require(msg.sender == owner); _; } constructor() public { owner = msg.sender; } function notarizeHash(bytes cid, bytes did) onlyOwner public { Proof proof proof.did = did proof.block = now DIDbyCID[cid] = proof; emit ProofCreated(cid, did); } function ProofRequest(bytes cid, bytes did) public view returns (Proof) { return DIDbyCID[cid]; } } This smart contract has the added benefit of listening to updates to the IPLD DID document via Ethereum events. Example 1: Demonstration of initialization of DID document { \"@context\": { \"/\": \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" }, \"created\": \"2018-12-01T03:00:00Z\", \"publicKey\": [ { \"curve\": \"ed25519\", \"expires\": \"2019-12-01T03:00:00Z\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\" } ], \"updated\": \"2018-12-01T03:00:01Z\" } Example 1 shows the DID document before publication and before a cryptographic signature has been added. Note that the id and signature fields are omitted as it is not associated yet with the DID method specific identifier. Unlike other DID methods, with the DID document being represented as IPLD, we can directly link the @context , which is also represented on IPLD as a resolvable CID cryptographic link. Notably absent is the previous field, as this is the genesis of the chain of objects that subsequent updates will reference (see below). This entire DID document when added to IPFS as IPLD has a CID of zdpuAqiExr6k4AbWF6BuGkgUbVMZ7jbJyNvRz9z9yyRBxosPi and will be used as the previous field in the subsequent updated DID document (Example 2). Example 2: Intermediate DID document updated with id field associating this DID document with a DID method specific identifier for future resolution { \"@context\": { \"/\": \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" }, \"authentication\": { \"publicKey\": [ \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\" ], \"type\": \"EdDsaSASignatureAuthentication2018\" }, \"created\": \"2018-12-01T03:00:00Z\", \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"previous\": { \"/\": \"zdpuAqiExr6k4AbWF6BuGkgUbVMZ7jbJyNvRz9z9yyRBxosPi\" }, \"proof\": { \"/\": \"z43AaGF42R2DXsU65bNnHRCypLPr9sg6D7CUws5raiqATVaB1jj\" }, \"publicKey\": [ { \"curve\": \"ed25519\", \"expires\": \"2019-12-01T03:00:00Z\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\" } ], \"updated\": \"2018-12-01T03:00:02Z\" } Example 2 shows the addition of the id field with the previous field linking to the hash of Example 1 Example 3: IPLD DID document updated with the addition of the signature field { \"@context\": { \"/\": \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" }, \"authentication\": { \"type\": \"EdDsaSASignatureAuthentication2018\", \"publicKey\": [ \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\" ] }, \"created\": \"2018-12-01T03:00:00Z\" , \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"previous\": { \"/\" : \"zdpuAosm9NYNW5kG2h3SBoCZz5DYqyTgf6qopkxpih5cFhqmU\" }, \"proof\" : { \"/\" : \"z43AaGF42R2DXsU65bNnHRCypLPr9sg6D7CUws5raiqATVaB1jj\" }, \"publicKey\": [ { \"curve\": \"ed25519\", \"expires\": \"2019-12-01T03:00:00Z\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\" } ], \"signature\": { \"created\": \"2018-12-01T03:00:02Z\", \"creator\": \"zdpuAohuM1RWMpzwQgWz5jXFCdPtz1rhD82fuZBauUDuRzknt/publicKey/0\", \"message\" : { \"/\" : \"zdpuAohuM1RWMpzwQgWz5jXFCdPtz1rhD82fuZBauUDuRzknt\" }, \"signatureValue\": \"o9r6LxgoGN8FoaeeUA6EdDcv12GvDzFEmCgjWzvpur2YSQyA8W2r0SSWUK+nH5tMqzaFLun6wwZ1Eot37amGDg==\", \"type\": \"ed25519Signature2018\" }, \"updated\": \"2018-12-01T03:00:00Z\" } Example 3 shows the final DID document after it is associated with the DID method specific identifier and signature. In this case, it was published to IPNS using the IPID method spec. Note that id field is now populated and the updated document was pushed to IPLD, resulting in a CID for the final document of zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP (link) . The chain of trust of the history of all edits can be done my simply following the previous link with the syntax zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP/previous and back to the original genesis of the document with a cid of zdpuAqiExr6k4AbWF6BuGkgUbVMZ7jbJyNvRz9z9yyRBxosPi with the syntax of zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP/previous/previous . Similarly, with this syntax the signature field links to the publicKey of the CID natively without the need of a referenced fragment and the message (payload) that was signed. Additionally, a proof field has been added which is itself CID link that resolves to a proof of existence smart contract on the Ethereum blockchain that resolves natively or can be externally validated. Retrieval of creator of the signature can be performed via any ipfs gateway > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP/publicKey/0 output: \"curve\":\"ed25519\",\"expires\":\"2019-12-01T03:00:00Z\",\"publicKeyBase64\":\"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\",\"type\":\"EdDsaPublicKey\"} Retrieval of the id of the creator can be performed via any ipfs gateway > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP/id output: \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\" Example 4: IPLD DID document updated after key rotation { \"@context\": { \"/\": \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" }, \"authentication\": { \"type\": \"EdDsaSASignatureAuthentication2018\", \"publicKey\": [ \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\" ] }, \"created\": \"2018-12-01T03:00:00Z\" , \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"previous\": { \"/\": \"zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP\" }, \"proof\": { \"/\": \"z43AaGF42R2DXsU65bNnHRCypLPr9sg6D7CUws5raiqATVaB1jj\" }, \"publicKey\": [ { \"curve\": \"ed25519\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\", \"status\": \"revoked\" }, { \"curve\": \"ed25519\", \"expires\": \"2021-07-08T16:02:20Z\", \"publicKeyBase64\": \"jGp4OT1GktrZMdrkOM+zj8iE1IqCiqg2iH+rUZ93jhE=\", \"type\": \"EdDsaPublicKey\" } ], \"signature\": { \"created\": \"2018-12-01T03:00:04Z\" , \"creator\": \"zdpuAt99xoa8i2BrcjwpY2H6ksXeaE28upionw2VuxRamBs6H/publicKey/1\", \"message\": { \"/\": \"zdpuAyvreXzQHqwv3rL8MaVPjNJjpLLa5Du3HcbpQL41XS35G\" }, \"signatureValue\": \"WDA3Dx7c+UWR37oglhkLNwxAbxXM4YbT7TpgmaCQ/rSqbtXgM3EpQ4mpkPXT5OBLH6bDai12Ank8SUHW47JxCQ==\", \"type\": \"ed25519Signature2018\" }, \"updated\": \"2018-12-01T03:00:04Z\" } Example 4 shows an updated DID document that has revoked key/0 and created a new key/1, which is used for signing the new DID document. The cid for this final document is zdpuAtrP6ZSDZj6izYQEbuUjuDRGHSa5L59BZDa1deRwAAZRQ and is used to publish and associate it with the DID. Use with Pairwise Identifiers The IPLD pattern may also be a good fit to use for pairwise identifiers. A pairwise identifier is a DID that is meant to be used only with one other entity. The idea is that when setting up a pairwise DID you can do it by generating the DID document, and send the DID as well as the DID document to the counterparty. The counterparty can verify that the DID document hash is the DID. Then if the user decides to update the DID document (normally through the use of a digital signature from a \"management key\" specified in the DID document) they can just present the updated version of the DID document to the counterparty, who can then verify the signatures and store the updated document in a local database and not necessarily publish it publically. Even though IPFS could be used for content addressing there would not be a need to connect to a wider IPFS network. The \"ledger\" in this case could just be a simple database hosted by each of the counterparties or a private ledger shared between them. Benefits One large advantage of the IPLD approach described here is that the identity owner does not need to use a blockchain when initially creating an identity, thus making creation of identities fast and low cost (if not free). The DID and DID document will be cryptographically coupled by hashing. Only when the identity owner needs to anchor their DID document will they need a more costly tool such as a blockchain. In addition, IPLD is perfectly suited for pairwise DID, when the creation and sharing of a DID document could be cryptographically generated and shared between only two parties and NOT saved to a public ledger. Each party can simply add the resulting CID to a local database. Summary of the benefits include: Low cost / free scalable solution for DIDs Ability to use Bitcoin, OpenTimeStamps, ZCash, Ethereum and/or Ethereum smart contracts, or Github as an optional proof of existence Ability to use CBOR as a schemaless data model Cryptographic verifiable resolution of a document Ability to use microledgers Perfectly suited for pairwise DIDs Ability to self-host documents or pay through future services such as Filecoin Easily resolve previous versions of the DID by cryptographic linking to the previous CID Resolvable anywhere (local networks, Mars, online, etc.) Drawbacks Not resolvable without hosting (This could be construed as a feature for pairwise DIDs) @context is not a reserved word in the IPLD specificatioin. { \"/\" : \"<CID>\" is not currently valid syntax for JSON-LD One additonal drawback to this approach results from the current DID draft specification, which designates publicKey as an array of objects; for example: { ... \"publicKey\": [{ \"id\": \"did:example:123456789abcdefghi#keys-1\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:example:123456789abcdefghi#keys-2\", \"type\": \"Ed25519VerificationKey2018\", \"owner\": \"did:example:pqrstuvwxyz0987654321\", \"publicKeyBase58\": \"H3C2AVvLMv6gmMNam3uVAjZpfkcJCwDwnZn6z3wXmqPV\" }, { \"id\": \"did:example:123456789abcdefghi#keys-3\", \"type\": \"Secp256k1VerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyHex\": \"02b97c30de767f084ce3080168ee293053ba33b235d7116a3263d29f1450936b71\" }], ... } The current approach is to iterate over each of the keys in this array to identify the key included by reference. Specifically, the algorithm to use when processing a publicKey property in a DID Document is: 1. Let value be the data associated with the publicKey property and initialize result to null. 2. If value is an object, the key material is embedded. Set result to value. 3. If value is a string, the key is included by reference. Assume value is a URL. 1. Dereference the URL and retrieve the publicKey properties associated with the URL (e.g., process the publicKey property at the top-level of the dereferenced document). 2. Iterate through each public key object. 1. If the id property of the object matches value, set result to the object. 4. If result does not contain at least the id, type, and owner properties as well as any mandatory public cryptographic material, as determined by the result's type property, throw an error. A better approach may be to name the key by name within the object of publicKey rather than as an array; for example: { ... \"publicKey\": { \"key-1\" : { \"curve\": \"ed25519\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\" }, \"key-2\" : { \"curve\": \"ed25519\", \"expires\": \"2021-07-08T16:02:20Z\", \"publicKeyBase64\": \"jGp4OT1GktrZMdrkOM+zj8iE1IqCiqg2iH+rUZ93jhE=\", \"type\": \"EdDsaPublicKey\" } }, ... } In IPLD, this will allow for a more straightforward reference without a convoluted algorithm to accomplish this. > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuAu918t7r8bv2wvuJiasiq78oDCbZ62ecZCj43oBWvspzr/publicKey/key-1 with output: {\"curve\":\"ed25519\",\"publicKeyBase64\":\"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\",\"type\":\"EdDsaPublicKey\"} This can be cryptographically linked back to the id with: > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuAu918t7r8bv2wvuJiasiq78oDCbZ62ecZCj43oBWvspzr/id Conclusion IPLD provides a robust framework for a lightweight microledger that cryptographically resolves the necessary data elements critical to the flow of distributed (not just decentralized) public key infrastructure. While this approach is already being used by IPID and muport we would encourage other DID methods to consider the merits of this approach. Discussion and Future Work In this brief paper, we have introduced the application of IPLD as a general pattern for representing the DID document and have highlighted its potential benefits and drawbacks and explained how it could be used across multiple DID method specifications. This technique enables a cost-effective and scalable solution for the creation and resolution of the data elements necessary to represent the cryptographic primitives to facilitate a distributed public key infrastructure. This model can be used to accelerate the adoption of truly self-sovereign digital identities. In the future, we hope to formalize this approach with additional stakeholders and standard bodies. We will also pursue adding @context as a SHOULD BE reserved attribute for IPLD and {\"/\" : \"<CID>\"} as valid syntax for JSON-LD. In order for IPLD to be more broadly adopted as a way of representing the DID document we will need to updated parsers to traverse and resolve IPLD links Finally, we look forward to the proof of spacetime that will be provided by Filecoin as an instrinsic anchoring mechanism that it will provide [5] . References [1] WebOfTrustInfo/rebooting-the-web-of-trust-fall2016 [2] https://www.w3.org/TR/json-ld/ [3] https://github.com/json-ld/json-ld.org/issues/547 [4] https://github.com/ipfs/ipfs/issues/36 [5] https://filecoin.io/filecoin.pdf","title":"IPLD as a general pattern for DID documents and Verifiable Claims"},{"location":"RWoT7/ipld-did/#ipld-as-a-general-pattern-for-did-documents-and-verifiable-claims","text":"By jonnycrunch , Anthony Ronning , Kim Duffy , Christian Lundkvist Rebooting the Web of Trust, Fall 2018","title":"IPLD as a general pattern for DID documents and Verifiable Claims"},{"location":"RWoT7/ipld-did/#abstract","text":"Since the emergence of the Decentralized Identifier (DID) specification at the Fall 2016 Rebooting the Web of Trust [1] , numerous DID method specifications have appeared. Each DID method specification defines how to resolve a cryptographically-tied DID document given a method-specific identifier. In this paper, we describe a way to represent the DID document as a content-addressed Merkle Directed Acyclic Graph (DAG) using Interplanetary Linked Data (IPLD) . This technique enables more cost-efficient, scaleable creation of DIDs and can be applied across different DID method specifications.","title":"Abstract"},{"location":"RWoT7/ipld-did/#why-ipld","text":"Content addressing through hashes has become a widely-used means of connecting data in distributed systems. IPLD is a way of representing hash-linked data to be used in content-addressed data retrieval systems such as IPFS . Other content that can be resolved using IPLD includes Git repositories and blockchains such as Bitcoin, Ethereum, and ZCash. IPLD enables creation of decentralized data-structures that are universally addressable, facilitating resolving content accross different protocols. It achieves this through an interoperable data model that represents various protocol formats. IPLD relies on Content Identifiers (CIDs) for content addressing. CIDs are a self-describing, flexible, and interoperable way of expressing cryptographic hashes. It uses several multiformats to achieve flexible self-description, namely multihash for hashes, multicodec for data content types, and multibase to represent the base encoding of the CID itself. This interoperability makes IPLD a valuable structure for the DID document that can be used across a variety of DID methods or distributed ledgers and ensures cryptographic validity of the DID document. In this paper, we demostrate how IPLD could be used as a general pattern for representing the DID document for numerous DID methods.","title":"Why IPLD"},{"location":"RWoT7/ipld-did/#ipld","text":"IPLD uses an abstract model for linking data via cryptographic hashes, which enables link traversal to the referenced data via path \"/\" notation. This path notation has its roots in the Linux Filesystem Hierarchy Standard . A link in IPLD is represented in JSON as a \"link object\" and uses the path syntax \"/\" as the key to the object that is followed by the CID of the link. For example: { \"/\" : \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" } This syntax for representing linked data can be expanded and used for other JSON structures. For instance, building on this example we can leverage JSON-LD semantics by inserting a @context to our IPLD object and apply the above as a link that resolves to the JSON-LD document that describes the attributes for our IPLD object. { \"@context\" : { \"/\" : \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" } } JSON-LD is a syntax to serialize Linked Data in JSON and provide semantics without the overhead of a large rdf model [2] . Since both IPLD and JSON-LD are 100% compatible with JSON, the large number of JSON parsers and libraries are already available. However, when using JSON-LD in a browser, it is impossible to discover the Base IRI after an http redirect ( see #316 ), and the content of the @context can potentially change over time [3] . Finally, since a URI depends upon the security of DNS, DNS spoofing/DNS poisoning could offer a simple attack vector. Essentially, without much effort, an attacker can adjust the cache of a DNS server and begin pointing traffic from 'schema.org' (or any other desired host) to anywhere else on the internet or local LAN. Given the critical nature of the JSON-LD @context resource, the attacker can make a fraudulent signature pass as being valid. Using IPLD, we can use the entire JSON data model and we can layer any JSON-LD on top of IPLD [4] . This will enable cryptographic guarantees to the authenticity of the JSON-LD schema and mitigate such an attack. One additional point to emphasize is that the content loaded into IPLD is serialized using Concise Binary Object Representation (CBOR) , allowing for deterministic representation and retrieval. Content stored on IPLD is cryptographically retrievable regardless of the domain. The content can be retrieved via numerous http gateways including: ipfs.io , ipfs.infura.io , and cloudflare-ipfs.com , as well as a locally hosted gateway or via the command-line ipfs application. Retrieval of IPLD content represented as dag-cbor from ipfs.io gateway and validating the content by reloading it into ipfs via command-line > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq | ipfs dag put outputs: zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq , thus validating the hash of the cid Retrieval of IPLD content represented as dag-cbor from ipfs command-line > ipfs dag get zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq outputs the original JSON-LD schema link .","title":"IPLD"},{"location":"RWoT7/ipld-did/#did-document-creation-and-updates-using-ipld","text":"At initialization of the DID document, the public/private keys pairs associated with the DID method specific identifier may or may not also have been generated. Additionally, the public/private key pair may or may not be the same as the master publishing key associated with the DID. In the example of the IPID DID method , the DID method specific identifier is the self-describing cryptographic multihash of the public key of an IPFS node and is different than the public/private key pair represented in the DID document. At present, RSA and Ed25519 elliptic curves are supported and there are plans to support secp256k1. Other DID methods that use IPLD to store the DID document may use alternative approaches to associate the DID document with the method specific identifier. For instance, in the muPort DID method, an Ethereum smart contract is used to point to the hash of the latest version of the DID document, and the document can be retrieved using the content-addressed IPFS system. The Sidetree DID method uses a scalable Merkle data structure that aggregates multiple DID document updates in a Merkle Tree and publishes the root hash in a blockchain such as Bitcoin or Ethereum. A sidetree node can use these data structures to aggregate the updates into a complete DID document Since the DID document needs to contain information about where to discover the latest version of the DID document (which is a requirement of DID resolution), in IPID the id field corresponds to the DID method specific identifier (i.e. did:ipid: ), but other DID methods specs can implement this in their own ways. While the exact update details are up to the DID method, Example 1 shows how the resulting structure might appear using IPID, which publishes the hash digest of the DID document to IPNS. Intrinsic to the integrity of any blockchain-based application is the existence of a previous field. In a blockchain data structure each block contains an ordered list of cyptographic transactions. A cryptographic reference to hash of the previous block is stored in the block header. Similarly, in this lightweight protocol based on IPLD, we have made use of the previous field with a cyptographic reference to the previous version of the DID document. This faciliatates traversal of the entire history of the DID document. One drawback to this approach is that this is a retrospective perspective. This highlights the importance of the id field, which allows an agent to follow the tip of the chain and at any point cryptographically resolve the latest version of the document. In IPID , associating the DID document with a DID is accomplished by cryptographically publishing the CID to the IPNS public key associated with the identity owner (DID method specific identifier). Any updates to the DID document are saved to IPLD and the resulting hash is published to IPNS cryptographically associating the new CID with the DID (for IPID this is the multihash of the public key). IPID uses a PubSub model for realtime updates to the DID. So far, on its own, this approach is not considered a comprehensive standalone blockchain solution. Most importantly: this does not faciliatate consensus of the document across peers, and timestamps are self attesting. The phrase \"microledger\" is often used to describe this approach. To overcome this shortcoming, the CID of the DID document can be anchored in a proof of existence smart contract (e.g. Truffle) created by the identity owner. Alternatively, cryptographic timestamping protocols such as openTimestamps (free) or Chainpoint could be utilized.","title":"DID Document Creation and Updates using IPLD"},{"location":"RWoT7/ipld-did/#examples","text":"Proof of Existence Smart Contract to anchor CID of a DID document pragma solidity ^0.4.23; contract ProofOfExistence { event ProofCreated( bytes indexed cid, bytes did ); struct Proof { bytes did; uint block; } address public owner; mapping (bytes => Proof) DIDbyCID; modifier onlyOwner() { require(msg.sender == owner); _; } constructor() public { owner = msg.sender; } function notarizeHash(bytes cid, bytes did) onlyOwner public { Proof proof proof.did = did proof.block = now DIDbyCID[cid] = proof; emit ProofCreated(cid, did); } function ProofRequest(bytes cid, bytes did) public view returns (Proof) { return DIDbyCID[cid]; } } This smart contract has the added benefit of listening to updates to the IPLD DID document via Ethereum events. Example 1: Demonstration of initialization of DID document { \"@context\": { \"/\": \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" }, \"created\": \"2018-12-01T03:00:00Z\", \"publicKey\": [ { \"curve\": \"ed25519\", \"expires\": \"2019-12-01T03:00:00Z\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\" } ], \"updated\": \"2018-12-01T03:00:01Z\" } Example 1 shows the DID document before publication and before a cryptographic signature has been added. Note that the id and signature fields are omitted as it is not associated yet with the DID method specific identifier. Unlike other DID methods, with the DID document being represented as IPLD, we can directly link the @context , which is also represented on IPLD as a resolvable CID cryptographic link. Notably absent is the previous field, as this is the genesis of the chain of objects that subsequent updates will reference (see below). This entire DID document when added to IPFS as IPLD has a CID of zdpuAqiExr6k4AbWF6BuGkgUbVMZ7jbJyNvRz9z9yyRBxosPi and will be used as the previous field in the subsequent updated DID document (Example 2). Example 2: Intermediate DID document updated with id field associating this DID document with a DID method specific identifier for future resolution { \"@context\": { \"/\": \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" }, \"authentication\": { \"publicKey\": [ \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\" ], \"type\": \"EdDsaSASignatureAuthentication2018\" }, \"created\": \"2018-12-01T03:00:00Z\", \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"previous\": { \"/\": \"zdpuAqiExr6k4AbWF6BuGkgUbVMZ7jbJyNvRz9z9yyRBxosPi\" }, \"proof\": { \"/\": \"z43AaGF42R2DXsU65bNnHRCypLPr9sg6D7CUws5raiqATVaB1jj\" }, \"publicKey\": [ { \"curve\": \"ed25519\", \"expires\": \"2019-12-01T03:00:00Z\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\" } ], \"updated\": \"2018-12-01T03:00:02Z\" } Example 2 shows the addition of the id field with the previous field linking to the hash of Example 1 Example 3: IPLD DID document updated with the addition of the signature field { \"@context\": { \"/\": \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" }, \"authentication\": { \"type\": \"EdDsaSASignatureAuthentication2018\", \"publicKey\": [ \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\" ] }, \"created\": \"2018-12-01T03:00:00Z\" , \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"previous\": { \"/\" : \"zdpuAosm9NYNW5kG2h3SBoCZz5DYqyTgf6qopkxpih5cFhqmU\" }, \"proof\" : { \"/\" : \"z43AaGF42R2DXsU65bNnHRCypLPr9sg6D7CUws5raiqATVaB1jj\" }, \"publicKey\": [ { \"curve\": \"ed25519\", \"expires\": \"2019-12-01T03:00:00Z\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\" } ], \"signature\": { \"created\": \"2018-12-01T03:00:02Z\", \"creator\": \"zdpuAohuM1RWMpzwQgWz5jXFCdPtz1rhD82fuZBauUDuRzknt/publicKey/0\", \"message\" : { \"/\" : \"zdpuAohuM1RWMpzwQgWz5jXFCdPtz1rhD82fuZBauUDuRzknt\" }, \"signatureValue\": \"o9r6LxgoGN8FoaeeUA6EdDcv12GvDzFEmCgjWzvpur2YSQyA8W2r0SSWUK+nH5tMqzaFLun6wwZ1Eot37amGDg==\", \"type\": \"ed25519Signature2018\" }, \"updated\": \"2018-12-01T03:00:00Z\" } Example 3 shows the final DID document after it is associated with the DID method specific identifier and signature. In this case, it was published to IPNS using the IPID method spec. Note that id field is now populated and the updated document was pushed to IPLD, resulting in a CID for the final document of zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP (link) . The chain of trust of the history of all edits can be done my simply following the previous link with the syntax zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP/previous and back to the original genesis of the document with a cid of zdpuAqiExr6k4AbWF6BuGkgUbVMZ7jbJyNvRz9z9yyRBxosPi with the syntax of zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP/previous/previous . Similarly, with this syntax the signature field links to the publicKey of the CID natively without the need of a referenced fragment and the message (payload) that was signed. Additionally, a proof field has been added which is itself CID link that resolves to a proof of existence smart contract on the Ethereum blockchain that resolves natively or can be externally validated. Retrieval of creator of the signature can be performed via any ipfs gateway > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP/publicKey/0 output: \"curve\":\"ed25519\",\"expires\":\"2019-12-01T03:00:00Z\",\"publicKeyBase64\":\"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\",\"type\":\"EdDsaPublicKey\"} Retrieval of the id of the creator can be performed via any ipfs gateway > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP/id output: \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\" Example 4: IPLD DID document updated after key rotation { \"@context\": { \"/\": \"zdpuAmoZixxJjvosviGeYcqduzDhSwGV2bL6ZTTXo1hbEJHfq\" }, \"authentication\": { \"type\": \"EdDsaSASignatureAuthentication2018\", \"publicKey\": [ \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\" ] }, \"created\": \"2018-12-01T03:00:00Z\" , \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"previous\": { \"/\": \"zdpuB1oR3vjYmkDc9ALfY7o6hSt1Hrg2ApXaYAFyiAW5E4NJP\" }, \"proof\": { \"/\": \"z43AaGF42R2DXsU65bNnHRCypLPr9sg6D7CUws5raiqATVaB1jj\" }, \"publicKey\": [ { \"curve\": \"ed25519\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\", \"status\": \"revoked\" }, { \"curve\": \"ed25519\", \"expires\": \"2021-07-08T16:02:20Z\", \"publicKeyBase64\": \"jGp4OT1GktrZMdrkOM+zj8iE1IqCiqg2iH+rUZ93jhE=\", \"type\": \"EdDsaPublicKey\" } ], \"signature\": { \"created\": \"2018-12-01T03:00:04Z\" , \"creator\": \"zdpuAt99xoa8i2BrcjwpY2H6ksXeaE28upionw2VuxRamBs6H/publicKey/1\", \"message\": { \"/\": \"zdpuAyvreXzQHqwv3rL8MaVPjNJjpLLa5Du3HcbpQL41XS35G\" }, \"signatureValue\": \"WDA3Dx7c+UWR37oglhkLNwxAbxXM4YbT7TpgmaCQ/rSqbtXgM3EpQ4mpkPXT5OBLH6bDai12Ank8SUHW47JxCQ==\", \"type\": \"ed25519Signature2018\" }, \"updated\": \"2018-12-01T03:00:04Z\" } Example 4 shows an updated DID document that has revoked key/0 and created a new key/1, which is used for signing the new DID document. The cid for this final document is zdpuAtrP6ZSDZj6izYQEbuUjuDRGHSa5L59BZDa1deRwAAZRQ and is used to publish and associate it with the DID.","title":"Examples"},{"location":"RWoT7/ipld-did/#use-with-pairwise-identifiers","text":"The IPLD pattern may also be a good fit to use for pairwise identifiers. A pairwise identifier is a DID that is meant to be used only with one other entity. The idea is that when setting up a pairwise DID you can do it by generating the DID document, and send the DID as well as the DID document to the counterparty. The counterparty can verify that the DID document hash is the DID. Then if the user decides to update the DID document (normally through the use of a digital signature from a \"management key\" specified in the DID document) they can just present the updated version of the DID document to the counterparty, who can then verify the signatures and store the updated document in a local database and not necessarily publish it publically. Even though IPFS could be used for content addressing there would not be a need to connect to a wider IPFS network. The \"ledger\" in this case could just be a simple database hosted by each of the counterparties or a private ledger shared between them.","title":"Use with Pairwise Identifiers"},{"location":"RWoT7/ipld-did/#benefits","text":"One large advantage of the IPLD approach described here is that the identity owner does not need to use a blockchain when initially creating an identity, thus making creation of identities fast and low cost (if not free). The DID and DID document will be cryptographically coupled by hashing. Only when the identity owner needs to anchor their DID document will they need a more costly tool such as a blockchain. In addition, IPLD is perfectly suited for pairwise DID, when the creation and sharing of a DID document could be cryptographically generated and shared between only two parties and NOT saved to a public ledger. Each party can simply add the resulting CID to a local database. Summary of the benefits include: Low cost / free scalable solution for DIDs Ability to use Bitcoin, OpenTimeStamps, ZCash, Ethereum and/or Ethereum smart contracts, or Github as an optional proof of existence Ability to use CBOR as a schemaless data model Cryptographic verifiable resolution of a document Ability to use microledgers Perfectly suited for pairwise DIDs Ability to self-host documents or pay through future services such as Filecoin Easily resolve previous versions of the DID by cryptographic linking to the previous CID Resolvable anywhere (local networks, Mars, online, etc.)","title":"Benefits"},{"location":"RWoT7/ipld-did/#drawbacks","text":"Not resolvable without hosting (This could be construed as a feature for pairwise DIDs) @context is not a reserved word in the IPLD specificatioin. { \"/\" : \"<CID>\" is not currently valid syntax for JSON-LD One additonal drawback to this approach results from the current DID draft specification, which designates publicKey as an array of objects; for example: { ... \"publicKey\": [{ \"id\": \"did:example:123456789abcdefghi#keys-1\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:example:123456789abcdefghi#keys-2\", \"type\": \"Ed25519VerificationKey2018\", \"owner\": \"did:example:pqrstuvwxyz0987654321\", \"publicKeyBase58\": \"H3C2AVvLMv6gmMNam3uVAjZpfkcJCwDwnZn6z3wXmqPV\" }, { \"id\": \"did:example:123456789abcdefghi#keys-3\", \"type\": \"Secp256k1VerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyHex\": \"02b97c30de767f084ce3080168ee293053ba33b235d7116a3263d29f1450936b71\" }], ... } The current approach is to iterate over each of the keys in this array to identify the key included by reference. Specifically, the algorithm to use when processing a publicKey property in a DID Document is: 1. Let value be the data associated with the publicKey property and initialize result to null. 2. If value is an object, the key material is embedded. Set result to value. 3. If value is a string, the key is included by reference. Assume value is a URL. 1. Dereference the URL and retrieve the publicKey properties associated with the URL (e.g., process the publicKey property at the top-level of the dereferenced document). 2. Iterate through each public key object. 1. If the id property of the object matches value, set result to the object. 4. If result does not contain at least the id, type, and owner properties as well as any mandatory public cryptographic material, as determined by the result's type property, throw an error. A better approach may be to name the key by name within the object of publicKey rather than as an array; for example: { ... \"publicKey\": { \"key-1\" : { \"curve\": \"ed25519\", \"publicKeyBase64\": \"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\", \"type\": \"EdDsaPublicKey\" }, \"key-2\" : { \"curve\": \"ed25519\", \"expires\": \"2021-07-08T16:02:20Z\", \"publicKeyBase64\": \"jGp4OT1GktrZMdrkOM+zj8iE1IqCiqg2iH+rUZ93jhE=\", \"type\": \"EdDsaPublicKey\" } }, ... } In IPLD, this will allow for a more straightforward reference without a convoluted algorithm to accomplish this. > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuAu918t7r8bv2wvuJiasiq78oDCbZ62ecZCj43oBWvspzr/publicKey/key-1 with output: {\"curve\":\"ed25519\",\"publicKeyBase64\":\"qmz7tpLNKKKdl7cD7PbejDiBVp7ONpmZbfmc7cEK9mg=\",\"type\":\"EdDsaPublicKey\"} This can be cryptographically linked back to the id with: > curl -s https://ipfs.io/api/v0/dag/get?arg=zdpuAu918t7r8bv2wvuJiasiq78oDCbZ62ecZCj43oBWvspzr/id","title":"Drawbacks"},{"location":"RWoT7/ipld-did/#conclusion","text":"IPLD provides a robust framework for a lightweight microledger that cryptographically resolves the necessary data elements critical to the flow of distributed (not just decentralized) public key infrastructure. While this approach is already being used by IPID and muport we would encourage other DID methods to consider the merits of this approach.","title":"Conclusion"},{"location":"RWoT7/ipld-did/#discussion-and-future-work","text":"In this brief paper, we have introduced the application of IPLD as a general pattern for representing the DID document and have highlighted its potential benefits and drawbacks and explained how it could be used across multiple DID method specifications. This technique enables a cost-effective and scalable solution for the creation and resolution of the data elements necessary to represent the cryptographic primitives to facilitate a distributed public key infrastructure. This model can be used to accelerate the adoption of truly self-sovereign digital identities. In the future, we hope to formalize this approach with additional stakeholders and standard bodies. We will also pursue adding @context as a SHOULD BE reserved attribute for IPLD and {\"/\" : \"<CID>\"} as valid syntax for JSON-LD. In order for IPLD to be more broadly adopted as a way of representing the DID document we will need to updated parsers to traverse and resolve IPLD links Finally, we look forward to the proof of spacetime that will be provided by Filecoin as an instrinsic anchoring mechanism that it will provide [5] .","title":"Discussion and Future Work"},{"location":"RWoT7/ipld-did/#references","text":"[1] WebOfTrustInfo/rebooting-the-web-of-trust-fall2016 [2] https://www.w3.org/TR/json-ld/ [3] https://github.com/json-ld/json-ld.org/issues/547 [4] https://github.com/ipfs/ipfs/issues/36 [5] https://filecoin.io/filecoin.pdf","title":"References"},{"location":"RWoT7/mental-models/","text":"Five Mental Models of Identity by Joe Andrieu (joe@legreq.com), Legendary Requirements; Nathan George (nathan@sovrin.org), Sovrin Foundation; Andrew Hughes (andrewhughes3000@gmail.com), ITIM Consulting; Christophe MacIntosh (cm@clearmatics.com), Clearmatics; and Antoine Rondelet (ar@clearmatics.com), Clearmatics Abstract Engineers of identity systems, both digital and non-digital, have assumptions and requirements that often lead to fundamentally different ideas about useful solutions. One\u2019s preferred use cases establish mental models tailored to those uses, which in turn shape discussion and engineering of identity systems. The differences between these mental models consistently cause confusion and disagreement when advocates of different models collaborate, often without the parties realizing that others may be speaking from a distinctly different, yet valid, notion of identity. Considering different mental models allows for constructive dialogue and reconciliation of requirements, creating opportunities to address a wider set of use cases and to build systems with better overall applicability and quality. We present five distinct mental models observed in conversations among technologists and laypeople when discussing identity. We then discuss observed patterns of discussion and design that result from the intersection of some pairs of mental models. Finally, we close with guidance for incorporating all five mental models when evaluating or designing any real-world or digital-identity system. We propose that understanding and considering these different mental models will result in more fruitful collaboration and ultimately in better identity systems. Background The authors have been participants, speakers, and organizers at identity-related professional gatherings for over a decade, including the Internet Identity Workshop[1], Rebooting the Web of Trust[2], ID2020[3], and EIC[4], to name just a few. They have held leadership roles in identity-related standards efforts at the W3C[5], IETF[6], Kantara Initiative[7], and ISO[8]. As active contributors to industry conversations about interoperable digital-identity systems, we have witnessed how different communities engage with emerging technologies to solve real-world identity problems. In these conversations we have seen participants consistently struggle to communicate clearly about \u201cidentity\u201d, \u201cdigital identity\u201d, and \u201cidentity systems\u201d. New participants often misconstrue established jargon used by well-versed veterans, who themselves tend to make several key assumptions. The motivation for simple and informal discourse leads to unrigorous language that makes it hard to establish a common understanding of identity. While simple and informal terms can communicate and share ideas, they risk introducing false understanding and confusion. Mistakes seep in around the edges as developers, regulators, decision makers, and end-users shift their use of language. What feels natural to one speaker may not be understood with the same nuance by listeners. What is intended to be specific and limited is often heard more broadly and generally. These informalities ultimately undermine the shared goals of building robust identity systems. As digital identity becomes increasingly relevant to businesses, governments, and society at large, it is vital that we find simpler, yet more rigorous, ways to communicate more clearly about identity. The ideas behind this paper were initially presented at Rebooting the Web of Trust V in Boston in 2017[9] as a brief one-hour discussion, followed by a session the next month at the 25th Internet Identity Workshop in Mountain View[10]. The initial discussion proposed three mental models. Since then, a fourth and fifth mental model have been incorporated based on feedback from fellow professionals. The first presentation of the current set of five mental models was submitted as an advanced reading topic paper [11] for the Rebooting the Web of Trust VII [12] workshop that was held in Toronto in September 2018. The same month, these mental models of identity were adopted as a key framework for the design workshop of the International Identity Summit[13] at the University of Washington. It was at Rebooting the Web of Trust VII that the current authors began collaborating on this paper. We look forward to receiving feedback and suggestions about this mental model approach and the five mental models we describe. Our hope is that the ideas we present will help identity professionals, developers, regulators, decision makers, and end-users communicate more clearly and effectively, and as a result create digital identity systems that better address the needs of all potential stakeholders. Mental Models Mental models are psychological representations of real, hypothetical, or imaginary situations [14]. In this paper, we are referring to the psychological representations people have for \u201cidentity\u201d. Our experiences have shown that different people approach identity in different ways, consistent with the mental models we discuss in this paper. From our observations, we have identified five distinct mental models for identity, each with its own framing, its own purpose, and its own defining question. These are five orthogonal mental models of identity. By understanding them, we can better understand how apparently disjoint ideas of identity relate to each other, enabling the discussion and engineering of better, more broadly useful, and more secure identity systems. These mental models are naturally emergent notions that arise from the needs people have for identity. While these models can be taught, our observations show that these models are more often argued for and assumed (with the exception of the Attribute mental model, which is enshrined in an international standard). This is the subtle nuance of language: when we have internalized a particular notion we generally assume that notion is what the concept means, even though a more rigorous understanding would recognize that the notion is merely what the concept means to us . It is exactly this implicit acceptance of \u201cwhat identity means\u201d that makes it so hard to communicate with people who have internalized a different notion of identity. The different purposes we have for these identity processes and assets - why we use identity in the first place - shapes our internal mental models, which in turn shapes the conversations we engage in and the kinds of solutions we build. While these different mental models are orthogonal and distinct, we believe that they are all, in fact, valid notions for identity. By recognizing these \u201calternative\u201d mental models, people will be able to communicate more clearly and collaborate more effectively to build identity systems that resolve the identity needs of all relevant stakeholders. Identity In this paper we use the definition of functional identity [15] when we refer to \u201cidentity\u201d. As a consequence, \u201cidentity\u201d is how we recognize , remember , and respond to specific people and things. The processes of identity system s acquire, correlate, apply, reason over, and govern the information assets of subjects, identifiers, attributes, raw data, and context . If a system recognizes , remembers , or responds to specific people or things, it is using identity in some form - even if those identities are neither linked to traditional identifying information such as names and addresses, nor represented in official documents like a birth certificate or passport. This paper addresses the search for a notionally objective \u201ctruth\u201d of the identity of a specific person or thing. Recognition of a subject depends on evaluating available evidence in search of that truth. The evidence may be flawed and the interpretation may be flawed, but the fundamental goal of each mental model is to answer truthfully its core question. We may not be able to determine that objective truth with 100% certainty, but we must try. Every identity system attempts to resolve the question of truth to its best ability based on the fundamental needs of its stakeholders. We distinguish essential truth of an identity from the means, quality, and confidence of the subjective conclusion derived from the available evidence. The question we are seeking to answer in this paper is the following: \u201cWhen we are evaluating the evidence, what are we trying to determine?\u201d . Each mental model approaches this differently. Five Mental Models Each mental model takes a different approach to recognizing, remembering, and responding to the \u201cWho\u201d. How do you determine uniqueness? How do you recognize a candidate individual? How do you record and correlate your observations about a subject? How do you correlate external assertions to subjects? And how do you apply that knowledge in interactions with the subject or with others? Answers to these questions vary not just based on different technologies and techniques, bust also based on the mental model brought to bear on the question of identity. Technical solutions optimized for different mental models answer different questions of identity. Failing to account for these different questions can lead to systems insufficient to the task, creating end-user frustration and system insecurities. The models we have identified may not be the complete set, and the language we have used here may not be the best \u201ccanonical\u201d definition. However, these models capture the breadth of conversations we have seen over time among and between identity professionals and laypeople. Space-time The space-time mental model sees identity as resolving the question of the physical continuity of an entity through space and time. Does the physical body under evaluation have a continuous link through space and time to a known entity? This is the dominant perspective in real-world security and is advocated by many for online security. The space-time mental model is necessary when enforcement targets the physical body. We put bodies in jail. We keep bodies out of restricted areas and off airplanes. If we are going to restrict the liberty of, apply harm to, or even protect a person, it is vital to know the target is in fact the right physical body, based on its literal physical continuity through space and time with the physical body we intend to restrict, harm, or protect. Presentation The presentation mental model sees identity as how we present ourselves to society. This is the mental model behind Vendor Relationship Management [16], user-centric identity, and self-sovereign identity. Is this how the subject chooses to be known? Whether you are gay, a Republican, or a gay Republican, it is your choice how you present yourself to the world. This mental model sees individually expressed identity as fundamental to privacy, self-determination, free speech, and a free society. Adherents advocate for technical and regulatory systems that allow people to fulfill their own self-actualization, to become who they want to be, rather than simply playing a role assigned by someone else. Attribute The attribute mental model sees identity as the set of attributes related to an entity as recorded in a specific system. Enshrined in ISO/IEC 24760-1, an international standard for identity management, this mental model is the primary focus for many engineers. Who is this data about? ISO/IEC 24760-1 limits the model of identity to correlated data within a single system, making such systems simpler and easier to build. It draws a bright line for engineers to focus on the accepted facts and information within the system they are developing, insofar as that data are related to specific individuals or entities. This model ignores the consequences of common identification across different systems for the sake of simplicity and feature management. It also ignores how that data came to be, what is done with it, and how it is governed. In effect, it focuses on the bits and bytes and not on the processes involved. It is the simplest model for engineering a system; in part, because of that, it is the only mental model that has a formal definition as an international standard. Relationship The relationship mental model sees identity emerging through interactions and relationships with others. Our identity is not about what we are in isolation from others, but is rather defined by the relationships we have. This is the fundamental model in the South African idea of \u201cUbuntu\u201d, meaning \u201cI am because we are.\u201d How is this person related? We are our children\u2019s parents. Our teachers\u2019 students. Our ex-spouse\u2019s ex-husband. We are the guy who helped a friend move, the kind neighbor who let a friend borrow a cup of sugar, the jerk in 4th grade who kept pulling a girl\u2019s pigtails. Or as Senator McCarthy famously asked \u201cAre you now, or have you ever been, a member of the communist party? \u201d [17] The relationship of membership in the communist party was used by McCarthy to argue that individuals were enemies of the state. Our appearance changes. Our age changes. Our height and weight change. Every cell in our body goes through a life cycle of birth, life, and death. We literally are not the same set of physical cells we were just a year ago. Presumed static traits often aren\u2019t. However, relationships often hold true across contexts when our physical traits do not. Relationships are given and earned through interactions with others, never to be fully perceived by any one observer, never to be fully known or captured at any point in time. The relationship mental model is the mental model that most directly embraces the fundamental fluidity of identity, rejecting the objective assessment of static, categorical identity for the emergent societal bonds that define us within our communities. Capability The capability mental model pragmatically defines identity in terms of an individual\u2019s actual capability to perform some task, including their physical ability now, in the past, or in the future. It is the inevitable approach for anyone in an emergency. What can a subject actually do? Capabilities combine both the ability and the will to execute. The question of an identity\u2019s capability is truly a test of one\u2019s willingness and ability to take action. Sometimes what you really need is a pilot[18]. It does not matter if they are licensed or if they physically have a license. What matters is whether or not they can safely land that plane. If they can, that\u2019s a defining element of who they are, just as much as if they can\u2019t. You are a killer if you can actually pull the trigger when faced with a situation that demands it. In theory, anyone in good health is able to physically pull a trigger, but not everyone can actually do it when it needs to be done. If you are capable of doing so, this very capability defines part of who you are. To spend bitcoin, you must be able to generate the correct ScriptSig [19], one that returns TRUE when evaluated against the scriptPubkey [20] of an input transaction. Bitcoin doesn\u2019t care what your name is or what is recorded in some database. If you can post a transaction that produces the right output, you can spend the coins. Included in this mental model are demonstrable physical traits, such as the test for being \u201cAt least this tall\u201d to ride a roller coaster. It also includes historically recognized abilities even though they may no longer apply, e.g., \u201cShe\u2019s a murderer,\u201d as well as speculation of future capability \u201cHe\u2019ll never last, he\u2019s a quitter.\u201d Ultimately, you can only truly know a capability by testing it, and that test may prove valid only in a highly specific context, like \u201cI didn\u2019t know I could do it, but when I saw my child in harms way, I had to stop that car.\u201d Intersections of mental models: Patterns and Pitfalls When two people discuss identity with different mental models, the conversation inevitably focuses on the intersection between those models, sometimes without either party realizing they are coming from different perspectives. In the classic science fiction story Flatland: A Romance of Many Dimensions by Edwin Abbot [21], the author uses a fictional world to explain how two-dimensional creatures would perceive three-dimensional objects as they travel through the plane of their two-dimensional world. The hero of the story realizes that these objects are, in fact, three dimensional, and struggles to explain his insight to his fellow two-dimensional beings. While they all think he is crazy, the hero eventually prevails. The romance is in the hero\u2019s struggle. If we think of our mental models as two-dimensional planes, and accept that most of us approach identity from a single perspective, then we can consider how the intersection of these planes shape conversations. The intersection of two planes is a line, which in both planes and, in our metaphor, is fully understandable from within each model\u2019s two-dimensional context. That is, people speaking from each mental model can fully perceive, understand, and discuss the line. In contrast, they would each struggle to understand concepts outside their mental model, just as people living in a single plane would struggle to understand the idea of things outside that plane. The following sections provide a few examples of intersections between mental models. Intersection between Space-time and Attribute When space-time-minded people discuss identity with attribute-minded people, the conversation often leads to two key questions: *\u201cWhat are the minimum attributes to identify a particular (physical) individual?\u201d, and * \u201cGiven the data we have, how can we identify particular individuals?\u201d Because all modern information systems rely on data, it is inevitable that space-time-minded applications intersect with the attribute mental model. In naive implementations, this can lead to oversampling in an attempt to increase certainty about a physical person, often referred to as surveillance. If you record all the data possible about every observable interaction throughout an individual\u2019s day, you can dramatically increase the likelihood that the system will correctly identify that the person from the start of the day is actually, physically the same person at the end of the day. If prison records the actions of a prisoner 24 hours/day, 7 days/week, the prison can be assured that the physical body being imprisoned has not escaped the cell without their knowledge. This intersection can also lead to overconfidence in the conclusions about a physical person based on attributes stored in a database, occasionally referred to as the \u201cTyranny of data\u201d. Humorous consequences can be seen in fiction when an official record indicates someone is dead, when clearly they are not. Less humorous consequences occur, for example, when subsidies people need to live are denied as a result of errors in the Aadhaar identity service [22]. Intersection between Capability and Relationship Capabilities and relationship mental models intersect when a capability implies a relationship or vice versa. For example, US military academies offer guaranteed admission for children of Congressional Medal of Honor recipients, the nation\u2019s highest honor. The reasoning is that, whether genetic or cultural, the child of a Medal of Honor winner has at least as high potential to become an outstanding officer as candidates vetted by congressional representatives (the primary route for admissions). We also see this in caste systems, including in European aristocracy and Indian society. The presumption is that your familial relationships are suitable indicators of your ability to perform certain societal roles. As a mythical example, King Arthur\u2019s ability to remove the sword from the stone proved his lineage and right to govern the kingdom. His capability re-established his relationship with the kingdom. Intersection between Presentation and Relationship An identity does not belong to one entity alone. It is built on the perception the observer has of the subject, based on how the subject presents themselves combined with other information. That information is applied by the observer on their own judgment, which may change, distort or recast the the presentation in non-obvious ways. While a subject may present however they want, different observers will interpret those presentations differently based on their relationship with the subject. How you react when a friend or lover claims a privilege is often different than if a rival or enemy claims the same. As relationships move beyond bilateral interactions to societal engagement, social norms dictate what information sharing is expected, conventional, and allowed. Social agreements compel the disclosure of certain data, such as giving a driver\u2019s license to a law enforcement officer or giving a passport to cross a border. They may also lead one to take a topic out of civil discourse altogether; consider the quaint but still relevant idea that it is rude to ask a woman about her age, a man about his income, or a gender-neutral individual about their Y chromosome. These social norms -- based on our relationships with others -- shape a framework for presenting information that both enables and restricts freedom of presentation [23]. Finally, self-asserting information may be enough for relying on certain types of data (\u201cthis is my favorite food\u201d), but not others (\u201cI am not a crook\u201d). Many systems build trust or authority by involving other parties, using systems of credentials to establish a sense of truth based on the assertions of known actors. This information is projected from the source, filtered and presented by the holder, and interpreted through the lens of the relying party. This shared meaning is based on multiple relationships and as such, is beyond the sole control of any of these parties. The relationships enable and transform the presentations. This is the architecture embodied in Verifiable Credentials [24] as well as physical credentials such as driver\u2019s licenses and passports. Intersection between Presentation and Attribute A persona represents, in data, information about a subject, as presented by that subject to others. It is a subset of the information that a subject could present, and independent of what the recipient may have recorded about the subject. It is the attribute-centric vehicle for presenting identity information. It is neither the totality of the subject\u2019s information, nor is it the only data known about them. An individual may present different persona in different context, sharing different information with different recipients or at different times. Self-asserted claims are fully in the intersection between presentation and attributes. When an individual self-asserts a statement about themselves, they are presenting an attribute on their own authority. Intersection between Attribute and Relationship We all interpret data differently based on the relationship we have with its source. If one of our siblings tells us something, we take it differently than if our parent tells us the same thing. A teacher informing us about our children\u2019s performance is perceived differently than that a child\u2019s self-reported assertion. Moreover, social constructs shape the way entities deal with matters of fact . The way to handle time, for example, differs tremendously depending on where you originally come from. While several calendars exist around the world, a person\u2019s date of birth is a matter of fact. A person is born at a specific moment in time. Nevertheless, the way to construct and attribute this moment in time as a birth date is a social construct. How we record and interpret that moment in time depends on the social context of those recording it. It is often assumed that entities with similar attributes are likely to establish relationships, i.e., \u201cbirds of a feather flock together\u201d. Dating or matchmaking services often try to create relationships between individuals based on their affinity as measured by their common attributes. Of course, such matchmaking ignores the equally resonant wisdom that \u201copposites attract\u201d. Both, however, are examples where attributes are used to evaluate the quality of a potential relationship. Data analytics do something similar when evaluating known attributes of individuals to seek out patterns and relationships. Sometimes this data is used to infer a common group dynamic related to life events (a belief the subject is pregnant) and buying patterns (the subject is likely to buy a car) or even explicit relationships (the subject is a supporter or member of a terrorist organization). In short, one can use attributes to infer relationships (and vice-versa) and relationships affect our interpretation of attributes. Recommendation and Conclusion When collaborating with others, consider multiple mental models for better communication and better identity systems. Whatever your own goals, we believe you are more likely to achieve them if you can communicate clearly in terms others understand and can incorporate the needs of others into your own work. None of us have a complete picture of the universe. None of us have a monopoly on the truth. The best identity solutions will come from a frank and open engagement where every individual\u2019s, and every organization\u2019s, needs are heard and considered. Implementations will necessarily be a collection of trade-offs between security and freedom, features and costs, and the needs of those building the systems and the needs of those using them. We believe the best way to build the most effective systems is to thoroughly understand where your collaborators are coming from and what they need from identity and then to work together to devise mechanisms that best achieve the common goals of all stakeholders. In short, be excellent to each other. Be open to new ideas and seek first to understand, then incorporate the perspectives of others. What might seem \u201cwrong\u201d at first hearing may turn out to be a vital component simply unperceivable from your own initial mental model. Working together is our best chance at getting it right. References [1] \u201cInternet Identity Workshop\u201d, https://internetidentityworkshop.com (Accessed March 23, 2019) [2] \u201cRebooting the Web of Trust\u201d, http://weboftrust.info (Accessed March 23, 2019) [3] \u201cID2020\u201d, https://id2020.org/ (Accessed March 23, 2019) [4] \u201cEuropean Identity & Cloud Conference\u201d, https://www.kuppingercole.com/events/eic2019 (Accessed March 23, 2019) [5] \u201cWorld Wide Web Consortium\u201d, https://www.w3.org/ (Accessed March 23, 2019) [6] \u201cInternet Engineering Task Force\u201d, https://www.ietf.org/ (Accessed March 23, 2019) [7] \u201cKantara Initiative\u201d, https://kantarainitiative.org/ (Accessed March 23, 2019) [8] \u201cInternational Organization for Standardization\u201d, https://www.iso.org/home.html (Accessed March 23, 2019) [9] \u201cRebooting the Web of Trust V\u201d, https://github.com/WebOfTrustInfo/rwot5-boston (Accessed March 23, 2019) [10] \u201cInternet Identity Workshop 25\u201d, PDF of Proceedings, https://github.com/windley/IIW_homepage/raw/gh-pages/assets/proceedings/IIWXXV_Book_of_Proceedings.pdf (Accessed March 23, 2019) [11] \u201cFive Mental Models of Identity \u201d, https://github.com/WebOfTrustInfo/rwot7-toronto/blob/master/topics-and-advance-readings/five-mental-models-of-identity.md (Accessed March 23, 2019) [12] \u201cRebooting the Web of Trust VII\u201d, https://github.com/WebOfTrustInfo/rwot7-toronto (Accessed March 23, 2019) [13] \u201cInternational Identity Summit\u201d, http://depts.washington.edu/uwconf/wordpress/idsummit/ (Accessed March 23, 2019) [14] \u201cWhat are mental models\u201d, http://mentalmodels.princeton.edu/about/what-are-mental-models/ (Accessed January 24, 2019). [15] \u201cA Primer on Functional Identity\u201d, https://github.com/WebOfTrustInfo/rwot7-toronto/blob/master/topics-and-advance-readings/functional-identity-primer.md (Accessed March 21, 2019) [16] \u201cVendor Relationship Management\u201d, https://en.wikipedia.org/wiki/Vendor_relationship_management (Accessed March 21, 2019) [17] \u201cJoseph McCarthy\u201d, https://en.wikipedia.org/wiki/Joseph_McCarthy (Accessed March 21, 2019) [18] Airplane. Clip from YouTube https://www.youtube.com/watch?v=KSQyW_l8OgE (Accessed March 23, 2019) [19] \u201cSignature Script, ScriptSig\u201d, https://bitcoin.org/en/glossary/signature-script (Accessed March 21, 2019) [20] \u201cPubkey Script, ScriptPubKey\u201d, https://bitcoin.org/en/glossary/pubkey-script (Accessed March 21, 2019) [21] \u201cFLATLAND: A Romance of Many Dimensions\u201d, https://ned.ipac.caltech.edu/level5/Abbott/paper.pdf (Accessed March 21, 2019) [22] \u201cUnique Identification Authority of India\u201d, https://uidai.gov.in/ (Accessed March 21, 2019) [23] \u201cOverton window\u201d https://en.wikipedia.org/wiki/Overton_window (Accessed March 21, 2019) [24] \u201cVerifiable Credentials Data Model 1.0\u201d, https://w3c.github.io/vc-data-model/ (Accessed March 21, 2019)","title":"Five Mental Models of Identity"},{"location":"RWoT7/mental-models/#five-mental-models-of-identity","text":"","title":"Five Mental Models of Identity"},{"location":"RWoT7/mental-models/#by-joe-andrieu-joelegreqcom-legendary-requirements-nathan-george-nathansovrinorg-sovrin-foundation-andrew-hughes-andrewhughes3000gmailcom-itim-consulting-christophe-macintosh-cmclearmaticscom-clearmatics-and-antoine-rondelet-arclearmaticscom-clearmatics","text":"","title":"by Joe Andrieu (joe@legreq.com), Legendary Requirements; Nathan George (nathan@sovrin.org), Sovrin Foundation; Andrew Hughes (andrewhughes3000@gmail.com), ITIM Consulting; Christophe MacIntosh (cm@clearmatics.com), Clearmatics; and Antoine Rondelet (ar@clearmatics.com), Clearmatics"},{"location":"RWoT7/mental-models/#abstract","text":"Engineers of identity systems, both digital and non-digital, have assumptions and requirements that often lead to fundamentally different ideas about useful solutions. One\u2019s preferred use cases establish mental models tailored to those uses, which in turn shape discussion and engineering of identity systems. The differences between these mental models consistently cause confusion and disagreement when advocates of different models collaborate, often without the parties realizing that others may be speaking from a distinctly different, yet valid, notion of identity. Considering different mental models allows for constructive dialogue and reconciliation of requirements, creating opportunities to address a wider set of use cases and to build systems with better overall applicability and quality. We present five distinct mental models observed in conversations among technologists and laypeople when discussing identity. We then discuss observed patterns of discussion and design that result from the intersection of some pairs of mental models. Finally, we close with guidance for incorporating all five mental models when evaluating or designing any real-world or digital-identity system. We propose that understanding and considering these different mental models will result in more fruitful collaboration and ultimately in better identity systems.","title":"Abstract"},{"location":"RWoT7/mental-models/#background","text":"The authors have been participants, speakers, and organizers at identity-related professional gatherings for over a decade, including the Internet Identity Workshop[1], Rebooting the Web of Trust[2], ID2020[3], and EIC[4], to name just a few. They have held leadership roles in identity-related standards efforts at the W3C[5], IETF[6], Kantara Initiative[7], and ISO[8]. As active contributors to industry conversations about interoperable digital-identity systems, we have witnessed how different communities engage with emerging technologies to solve real-world identity problems. In these conversations we have seen participants consistently struggle to communicate clearly about \u201cidentity\u201d, \u201cdigital identity\u201d, and \u201cidentity systems\u201d. New participants often misconstrue established jargon used by well-versed veterans, who themselves tend to make several key assumptions. The motivation for simple and informal discourse leads to unrigorous language that makes it hard to establish a common understanding of identity. While simple and informal terms can communicate and share ideas, they risk introducing false understanding and confusion. Mistakes seep in around the edges as developers, regulators, decision makers, and end-users shift their use of language. What feels natural to one speaker may not be understood with the same nuance by listeners. What is intended to be specific and limited is often heard more broadly and generally. These informalities ultimately undermine the shared goals of building robust identity systems. As digital identity becomes increasingly relevant to businesses, governments, and society at large, it is vital that we find simpler, yet more rigorous, ways to communicate more clearly about identity. The ideas behind this paper were initially presented at Rebooting the Web of Trust V in Boston in 2017[9] as a brief one-hour discussion, followed by a session the next month at the 25th Internet Identity Workshop in Mountain View[10]. The initial discussion proposed three mental models. Since then, a fourth and fifth mental model have been incorporated based on feedback from fellow professionals. The first presentation of the current set of five mental models was submitted as an advanced reading topic paper [11] for the Rebooting the Web of Trust VII [12] workshop that was held in Toronto in September 2018. The same month, these mental models of identity were adopted as a key framework for the design workshop of the International Identity Summit[13] at the University of Washington. It was at Rebooting the Web of Trust VII that the current authors began collaborating on this paper. We look forward to receiving feedback and suggestions about this mental model approach and the five mental models we describe. Our hope is that the ideas we present will help identity professionals, developers, regulators, decision makers, and end-users communicate more clearly and effectively, and as a result create digital identity systems that better address the needs of all potential stakeholders.","title":"Background"},{"location":"RWoT7/mental-models/#mental-models","text":"Mental models are psychological representations of real, hypothetical, or imaginary situations [14]. In this paper, we are referring to the psychological representations people have for \u201cidentity\u201d. Our experiences have shown that different people approach identity in different ways, consistent with the mental models we discuss in this paper. From our observations, we have identified five distinct mental models for identity, each with its own framing, its own purpose, and its own defining question. These are five orthogonal mental models of identity. By understanding them, we can better understand how apparently disjoint ideas of identity relate to each other, enabling the discussion and engineering of better, more broadly useful, and more secure identity systems. These mental models are naturally emergent notions that arise from the needs people have for identity. While these models can be taught, our observations show that these models are more often argued for and assumed (with the exception of the Attribute mental model, which is enshrined in an international standard). This is the subtle nuance of language: when we have internalized a particular notion we generally assume that notion is what the concept means, even though a more rigorous understanding would recognize that the notion is merely what the concept means to us . It is exactly this implicit acceptance of \u201cwhat identity means\u201d that makes it so hard to communicate with people who have internalized a different notion of identity. The different purposes we have for these identity processes and assets - why we use identity in the first place - shapes our internal mental models, which in turn shapes the conversations we engage in and the kinds of solutions we build. While these different mental models are orthogonal and distinct, we believe that they are all, in fact, valid notions for identity. By recognizing these \u201calternative\u201d mental models, people will be able to communicate more clearly and collaborate more effectively to build identity systems that resolve the identity needs of all relevant stakeholders.","title":"Mental Models"},{"location":"RWoT7/mental-models/#identity","text":"In this paper we use the definition of functional identity [15] when we refer to \u201cidentity\u201d. As a consequence, \u201cidentity\u201d is how we recognize , remember , and respond to specific people and things. The processes of identity system s acquire, correlate, apply, reason over, and govern the information assets of subjects, identifiers, attributes, raw data, and context . If a system recognizes , remembers , or responds to specific people or things, it is using identity in some form - even if those identities are neither linked to traditional identifying information such as names and addresses, nor represented in official documents like a birth certificate or passport. This paper addresses the search for a notionally objective \u201ctruth\u201d of the identity of a specific person or thing. Recognition of a subject depends on evaluating available evidence in search of that truth. The evidence may be flawed and the interpretation may be flawed, but the fundamental goal of each mental model is to answer truthfully its core question. We may not be able to determine that objective truth with 100% certainty, but we must try. Every identity system attempts to resolve the question of truth to its best ability based on the fundamental needs of its stakeholders. We distinguish essential truth of an identity from the means, quality, and confidence of the subjective conclusion derived from the available evidence. The question we are seeking to answer in this paper is the following: \u201cWhen we are evaluating the evidence, what are we trying to determine?\u201d . Each mental model approaches this differently.","title":"Identity"},{"location":"RWoT7/mental-models/#five-mental-models","text":"Each mental model takes a different approach to recognizing, remembering, and responding to the \u201cWho\u201d. How do you determine uniqueness? How do you recognize a candidate individual? How do you record and correlate your observations about a subject? How do you correlate external assertions to subjects? And how do you apply that knowledge in interactions with the subject or with others? Answers to these questions vary not just based on different technologies and techniques, bust also based on the mental model brought to bear on the question of identity. Technical solutions optimized for different mental models answer different questions of identity. Failing to account for these different questions can lead to systems insufficient to the task, creating end-user frustration and system insecurities. The models we have identified may not be the complete set, and the language we have used here may not be the best \u201ccanonical\u201d definition. However, these models capture the breadth of conversations we have seen over time among and between identity professionals and laypeople.","title":"Five Mental Models"},{"location":"RWoT7/mental-models/#space-time","text":"The space-time mental model sees identity as resolving the question of the physical continuity of an entity through space and time. Does the physical body under evaluation have a continuous link through space and time to a known entity? This is the dominant perspective in real-world security and is advocated by many for online security. The space-time mental model is necessary when enforcement targets the physical body. We put bodies in jail. We keep bodies out of restricted areas and off airplanes. If we are going to restrict the liberty of, apply harm to, or even protect a person, it is vital to know the target is in fact the right physical body, based on its literal physical continuity through space and time with the physical body we intend to restrict, harm, or protect.","title":"Space-time"},{"location":"RWoT7/mental-models/#presentation","text":"The presentation mental model sees identity as how we present ourselves to society. This is the mental model behind Vendor Relationship Management [16], user-centric identity, and self-sovereign identity. Is this how the subject chooses to be known? Whether you are gay, a Republican, or a gay Republican, it is your choice how you present yourself to the world. This mental model sees individually expressed identity as fundamental to privacy, self-determination, free speech, and a free society. Adherents advocate for technical and regulatory systems that allow people to fulfill their own self-actualization, to become who they want to be, rather than simply playing a role assigned by someone else.","title":"Presentation"},{"location":"RWoT7/mental-models/#attribute","text":"The attribute mental model sees identity as the set of attributes related to an entity as recorded in a specific system. Enshrined in ISO/IEC 24760-1, an international standard for identity management, this mental model is the primary focus for many engineers. Who is this data about? ISO/IEC 24760-1 limits the model of identity to correlated data within a single system, making such systems simpler and easier to build. It draws a bright line for engineers to focus on the accepted facts and information within the system they are developing, insofar as that data are related to specific individuals or entities. This model ignores the consequences of common identification across different systems for the sake of simplicity and feature management. It also ignores how that data came to be, what is done with it, and how it is governed. In effect, it focuses on the bits and bytes and not on the processes involved. It is the simplest model for engineering a system; in part, because of that, it is the only mental model that has a formal definition as an international standard.","title":"Attribute"},{"location":"RWoT7/mental-models/#relationship","text":"The relationship mental model sees identity emerging through interactions and relationships with others. Our identity is not about what we are in isolation from others, but is rather defined by the relationships we have. This is the fundamental model in the South African idea of \u201cUbuntu\u201d, meaning \u201cI am because we are.\u201d How is this person related? We are our children\u2019s parents. Our teachers\u2019 students. Our ex-spouse\u2019s ex-husband. We are the guy who helped a friend move, the kind neighbor who let a friend borrow a cup of sugar, the jerk in 4th grade who kept pulling a girl\u2019s pigtails. Or as Senator McCarthy famously asked \u201cAre you now, or have you ever been, a member of the communist party? \u201d [17] The relationship of membership in the communist party was used by McCarthy to argue that individuals were enemies of the state. Our appearance changes. Our age changes. Our height and weight change. Every cell in our body goes through a life cycle of birth, life, and death. We literally are not the same set of physical cells we were just a year ago. Presumed static traits often aren\u2019t. However, relationships often hold true across contexts when our physical traits do not. Relationships are given and earned through interactions with others, never to be fully perceived by any one observer, never to be fully known or captured at any point in time. The relationship mental model is the mental model that most directly embraces the fundamental fluidity of identity, rejecting the objective assessment of static, categorical identity for the emergent societal bonds that define us within our communities.","title":"Relationship"},{"location":"RWoT7/mental-models/#capability","text":"The capability mental model pragmatically defines identity in terms of an individual\u2019s actual capability to perform some task, including their physical ability now, in the past, or in the future. It is the inevitable approach for anyone in an emergency. What can a subject actually do? Capabilities combine both the ability and the will to execute. The question of an identity\u2019s capability is truly a test of one\u2019s willingness and ability to take action. Sometimes what you really need is a pilot[18]. It does not matter if they are licensed or if they physically have a license. What matters is whether or not they can safely land that plane. If they can, that\u2019s a defining element of who they are, just as much as if they can\u2019t. You are a killer if you can actually pull the trigger when faced with a situation that demands it. In theory, anyone in good health is able to physically pull a trigger, but not everyone can actually do it when it needs to be done. If you are capable of doing so, this very capability defines part of who you are. To spend bitcoin, you must be able to generate the correct ScriptSig [19], one that returns TRUE when evaluated against the scriptPubkey [20] of an input transaction. Bitcoin doesn\u2019t care what your name is or what is recorded in some database. If you can post a transaction that produces the right output, you can spend the coins. Included in this mental model are demonstrable physical traits, such as the test for being \u201cAt least this tall\u201d to ride a roller coaster. It also includes historically recognized abilities even though they may no longer apply, e.g., \u201cShe\u2019s a murderer,\u201d as well as speculation of future capability \u201cHe\u2019ll never last, he\u2019s a quitter.\u201d Ultimately, you can only truly know a capability by testing it, and that test may prove valid only in a highly specific context, like \u201cI didn\u2019t know I could do it, but when I saw my child in harms way, I had to stop that car.\u201d","title":"Capability"},{"location":"RWoT7/mental-models/#intersections-of-mental-models-patterns-and-pitfalls","text":"When two people discuss identity with different mental models, the conversation inevitably focuses on the intersection between those models, sometimes without either party realizing they are coming from different perspectives. In the classic science fiction story Flatland: A Romance of Many Dimensions by Edwin Abbot [21], the author uses a fictional world to explain how two-dimensional creatures would perceive three-dimensional objects as they travel through the plane of their two-dimensional world. The hero of the story realizes that these objects are, in fact, three dimensional, and struggles to explain his insight to his fellow two-dimensional beings. While they all think he is crazy, the hero eventually prevails. The romance is in the hero\u2019s struggle. If we think of our mental models as two-dimensional planes, and accept that most of us approach identity from a single perspective, then we can consider how the intersection of these planes shape conversations. The intersection of two planes is a line, which in both planes and, in our metaphor, is fully understandable from within each model\u2019s two-dimensional context. That is, people speaking from each mental model can fully perceive, understand, and discuss the line. In contrast, they would each struggle to understand concepts outside their mental model, just as people living in a single plane would struggle to understand the idea of things outside that plane. The following sections provide a few examples of intersections between mental models.","title":"Intersections of mental models: Patterns and Pitfalls"},{"location":"RWoT7/mental-models/#intersection-between-space-time-and-attribute","text":"When space-time-minded people discuss identity with attribute-minded people, the conversation often leads to two key questions: *\u201cWhat are the minimum attributes to identify a particular (physical) individual?\u201d, and * \u201cGiven the data we have, how can we identify particular individuals?\u201d Because all modern information systems rely on data, it is inevitable that space-time-minded applications intersect with the attribute mental model. In naive implementations, this can lead to oversampling in an attempt to increase certainty about a physical person, often referred to as surveillance. If you record all the data possible about every observable interaction throughout an individual\u2019s day, you can dramatically increase the likelihood that the system will correctly identify that the person from the start of the day is actually, physically the same person at the end of the day. If prison records the actions of a prisoner 24 hours/day, 7 days/week, the prison can be assured that the physical body being imprisoned has not escaped the cell without their knowledge. This intersection can also lead to overconfidence in the conclusions about a physical person based on attributes stored in a database, occasionally referred to as the \u201cTyranny of data\u201d. Humorous consequences can be seen in fiction when an official record indicates someone is dead, when clearly they are not. Less humorous consequences occur, for example, when subsidies people need to live are denied as a result of errors in the Aadhaar identity service [22].","title":"Intersection between Space-time and Attribute"},{"location":"RWoT7/mental-models/#intersection-between-capability-and-relationship","text":"Capabilities and relationship mental models intersect when a capability implies a relationship or vice versa. For example, US military academies offer guaranteed admission for children of Congressional Medal of Honor recipients, the nation\u2019s highest honor. The reasoning is that, whether genetic or cultural, the child of a Medal of Honor winner has at least as high potential to become an outstanding officer as candidates vetted by congressional representatives (the primary route for admissions). We also see this in caste systems, including in European aristocracy and Indian society. The presumption is that your familial relationships are suitable indicators of your ability to perform certain societal roles. As a mythical example, King Arthur\u2019s ability to remove the sword from the stone proved his lineage and right to govern the kingdom. His capability re-established his relationship with the kingdom.","title":"Intersection between Capability and Relationship"},{"location":"RWoT7/mental-models/#intersection-between-presentation-and-relationship","text":"An identity does not belong to one entity alone. It is built on the perception the observer has of the subject, based on how the subject presents themselves combined with other information. That information is applied by the observer on their own judgment, which may change, distort or recast the the presentation in non-obvious ways. While a subject may present however they want, different observers will interpret those presentations differently based on their relationship with the subject. How you react when a friend or lover claims a privilege is often different than if a rival or enemy claims the same. As relationships move beyond bilateral interactions to societal engagement, social norms dictate what information sharing is expected, conventional, and allowed. Social agreements compel the disclosure of certain data, such as giving a driver\u2019s license to a law enforcement officer or giving a passport to cross a border. They may also lead one to take a topic out of civil discourse altogether; consider the quaint but still relevant idea that it is rude to ask a woman about her age, a man about his income, or a gender-neutral individual about their Y chromosome. These social norms -- based on our relationships with others -- shape a framework for presenting information that both enables and restricts freedom of presentation [23]. Finally, self-asserting information may be enough for relying on certain types of data (\u201cthis is my favorite food\u201d), but not others (\u201cI am not a crook\u201d). Many systems build trust or authority by involving other parties, using systems of credentials to establish a sense of truth based on the assertions of known actors. This information is projected from the source, filtered and presented by the holder, and interpreted through the lens of the relying party. This shared meaning is based on multiple relationships and as such, is beyond the sole control of any of these parties. The relationships enable and transform the presentations. This is the architecture embodied in Verifiable Credentials [24] as well as physical credentials such as driver\u2019s licenses and passports.","title":"Intersection between Presentation and Relationship"},{"location":"RWoT7/mental-models/#intersection-between-presentation-and-attribute","text":"A persona represents, in data, information about a subject, as presented by that subject to others. It is a subset of the information that a subject could present, and independent of what the recipient may have recorded about the subject. It is the attribute-centric vehicle for presenting identity information. It is neither the totality of the subject\u2019s information, nor is it the only data known about them. An individual may present different persona in different context, sharing different information with different recipients or at different times. Self-asserted claims are fully in the intersection between presentation and attributes. When an individual self-asserts a statement about themselves, they are presenting an attribute on their own authority.","title":"Intersection between Presentation and Attribute"},{"location":"RWoT7/mental-models/#intersection-between-attribute-and-relationship","text":"We all interpret data differently based on the relationship we have with its source. If one of our siblings tells us something, we take it differently than if our parent tells us the same thing. A teacher informing us about our children\u2019s performance is perceived differently than that a child\u2019s self-reported assertion. Moreover, social constructs shape the way entities deal with matters of fact . The way to handle time, for example, differs tremendously depending on where you originally come from. While several calendars exist around the world, a person\u2019s date of birth is a matter of fact. A person is born at a specific moment in time. Nevertheless, the way to construct and attribute this moment in time as a birth date is a social construct. How we record and interpret that moment in time depends on the social context of those recording it. It is often assumed that entities with similar attributes are likely to establish relationships, i.e., \u201cbirds of a feather flock together\u201d. Dating or matchmaking services often try to create relationships between individuals based on their affinity as measured by their common attributes. Of course, such matchmaking ignores the equally resonant wisdom that \u201copposites attract\u201d. Both, however, are examples where attributes are used to evaluate the quality of a potential relationship. Data analytics do something similar when evaluating known attributes of individuals to seek out patterns and relationships. Sometimes this data is used to infer a common group dynamic related to life events (a belief the subject is pregnant) and buying patterns (the subject is likely to buy a car) or even explicit relationships (the subject is a supporter or member of a terrorist organization). In short, one can use attributes to infer relationships (and vice-versa) and relationships affect our interpretation of attributes.","title":"Intersection between Attribute and Relationship"},{"location":"RWoT7/mental-models/#recommendation-and-conclusion","text":"When collaborating with others, consider multiple mental models for better communication and better identity systems. Whatever your own goals, we believe you are more likely to achieve them if you can communicate clearly in terms others understand and can incorporate the needs of others into your own work. None of us have a complete picture of the universe. None of us have a monopoly on the truth. The best identity solutions will come from a frank and open engagement where every individual\u2019s, and every organization\u2019s, needs are heard and considered. Implementations will necessarily be a collection of trade-offs between security and freedom, features and costs, and the needs of those building the systems and the needs of those using them. We believe the best way to build the most effective systems is to thoroughly understand where your collaborators are coming from and what they need from identity and then to work together to devise mechanisms that best achieve the common goals of all stakeholders. In short, be excellent to each other. Be open to new ideas and seek first to understand, then incorporate the perspectives of others. What might seem \u201cwrong\u201d at first hearing may turn out to be a vital component simply unperceivable from your own initial mental model. Working together is our best chance at getting it right.","title":"Recommendation and Conclusion"},{"location":"RWoT7/mental-models/#references","text":"[1] \u201cInternet Identity Workshop\u201d, https://internetidentityworkshop.com (Accessed March 23, 2019) [2] \u201cRebooting the Web of Trust\u201d, http://weboftrust.info (Accessed March 23, 2019) [3] \u201cID2020\u201d, https://id2020.org/ (Accessed March 23, 2019) [4] \u201cEuropean Identity & Cloud Conference\u201d, https://www.kuppingercole.com/events/eic2019 (Accessed March 23, 2019) [5] \u201cWorld Wide Web Consortium\u201d, https://www.w3.org/ (Accessed March 23, 2019) [6] \u201cInternet Engineering Task Force\u201d, https://www.ietf.org/ (Accessed March 23, 2019) [7] \u201cKantara Initiative\u201d, https://kantarainitiative.org/ (Accessed March 23, 2019) [8] \u201cInternational Organization for Standardization\u201d, https://www.iso.org/home.html (Accessed March 23, 2019) [9] \u201cRebooting the Web of Trust V\u201d, https://github.com/WebOfTrustInfo/rwot5-boston (Accessed March 23, 2019) [10] \u201cInternet Identity Workshop 25\u201d, PDF of Proceedings, https://github.com/windley/IIW_homepage/raw/gh-pages/assets/proceedings/IIWXXV_Book_of_Proceedings.pdf (Accessed March 23, 2019) [11] \u201cFive Mental Models of Identity \u201d, https://github.com/WebOfTrustInfo/rwot7-toronto/blob/master/topics-and-advance-readings/five-mental-models-of-identity.md (Accessed March 23, 2019) [12] \u201cRebooting the Web of Trust VII\u201d, https://github.com/WebOfTrustInfo/rwot7-toronto (Accessed March 23, 2019) [13] \u201cInternational Identity Summit\u201d, http://depts.washington.edu/uwconf/wordpress/idsummit/ (Accessed March 23, 2019) [14] \u201cWhat are mental models\u201d, http://mentalmodels.princeton.edu/about/what-are-mental-models/ (Accessed January 24, 2019). [15] \u201cA Primer on Functional Identity\u201d, https://github.com/WebOfTrustInfo/rwot7-toronto/blob/master/topics-and-advance-readings/functional-identity-primer.md (Accessed March 21, 2019) [16] \u201cVendor Relationship Management\u201d, https://en.wikipedia.org/wiki/Vendor_relationship_management (Accessed March 21, 2019) [17] \u201cJoseph McCarthy\u201d, https://en.wikipedia.org/wiki/Joseph_McCarthy (Accessed March 21, 2019) [18] Airplane. Clip from YouTube https://www.youtube.com/watch?v=KSQyW_l8OgE (Accessed March 23, 2019) [19] \u201cSignature Script, ScriptSig\u201d, https://bitcoin.org/en/glossary/signature-script (Accessed March 21, 2019) [20] \u201cPubkey Script, ScriptPubKey\u201d, https://bitcoin.org/en/glossary/pubkey-script (Accessed March 21, 2019) [21] \u201cFLATLAND: A Romance of Many Dimensions\u201d, https://ned.ipac.caltech.edu/level5/Abbott/paper.pdf (Accessed March 21, 2019) [22] \u201cUnique Identification Authority of India\u201d, https://uidai.gov.in/ (Accessed March 21, 2019) [23] \u201cOverton window\u201d https://en.wikipedia.org/wiki/Overton_window (Accessed March 21, 2019) [24] \u201cVerifiable Credentials Data Model 1.0\u201d, https://w3c.github.io/vc-data-model/ (Accessed March 21, 2019)","title":"References"},{"location":"RWoT7/offline-use-cases/","text":"Use Cases and Proposed Solutions for Verifiable Offline Credentials Authors: Michael Lodder, Samantha Mathews Chase, Wolf McNally mike@sovrin.org wolf@wolfmcnally.com samantha@venn.agency Abstract Self-Sovereign Identity is now a widely discussed topic, especially in the context of verifiable credentials/attributes attested by third-parties about an individual or entity that can be used as proof about them, such as a digital driver\u2019s license or passport. This has enabled new systems to be developed to address security and privacy issues. In this paper we cover various scenarios where some or all parties have intermittent, unreliable, untrusted, insecure, or no network access, but require cryptographic verification (message protection and/or proofs). Furthermore, communications between the parties may be only via legacy voice channels. Applicable situations include marine, subterranean, remote expeditions, disaster areas, refugee camps, and high-security installations. This paper then recommends solutions for addressing offline deployments. Introduction All current solutions for verifiable credentials involve computer systems and networks. These systems perform all the complex cryptographic algorithms on a user's behalf, communicate with all involved parties, and are responsible for safeguarding the information. Developed countries such as those in Europe and North America have no problem using these platforms, but these solutions exclude parts of the word that do not have the same sophistication. For example, the vast majority of Africa does not have access to the internet; though many Africa locations do, connection speeds are incredibly slow. There are also possible scenarios where a computer system may not be available to individuals like refugees or missionaries. Finally, some high-security environments do not allow outside network connections. All of these situations must still permit users to prove attributes about themselves and for relying parties to validate that information. The goal of this paper is to provide and encourage consideration for situations where users operate in internet-hostile conditions. Premise For purposes presented in this paper, offline means any device or method that does not require an active internet connection. Offline credentials aim to fill this niche. The benefits to such a system include an individual being able to carry the credentials with them and being able to safeguard them with minimal devices while still allowing relying parties to cryptographically verify them. Offline credentials should not require the use of major computer systems or other powerful electronic devices, but may use them to implement pieces of the process. Otherwise, this voids the entire idea of offline credentials. Some scenarios require cryptographic material processing remain offline to prevent electronic compromises, alterations, or theft. Offline encryption is also not susceptible to a malware attack; and side channel attacks require cameras to record the person performing the encryption. Offline encryption systems have existed for centuries as ciphers. Only in the last few decades have offline cryptography algorithms become sophisticated enough to offer the more powerful features of modern cryptography like authenticated encryption and yield ciphertexts with uniform random distribution of character frequencies. Understanding simple encryption can also help to establish trust in systems. Offline benefits encompass three categories: usability, deployability, and security. Each of these come with risks and limits that computers already handle or solve. One question is how this is different than having a physical credential like a drivers license today. A drivers license or passport cannot be cryptographically verified by hand, and all information must be shown when presented to a relying party. Offline credentials should also support selective disclosure where only the information that a credential holder allows is shared with a verifier. This paper discusses a toolkit that allows possible solutions to be implemented to create verifiable offline credentials along with their pros and cons. Considerations Before detailing offline use cases and solutions, it is necessary to cover the aspects of offline credentials that will be used to measure solutions before they are considered viable. Some offline ciphers require more sophistication and are more prone to mistakes but hard to break, while others may be simpler, with fewer mistakes possible, but not hard enough for an attacker with sufficient resources to break. Many of these terms are borrowed from The quest to replace passwords (particularly concerning usability and security) but adapted to offline credentials. Below is a list of those considered relevant for offline credential systems. Parties Holder : A person or entity that physically holds offline credentials. Issuer : A person or entity that creates offline credentials for Holders . Verifier : A person or entity that receives a presentation of credentials from Holders and verifies their truthfulness. Usability We define usability to mean the following: Memory Wise-Effortless : Holders do not have to remember any secrets at all or possibly one secret for everything (e.g., pin to unlock offline device). Scalable : Dozens of credentials shouldn't increase the burden for the user. \"Scalable\" is only from the user's perspective. Simple-to-carry : Users carry a minimal additional physical object (electronic, mechanical key, piece of paper) that stores credential and cryptographic material and is powerful enough to perform the necessary proofs. Physically-Effortless : Process does not require physical user effort beyond, say, performing a simple task like pressing a button or entering a passcode. Easy-to-learn : Users who don't know the process can figure it out and learn it without too much trouble, and then easily recall how to use it. Efficient-to-Use : The time the user must spend for each presentation is acceptably short. The time required for enrolling with a new issuer, although possibly longer than presentation to a verifier, is also reasonable. Infrequent-Errors : The process a user must perform should succeed when done by an honest and legitimate person. In other words, the system isn't so hard to use or unreliable that genuine users are routinely rejected (as might occur when performing an authenticated encryption scheme by hand). Easy-Recovery-from-Loss : Users can conveniently regain their credentials if lost or stolen. This combines other aspects like low latency before restored credentials; low user inconvenience (e.g., no requirement for physically standing in line); and assurance that recovery will be possible. Selective-disclosure : Users can easily choose which attributes to present and withhold the rest. Deployability We define deployability to be the following: Accessible : Holders are not prevented from using the system by disabilities or other physical (not cognitive) conditions. Negligible-Cost-per-User : The total cost per Holder is negligible. System-compatible : The process could be done by computers if needed. Non-Proprietary : Anyone can implement or use the process for any purpose without having to pay royalties to anyone else. Relevant techniques are generally known, published openly, and not protected by patents or trade secrets. This category is often the barrier for moving offline. Data collected offline could be lost before it has a chance to be saved to online resources. Security We define security to be the following: Resilient-to-Physical-Observation : An attacker cannot impersonate a user after observing them present a credential. Attacks include any form of observation. Resilient-to-Targeted-Impersonation : It is not possible for an attacker to impersonate a holder by exploiting knowledge of personal details without having their credentials. Resilient-to-Guessing : Since offline credential presentations are done in person, relying parties can constrain guessing or detect an impersonator trying to guess. Resilient-to-Leaks-from-Other-Verifiers : Nothing a verifier could possibly leak can help an attacker impersonate the user to another verifier. Resilient-to-Theft : An attacker in possession of a Holder's credentials cannot use them for presentation to another party. No-Trusted-Third-Party : The process does not rely on a trusted third party who could, upon being attacked or otherwise becoming untrustworthy, compromise a holder's security or privacy. Unlinkable : Colluding verifiers cannot determine whether the same holder is presenting to both. Use Cases We begin by describing a scenario where offline credentials could be deployed, detailing their respective environments, and we conclude by illustrating how offline credentials are helpful. Scenario: Marine, Subterranean Underwater and underground operations present interesting conditions for internet connectivity. Oil rigs, while stationary, are constructed many miles offshore and may have consistent connections to land-based internet or satellites but may not always be online. Offshore rescue operations like the US Coast Guard tend to have short gaps of time where connectivity does not exist, but usually return to land after a few minutes or a few hours. Boats can travel longer distances away from land and may be away weeks or months at a time. Since boats travel at the surface, they do have more internet accessibility than submarines. Submarines only connect to the outside world when at or near the surface. Information routing is critical for marine-based scenarios because often the data to be sent have directed destinations. An oil rig in international waters may not want to send highly sensitive data over the nearest countries internet cables but instead chooses to send it by human carriers. In [2], the authors describe certain weaknesses in underwater security communications and recommend \u201cnon-interactive data transmission schemes that ensure the underwater vehicles do not transmit additional messages for authentication and key establishment.\u201d Underground inhibits wireless signals and may require long cables to connect to operators. They might choose to report in or to sync to shore or surface at longer intervals, such as once a day. Scenario: Remote Expeditions There are remote locations like the Amazon jungle, Antarctica, or Mount Everest where it's possible to receive communications but which require expensive equipment to reach satellites or remote substations. Scenario: Sensitive Compartmented Information Facility (SCIF) SCIFs store high security information. The most restrictive security measures are implemented, such as fences, guards, faraday cages, concrete walls, and no internet connection. Personnel are required to undergo strict vetting for security clearances and utilize multiple factors for authentication. At designated intervals, updates to data are performed but methods vary, such as using USB thumb drives containing the newest data for transportation and synchronization. Scenario: Epidemic Outbreaks, Public Health Events Epidemics can occur with or without warning. When they happen, there are no guarantees whether internet connections are available, especially in developing countries like Africa. Tracking individuals who have received a clean bill of health and those infected becomes paramount. Affected areas are isolated via containment procedures, with restricted access to qualified medical professionals [5, 6]. Medical workers must be able to prove their qualifications and document their actions with or without internet access. Scenario: Disaster Zones Fraud also becomes prevalent as charitable donations are given to malicious parties. Internet connectivity can be sparse for emergency responders because important technology infrastructure is commonly destroyed or otherwise unavailable or disrupted. It is critical when disaster strikes to be able to identify those affected. Any rescued personnel\u2019s infirmity or disability are necessary to know before administering medical services, in case of allergies or adverse side effects to medicine or treatment. Remote operated vehicles (ROVs) are commonly deployed to assist in rescue efforts. ROVs solely communicate with operators and over Radio Frequency and not the internet. ROVs could facilitate identification if provided with features to identify people. Scenario: Missionary, Humanitarian Service, Refugee Camps Religious missionaries or humanitarian aid workers are required to prove immunizations and to present visas and other documentation when entering countries for service [3, 4]. Their service could include documenting vital information on behalf of the local population and recording their activities while in internet dead zones. The information is eventually relocated to internet-connected areas but the time period is unknown. Many locals do not have a digital presence at all or have very little of a presence. Refugee camps are often required to immunize and document other medical facts and procedures on refugees. Scenario: Resource Allocation and Management Areas where essential resources like food, water, or medicine are scarce require careful allocation and management. Often such areas are poorly connected to the Internet and visited by personnel who need permission to access and distribute the resources. Scenario: Delegative Democracy Democracy is often implemented using \u201crepresentative democracy\u201d where citizens elect representatives in a particular district for a particular term, who \u201crepresent\u201d their district as they please. This technique was developed when distances were large and communication was slow; it often concentrates power in the hands of elected representatives who are not very accountable to their constituents. By contrast, delegative democracy (also called \u201cliquid democracy\u201d) allows anyone to delegate their vote to anyone else. This also works well over long distances and in areas with poor connectivity, but requires more bookkeeping and auditing to hold delegates accountable. In particular, the ability to assign proxy credentials to delegates and remain anonymous, the ability for delegates to vote their proxies and be accountable, and the ability to prove the results of voting are all necessary. Solutions The scenarios and the variance in requirements are vast. However, many of the challenges can be solved in similar ways. We propose a toolkit that can be used to address most of the concerns, but implementation is left to developers. The toolkit includes methods for confidential identification, authentication, authorization, and auditing. It is the hope that implementers consider basing their solutions on this toolkit and the ideas proposed. Sovrin is a blockchain for enabling secure privacy-preserving identity management, but any blockchain that facilitates identity management should suffice. The Sovrin ledger enables users to easily and securely manage identities by using verifiable credentials and zero-knowledge proofs. Organizations can securely issue credentials containing various attributes to personnel, which can be used to generate zero-knowledge proofs to relying parties. Issuers use Sovrin to indicate which credentials are valid without disclosing to whom they were issued, and Relying Parties can use the ledger to verify proofs from credential holders. In order to enable offline verification, Sovrin supports creating a proof of ledger state: a snapshot of the ledger at the latest moment in time. Devices can store state proofs and receive periodic updates as permitted, but would not require persistent internet connections. Updates can be performed according to best security and industry practices. Credentials can be stored in offline digital wallets, i.e. smart cards, USB keys, ROVs, and custom electronics. These devices can also support proof-request parsing, proof generation, and proof verification. All of the code to do this is open source in Hyperledger Indy-SDK . Sovrin supports many of the considerations stated previously and selective disclosure. Offline still requires devices with sufficient computational power to perform complex math operations for issuing credentials and generating zero-knowledge proofs such as modular exponentiation and elliptic curve pairings but would not require anything else. It is recommended they also provide cryptographic primitives for encryption, digital signatures, and key exchanges to meet simplicity requirements for users. Offline methods can securely transmit issued credential from Issuer to Holder over any medium: QR codes, audio codes, RF, downloaded to portable thumb drive, dead drops, etc. Offline transmissions can be secured by using modern key-exchange protocols like Diffie Hellman if supported but may also be done via pen-and-paper ciphers. If using pen-and-paper ciphers, care should be taken for the time it takes to perform encryption and to use authenticated encryption scheme. LC4 is a new cipher that supports authenticated encryption and nonces to prevent key leaks. The algorithm is easy to perform which makes it susceptible to some attacks but simple to learn. It supports up to 36 characters, keys are 36 characters long, and the algorithm claims strengths equal to 136 bits. LC4 has most of the considerations and allows the scheme to meet all the requirements. Zero-knowledge proofs do not need to be encrypted as they reveal no information,` unlike credentials. This could simplify designs and reduce power consumption if needed. It is recommended to use encryption in all message transmissions to provide security by default. Some scenarios call for storing non-credential data that must be shared. IPFS is a distributed hash table (DHT) tool that allows offline state caching, modification and later resyncing. Again, any distributed data technology could work. This is helpful for collecting data about situations like radiation or atmosphere readings, soil samples, patient blood samples, immunizations, etc. This data can later be read and additional credentials can be issued, or it can be published to for global consumption like IXO to measure impact. IXO allows Sovrin credentials for authentically publishing data. This toolkit of three technologies enable solutions to be easily implemented that meet scenario requirements. The blockchains have been created to be usable, deployable, and secure. We envision future technology built from this toolkit that solves the challenges from these scenarios in ways that consumers couldn\u2019t imagine. The authors are working on basic implementations that can be consumed for more complicated deployments. To conclude, we explore how the marine use case could be solved using this toolkit: Maritime communications happen via SATCOM (satellite communications). According to [7], security is rudimentary, weak, or non existent. The first problem is identification and authentication. Ship workers, sailors, and any electronic device can be given credentials that contain essential attributes. Credentials can be stored in smart cards for people, system memory for devices. Any device or system requiring authentication, proof of training, or proof of rank to make decisions can store state proofs from Sovrin. If the company demanded more security, Hyperledger Indy could be deployed on their own networks but would limit portability for cross-company collaboration. State proof updates can be done over SATCOM using authentication from credentials issued to the fleet while at port. Updates to devices on the boat can be done any viable method but for our example we state the boat has networked devices to a main computer which functions as the external gateway. Workers can access their work areas via smart cards that perform authentication with ZKPs. Audits for work completed can be recorded in the devices using IPFS. When the next resync window happens on this data can be sent securely to where it needs to go. This fulfills the usability requirements because workers just need to remember to carry their smart cards; it\u2019s easy to use by simply sliding in a card or presenting the card via a proximity scanner. It's secure because it uses zero-knowledge proofs to verify identities, and deployable because the smart cards can be made from inexpensive hardware like a raspberry pi or embedded systems. Once at port, all the public IPFS data could be aggregated and uploaded for impact analysis to IXO for shipping companies to review later for improving their trade. Implementation does not change much or at all for offshore operations or submarines. References [1] Pacific Life Research Center - http://www.plrc.org/docs/941005B.pdf [2] Changsheng Wan, Vir Virander Phoha, Yuzhe Tang, Aiqun Hu, \"Non-interactive Identity-Based Underwater Data Transmission With Anonymity and Zero Knowledge\", Vehicular Technology IEEE Transactions on, vol. 67, no. 2, pp. 1726-1739, 2018. [3] World Health Organization Humanitarian Requirements Document - http://www.who.int/health-cluster/countries/ethiopia/ethiopia-humanitarian-response-plan-2017.pdf [4] Ethiopia Requirements Document - https://www.usaid.gov/sites/default/files/documents/1860/Ethiopia%20HRD%202016.pdf [5] International Health Regulations and Epidemic Control - http://www.who.int/trade/distance_learning/gpgh/gpgh8/en/ [6] Control of communicable diseases and prevention of epidemics - http://www.searo.who.int/entity/emergencies/documents/who_control_of_communicable_disease.pdf?ua=1 [7] Hacking, tracking, stealing and sinking ships - https://www.pentestpartners.com/security-blog/hacking-tracking-stealing-and-sinking-ships/","title":"Use Cases and Proposed Solutions for Verifiable Offline Credentials"},{"location":"RWoT7/offline-use-cases/#use-cases-and-proposed-solutions-for-verifiable-offline-credentials","text":"Authors: Michael Lodder, Samantha Mathews Chase, Wolf McNally mike@sovrin.org wolf@wolfmcnally.com samantha@venn.agency","title":"Use Cases and Proposed Solutions for Verifiable Offline Credentials"},{"location":"RWoT7/offline-use-cases/#abstract","text":"Self-Sovereign Identity is now a widely discussed topic, especially in the context of verifiable credentials/attributes attested by third-parties about an individual or entity that can be used as proof about them, such as a digital driver\u2019s license or passport. This has enabled new systems to be developed to address security and privacy issues. In this paper we cover various scenarios where some or all parties have intermittent, unreliable, untrusted, insecure, or no network access, but require cryptographic verification (message protection and/or proofs). Furthermore, communications between the parties may be only via legacy voice channels. Applicable situations include marine, subterranean, remote expeditions, disaster areas, refugee camps, and high-security installations. This paper then recommends solutions for addressing offline deployments.","title":"Abstract"},{"location":"RWoT7/offline-use-cases/#introduction","text":"All current solutions for verifiable credentials involve computer systems and networks. These systems perform all the complex cryptographic algorithms on a user's behalf, communicate with all involved parties, and are responsible for safeguarding the information. Developed countries such as those in Europe and North America have no problem using these platforms, but these solutions exclude parts of the word that do not have the same sophistication. For example, the vast majority of Africa does not have access to the internet; though many Africa locations do, connection speeds are incredibly slow. There are also possible scenarios where a computer system may not be available to individuals like refugees or missionaries. Finally, some high-security environments do not allow outside network connections. All of these situations must still permit users to prove attributes about themselves and for relying parties to validate that information. The goal of this paper is to provide and encourage consideration for situations where users operate in internet-hostile conditions.","title":"Introduction"},{"location":"RWoT7/offline-use-cases/#premise","text":"For purposes presented in this paper, offline means any device or method that does not require an active internet connection. Offline credentials aim to fill this niche. The benefits to such a system include an individual being able to carry the credentials with them and being able to safeguard them with minimal devices while still allowing relying parties to cryptographically verify them. Offline credentials should not require the use of major computer systems or other powerful electronic devices, but may use them to implement pieces of the process. Otherwise, this voids the entire idea of offline credentials. Some scenarios require cryptographic material processing remain offline to prevent electronic compromises, alterations, or theft. Offline encryption is also not susceptible to a malware attack; and side channel attacks require cameras to record the person performing the encryption. Offline encryption systems have existed for centuries as ciphers. Only in the last few decades have offline cryptography algorithms become sophisticated enough to offer the more powerful features of modern cryptography like authenticated encryption and yield ciphertexts with uniform random distribution of character frequencies. Understanding simple encryption can also help to establish trust in systems. Offline benefits encompass three categories: usability, deployability, and security. Each of these come with risks and limits that computers already handle or solve. One question is how this is different than having a physical credential like a drivers license today. A drivers license or passport cannot be cryptographically verified by hand, and all information must be shown when presented to a relying party. Offline credentials should also support selective disclosure where only the information that a credential holder allows is shared with a verifier. This paper discusses a toolkit that allows possible solutions to be implemented to create verifiable offline credentials along with their pros and cons.","title":"Premise"},{"location":"RWoT7/offline-use-cases/#considerations","text":"Before detailing offline use cases and solutions, it is necessary to cover the aspects of offline credentials that will be used to measure solutions before they are considered viable. Some offline ciphers require more sophistication and are more prone to mistakes but hard to break, while others may be simpler, with fewer mistakes possible, but not hard enough for an attacker with sufficient resources to break. Many of these terms are borrowed from The quest to replace passwords (particularly concerning usability and security) but adapted to offline credentials. Below is a list of those considered relevant for offline credential systems.","title":"Considerations"},{"location":"RWoT7/offline-use-cases/#parties","text":"Holder : A person or entity that physically holds offline credentials. Issuer : A person or entity that creates offline credentials for Holders . Verifier : A person or entity that receives a presentation of credentials from Holders and verifies their truthfulness.","title":"Parties"},{"location":"RWoT7/offline-use-cases/#usability","text":"We define usability to mean the following: Memory Wise-Effortless : Holders do not have to remember any secrets at all or possibly one secret for everything (e.g., pin to unlock offline device). Scalable : Dozens of credentials shouldn't increase the burden for the user. \"Scalable\" is only from the user's perspective. Simple-to-carry : Users carry a minimal additional physical object (electronic, mechanical key, piece of paper) that stores credential and cryptographic material and is powerful enough to perform the necessary proofs. Physically-Effortless : Process does not require physical user effort beyond, say, performing a simple task like pressing a button or entering a passcode. Easy-to-learn : Users who don't know the process can figure it out and learn it without too much trouble, and then easily recall how to use it. Efficient-to-Use : The time the user must spend for each presentation is acceptably short. The time required for enrolling with a new issuer, although possibly longer than presentation to a verifier, is also reasonable. Infrequent-Errors : The process a user must perform should succeed when done by an honest and legitimate person. In other words, the system isn't so hard to use or unreliable that genuine users are routinely rejected (as might occur when performing an authenticated encryption scheme by hand). Easy-Recovery-from-Loss : Users can conveniently regain their credentials if lost or stolen. This combines other aspects like low latency before restored credentials; low user inconvenience (e.g., no requirement for physically standing in line); and assurance that recovery will be possible. Selective-disclosure : Users can easily choose which attributes to present and withhold the rest.","title":"Usability"},{"location":"RWoT7/offline-use-cases/#deployability","text":"We define deployability to be the following: Accessible : Holders are not prevented from using the system by disabilities or other physical (not cognitive) conditions. Negligible-Cost-per-User : The total cost per Holder is negligible. System-compatible : The process could be done by computers if needed. Non-Proprietary : Anyone can implement or use the process for any purpose without having to pay royalties to anyone else. Relevant techniques are generally known, published openly, and not protected by patents or trade secrets. This category is often the barrier for moving offline. Data collected offline could be lost before it has a chance to be saved to online resources.","title":"Deployability"},{"location":"RWoT7/offline-use-cases/#security","text":"We define security to be the following: Resilient-to-Physical-Observation : An attacker cannot impersonate a user after observing them present a credential. Attacks include any form of observation. Resilient-to-Targeted-Impersonation : It is not possible for an attacker to impersonate a holder by exploiting knowledge of personal details without having their credentials. Resilient-to-Guessing : Since offline credential presentations are done in person, relying parties can constrain guessing or detect an impersonator trying to guess. Resilient-to-Leaks-from-Other-Verifiers : Nothing a verifier could possibly leak can help an attacker impersonate the user to another verifier. Resilient-to-Theft : An attacker in possession of a Holder's credentials cannot use them for presentation to another party. No-Trusted-Third-Party : The process does not rely on a trusted third party who could, upon being attacked or otherwise becoming untrustworthy, compromise a holder's security or privacy. Unlinkable : Colluding verifiers cannot determine whether the same holder is presenting to both.","title":"Security"},{"location":"RWoT7/offline-use-cases/#use-cases","text":"We begin by describing a scenario where offline credentials could be deployed, detailing their respective environments, and we conclude by illustrating how offline credentials are helpful.","title":"Use Cases"},{"location":"RWoT7/offline-use-cases/#scenario-marine-subterranean","text":"Underwater and underground operations present interesting conditions for internet connectivity. Oil rigs, while stationary, are constructed many miles offshore and may have consistent connections to land-based internet or satellites but may not always be online. Offshore rescue operations like the US Coast Guard tend to have short gaps of time where connectivity does not exist, but usually return to land after a few minutes or a few hours. Boats can travel longer distances away from land and may be away weeks or months at a time. Since boats travel at the surface, they do have more internet accessibility than submarines. Submarines only connect to the outside world when at or near the surface. Information routing is critical for marine-based scenarios because often the data to be sent have directed destinations. An oil rig in international waters may not want to send highly sensitive data over the nearest countries internet cables but instead chooses to send it by human carriers. In [2], the authors describe certain weaknesses in underwater security communications and recommend \u201cnon-interactive data transmission schemes that ensure the underwater vehicles do not transmit additional messages for authentication and key establishment.\u201d Underground inhibits wireless signals and may require long cables to connect to operators. They might choose to report in or to sync to shore or surface at longer intervals, such as once a day.","title":"Scenario: Marine, Subterranean"},{"location":"RWoT7/offline-use-cases/#scenario-remote-expeditions","text":"There are remote locations like the Amazon jungle, Antarctica, or Mount Everest where it's possible to receive communications but which require expensive equipment to reach satellites or remote substations.","title":"Scenario: Remote Expeditions"},{"location":"RWoT7/offline-use-cases/#scenario-sensitive-compartmented-information-facility-scif","text":"SCIFs store high security information. The most restrictive security measures are implemented, such as fences, guards, faraday cages, concrete walls, and no internet connection. Personnel are required to undergo strict vetting for security clearances and utilize multiple factors for authentication. At designated intervals, updates to data are performed but methods vary, such as using USB thumb drives containing the newest data for transportation and synchronization.","title":"Scenario: Sensitive Compartmented Information Facility (SCIF)"},{"location":"RWoT7/offline-use-cases/#scenario-epidemic-outbreaks-public-health-events","text":"Epidemics can occur with or without warning. When they happen, there are no guarantees whether internet connections are available, especially in developing countries like Africa. Tracking individuals who have received a clean bill of health and those infected becomes paramount. Affected areas are isolated via containment procedures, with restricted access to qualified medical professionals [5, 6]. Medical workers must be able to prove their qualifications and document their actions with or without internet access.","title":"Scenario: Epidemic Outbreaks, Public Health Events"},{"location":"RWoT7/offline-use-cases/#scenario-disaster-zones","text":"Fraud also becomes prevalent as charitable donations are given to malicious parties. Internet connectivity can be sparse for emergency responders because important technology infrastructure is commonly destroyed or otherwise unavailable or disrupted. It is critical when disaster strikes to be able to identify those affected. Any rescued personnel\u2019s infirmity or disability are necessary to know before administering medical services, in case of allergies or adverse side effects to medicine or treatment. Remote operated vehicles (ROVs) are commonly deployed to assist in rescue efforts. ROVs solely communicate with operators and over Radio Frequency and not the internet. ROVs could facilitate identification if provided with features to identify people.","title":"Scenario: Disaster Zones"},{"location":"RWoT7/offline-use-cases/#scenario-missionary-humanitarian-service-refugee-camps","text":"Religious missionaries or humanitarian aid workers are required to prove immunizations and to present visas and other documentation when entering countries for service [3, 4]. Their service could include documenting vital information on behalf of the local population and recording their activities while in internet dead zones. The information is eventually relocated to internet-connected areas but the time period is unknown. Many locals do not have a digital presence at all or have very little of a presence. Refugee camps are often required to immunize and document other medical facts and procedures on refugees.","title":"Scenario: Missionary, Humanitarian Service, Refugee Camps"},{"location":"RWoT7/offline-use-cases/#scenario-resource-allocation-and-management","text":"Areas where essential resources like food, water, or medicine are scarce require careful allocation and management. Often such areas are poorly connected to the Internet and visited by personnel who need permission to access and distribute the resources.","title":"Scenario: Resource Allocation and Management"},{"location":"RWoT7/offline-use-cases/#scenario-delegative-democracy","text":"Democracy is often implemented using \u201crepresentative democracy\u201d where citizens elect representatives in a particular district for a particular term, who \u201crepresent\u201d their district as they please. This technique was developed when distances were large and communication was slow; it often concentrates power in the hands of elected representatives who are not very accountable to their constituents. By contrast, delegative democracy (also called \u201cliquid democracy\u201d) allows anyone to delegate their vote to anyone else. This also works well over long distances and in areas with poor connectivity, but requires more bookkeeping and auditing to hold delegates accountable. In particular, the ability to assign proxy credentials to delegates and remain anonymous, the ability for delegates to vote their proxies and be accountable, and the ability to prove the results of voting are all necessary.","title":"Scenario: Delegative Democracy"},{"location":"RWoT7/offline-use-cases/#solutions","text":"The scenarios and the variance in requirements are vast. However, many of the challenges can be solved in similar ways. We propose a toolkit that can be used to address most of the concerns, but implementation is left to developers. The toolkit includes methods for confidential identification, authentication, authorization, and auditing. It is the hope that implementers consider basing their solutions on this toolkit and the ideas proposed. Sovrin is a blockchain for enabling secure privacy-preserving identity management, but any blockchain that facilitates identity management should suffice. The Sovrin ledger enables users to easily and securely manage identities by using verifiable credentials and zero-knowledge proofs. Organizations can securely issue credentials containing various attributes to personnel, which can be used to generate zero-knowledge proofs to relying parties. Issuers use Sovrin to indicate which credentials are valid without disclosing to whom they were issued, and Relying Parties can use the ledger to verify proofs from credential holders. In order to enable offline verification, Sovrin supports creating a proof of ledger state: a snapshot of the ledger at the latest moment in time. Devices can store state proofs and receive periodic updates as permitted, but would not require persistent internet connections. Updates can be performed according to best security and industry practices. Credentials can be stored in offline digital wallets, i.e. smart cards, USB keys, ROVs, and custom electronics. These devices can also support proof-request parsing, proof generation, and proof verification. All of the code to do this is open source in Hyperledger Indy-SDK . Sovrin supports many of the considerations stated previously and selective disclosure. Offline still requires devices with sufficient computational power to perform complex math operations for issuing credentials and generating zero-knowledge proofs such as modular exponentiation and elliptic curve pairings but would not require anything else. It is recommended they also provide cryptographic primitives for encryption, digital signatures, and key exchanges to meet simplicity requirements for users. Offline methods can securely transmit issued credential from Issuer to Holder over any medium: QR codes, audio codes, RF, downloaded to portable thumb drive, dead drops, etc. Offline transmissions can be secured by using modern key-exchange protocols like Diffie Hellman if supported but may also be done via pen-and-paper ciphers. If using pen-and-paper ciphers, care should be taken for the time it takes to perform encryption and to use authenticated encryption scheme. LC4 is a new cipher that supports authenticated encryption and nonces to prevent key leaks. The algorithm is easy to perform which makes it susceptible to some attacks but simple to learn. It supports up to 36 characters, keys are 36 characters long, and the algorithm claims strengths equal to 136 bits. LC4 has most of the considerations and allows the scheme to meet all the requirements. Zero-knowledge proofs do not need to be encrypted as they reveal no information,` unlike credentials. This could simplify designs and reduce power consumption if needed. It is recommended to use encryption in all message transmissions to provide security by default. Some scenarios call for storing non-credential data that must be shared. IPFS is a distributed hash table (DHT) tool that allows offline state caching, modification and later resyncing. Again, any distributed data technology could work. This is helpful for collecting data about situations like radiation or atmosphere readings, soil samples, patient blood samples, immunizations, etc. This data can later be read and additional credentials can be issued, or it can be published to for global consumption like IXO to measure impact. IXO allows Sovrin credentials for authentically publishing data. This toolkit of three technologies enable solutions to be easily implemented that meet scenario requirements. The blockchains have been created to be usable, deployable, and secure. We envision future technology built from this toolkit that solves the challenges from these scenarios in ways that consumers couldn\u2019t imagine. The authors are working on basic implementations that can be consumed for more complicated deployments. To conclude, we explore how the marine use case could be solved using this toolkit: Maritime communications happen via SATCOM (satellite communications). According to [7], security is rudimentary, weak, or non existent. The first problem is identification and authentication. Ship workers, sailors, and any electronic device can be given credentials that contain essential attributes. Credentials can be stored in smart cards for people, system memory for devices. Any device or system requiring authentication, proof of training, or proof of rank to make decisions can store state proofs from Sovrin. If the company demanded more security, Hyperledger Indy could be deployed on their own networks but would limit portability for cross-company collaboration. State proof updates can be done over SATCOM using authentication from credentials issued to the fleet while at port. Updates to devices on the boat can be done any viable method but for our example we state the boat has networked devices to a main computer which functions as the external gateway. Workers can access their work areas via smart cards that perform authentication with ZKPs. Audits for work completed can be recorded in the devices using IPFS. When the next resync window happens on this data can be sent securely to where it needs to go. This fulfills the usability requirements because workers just need to remember to carry their smart cards; it\u2019s easy to use by simply sliding in a card or presenting the card via a proximity scanner. It's secure because it uses zero-knowledge proofs to verify identities, and deployable because the smart cards can be made from inexpensive hardware like a raspberry pi or embedded systems. Once at port, all the public IPFS data could be aggregated and uploaded for impact analysis to IXO for shipping companies to review later for improving their trade. Implementation does not change much or at all for offshore operations or submarines.","title":"Solutions"},{"location":"RWoT7/offline-use-cases/#references","text":"[1] Pacific Life Research Center - http://www.plrc.org/docs/941005B.pdf [2] Changsheng Wan, Vir Virander Phoha, Yuzhe Tang, Aiqun Hu, \"Non-interactive Identity-Based Underwater Data Transmission With Anonymity and Zero Knowledge\", Vehicular Technology IEEE Transactions on, vol. 67, no. 2, pp. 1726-1739, 2018. [3] World Health Organization Humanitarian Requirements Document - http://www.who.int/health-cluster/countries/ethiopia/ethiopia-humanitarian-response-plan-2017.pdf [4] Ethiopia Requirements Document - https://www.usaid.gov/sites/default/files/documents/1860/Ethiopia%20HRD%202016.pdf [5] International Health Regulations and Epidemic Control - http://www.who.int/trade/distance_learning/gpgh/gpgh8/en/ [6] Control of communicable diseases and prevention of epidemics - http://www.searo.who.int/entity/emergencies/documents/who_control_of_communicable_disease.pdf?ua=1 [7] Hacking, tracking, stealing and sinking ships - https://www.pentestpartners.com/security-blog/hacking-tracking-stealing-and-sinking-ships/","title":"References"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/","text":"Peer to Peer Degrees of Trust Harrison Stahl, Titus Capilnean, Peter Snyder, Tyler Yasaka Rebooting the Web of Trust, Fall 2018 Abstract Aunthenticity is a challenge for any identity solution. In the physical world, at least in America, it is not difficult to change one's identity [1]. In the digital world, there is the problem of bots. The botnet detection market is expected to be worth over one billion USD by 2023 [2], in a landscape where most digital activity is still heavily centralized. These centralized digital solutions have the advantage of being able to track IP addresses, request phone verification, and present CAPTCHAs to users in order to authenticate them. If this problem is so difficult to solve in the centralized world, how much more challenging will it be in the decentralized world, where none of these techniques are available? In this paper, we explore the idea of using a web of trust as a tool to add authenticity to decentralized identifiers (DIDs). We define a framework for deriving relative trust degrees using a given trust metric : a \"trustworthiness\" score for a given identity from the perspective of another identity. It is our intent that this framework may be used as a starting point for an ongoing exploration of graph-based, decentralized trust. We believe this approach may ultimately be used as a foundation for decentralized reputation. Related Work on Graph-Based Reputation Systems PGP PGP (Pretty Good Privacy) is a system for asymmetric encryption. It was originally designed as a system for encrypting email contents, but it is also used for a variety of other file-encryption purposes. In addition to defining a system for asymmetric message encryption, PGP also defines a system for key discovery, based off of key servers (which are conceptually distributed, but in practice use a small number of well-known and trusted servers), a method for assigning trust to keys that have been uploaded to these servers, and an algorithm for determining how much trust to assign to a key that was uploaded by a given email address, but which belongs to an account unfamiliar to the email sender. Collectively this system is called the \\\"Web of Trust\\\". This system has many similarities to the kinds of systems envisioned in this paper. The design is fundamentally decentralized, it allows parties to reason about whether, and how much, to trust unfamiliar peers, and it allows participants to model trust in a non-binary way (i.e. users can be trusted \\\"some\\\", but not fully or not at all). However, the PGP \\\"Web of Trust\\\" model differs from the model envisioned in this paper in several ways. First, while trust can be assigned in a non-binary manner, it can only be \\\"derived\\\" binary (i.e. a key is either trusted or untrusted, but the \\\"Web of Trust\\\" algorithm isn\\'t intended to be used to provide finer grained trust information). Additionally, the PGP Web of Trust does not include a natural way to describe different domains of trust. One might wish to say \\\"I trust party X very much when they\\'re discussing music, but very little when they\\'re discussing politics\\\". The PGP model is not a natural fit for such statements. Social Networks Online social networks such as Facebook, Twitter and Instagram conceptually model their systems as graphs, with participants as nodes, and connections between the participants as edges with labels like \\\"friend\\\", \\\"follower\\\" or \\\"boyfriend\\\". These edges can be directed or bi-directional, depending on the relationship being represented. The maintainers of these systems use the graph representation to make decisions about relationship, event or interest recommendations: a friend of your friend is more likely to also be your friend than a randomly selected person in the graph. While these systems are not primarily intended to be reputation or trust-management systems, the same algorithms and representations used in online social networks could guide the development of decentralized trust systems. Recommendation Systems Graph-based recommendation systems have been deployed for many commercial purposes. Examples include the video recommendation systems used in YouTube and Netflix and the product-recommendation systems used in sites like Amazon and eBay. In such systems, videos and products can be modeled as nodes. The users and customers of such sites are also represented as nodes. Recommendations are represented as edges from the latter to the former, possibly weighed for systems that allow non-binary edges (e.g. 1-5 star ratings). The structure of such graphs are used to recommend similar, popular products and videos to other users. As in the \\\"Social Networks\\\" case, the same representations and techniques used in such systems can inform the design and algorithms used in a decentralized reputation / trust system. Degrees of Trust: A Framework A Relative Trust Score Google's PageRank algorithm is famously able to assign ranking scores to web pages from a directed graph, where edges represent links from one website to another. One major problem with PageRank, however, is that it is vulnerable to Sybil attacks. Indeed, after Google implemented PageRank we witnessed the phenomenon of link farms: a form of Sybil attack that takes advantage of the fact that (in the pure form of PageRank) all pages are first-class citizens with equal ability to cast \"votes\" in the algorithm, in the form of web links. Dummy web pages could be created cheaply to deceive the algorithm. Of course, it is now widely known that Google uses a much more sophisticated version of PageRank that is less susceptible to link farms (though the algorithm itself is kept secret). In many respects, deriving trust scores in an open web of trust is similar to the problem of deriving web page rankings in an open internet. Both the web of trust and the internet can be represented as a directed graph, where edges represent votes from one node in favor of another. Indeed, the defunct website Advogato used a web of trust with designated \"seed nodes\" in order to generate trust scores for members of the site [3]. Advogato's solution was Sybil resistant thanks to the seed nodes, and Google's updated ranking algorithm may very well use a similar concept of \"seed\" websites to protect against link farms. There is a problem with seed nodes, however. Seed nodes are inherently more powerful than other nodes -- an inequality of power that we do not want in a decentralized system. Seed nodes seem to be necessary when these two requirements exist for a graph-based system: (1) that it must produce absolute scores for each node, and (2) that it must be Sybil resistant. We want to keep the second requirement, obviously. But what if we were to relax the first? Next we explore the concept of deriving relative trust scores. We define a trust score to be relative if it varies based on the observer. Assumptions In this framework, we assume an underlying web of trust that is represented as a directed graph. Each edge in the graph represents a trust link : a statement that one node trusts another. These links may be weighted or unweighted, depending on the implementation. This web of trust may be created explicitly (e.g. by explicit statements among users) or it may be derived from some other data (e.g. \"follows\" on a social media platform). We do not make any assumptions as to what is represented by a node and what is represented by an edge. The most intuitive interpretation is that a node represents a human and that an edge represents a statement of trust made by one human about another. However, this framework may be applied to any application where nodes have use for decentralized reputation, and where there is some data available regarding which nodes are considered reliable by other nodes. These nodes could just as well represent devices, organizations, or something else entirely. Trust Metrics We define a trust metric as a function that takes as input a web of trust, a source node, and a target node, and outputs a value that represents the degree of trust from the source to the target. In other words, this function yields a degree of trust (how trustworthy one node is from the perspective of another node), given a web of trust. A simple example of a trust metric would be a function that counts the number of hops along the shortest path from a source node s to a target node t . The inverse of this number could be returned as the result of this function. The effect of such a metric would be that the degree of trust is inversely proportional to the number of hops required to get from s to t . However, other metrics could also be used. Another similar metric might be to take the inverse of two raised to the power of the number of hops. In this metric, the degree of trust would decrease logarithmically as the distance from s to t increased. More complex metrics might do other interesting things, such as taking into account multiple paths from s to t . Ultimately then, these metrics are all performing some sort of graph analysis, where the graph is centered around the source (i.e. from the perspective of the source). There is certainly a limit to what can be known purely based on a graph analysis, and intuitively one would expect this limit to depend on the connectedness of the graph as well as the error rate of trust links. It is assumed that there would be at least some error rate (i.e. some percentage of trust links that are directed to untrustworthy nodes). With too high of an error rate and/or too sparse of a graph, it might be difficult to distinguish with much certainty a trustworthy node from a malicious node. Despite these limitations, we believe that in some systems there may exist some useful information related to trust. It is the intent of this framework is to allow as much of this information to be utilized as possible. Sybil Resistance So far we have been using the term Sybil resistance rather vaguely. Here we will provide a precise definition that applies to the manipulation of trust scores in a web of trust. To do this, we will borrow the concept of good nodes, confused nodes, and bad nodes from Advogato. Good nodes in a web of trust are honest and will not attempt to manipulate trust scores. Confused nodes are also honest and will not attempt to manipulate trust scores, but may mistakenly trust one or more bad nodes. Bad nodes, then, are those that will create fake nodes in order to game the system in some way. The definition of a confused node is rather open-ended, however. Does a confused node trust a single bad node, or multiple bad nodes, or an unlimited number of bad nodes? For the sake of clarity, we will define an edge from a confused node to a bad node as a confused edge. We will also define a puppet node as a fake identity in a web of trust created by an attacker for the purpose of gaming the system. More precisely: puppet nodes are the subset of bad nodes that are not adjacent to a confused edge. We propose the following definition: a trust metric is Sybil resistant if and only if the upper limit of the combined trust scores that can belong to bad nodes is bounded by the number of confused edges. Stated another way, a trust metric is Sybil resistant if and only if there is a finite amount of combined trust an attacker can gain in a system merely by creating puppet nodes. This definition alone is not sufficient when considering Sybil attacks. While it helps to know that the impact that a Sybil attack can have is bounded, we might also wish to know the degree of Sybil resistance that a metric has. We will define the degree of Sybil resistance as the maximum trust that an attacker can have with a single bad node, divided by the maximum combined trust that can be accumulated by an attacker with an unlimited number of puppet nodes. A metric where puppet nodes have no effect has a Sybil-resistance degree of one; a metric where puppet nodes can increase the combined trust score of an attacker has a Sybil-resistance degree less than one; and a metric where puppet nodes can increase the combined trust score of an attacker without bound has a Sybil-resistance degree of zero (i.e. is not Sybil resistant). Inclusion Status Here we want to introduce the concept of inclusion status . We envision that many applications that use webs of trust will need to determine whether some identity is included in a set of valid identities. This inclusion status is subjective, just like the trust degree; the set of included identities will vary based on the source identity. We define an inclusion function to be a function that takes as input a trust degree and an identity cost and returns a boolean (true or false). This trust degree and identity cost should pertain to a target identity in a web of trust, and the returned value should indicate whether or not the target identity is included in the source's trusted network. We of course have not yet defined identity cost. We will define an identity cost to be some cost required for an identity to be considered valid. In the physical world this might be something such as an IP address, but this could also be a monetary deposit (e.g. burning a token of a cryptocurrency) or a proof-of-work based deposit (where proof of some computational work must be provided for validation). This measure is tied to that identity for its lifetime. The identity cost is assumed to be an objective measure that is not disputable. The intent behind the identity cost is to mitigate abuse through Sybil attacks. It can act in a similar matter to rate limiting, significantly reducing the damage that might be done by creating fake identities to manipulate or simply overwhelm the system. By attaching a cost to identities, sybil attacks become less economically viable. Discussion The Intersubjectivity of Trust Intersubjectivity has been defined as \"mutual awareness of agreement or disagreement and even the realisation of such understanding or misunderstanding\" [4]. Trust cannot be connected to objective realities or pure mathematical algorithms as it is highly connected to the concept of intersubjectivity. Trust is relative based on our perceptions of other human actors in a web of trust. In the context of trust, this means that an actor's trust perception within a web of trust evolves continuously and subjectively based on their interactions and their observation of other interactions. Isolation Prevention in Intersubjective Systems In the section on Sybil resistance, we introduced the concept of good nodes, bad nodes, and confused nodes. While this is a useful framework for considering Sybil attacks, it is important to keep in mind that these labels are subjective and do not exist in the real world. What one node considers bad might be considered good by another. Graphs and groups that routinely exclude perceived \"bad\" nodes based on fixed rules will have a tendency to create isolated subgraphs and groups. Nodes will cluster around shared beliefs, perspectives, and interests. In doing so, they block communication channels and as a consequence continue to separate and draw groups apart. The result is a feedback loop that creates factions which are highly intra- connected but not well inter -connected. We can describe this principle more generally: clusters of nodes in a system are bound to make contact, either continuously or intermittently. If they are continuously integrated, this process will be smooth and manageable. If they are allowed to drift apart for too long, they will eventually collide, and the collision may be explosive and unpredictable. The challenge we face is thus: how can we keep clusters continuously integrated? Interestingly, the confused nodes that we described in the section on Sybil attacks may best be equipped to perform this role. Though they were considered an undesirable aspect previously, they may serve an important function in keeping disparate groups from drifting too far apart. We present the following questions as food for thought: how can we create confused nodes that connect the groups that tend to move further apart? And at the same time: how do we design webs of trust where while encouraging confused nodes, we don't turn everyone into one? Future Work Here we have attempted to introduce and define a framework for measuring degrees of trust. There is much follow-up work to be done. First, we welcome criticisms, extensions, and modifications of the framework. Second, we are interested in seeing various trust metrics proposed, in addition to analyses of the strengths and weaknesses of each metric. Game-theoretic analyses, models, and simulations are welcomed. We are interested in how this framework might be combined with other techniques to create real-world reputation systems. Finally, we are interested in further consideration of the human, social implications of the ideas presented. References [1] [https://www.nytimes.com/2011/07/17/sunday-review/17disappear.html]{.underline} [2] [https://www.prnewswire.com/news-releases/botnet-detection-market-worth-11911-million-usd-by-2023-681515921.html]{.underline} [3] [https://web.archive.org/web/20170627230829/http://www.advogato.org/trust-metric.html]{.underline} [4] [http://users.utu.fi/freder/gillespie.pdf]{.underline}","title":"Peer to peer degrees of trust"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#peer-to-peer-degrees-of-trust","text":"Harrison Stahl, Titus Capilnean, Peter Snyder, Tyler Yasaka Rebooting the Web of Trust, Fall 2018","title":"Peer to Peer Degrees of Trust"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#abstract","text":"Aunthenticity is a challenge for any identity solution. In the physical world, at least in America, it is not difficult to change one's identity [1]. In the digital world, there is the problem of bots. The botnet detection market is expected to be worth over one billion USD by 2023 [2], in a landscape where most digital activity is still heavily centralized. These centralized digital solutions have the advantage of being able to track IP addresses, request phone verification, and present CAPTCHAs to users in order to authenticate them. If this problem is so difficult to solve in the centralized world, how much more challenging will it be in the decentralized world, where none of these techniques are available? In this paper, we explore the idea of using a web of trust as a tool to add authenticity to decentralized identifiers (DIDs). We define a framework for deriving relative trust degrees using a given trust metric : a \"trustworthiness\" score for a given identity from the perspective of another identity. It is our intent that this framework may be used as a starting point for an ongoing exploration of graph-based, decentralized trust. We believe this approach may ultimately be used as a foundation for decentralized reputation.","title":"Abstract"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#related-work-on-graph-based-reputation-systems","text":"","title":"Related Work on Graph-Based Reputation Systems"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#pgp","text":"PGP (Pretty Good Privacy) is a system for asymmetric encryption. It was originally designed as a system for encrypting email contents, but it is also used for a variety of other file-encryption purposes. In addition to defining a system for asymmetric message encryption, PGP also defines a system for key discovery, based off of key servers (which are conceptually distributed, but in practice use a small number of well-known and trusted servers), a method for assigning trust to keys that have been uploaded to these servers, and an algorithm for determining how much trust to assign to a key that was uploaded by a given email address, but which belongs to an account unfamiliar to the email sender. Collectively this system is called the \\\"Web of Trust\\\". This system has many similarities to the kinds of systems envisioned in this paper. The design is fundamentally decentralized, it allows parties to reason about whether, and how much, to trust unfamiliar peers, and it allows participants to model trust in a non-binary way (i.e. users can be trusted \\\"some\\\", but not fully or not at all). However, the PGP \\\"Web of Trust\\\" model differs from the model envisioned in this paper in several ways. First, while trust can be assigned in a non-binary manner, it can only be \\\"derived\\\" binary (i.e. a key is either trusted or untrusted, but the \\\"Web of Trust\\\" algorithm isn\\'t intended to be used to provide finer grained trust information). Additionally, the PGP Web of Trust does not include a natural way to describe different domains of trust. One might wish to say \\\"I trust party X very much when they\\'re discussing music, but very little when they\\'re discussing politics\\\". The PGP model is not a natural fit for such statements.","title":"PGP"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#social-networks","text":"Online social networks such as Facebook, Twitter and Instagram conceptually model their systems as graphs, with participants as nodes, and connections between the participants as edges with labels like \\\"friend\\\", \\\"follower\\\" or \\\"boyfriend\\\". These edges can be directed or bi-directional, depending on the relationship being represented. The maintainers of these systems use the graph representation to make decisions about relationship, event or interest recommendations: a friend of your friend is more likely to also be your friend than a randomly selected person in the graph. While these systems are not primarily intended to be reputation or trust-management systems, the same algorithms and representations used in online social networks could guide the development of decentralized trust systems.","title":"Social Networks"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#recommendation-systems","text":"Graph-based recommendation systems have been deployed for many commercial purposes. Examples include the video recommendation systems used in YouTube and Netflix and the product-recommendation systems used in sites like Amazon and eBay. In such systems, videos and products can be modeled as nodes. The users and customers of such sites are also represented as nodes. Recommendations are represented as edges from the latter to the former, possibly weighed for systems that allow non-binary edges (e.g. 1-5 star ratings). The structure of such graphs are used to recommend similar, popular products and videos to other users. As in the \\\"Social Networks\\\" case, the same representations and techniques used in such systems can inform the design and algorithms used in a decentralized reputation / trust system.","title":"Recommendation Systems"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#degrees-of-trust-a-framework","text":"","title":"Degrees of Trust: A Framework"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#a-relative-trust-score","text":"Google's PageRank algorithm is famously able to assign ranking scores to web pages from a directed graph, where edges represent links from one website to another. One major problem with PageRank, however, is that it is vulnerable to Sybil attacks. Indeed, after Google implemented PageRank we witnessed the phenomenon of link farms: a form of Sybil attack that takes advantage of the fact that (in the pure form of PageRank) all pages are first-class citizens with equal ability to cast \"votes\" in the algorithm, in the form of web links. Dummy web pages could be created cheaply to deceive the algorithm. Of course, it is now widely known that Google uses a much more sophisticated version of PageRank that is less susceptible to link farms (though the algorithm itself is kept secret). In many respects, deriving trust scores in an open web of trust is similar to the problem of deriving web page rankings in an open internet. Both the web of trust and the internet can be represented as a directed graph, where edges represent votes from one node in favor of another. Indeed, the defunct website Advogato used a web of trust with designated \"seed nodes\" in order to generate trust scores for members of the site [3]. Advogato's solution was Sybil resistant thanks to the seed nodes, and Google's updated ranking algorithm may very well use a similar concept of \"seed\" websites to protect against link farms. There is a problem with seed nodes, however. Seed nodes are inherently more powerful than other nodes -- an inequality of power that we do not want in a decentralized system. Seed nodes seem to be necessary when these two requirements exist for a graph-based system: (1) that it must produce absolute scores for each node, and (2) that it must be Sybil resistant. We want to keep the second requirement, obviously. But what if we were to relax the first? Next we explore the concept of deriving relative trust scores. We define a trust score to be relative if it varies based on the observer.","title":"A Relative Trust Score"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#assumptions","text":"In this framework, we assume an underlying web of trust that is represented as a directed graph. Each edge in the graph represents a trust link : a statement that one node trusts another. These links may be weighted or unweighted, depending on the implementation. This web of trust may be created explicitly (e.g. by explicit statements among users) or it may be derived from some other data (e.g. \"follows\" on a social media platform). We do not make any assumptions as to what is represented by a node and what is represented by an edge. The most intuitive interpretation is that a node represents a human and that an edge represents a statement of trust made by one human about another. However, this framework may be applied to any application where nodes have use for decentralized reputation, and where there is some data available regarding which nodes are considered reliable by other nodes. These nodes could just as well represent devices, organizations, or something else entirely.","title":"Assumptions"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#trust-metrics","text":"We define a trust metric as a function that takes as input a web of trust, a source node, and a target node, and outputs a value that represents the degree of trust from the source to the target. In other words, this function yields a degree of trust (how trustworthy one node is from the perspective of another node), given a web of trust. A simple example of a trust metric would be a function that counts the number of hops along the shortest path from a source node s to a target node t . The inverse of this number could be returned as the result of this function. The effect of such a metric would be that the degree of trust is inversely proportional to the number of hops required to get from s to t . However, other metrics could also be used. Another similar metric might be to take the inverse of two raised to the power of the number of hops. In this metric, the degree of trust would decrease logarithmically as the distance from s to t increased. More complex metrics might do other interesting things, such as taking into account multiple paths from s to t . Ultimately then, these metrics are all performing some sort of graph analysis, where the graph is centered around the source (i.e. from the perspective of the source). There is certainly a limit to what can be known purely based on a graph analysis, and intuitively one would expect this limit to depend on the connectedness of the graph as well as the error rate of trust links. It is assumed that there would be at least some error rate (i.e. some percentage of trust links that are directed to untrustworthy nodes). With too high of an error rate and/or too sparse of a graph, it might be difficult to distinguish with much certainty a trustworthy node from a malicious node. Despite these limitations, we believe that in some systems there may exist some useful information related to trust. It is the intent of this framework is to allow as much of this information to be utilized as possible.","title":"Trust Metrics"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#sybil-resistance","text":"So far we have been using the term Sybil resistance rather vaguely. Here we will provide a precise definition that applies to the manipulation of trust scores in a web of trust. To do this, we will borrow the concept of good nodes, confused nodes, and bad nodes from Advogato. Good nodes in a web of trust are honest and will not attempt to manipulate trust scores. Confused nodes are also honest and will not attempt to manipulate trust scores, but may mistakenly trust one or more bad nodes. Bad nodes, then, are those that will create fake nodes in order to game the system in some way. The definition of a confused node is rather open-ended, however. Does a confused node trust a single bad node, or multiple bad nodes, or an unlimited number of bad nodes? For the sake of clarity, we will define an edge from a confused node to a bad node as a confused edge. We will also define a puppet node as a fake identity in a web of trust created by an attacker for the purpose of gaming the system. More precisely: puppet nodes are the subset of bad nodes that are not adjacent to a confused edge. We propose the following definition: a trust metric is Sybil resistant if and only if the upper limit of the combined trust scores that can belong to bad nodes is bounded by the number of confused edges. Stated another way, a trust metric is Sybil resistant if and only if there is a finite amount of combined trust an attacker can gain in a system merely by creating puppet nodes. This definition alone is not sufficient when considering Sybil attacks. While it helps to know that the impact that a Sybil attack can have is bounded, we might also wish to know the degree of Sybil resistance that a metric has. We will define the degree of Sybil resistance as the maximum trust that an attacker can have with a single bad node, divided by the maximum combined trust that can be accumulated by an attacker with an unlimited number of puppet nodes. A metric where puppet nodes have no effect has a Sybil-resistance degree of one; a metric where puppet nodes can increase the combined trust score of an attacker has a Sybil-resistance degree less than one; and a metric where puppet nodes can increase the combined trust score of an attacker without bound has a Sybil-resistance degree of zero (i.e. is not Sybil resistant).","title":"Sybil Resistance"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#inclusion-status","text":"Here we want to introduce the concept of inclusion status . We envision that many applications that use webs of trust will need to determine whether some identity is included in a set of valid identities. This inclusion status is subjective, just like the trust degree; the set of included identities will vary based on the source identity. We define an inclusion function to be a function that takes as input a trust degree and an identity cost and returns a boolean (true or false). This trust degree and identity cost should pertain to a target identity in a web of trust, and the returned value should indicate whether or not the target identity is included in the source's trusted network. We of course have not yet defined identity cost. We will define an identity cost to be some cost required for an identity to be considered valid. In the physical world this might be something such as an IP address, but this could also be a monetary deposit (e.g. burning a token of a cryptocurrency) or a proof-of-work based deposit (where proof of some computational work must be provided for validation). This measure is tied to that identity for its lifetime. The identity cost is assumed to be an objective measure that is not disputable. The intent behind the identity cost is to mitigate abuse through Sybil attacks. It can act in a similar matter to rate limiting, significantly reducing the damage that might be done by creating fake identities to manipulate or simply overwhelm the system. By attaching a cost to identities, sybil attacks become less economically viable.","title":"Inclusion Status"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#discussion","text":"","title":"Discussion"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#the-intersubjectivity-of-trust","text":"Intersubjectivity has been defined as \"mutual awareness of agreement or disagreement and even the realisation of such understanding or misunderstanding\" [4]. Trust cannot be connected to objective realities or pure mathematical algorithms as it is highly connected to the concept of intersubjectivity. Trust is relative based on our perceptions of other human actors in a web of trust. In the context of trust, this means that an actor's trust perception within a web of trust evolves continuously and subjectively based on their interactions and their observation of other interactions.","title":"The Intersubjectivity of Trust"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#isolation-prevention-in-intersubjective-systems","text":"In the section on Sybil resistance, we introduced the concept of good nodes, bad nodes, and confused nodes. While this is a useful framework for considering Sybil attacks, it is important to keep in mind that these labels are subjective and do not exist in the real world. What one node considers bad might be considered good by another. Graphs and groups that routinely exclude perceived \"bad\" nodes based on fixed rules will have a tendency to create isolated subgraphs and groups. Nodes will cluster around shared beliefs, perspectives, and interests. In doing so, they block communication channels and as a consequence continue to separate and draw groups apart. The result is a feedback loop that creates factions which are highly intra- connected but not well inter -connected. We can describe this principle more generally: clusters of nodes in a system are bound to make contact, either continuously or intermittently. If they are continuously integrated, this process will be smooth and manageable. If they are allowed to drift apart for too long, they will eventually collide, and the collision may be explosive and unpredictable. The challenge we face is thus: how can we keep clusters continuously integrated? Interestingly, the confused nodes that we described in the section on Sybil attacks may best be equipped to perform this role. Though they were considered an undesirable aspect previously, they may serve an important function in keeping disparate groups from drifting too far apart. We present the following questions as food for thought: how can we create confused nodes that connect the groups that tend to move further apart? And at the same time: how do we design webs of trust where while encouraging confused nodes, we don't turn everyone into one?","title":"Isolation Prevention in Intersubjective Systems"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#future-work","text":"Here we have attempted to introduce and define a framework for measuring degrees of trust. There is much follow-up work to be done. First, we welcome criticisms, extensions, and modifications of the framework. Second, we are interested in seeing various trust metrics proposed, in addition to analyses of the strengths and weaknesses of each metric. Game-theoretic analyses, models, and simulations are welcomed. We are interested in how this framework might be combined with other techniques to create real-world reputation systems. Finally, we are interested in further consideration of the human, social implications of the ideas presented.","title":"Future Work"},{"location":"RWoT7/peer-to-peer-degrees-of-trust/#references","text":"[1] [https://www.nytimes.com/2011/07/17/sunday-review/17disappear.html]{.underline} [2] [https://www.prnewswire.com/news-releases/botnet-detection-market-worth-11911-million-usd-by-2023-681515921.html]{.underline} [3] [https://web.archive.org/web/20170627230829/http://www.advogato.org/trust-metric.html]{.underline} [4] [http://users.utu.fi/freder/gillespie.pdf]{.underline}","title":"References"},{"location":"RWoT7/resource-integrity-proofs/","text":"Resource Integrity Proofs Cryptographic linking provides discoverability, integrity, and scheme agility Authors: Ganesh Annan and Kim Hamilton Duffy Contributors: Manu Sporny, Dave Longley, David Lehn, and Bohdan Andriyiv Abstract Currently, the Web provides a simple yet powerful mechanism for the dissemination of information via links. Unfortunately, there is no generalized mechanism that enables verifying that a fetched resource has been delivered without unexpected manipulation. Would it be possible to create an extensible and multipurpose cryptographic link that provides discoverability, integrity, and scheme agility? This paper proposes a linking solution that decouples integrity information from link and resource syntaxes, enabling verification of any representation of a resource from any type of link. We call this approach Resource Integrity Proofs (RIPs). RIPs provide a succinct way to link to resources with cryptographically verifiable content integrity. RIPs can be combined with blockchain technology to create discoverable proofs of existence to off-chain resources. Introduction Cryptographic linking solutions today have yet to provide a generalized mechanism for creating tamper-evident links. The Subresource Integrity standard limits this guarantee to script and link resources loaded on Web pages via the use of HTML attributes. IPFS provides a verification mechanism that is constrained to hash-based, content-addressable links, with no ability to complete content negotiation. RFC6920 proposes another mechanism that cannot be applied to existing links: it recommends the use of named information hashes and a resolution method that creates a content addressable URL [ 1 ]. Resource Integrity Proofs incorporates ideas from these standards and solutions to provide a new data format for cryptographic links that is fit for the open world. This paper describes use cases benefitting from RIPs, such as enabling Verifiable Displays and meeting regulatory compliance. Features Integrity Resource Integrity Proofs use the representation of a resource as the input to a cryptographic hash function to generate a digest value. We can reproduce the digest value because the RIP data model requires recording of the content type and digest algorithm. Third parties can easily verify data received by 1) dereferencing the URL of the desired resource and 2) using the digest algorithm on the data to generate a matching digest value, ensuring that the data was not unexpectedly manipulated. The content received is now tamper-evident. RIPs may be placed on blockchains to simultaneously enable discoverability of off-chain resources and establish a proof of existence. Discoverability Resource Integrity Proofs allow any party to find a given resource. This is achieved simply by including the URL of the resource in the data model. RIPs may be placed on blockchains to enable discovery of data whilst keeping sensitive data off-chain (i.e., private and secure). Scheme Agility Resource Integrity Proofs make no assumptions on the URL scheme used. This scheme agility means that one can enable verification of the integrity of a resource using any URL scheme with any content type. Data Model The Resource Integrity Proof (RIP) is a data model built using the Linked Data Proofs specification. It can be represented using many different syntaxes; examples are given here in JSON-LD , N-Quads, and in a simple table. id The location of the resource. proof The Linked Data digital proof. type : The identifier for the digital proof suite . contentType : The content type for the resource. multiDigest : The multibase encoded multihash [ 2 ]. JSON-LD Syntax { \"@context\": \"https://w3id.org/security/v2\", \"id\": \"https://example.com/storage/ndBRHU8gqjRzkcRdrPC2XQ\", \"proof\": { \"type\": \"Multihash2018\", \"contentType\": \"application/json\", \"multiDigest\": \"zQmUvZSaVzgjVHCDDDAoNNBgpiAkN6wKmCcD37vvnmoKq6e\" } } N-Quads Syntax <https://example.com/storage/ndBRHU8gqjRzkcRdrPC2XQ> <https://w3id.org/security#proof> _:b0 . _:b0 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://w3id.org/security#Multihash2018> . _:b0 <http://schema.org/contentType> \"application/json\" . _:b0 <https://w3id.org/security#multiDigest> \"zQmUvZSaVzgjVHCDDDAoNNBgpiAkN6wKmCcD37vvnmoKq6e\" . Table Subject Predicate Object Graph https://example.com/storage/ndBRHU8gqjRzkcRdrPC2XQ https://w3id.org/security#proof :b0 :b1 http://www.w3.org/1999/02/22-rdf-syntax-ns#type https://w3id.org/security#Multihash2018 :b0 :b1 http://schema.org/contentType application/pdf :b0 :b1 https://w3id.org/security#multiDigest zQmUvZSaVzgjVHCDDDAoNNBgpiAkN6wKmCcD37vvnmoKq6e _:b0 Use Cases There are many compelling applications of RIPs in a decentralized ecosystem. We will first dive into the problem of Verifiable Displays , which seeks to ensure that the rendering of the Verifiable Credential content matches what the issuer intended. Next, we will envision a new age regulatory compliance system built on top of Decentralized Identifiers (DID) , Verifiable Credentials (VC) , and Object Capabilities (OCAP) . Verifiable Displays In the Educational/Occupational Credentials space, RIPs allow issuers to specify a set of approved visual renderings associated with a signed claim. This enables any viewer of the claim to determine if the visual rendering differs from what was intended by the issuer -- an ability that's critical for detecting social engineering attacks introduced by tampering with the rendered image. The \"verifiability\" of a Verifiable Credential applies to the content of the claim, not necessarily the human-readable display. As described in Verifiable Displays , this risk has been generally been addressed in an ad-hoc, use-case-dependent way. But there is no clear standard or convention for tamper detection across different credential schemas and use cases. This example shows how we might use a RIP to address the problem of proving that a PNG file hashes to the value expected by a referencing Verifiable Credential: { \"@context\": [\"https://w3id.org/credentials/v1\", \"https://w3id.org/security/v2\"], \"id\": \"http://credentials.example.org/credentials/3732\", \"type\": [\"VerifiableCredential\", \"EmployeeOfTheMonthCredential\"], \"issuer\": \"did:example:12345678\", \"issuanceDate\": \"2014-01-02\", \"expirationDate\": \"2014-02-02\", \"claim\": { \"id\": \"did:example:10011872\", \"accomplishment\": \"Employee of the Month Demonstrating Excellent Leadership Skills\" }, \"verifiableDisplay\": { \"id\": \"https://raw.githubusercontent.com/WebOfTrustInfo/rwot7/master/draft-documents/images/exampleVerifiableDisplay.png\", \"proof\": { \"type\": [\"ResourceIntegrityProof\", \"Multihash2018\"], \"contentType: \"image/png\", \"multiDigest\": \"zQmUvZSaVzgjVHCDDDAoNNBgpiAkN6wKmCcD37vvnmoKq6e\" } }, \"proof\": { ... } } In this example, we leverage the ResourceIntegrityProof and Multihash2018 type to say that the image identified by id is expected to have a multibase encoded multihash that matches the value in multiDigest . Extensions to General Linked Data Expanding on linked visual data examples, this method could enable a pharmacist to ensure the prescription they are viewing matches the associated machine-readable content. If the credential contained sensitive data, we wouldn't want the image to be publicly-hosted. But this is also supported: id can be any URI, so the referenced visual rendering could be stored offline. RIPs enable snapshot integrity proofs for general linked data; this can be used for credentials bridging legacy systems where data is stored in a mutable store. Meeting Regulatory Compliance Organizations must provide documentation to regulators in order to maintain compliance. We can implement software to meet the full requirements of this use case by adding RIPs to the already composable Lego-like ecosystem of interoperable decentralized technologies such as DIDs, VCs, and OCAPs -- and by combining this ecosystem with a cryptographically auditable system, such as a blockchain. When an organization is preparing supporting documentation to meet compliance, they can post one or more RIPs and an OCAP for accessing each resource to a blockchain. This OCAP only grants access to the regulator and only to the specific items and for the duration that they need. Posting the RIP to a blockchain enables discoverability of the resource and establishes a proof of existence. The tamper-evident characteristics of the blockchain prove that the data existed at some point in the past, establishing trust via the cryptosystem rather than requiring it in the organization. The regulator then uses the delegated OCAP to dereference the url in the RIP and to ensure the data was not changed since the time of submission. Appendix [1] Example from Naming Things with Hashes ( RFC6920 ) Using an Authority of example.com and the sha-256 hash of the text \"Hello World!\" we can generate the following ni URI: ni://example.com/sha-256;f4OxZX_x_FO5LcGBSKHWXfwtSx-j1ncoSt3SABJtkGk The generated ni URI takes advantage of the .well-known URI ( RFC5785 ) format so that we can dereference the information using HTTP(S): http://example.com/.well-known/ni/sha-256/f4OxZX_x_FO5LcGBSKHWXfwtSx-j1ncoSt3SABJtkGk [2] Generating a Multibase encoded Multihash An example using Node.js v8.12.0 const multibase = require(\"multibase\"); // v0.5.0 const multihash = require(\"multihashes\"); // v0.4.14 const crypto = require(\"crypto\"); const hash = crypto.createHash('sha256').update('Hello World!', 'utf8').digest(); // hash is 7f83b1657ff1fc53b92dc18148a1d65dfc2d4b1fa3d677284addd200126d9069 const mh = multihash.encode(hash, 'sha2-256'); // mh is 12207f83b1657ff1fc53b92dc18148a1d65dfc2d4b1fa3d677284addd200126d9069 const mb = multibase.encode('base58btc', mh); // mb is zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e","title":"Resource Integrity Proofs"},{"location":"RWoT7/resource-integrity-proofs/#resource-integrity-proofs","text":"Cryptographic linking provides discoverability, integrity, and scheme agility Authors: Ganesh Annan and Kim Hamilton Duffy Contributors: Manu Sporny, Dave Longley, David Lehn, and Bohdan Andriyiv","title":"Resource Integrity Proofs"},{"location":"RWoT7/resource-integrity-proofs/#abstract","text":"Currently, the Web provides a simple yet powerful mechanism for the dissemination of information via links. Unfortunately, there is no generalized mechanism that enables verifying that a fetched resource has been delivered without unexpected manipulation. Would it be possible to create an extensible and multipurpose cryptographic link that provides discoverability, integrity, and scheme agility? This paper proposes a linking solution that decouples integrity information from link and resource syntaxes, enabling verification of any representation of a resource from any type of link. We call this approach Resource Integrity Proofs (RIPs). RIPs provide a succinct way to link to resources with cryptographically verifiable content integrity. RIPs can be combined with blockchain technology to create discoverable proofs of existence to off-chain resources.","title":"Abstract"},{"location":"RWoT7/resource-integrity-proofs/#introduction","text":"Cryptographic linking solutions today have yet to provide a generalized mechanism for creating tamper-evident links. The Subresource Integrity standard limits this guarantee to script and link resources loaded on Web pages via the use of HTML attributes. IPFS provides a verification mechanism that is constrained to hash-based, content-addressable links, with no ability to complete content negotiation. RFC6920 proposes another mechanism that cannot be applied to existing links: it recommends the use of named information hashes and a resolution method that creates a content addressable URL [ 1 ]. Resource Integrity Proofs incorporates ideas from these standards and solutions to provide a new data format for cryptographic links that is fit for the open world. This paper describes use cases benefitting from RIPs, such as enabling Verifiable Displays and meeting regulatory compliance.","title":"Introduction"},{"location":"RWoT7/resource-integrity-proofs/#features","text":"","title":"Features"},{"location":"RWoT7/resource-integrity-proofs/#integrity","text":"Resource Integrity Proofs use the representation of a resource as the input to a cryptographic hash function to generate a digest value. We can reproduce the digest value because the RIP data model requires recording of the content type and digest algorithm. Third parties can easily verify data received by 1) dereferencing the URL of the desired resource and 2) using the digest algorithm on the data to generate a matching digest value, ensuring that the data was not unexpectedly manipulated. The content received is now tamper-evident. RIPs may be placed on blockchains to simultaneously enable discoverability of off-chain resources and establish a proof of existence.","title":"Integrity"},{"location":"RWoT7/resource-integrity-proofs/#discoverability","text":"Resource Integrity Proofs allow any party to find a given resource. This is achieved simply by including the URL of the resource in the data model. RIPs may be placed on blockchains to enable discovery of data whilst keeping sensitive data off-chain (i.e., private and secure).","title":"Discoverability"},{"location":"RWoT7/resource-integrity-proofs/#scheme-agility","text":"Resource Integrity Proofs make no assumptions on the URL scheme used. This scheme agility means that one can enable verification of the integrity of a resource using any URL scheme with any content type.","title":"Scheme Agility"},{"location":"RWoT7/resource-integrity-proofs/#data-model","text":"The Resource Integrity Proof (RIP) is a data model built using the Linked Data Proofs specification. It can be represented using many different syntaxes; examples are given here in JSON-LD , N-Quads, and in a simple table. id The location of the resource. proof The Linked Data digital proof. type : The identifier for the digital proof suite . contentType : The content type for the resource. multiDigest : The multibase encoded multihash [ 2 ]. JSON-LD Syntax { \"@context\": \"https://w3id.org/security/v2\", \"id\": \"https://example.com/storage/ndBRHU8gqjRzkcRdrPC2XQ\", \"proof\": { \"type\": \"Multihash2018\", \"contentType\": \"application/json\", \"multiDigest\": \"zQmUvZSaVzgjVHCDDDAoNNBgpiAkN6wKmCcD37vvnmoKq6e\" } } N-Quads Syntax <https://example.com/storage/ndBRHU8gqjRzkcRdrPC2XQ> <https://w3id.org/security#proof> _:b0 . _:b0 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://w3id.org/security#Multihash2018> . _:b0 <http://schema.org/contentType> \"application/json\" . _:b0 <https://w3id.org/security#multiDigest> \"zQmUvZSaVzgjVHCDDDAoNNBgpiAkN6wKmCcD37vvnmoKq6e\" . Table Subject Predicate Object Graph https://example.com/storage/ndBRHU8gqjRzkcRdrPC2XQ https://w3id.org/security#proof :b0 :b1 http://www.w3.org/1999/02/22-rdf-syntax-ns#type https://w3id.org/security#Multihash2018 :b0 :b1 http://schema.org/contentType application/pdf :b0 :b1 https://w3id.org/security#multiDigest zQmUvZSaVzgjVHCDDDAoNNBgpiAkN6wKmCcD37vvnmoKq6e _:b0","title":"Data Model"},{"location":"RWoT7/resource-integrity-proofs/#use-cases","text":"There are many compelling applications of RIPs in a decentralized ecosystem. We will first dive into the problem of Verifiable Displays , which seeks to ensure that the rendering of the Verifiable Credential content matches what the issuer intended. Next, we will envision a new age regulatory compliance system built on top of Decentralized Identifiers (DID) , Verifiable Credentials (VC) , and Object Capabilities (OCAP) .","title":"Use Cases"},{"location":"RWoT7/resource-integrity-proofs/#verifiable-displays","text":"In the Educational/Occupational Credentials space, RIPs allow issuers to specify a set of approved visual renderings associated with a signed claim. This enables any viewer of the claim to determine if the visual rendering differs from what was intended by the issuer -- an ability that's critical for detecting social engineering attacks introduced by tampering with the rendered image. The \"verifiability\" of a Verifiable Credential applies to the content of the claim, not necessarily the human-readable display. As described in Verifiable Displays , this risk has been generally been addressed in an ad-hoc, use-case-dependent way. But there is no clear standard or convention for tamper detection across different credential schemas and use cases. This example shows how we might use a RIP to address the problem of proving that a PNG file hashes to the value expected by a referencing Verifiable Credential: { \"@context\": [\"https://w3id.org/credentials/v1\", \"https://w3id.org/security/v2\"], \"id\": \"http://credentials.example.org/credentials/3732\", \"type\": [\"VerifiableCredential\", \"EmployeeOfTheMonthCredential\"], \"issuer\": \"did:example:12345678\", \"issuanceDate\": \"2014-01-02\", \"expirationDate\": \"2014-02-02\", \"claim\": { \"id\": \"did:example:10011872\", \"accomplishment\": \"Employee of the Month Demonstrating Excellent Leadership Skills\" }, \"verifiableDisplay\": { \"id\": \"https://raw.githubusercontent.com/WebOfTrustInfo/rwot7/master/draft-documents/images/exampleVerifiableDisplay.png\", \"proof\": { \"type\": [\"ResourceIntegrityProof\", \"Multihash2018\"], \"contentType: \"image/png\", \"multiDigest\": \"zQmUvZSaVzgjVHCDDDAoNNBgpiAkN6wKmCcD37vvnmoKq6e\" } }, \"proof\": { ... } } In this example, we leverage the ResourceIntegrityProof and Multihash2018 type to say that the image identified by id is expected to have a multibase encoded multihash that matches the value in multiDigest .","title":"Verifiable Displays"},{"location":"RWoT7/resource-integrity-proofs/#extensions-to-general-linked-data","text":"Expanding on linked visual data examples, this method could enable a pharmacist to ensure the prescription they are viewing matches the associated machine-readable content. If the credential contained sensitive data, we wouldn't want the image to be publicly-hosted. But this is also supported: id can be any URI, so the referenced visual rendering could be stored offline. RIPs enable snapshot integrity proofs for general linked data; this can be used for credentials bridging legacy systems where data is stored in a mutable store.","title":"Extensions to General Linked Data"},{"location":"RWoT7/resource-integrity-proofs/#meeting-regulatory-compliance","text":"Organizations must provide documentation to regulators in order to maintain compliance. We can implement software to meet the full requirements of this use case by adding RIPs to the already composable Lego-like ecosystem of interoperable decentralized technologies such as DIDs, VCs, and OCAPs -- and by combining this ecosystem with a cryptographically auditable system, such as a blockchain. When an organization is preparing supporting documentation to meet compliance, they can post one or more RIPs and an OCAP for accessing each resource to a blockchain. This OCAP only grants access to the regulator and only to the specific items and for the duration that they need. Posting the RIP to a blockchain enables discoverability of the resource and establishes a proof of existence. The tamper-evident characteristics of the blockchain prove that the data existed at some point in the past, establishing trust via the cryptosystem rather than requiring it in the organization. The regulator then uses the delegated OCAP to dereference the url in the RIP and to ensure the data was not changed since the time of submission.","title":"Meeting Regulatory Compliance"},{"location":"RWoT7/resource-integrity-proofs/#appendix","text":"","title":"Appendix"},{"location":"RWoT7/resource-integrity-proofs/#1-example-from-naming-things-with-hashes-rfc6920","text":"Using an Authority of example.com and the sha-256 hash of the text \"Hello World!\" we can generate the following ni URI: ni://example.com/sha-256;f4OxZX_x_FO5LcGBSKHWXfwtSx-j1ncoSt3SABJtkGk The generated ni URI takes advantage of the .well-known URI ( RFC5785 ) format so that we can dereference the information using HTTP(S): http://example.com/.well-known/ni/sha-256/f4OxZX_x_FO5LcGBSKHWXfwtSx-j1ncoSt3SABJtkGk","title":"[1] Example from Naming Things with Hashes (RFC6920)"},{"location":"RWoT7/resource-integrity-proofs/#2-generating-a-multibase-encoded-multihash","text":"An example using Node.js v8.12.0 const multibase = require(\"multibase\"); // v0.5.0 const multihash = require(\"multihashes\"); // v0.4.14 const crypto = require(\"crypto\"); const hash = crypto.createHash('sha256').update('Hello World!', 'utf8').digest(); // hash is 7f83b1657ff1fc53b92dc18148a1d65dfc2d4b1fa3d677284addd200126d9069 const mh = multihash.encode(hash, 'sha2-256'); // mh is 12207f83b1657ff1fc53b92dc18148a1d65dfc2d4b1fa3d677284addd200126d9069 const mb = multibase.encode('base58btc', mh); // mb is zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e","title":"[2] Generating a Multibase encoded Multihash"},{"location":"RWoT8/","text":"Rebooting the Web of Trust VIII: Barcelona (March 2019) This repository contains documents related to RWOT8, the eighth Rebooting the Web of Trust design workshop, which ran in Barcelona, Spain on March 1st to 3rd, 2019. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Visit http://rwot8.eventbrite.com for more information and to purchase tickets. Event details for attendees (schedule, hotels, transportation) (pdf 14MB) Draft Documents These are the documents currently in process from RWOT8. Paper Lead CRUD and multi-device for IPID Andr\u00e9 Cruz Crypto Jurisdiction Toni DID Key Management in the Browser Alberto DID Spec Work Manu The Digital Citizen Chris C. Driving Adoption Based on Basic Human Needs Sam Enabling a Web of Trust with Self-Signed and Self-Issued Credentials Nader Establishing the Identity of the Issuer by the Verifier in Verifiable Credentials Matt How SSI Will Survive Capitalism Adrian G. Human-Meaningful Names for SSI Boyma Identity Containers Alex P. JORAM Illustrations Katie JORAM 2 Joe LibP2P for DID Auth jonnycrunch Peer DID Method Spec Report Brent Satyrn Joe Social Key Recovery #1: Shamir Secret Sharing Best Practices Christopher A. Social Key Recovery #1a: New SSS Library Daan & Mark Social Key Recovery #2: Evaluating Social Schemes for Recovering Control of an Identifier Peg Terminology Reiks Sociolegal frameworks for the phygital age Elizabeth Understanding DIDs Drummond Use Cases and Research Directions for self-sovereign publication and journalism Juan C. Using OpenID Connect Self-Issued to Achieve DID Auth Dmitri Z Using OpenID Connect Self-Issued to achieve Verifiable Credentials Presentation Dmitri Z Verifiable Credential Museum/Playground Ryan Topics & Advance Readings Please see Topics & Advance Readings for a list of readings prepared in advance of the conference.","title":"Rebooting the Web of Trust VIII: Barcelona (March 2019)"},{"location":"RWoT8/#rebooting-the-web-of-trust-viii-barcelona-march-2019","text":"This repository contains documents related to RWOT8, the eighth Rebooting the Web of Trust design workshop, which ran in Barcelona, Spain on March 1st to 3rd, 2019. The goal of the workshop was to generate five technical white papers and/or proposals on topics decided by the group that would have the greatest impact on the future. Visit http://rwot8.eventbrite.com for more information and to purchase tickets. Event details for attendees (schedule, hotels, transportation) (pdf 14MB)","title":"Rebooting the Web of Trust VIII: Barcelona (March 2019)"},{"location":"RWoT8/#draft-documents","text":"These are the documents currently in process from RWOT8. Paper Lead CRUD and multi-device for IPID Andr\u00e9 Cruz Crypto Jurisdiction Toni DID Key Management in the Browser Alberto DID Spec Work Manu The Digital Citizen Chris C. Driving Adoption Based on Basic Human Needs Sam Enabling a Web of Trust with Self-Signed and Self-Issued Credentials Nader Establishing the Identity of the Issuer by the Verifier in Verifiable Credentials Matt How SSI Will Survive Capitalism Adrian G. Human-Meaningful Names for SSI Boyma Identity Containers Alex P. JORAM Illustrations Katie JORAM 2 Joe LibP2P for DID Auth jonnycrunch Peer DID Method Spec Report Brent Satyrn Joe Social Key Recovery #1: Shamir Secret Sharing Best Practices Christopher A. Social Key Recovery #1a: New SSS Library Daan & Mark Social Key Recovery #2: Evaluating Social Schemes for Recovering Control of an Identifier Peg Terminology Reiks Sociolegal frameworks for the phygital age Elizabeth Understanding DIDs Drummond Use Cases and Research Directions for self-sovereign publication and journalism Juan C. Using OpenID Connect Self-Issued to Achieve DID Auth Dmitri Z Using OpenID Connect Self-Issued to achieve Verifiable Credentials Presentation Dmitri Z Verifiable Credential Museum/Playground Ryan","title":"Draft Documents"},{"location":"RWoT8/#topics-advance-readings","text":"Please see Topics & Advance Readings for a list of readings prepared in advance of the conference.","title":"Topics &amp; Advance Readings"},{"location":"RWoT8/did-spec-refinement/","text":"RWoT8 DID Specification Refinement Paper Lead: Manu Sporny Team (in alphabetical order): Dan Burnett, Ken Ebert, Amy Guy, Drummond Reed, Manu Sporny 2019-03-01 https://tinyurl.com/rwot8did Abstract The Decentralized Identifier (DID) specification describes a new type of URL that is globally unique, highly available, and cryptographically verifiable and which has no central authority. The DID spec document describes the expected ecosystem, data model, and syntaxes for DIDs. In December 2018, the W3C held a Strong Authentication and Identity Workshop that determined that a reasonable next step would be to create a W3C Working Group to standardize the DID specification. As a result, the W3C Credentials Community Group, which has been incubating the specification, will eventually need to hand the specification over to the newly formed W3C DID Working Group. In preparation for this hand off, a group at Rebooting the Web of Trust triaged issues related to the DID specification, refined existing proposals related to the specification, and gathered new features and requirements from the community. The result of this work is outlined in this document. Introduction In a recently published W3C Workshop Report on Strong Authentication and Identity , it was suggested that the DID Specification should be considered for standardization at the W3C via a new group called the W3C Decentralized Identifier Working Group (DID WG). The current Decentralized Identifier Specification maintains a list of issues that have been growing over the past year. Amy Guy and Dmitri Zagidulin performed an initial triage of the DID specification issues as an advanced reading topic paper for Rebooting the Web of Trust 8 (RWoT8). In an attempt to provide the DID WG with a good foundation to build upon, the W3C Credentials Community Group, with aid from the Rebooting the Web of Trust community, worked on the specification at the Rebooting the Web of Trust 8 (RWoT8) event in Barcelona, Spain in early March 2019. The group processed issues related to the DID specification, refined existing proposals related to the specification, and gathered new features and requirements from the community. The state of the work as of the end of the RWoT8 event is outlined below. The first day consisted of triaging issues that resulted in three broad categories: Issues that are editorial and did not require debate. Current features that required debate and refinement. New features and requirements that needed to be captured. The second day consisted of communicating open questions and answering them so that the group could divide the work up into different work tracks that could progress independently. The third day consisted of executing on the work tracks and making as much progress as we could. The group had wanted to produce a document to detail a plan for the W3C Credentials Community Group to summarize discussion or propose one or more PRs that add the feature to the specification Issue Triage One of the results of the issue triage was the desire to restructure the specification in order to communicate the concepts in the document more cleanly. The following structure was agreed upon by the group as an improvement that would need to be further refined: Intro A Simple Example Terminology Core Data Model (simple, short, overview) (~~151~~) Decentralised Identifiers (ABNF etc) DID Documents (detail on core concepts) DID Document Syntax DID Methods DID Resolvers Security Concerns Privacy Concerns Accessibility The result of the issue triage follow. Issue numbers with strike-throughs were resolved during the event. Update relationship between DID spec and URI spec: https://github.com/w3c-ccg/did-spec/issues/81 Does the did-spec need to be specific about which parts of the URI RFC it is conformant with? https://github.com/w3c-ccg/did-spec/issues/169 Intro overhaul Improve abstract 115, 112 Define target audience of the DID spec: https://github.com/w3c-ccg/did-spec/issues/157 Terminology and explanations, what is a DID? ~~122~~ (Motivations) ~~121~~ (Purpose, terminology) ~~117~~ (entities in Overview) 156 & 155 ~~123, 124~~ (management of identifiers, Design Goals, terminology) 130 title - close this once definition of DID and related parts are solidified throughout the rest of the spec 127 Remove uses of 'entity' and other fluffy terms when we mean DID Subject: ~~125, 145 139 154~~ Clarify language about keys: 147 143 105 166 Clarify language around ledgers: 150 119 149 ACTION: ledger -> Verifiable Data Registry (with clarification that there are multiple registries, see VC spec) Michael>> You mean Verifiable Data Registry (VDR), n'est pas? Quick fixes: ~~137~~, ~~144~~, 133, ~~116~~, ~~118~~, ~~129~~, ~~140, 141~~ Clarify language about requirements: ~~120~~ Feature Refinement MIME Types 1: https://github.com/w3c-ccg/did-spec/issues/84 MIME Types 2: https://github.com/w3c-ccg/did-spec/issues/82 ABNF for DID Syntax Use case: changing file hosting providers Use case: fragments continue to function after moving a resource that supports fragments (DB: and what, precisely, is the resource ?) Use case: change ActivityPub from one provider to another provider Use case: I want to advertise a legacy URL (Twitter, Linkedin, etc.) Use case: Citations Use case: Timo??? Use case: MS services by type??? Need more details. Semantics of DID URL vs DID Which URL spec? https://github.com/w3c-ccg/did-spec/issues/163 If we don't need WHAT WG URL spec features, then cite only the RFC3986. New Features and Requirements The following new features and requirements were identified at RWoT8 The need for a place to express keys that are used to digitally sign Verifiable Credentials (e.g. a \"credentialIssuer\" property) The need for a place to express keys that are used to digitally sign legal agreements (e.g. a \"legalAgreement\" property) Michael Herman's Key Feedback DID Resolution Technical Use Cases: https://github.com/w3c-ccg/did-resolution/issues/32 Check that small-e \"entities\" references are removed or better qualified Verifiable Data Registry - DID Resolver Architectures: https://github.com/mwherman2000/indy-arm/blob/master/images/HBB-DID-Resolver-Architecture%20v0.2.png DID Data Object References feedback: https://github.com/WebOfTrustInfo/rwot8-barcelona/issues DID Namespaces feedback: https://github.com/WebOfTrustInfo/rwot8-barcelona/issues Editorial work - WIP: https://github.com/w3c-ccg/did-spec/commits/rwot8-editorial","title":"RWoT8 DID Specification Refinement"},{"location":"RWoT8/did-spec-refinement/#rwot8-did-specification-refinement","text":"Paper Lead: Manu Sporny Team (in alphabetical order): Dan Burnett, Ken Ebert, Amy Guy, Drummond Reed, Manu Sporny 2019-03-01 https://tinyurl.com/rwot8did","title":"RWoT8 DID Specification Refinement"},{"location":"RWoT8/did-spec-refinement/#abstract","text":"The Decentralized Identifier (DID) specification describes a new type of URL that is globally unique, highly available, and cryptographically verifiable and which has no central authority. The DID spec document describes the expected ecosystem, data model, and syntaxes for DIDs. In December 2018, the W3C held a Strong Authentication and Identity Workshop that determined that a reasonable next step would be to create a W3C Working Group to standardize the DID specification. As a result, the W3C Credentials Community Group, which has been incubating the specification, will eventually need to hand the specification over to the newly formed W3C DID Working Group. In preparation for this hand off, a group at Rebooting the Web of Trust triaged issues related to the DID specification, refined existing proposals related to the specification, and gathered new features and requirements from the community. The result of this work is outlined in this document.","title":"Abstract"},{"location":"RWoT8/did-spec-refinement/#introduction","text":"In a recently published W3C Workshop Report on Strong Authentication and Identity , it was suggested that the DID Specification should be considered for standardization at the W3C via a new group called the W3C Decentralized Identifier Working Group (DID WG). The current Decentralized Identifier Specification maintains a list of issues that have been growing over the past year. Amy Guy and Dmitri Zagidulin performed an initial triage of the DID specification issues as an advanced reading topic paper for Rebooting the Web of Trust 8 (RWoT8). In an attempt to provide the DID WG with a good foundation to build upon, the W3C Credentials Community Group, with aid from the Rebooting the Web of Trust community, worked on the specification at the Rebooting the Web of Trust 8 (RWoT8) event in Barcelona, Spain in early March 2019. The group processed issues related to the DID specification, refined existing proposals related to the specification, and gathered new features and requirements from the community. The state of the work as of the end of the RWoT8 event is outlined below. The first day consisted of triaging issues that resulted in three broad categories: Issues that are editorial and did not require debate. Current features that required debate and refinement. New features and requirements that needed to be captured. The second day consisted of communicating open questions and answering them so that the group could divide the work up into different work tracks that could progress independently. The third day consisted of executing on the work tracks and making as much progress as we could. The group had wanted to produce a document to detail a plan for the W3C Credentials Community Group to summarize discussion or propose one or more PRs that add the feature to the specification","title":"Introduction"},{"location":"RWoT8/did-spec-refinement/#issue-triage","text":"One of the results of the issue triage was the desire to restructure the specification in order to communicate the concepts in the document more cleanly. The following structure was agreed upon by the group as an improvement that would need to be further refined: Intro A Simple Example Terminology Core Data Model (simple, short, overview) (~~151~~) Decentralised Identifiers (ABNF etc) DID Documents (detail on core concepts) DID Document Syntax DID Methods DID Resolvers Security Concerns Privacy Concerns Accessibility The result of the issue triage follow. Issue numbers with strike-throughs were resolved during the event. Update relationship between DID spec and URI spec: https://github.com/w3c-ccg/did-spec/issues/81 Does the did-spec need to be specific about which parts of the URI RFC it is conformant with? https://github.com/w3c-ccg/did-spec/issues/169 Intro overhaul Improve abstract 115, 112 Define target audience of the DID spec: https://github.com/w3c-ccg/did-spec/issues/157 Terminology and explanations, what is a DID? ~~122~~ (Motivations) ~~121~~ (Purpose, terminology) ~~117~~ (entities in Overview) 156 & 155 ~~123, 124~~ (management of identifiers, Design Goals, terminology) 130 title - close this once definition of DID and related parts are solidified throughout the rest of the spec 127 Remove uses of 'entity' and other fluffy terms when we mean DID Subject: ~~125, 145 139 154~~ Clarify language about keys: 147 143 105 166 Clarify language around ledgers: 150 119 149 ACTION: ledger -> Verifiable Data Registry (with clarification that there are multiple registries, see VC spec) Michael>> You mean Verifiable Data Registry (VDR), n'est pas? Quick fixes: ~~137~~, ~~144~~, 133, ~~116~~, ~~118~~, ~~129~~, ~~140, 141~~ Clarify language about requirements: ~~120~~","title":"Issue Triage"},{"location":"RWoT8/did-spec-refinement/#feature-refinement","text":"MIME Types 1: https://github.com/w3c-ccg/did-spec/issues/84 MIME Types 2: https://github.com/w3c-ccg/did-spec/issues/82 ABNF for DID Syntax Use case: changing file hosting providers Use case: fragments continue to function after moving a resource that supports fragments (DB: and what, precisely, is the resource ?) Use case: change ActivityPub from one provider to another provider Use case: I want to advertise a legacy URL (Twitter, Linkedin, etc.) Use case: Citations Use case: Timo??? Use case: MS services by type??? Need more details. Semantics of DID URL vs DID Which URL spec? https://github.com/w3c-ccg/did-spec/issues/163 If we don't need WHAT WG URL spec features, then cite only the RFC3986.","title":"Feature Refinement"},{"location":"RWoT8/did-spec-refinement/#new-features-and-requirements","text":"The following new features and requirements were identified at RWoT8 The need for a place to express keys that are used to digitally sign Verifiable Credentials (e.g. a \"credentialIssuer\" property) The need for a place to express keys that are used to digitally sign legal agreements (e.g. a \"legalAgreement\" property) Michael Herman's Key Feedback DID Resolution Technical Use Cases: https://github.com/w3c-ccg/did-resolution/issues/32 Check that small-e \"entities\" references are removed or better qualified Verifiable Data Registry - DID Resolver Architectures: https://github.com/mwherman2000/indy-arm/blob/master/images/HBB-DID-Resolver-Architecture%20v0.2.png DID Data Object References feedback: https://github.com/WebOfTrustInfo/rwot8-barcelona/issues DID Namespaces feedback: https://github.com/WebOfTrustInfo/rwot8-barcelona/issues Editorial work - WIP: https://github.com/w3c-ccg/did-spec/commits/rwot8-editorial","title":"New Features and Requirements"},{"location":"RWoT8/how-ssi-will-survive-capitalism/","text":"How SSI Will Survive Capitalism Adrian Gropper, Michael Shea, Martin Riedel Abstract 1 Introduction 2 SWOT Analysis 2 Strengths of SSI 2 Weaknesses 3 Opportunities 3 Threats 4 Barriers, Pathways and the Third Rail (\"The hard way out?\") 4 Barriers 5 Pathway Choices 5 Desired End State 5 Community-driven Wallet reference implementation 5 AaaS First 5 The Third Rail 6 The Health Care Example 6 Conclusion 7 Abstract The Self-Sovereign Identity (SSI) community has described several groundbreaking properties that arise from the adoption of its principles. Governance, as in business and financing structure, is arguably the most challenging of these properties, captured succinctly by Shoshana Zuboff as: \"Who decides? Who decides who decides?\" However, even though the technology has matured greatly over recent years, bootstrapping an SSI product within the existing capitalistic market environment is complicated and has not been achieved at scale within any functional domain. A RWOT6 paper explored the challenges to a sustainable commons. In this paper, we apply the SWOT framework (Strengths, Weaknesses, Opportunities, and Threats) to identify potential paths to adoption. For example, what are the general implications of introducing a credential holder into existing issuer/verifier relationships? Our analysis leads to cooperative (in the legal sense) governance with focus on the holder (the wallet) as the key innovation, since issuers and verifiers already exist. The healthcare industry is used as an example. Introduction Over the past twenty years there have been a consistent wish that individuals would be able to control their personal information in a manner equivalent to the pre-internet age. There have been multiple \"turns of the wheel\" (OAuth, SAML) of technological standards that have attempted to enable this. Self-Sovereign Identity (SSI) is the latest attempt to do so. In each of the previous attempts, the technology and work that was supposed to strengthen the position of the individual was subverted and the result is the current situation of strong corporate players (Google, Facebook) leveraging convenience to further mine behavioral data on individuals. There can be no argument that there is a ground swell around SSI currently; the real unknown is whether SSI will maintain its current trajectory of empowering the individual to regain control of their information. However there are still real challenges ahead around enabling adoption and countering attempts to hijack the sector and direct it in some unforeseen manner. This paper provides a strategic analysis (SWOT) of the SSI community and sector to reveal where we believe the threats may appear and to identify the best path to adoption. SWOT Analysis This paper applies the SWOT Analysis, a strategic planning technique that can help an individual or an organization in determining Strengths, Weaknesses, Opportunities, and Threats (\"SWOT\") to identify possible paths to adoption around Self-Sovereign Identity concepts. SWOT analysis may be used in any decision-making situation when a desired end-state (objective) is known. It should take into account internal and external factors that contribute to achieving or failing to achieve a given end result. The results of the SWOT Analysis can inform about possible measures or counter-measures in order to support or prevent certain anticipated factors. Strengths of SSI Avoids an enterprise sale: adoption can be decentralized, one doctor, one patient at a time. Separating the SSI strategy in this way avoids having to create both supply & demand (bootstrapping cost, typical in a platform) Ease of implementing Privacy by Default Transparency is controlled by principals (doctors, patients) not by intermediaries Non-sectoral: the same personal agent creates opportunities across industries/sectors No corporate data aggregation Builds on top of object capabilities and delegation framework An Identity-layered Internet allows for the development of new business models in a privacy-preserving way without expensive platform intermediaries (decentralized Uber, decentralized eBay, decentralized AirBnB). Weaknesses of SSI A barrier or friction point in adoption is created by introducing the holder into an incumbent verifier-issuer relationship. Lack of upfront financing due to lack of platform leverage (chicken & egg problem) Risk of increasing transaction costs Every technology can be used for good or bad and in many instances the first 'commercial' successes are for illicit purposes; quacks and fraudsters, are going to rise, creaing the potential for social media hysteria and significant shifting of market perception. Correlation of agent endpoint (we need tunnels, Tor, ...) Cellphones, as a secure element, are not inherently SSI Walled garden App Stores are not inherently SSI Opportunities of SSI The value (cost) of data brokers/intermediaries can be redistributed to the principals (issuer, subject/holder, verifier) Decentralization of policy making promotes diversity and resilience Competition for governance Authorization agent improves data quality: elimination of rekeying, provenance implicit, streaming, query, single of point of monitoring, policies stay private Use of Zero-knowledge proofs for reputation, nullification (e.g., vote only once), data minimization Globalization of medicine where the patient selects the jurisdiction of physician; decentralization of reputation and issuance (e.g., guilds vs states) AI cost is 80% data and data cleansing, 10% data science and technology, 10% verification/validation (elimination/reduction of data brokers does not remove/limit AI) Decentralization of AI to reflect SSI principles through Self-Sovereign AI-enhanced agents based on federated machine learning. Federated machine learning means that one AI can teach another AI without actually sharing the training data itself. Human-in-the-loop AI is another example of Self-Sovereign AI. Standardized data governance labeling ( http://bit.ly/PPR-IGL ), Quinn Grundy et al. - Data sharing practices of medicines related apps and the mobile ecosystem: traffic, content, and network analysis, BMJ2019 ) Agent capability improves with greater API standardization within the market (API effectively defines the data model. Semantic compatibility makes the agent more reliable.) Threats of SSI Data brokers and intermediaries will fight to keep their current roles. Platform mentality and political power of platform, through the vehicle of regulatory capture, can delay effective standards and raise regulatory barriers. Need for standards, and lead times for standards. Market inertia: data brokers are here and built into the value chain. May require governmental action (antitrust for oligopolies/monopolies, human rights of access and privacy) Requiring change by multiple stakeholder categories (issuer, holder, verifier) magnifies barriers. Existence of government regulation (NIST-regulated trust framework models), where stakeholders may be willing to change but regulatory inertia causes delay. Data brokers are very large businesses, like icebergs, 90% hidden. Crypto wallets that are not SSI get integrated natively into iOS and Android. Agents as a Service (AaaS) (Microsoft, Amazon, Google, Telco's, ISPs) Intel SGX favors platforms and centralization Elimination of data brokers requires an alternative for all of a particular data broker's roles. Solutions may be required in as many as six use cases (reputation, matchmaker, transaction convenience, data brokerage, mapping data for a common ontology, and record retention) Lack of business knowledge for tech-first SSI industry participants Collusion or Regulatory Capture: direction is taken over by a limited number of private corporations that organize in \"community groups\", that do not truly represent commons or effectively create a barrier to entry. SWOT Result Interpretation: \"Barriers, Pathways and the Third Rail\" With the identified factors of the SWOT analysis we are able to deduce further implications around the adoption of SSI. Specifically, we want to point out possible barriers, pathways, and distractions that may arise. Barriers Want to bootstrap use without single entity dominance. Non-traditional economic model; SSI model has by definition no equity upfront. All value is created in the Commons. A Commons does generate enough equity for traditional finance models. Platform tokens have security regulatory risk. Surveillance capitalism has very low friction. Cultural norms (China) resistant to full agency (want backdoors). India (Aadhaar) wants to centralize and maintain control of identifiers. Pathway Choices Desired End State Open-source standards-based code in a cooperative-based financial model, with each co-op in control of their specific policies while agreeing to allow any individual or entity to exit the cooperative, start their own cooperative, or operate outside of any cooperative. See Rochdale Society and seven principles of cooperatives and Past, Present, Future: From Co-ops to Cryptonetworks . Community-driven Wallet reference implementation Given the novelty and importance of the introduction of holder responsibility, the community should strive towards providing a non-commercial wallet implementation to support this process and lower the barrier to entry. AaaS First Agent as a Service (AaaS) as an intermediate pathway before completely moving the responsibility of the agent into the User space (e.g., \"access recovery via the centralized service, like people are used to\"). However, these services would need to be tightly regulated and allow a seamless transition into a fully self-sovereign internet. Regulation should aim at removing them altogether as soon as a critical size has been reached. The Third Rail A megamerger of platforms across hosting, telecom, and payment creates the universal identity based on transaction surveillance. Regulating data use instead of regulating aggregation itself is a prescription for disaster. Apple fails at privacy by default; Facebook privacy theater wins. Private telecom uses 5G to eliminate the opportunity for personal firewalls as agents. Alternatives, such as mesh networks and free community broadband, are not actively pursued. (Need regulatory measures to give control to the owner of 5G-enabled equipment, e.g. keys, triple-blind, etc.) Our children join corporate America with blind respect for authority and platforms. The SSI infrastructure need for diverse holders/subjects is seen as a risk. Private authority does not have the ability to deter bad behavior the same way that government can, but government is weak compared to private sector. The Health Care Example Healthcare (the industry) is large and more about personal data than most markets. Health care (the value transaction) is prone to decentralization because the principals, patients, and licensed practitioners are able to transact independent of any institution and in some cases independent of any particular jurisdiction. This kind of decentralization is reminiscent of public blockchains. SSI, in this example, applies to the licensed practitioner as well as the patient. Each of them is a holder. The practitioner holds credentials issued by testing institutions and benefits from appropriate reputation mechanisms tied to their identity. The patient holds health records, including notes, diagnoses, and prescriptions, issued by the licensed practitioners. These two rather different holder technologies must interact with each other independently of institutional issuers and verifiers. The healthcare industry is dominated by data broker intermediaries that aggregate practitioner licenses and reputation on one side and aggregate patient health records on the other. We call them hospitals. Under what circumstances would the hospital support the introduction of SSI holders on behalf of either the practitioners or the patients? Competitive pressure will force hospitals to accept SSI, effectively driving them out of the data brokerage business if they wish to remain in the principal business of operating ERs, surgery suites, and other health care services. But for that to happen the data brokerage function of hospitals will need to shift to an SSI model that respects a diversity of standards-based holders. What institutions will organize the tech and support systems to provide holders to the practitioners and patients? Patient and physician cooperatives could provide governance and support for SSI holders without need for a proprietary business model or private equity. Alternative structures for support of SSI holders could be government (as a public commons) or a not-for-profit collaboration by interested principals such as pharmaceutical and device manufacturers. Strategic support could also come from employers and other players that stand to benefit from reducing the cost of data brokerage in healthcare. Another possibility for support of SSI would be a new class of data brokers built around a fiduciary relationship with the subjects. These Agent as a Service models will pick and choose among the SSI principles and standards and possibly focus on healthcare to the exclusion of other markets in order to develop brand and platform economics with private equity support. Conclusion Adoption of SSI is dominated by the challenge to the data brokers and intermediaries that would be replaced by holder technology. Even if some data brokers are able to pivot to Agent as a Service during a transition to SSI, the highly redundant and hidden data aggregation business would be decimated. SSI can overcome these incumbents while staying true to its principles by developing and supporting holder technology through a cooperative model. Cooperatives can focus on the needs of issuers and verifiers in specific markets while at the same time adopting and supporting standards that will allow holders to operate seamlessly across diverse markets. Transitional states on the path to decentralized SSI could include holder services with a more or less fiduciary relationship to the subject. In the long-run, self-sovereign identity and individual agency require both Free software and standards with support through cooperative governance. The disintermediation and destruction of current data broker business models provide very strong economic incentive to hijack and subvert the goals of the SSI community. This is something that community must be constantly aware of and be prepared to counter.","title":"How SSI Will Survive Capitalism"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#how-ssi-will-survive-capitalism","text":"Adrian Gropper, Michael Shea, Martin Riedel Abstract 1 Introduction 2 SWOT Analysis 2 Strengths of SSI 2 Weaknesses 3 Opportunities 3 Threats 4 Barriers, Pathways and the Third Rail (\"The hard way out?\") 4 Barriers 5 Pathway Choices 5 Desired End State 5 Community-driven Wallet reference implementation 5 AaaS First 5 The Third Rail 6 The Health Care Example 6 Conclusion 7","title":"How SSI Will Survive Capitalism"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#abstract","text":"The Self-Sovereign Identity (SSI) community has described several groundbreaking properties that arise from the adoption of its principles. Governance, as in business and financing structure, is arguably the most challenging of these properties, captured succinctly by Shoshana Zuboff as: \"Who decides? Who decides who decides?\" However, even though the technology has matured greatly over recent years, bootstrapping an SSI product within the existing capitalistic market environment is complicated and has not been achieved at scale within any functional domain. A RWOT6 paper explored the challenges to a sustainable commons. In this paper, we apply the SWOT framework (Strengths, Weaknesses, Opportunities, and Threats) to identify potential paths to adoption. For example, what are the general implications of introducing a credential holder into existing issuer/verifier relationships? Our analysis leads to cooperative (in the legal sense) governance with focus on the holder (the wallet) as the key innovation, since issuers and verifiers already exist. The healthcare industry is used as an example.","title":"Abstract"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#introduction","text":"Over the past twenty years there have been a consistent wish that individuals would be able to control their personal information in a manner equivalent to the pre-internet age. There have been multiple \"turns of the wheel\" (OAuth, SAML) of technological standards that have attempted to enable this. Self-Sovereign Identity (SSI) is the latest attempt to do so. In each of the previous attempts, the technology and work that was supposed to strengthen the position of the individual was subverted and the result is the current situation of strong corporate players (Google, Facebook) leveraging convenience to further mine behavioral data on individuals. There can be no argument that there is a ground swell around SSI currently; the real unknown is whether SSI will maintain its current trajectory of empowering the individual to regain control of their information. However there are still real challenges ahead around enabling adoption and countering attempts to hijack the sector and direct it in some unforeseen manner. This paper provides a strategic analysis (SWOT) of the SSI community and sector to reveal where we believe the threats may appear and to identify the best path to adoption.","title":"Introduction"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#swot-analysis","text":"This paper applies the SWOT Analysis, a strategic planning technique that can help an individual or an organization in determining Strengths, Weaknesses, Opportunities, and Threats (\"SWOT\") to identify possible paths to adoption around Self-Sovereign Identity concepts. SWOT analysis may be used in any decision-making situation when a desired end-state (objective) is known. It should take into account internal and external factors that contribute to achieving or failing to achieve a given end result. The results of the SWOT Analysis can inform about possible measures or counter-measures in order to support or prevent certain anticipated factors.","title":"SWOT Analysis"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#strengths-of-ssi","text":"Avoids an enterprise sale: adoption can be decentralized, one doctor, one patient at a time. Separating the SSI strategy in this way avoids having to create both supply & demand (bootstrapping cost, typical in a platform) Ease of implementing Privacy by Default Transparency is controlled by principals (doctors, patients) not by intermediaries Non-sectoral: the same personal agent creates opportunities across industries/sectors No corporate data aggregation Builds on top of object capabilities and delegation framework An Identity-layered Internet allows for the development of new business models in a privacy-preserving way without expensive platform intermediaries (decentralized Uber, decentralized eBay, decentralized AirBnB).","title":"Strengths of SSI"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#weaknesses-of-ssi","text":"A barrier or friction point in adoption is created by introducing the holder into an incumbent verifier-issuer relationship. Lack of upfront financing due to lack of platform leverage (chicken & egg problem) Risk of increasing transaction costs Every technology can be used for good or bad and in many instances the first 'commercial' successes are for illicit purposes; quacks and fraudsters, are going to rise, creaing the potential for social media hysteria and significant shifting of market perception. Correlation of agent endpoint (we need tunnels, Tor, ...) Cellphones, as a secure element, are not inherently SSI Walled garden App Stores are not inherently SSI","title":"Weaknesses of SSI"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#opportunities-of-ssi","text":"The value (cost) of data brokers/intermediaries can be redistributed to the principals (issuer, subject/holder, verifier) Decentralization of policy making promotes diversity and resilience Competition for governance Authorization agent improves data quality: elimination of rekeying, provenance implicit, streaming, query, single of point of monitoring, policies stay private Use of Zero-knowledge proofs for reputation, nullification (e.g., vote only once), data minimization Globalization of medicine where the patient selects the jurisdiction of physician; decentralization of reputation and issuance (e.g., guilds vs states) AI cost is 80% data and data cleansing, 10% data science and technology, 10% verification/validation (elimination/reduction of data brokers does not remove/limit AI) Decentralization of AI to reflect SSI principles through Self-Sovereign AI-enhanced agents based on federated machine learning. Federated machine learning means that one AI can teach another AI without actually sharing the training data itself. Human-in-the-loop AI is another example of Self-Sovereign AI. Standardized data governance labeling ( http://bit.ly/PPR-IGL ), Quinn Grundy et al. - Data sharing practices of medicines related apps and the mobile ecosystem: traffic, content, and network analysis, BMJ2019 ) Agent capability improves with greater API standardization within the market (API effectively defines the data model. Semantic compatibility makes the agent more reliable.)","title":"Opportunities of SSI"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#threats-of-ssi","text":"Data brokers and intermediaries will fight to keep their current roles. Platform mentality and political power of platform, through the vehicle of regulatory capture, can delay effective standards and raise regulatory barriers. Need for standards, and lead times for standards. Market inertia: data brokers are here and built into the value chain. May require governmental action (antitrust for oligopolies/monopolies, human rights of access and privacy) Requiring change by multiple stakeholder categories (issuer, holder, verifier) magnifies barriers. Existence of government regulation (NIST-regulated trust framework models), where stakeholders may be willing to change but regulatory inertia causes delay. Data brokers are very large businesses, like icebergs, 90% hidden. Crypto wallets that are not SSI get integrated natively into iOS and Android. Agents as a Service (AaaS) (Microsoft, Amazon, Google, Telco's, ISPs) Intel SGX favors platforms and centralization Elimination of data brokers requires an alternative for all of a particular data broker's roles. Solutions may be required in as many as six use cases (reputation, matchmaker, transaction convenience, data brokerage, mapping data for a common ontology, and record retention) Lack of business knowledge for tech-first SSI industry participants Collusion or Regulatory Capture: direction is taken over by a limited number of private corporations that organize in \"community groups\", that do not truly represent commons or effectively create a barrier to entry.","title":"Threats of SSI"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#swot-result-interpretation-barriers-pathways-and-the-third-rail","text":"With the identified factors of the SWOT analysis we are able to deduce further implications around the adoption of SSI. Specifically, we want to point out possible barriers, pathways, and distractions that may arise.","title":"SWOT Result Interpretation: \"Barriers, Pathways and the Third Rail\""},{"location":"RWoT8/how-ssi-will-survive-capitalism/#barriers","text":"Want to bootstrap use without single entity dominance. Non-traditional economic model; SSI model has by definition no equity upfront. All value is created in the Commons. A Commons does generate enough equity for traditional finance models. Platform tokens have security regulatory risk. Surveillance capitalism has very low friction. Cultural norms (China) resistant to full agency (want backdoors). India (Aadhaar) wants to centralize and maintain control of identifiers.","title":"Barriers"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#pathway-choices","text":"","title":"Pathway Choices"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#desired-end-state","text":"Open-source standards-based code in a cooperative-based financial model, with each co-op in control of their specific policies while agreeing to allow any individual or entity to exit the cooperative, start their own cooperative, or operate outside of any cooperative. See Rochdale Society and seven principles of cooperatives and Past, Present, Future: From Co-ops to Cryptonetworks .","title":"Desired End State"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#community-driven-wallet-reference-implementation","text":"Given the novelty and importance of the introduction of holder responsibility, the community should strive towards providing a non-commercial wallet implementation to support this process and lower the barrier to entry.","title":"Community-driven Wallet reference implementation"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#aaas-first","text":"Agent as a Service (AaaS) as an intermediate pathway before completely moving the responsibility of the agent into the User space (e.g., \"access recovery via the centralized service, like people are used to\"). However, these services would need to be tightly regulated and allow a seamless transition into a fully self-sovereign internet. Regulation should aim at removing them altogether as soon as a critical size has been reached.","title":"AaaS First"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#the-third-rail","text":"A megamerger of platforms across hosting, telecom, and payment creates the universal identity based on transaction surveillance. Regulating data use instead of regulating aggregation itself is a prescription for disaster. Apple fails at privacy by default; Facebook privacy theater wins. Private telecom uses 5G to eliminate the opportunity for personal firewalls as agents. Alternatives, such as mesh networks and free community broadband, are not actively pursued. (Need regulatory measures to give control to the owner of 5G-enabled equipment, e.g. keys, triple-blind, etc.) Our children join corporate America with blind respect for authority and platforms. The SSI infrastructure need for diverse holders/subjects is seen as a risk. Private authority does not have the ability to deter bad behavior the same way that government can, but government is weak compared to private sector.","title":"The Third Rail"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#the-health-care-example","text":"Healthcare (the industry) is large and more about personal data than most markets. Health care (the value transaction) is prone to decentralization because the principals, patients, and licensed practitioners are able to transact independent of any institution and in some cases independent of any particular jurisdiction. This kind of decentralization is reminiscent of public blockchains. SSI, in this example, applies to the licensed practitioner as well as the patient. Each of them is a holder. The practitioner holds credentials issued by testing institutions and benefits from appropriate reputation mechanisms tied to their identity. The patient holds health records, including notes, diagnoses, and prescriptions, issued by the licensed practitioners. These two rather different holder technologies must interact with each other independently of institutional issuers and verifiers. The healthcare industry is dominated by data broker intermediaries that aggregate practitioner licenses and reputation on one side and aggregate patient health records on the other. We call them hospitals. Under what circumstances would the hospital support the introduction of SSI holders on behalf of either the practitioners or the patients? Competitive pressure will force hospitals to accept SSI, effectively driving them out of the data brokerage business if they wish to remain in the principal business of operating ERs, surgery suites, and other health care services. But for that to happen the data brokerage function of hospitals will need to shift to an SSI model that respects a diversity of standards-based holders. What institutions will organize the tech and support systems to provide holders to the practitioners and patients? Patient and physician cooperatives could provide governance and support for SSI holders without need for a proprietary business model or private equity. Alternative structures for support of SSI holders could be government (as a public commons) or a not-for-profit collaboration by interested principals such as pharmaceutical and device manufacturers. Strategic support could also come from employers and other players that stand to benefit from reducing the cost of data brokerage in healthcare. Another possibility for support of SSI would be a new class of data brokers built around a fiduciary relationship with the subjects. These Agent as a Service models will pick and choose among the SSI principles and standards and possibly focus on healthcare to the exclusion of other markets in order to develop brand and platform economics with private equity support.","title":"The Health Care Example"},{"location":"RWoT8/how-ssi-will-survive-capitalism/#conclusion","text":"Adoption of SSI is dominated by the challenge to the data brokers and intermediaries that would be replaced by holder technology. Even if some data brokers are able to pivot to Agent as a Service during a transition to SSI, the highly redundant and hidden data aggregation business would be decimated. SSI can overcome these incumbents while staying true to its principles by developing and supporting holder technology through a cooperative model. Cooperatives can focus on the needs of issuers and verifiers in specific markets while at the same time adopting and supporting standards that will allow holders to operate seamlessly across diverse markets. Transitional states on the path to decentralized SSI could include holder services with a more or less fiduciary relationship to the subject. In the long-run, self-sovereign identity and individual agency require both Free software and standards with support through cooperative governance. The disintermediation and destruction of current data broker business models provide very strong economic incentive to hijack and subvert the goals of the SSI community. This is something that community must be constantly aware of and be prepared to counter.","title":"Conclusion"},{"location":"RWoT8/peer-DID-method-spec-report/","text":"Peer DID Method Specification Report Authors Brent Zundel, Timo Welde, Mike Varley, Marton Csernai Contributors Abstract This paper consists of objectives, use cases and observations around a \"peer\" DID method, based off a draft specification submitted to RWOT8. The following abstract is from that draft specification, located here . \"This DID method spec conforms to the requirements in the DID specification currently published by the W3C Credentials Community Group. For more information about DIDs and DID method specifications, please see the DID Primer and DID Spec. \"This document defines a 'peer' DID Method that can be used independent of any source of truth external to the relationship in which it is used. The method is cheap, fast, scalable, and secure. It is suitable for most private relationships between people, organizations, and IoT things. DIDs associated with this method are also promotable to a more public context. That is, blockchains with different DID methods can graft some or all peer DIDs into their namespace(s) with no risk of accidental collision, and no loss of meaning. Peer DID will have a recognizable and consistent identity in all of them.\" Objectives Peer DID resolution does not require a Universal Resolver: documents are self-contained in a message protocol. Peer DID exchange is for the purpose of establishing secure communication, but Trust in the peers must be established at another level (in person, out of band, using Verifiable Credentials, using other attestations). Peer DIDs may be created on the fly for each new session between parties. This enables privacy and anonymity features. Peer DIDs may be persisted for subsequent sessions between the parties. This enables a persistent trust relationship between parties. Peer DID communication protocol is not bound to any specific ledger-based DID service or design model. (Someone who wishes to use a peer DID is not bound to any 'anchor,' such as a ledger). Peer DIDs may be interoperable with ledger-backed (anchored) DIDs; the peers group do not all need to be using peer DIDs (e.g. Alice wants to use a did:sov DID, and Bob wants to use a did:peer DID). Create an n-wise peer DID spec, of which one use case is pairwise DID exchange. Use Cases Two or more individuals can create DIDs \u201cwithout any overhead\u201d such as infrastructure, registry costs, time penalty, or even network requirement. Two service entities wish to communicate in an \"anonymous but trusted\" way for a data exchange transaction, but do not need this relationship persisted beyond the transaction lifetime. In a doctor/hospital/patient context these three entities may wish to establish trusted communication channels for delegating care or sharing information (securely) regarding the other parties (the hospital sharing a record with the doctor and the patient seeing the exchange has occurred). Spec Review Observations Groups Section It should be made clear that if Alice and Bob are already connected (through peer DIDs), but wish to add another party, they should first create new peer DIDs with one another then invite Carol to that group. Removing participants from a Group is basically recreating the group without the person who is 'removed'. Namestring Generation: keyfmtchar We understand the need for keyfmtchar, but it needs a definition, and an example of when to use it (i.e., when to make a \"2\") would be helpful. Protocol: Message Format Section Indy HIPE message protocol is referenced - what extensions are required (multiplexed encryption) and why? The observation here is that pure JWE may be better for adoption, so understanding the need for extensions would be helpful. Comments on the Spec The language of the abstract is \"marketing speak\". We would suggest changing it to state just the intent. \"The method is cheap, fast, scalable, and secure\" -> \"The method is intended to be cheap, fast, scalable, and secure\". Section 2.1 links to a non-existing section \"cross-registration\" at the end. Section 2.3 could be better structured with subsections (e.g. for keyfmtstring and idstring ). Section 3.4 contains a lot of prose, which doesn't fit the structure of the rest of the document. Also it is not clear, what is meant by \"The significance of the error situation described above, ...\" Next Steps The next step is to form a working group and establish a regular cadence of meetings to continue working. The working group will work to: - Address the issues outlined in this document. - Continue to refine the objectives for peer DIDs. - Iteratively modify the draft Peer DID Method Spec to reflect the objectives. - Seek feedback on the draft Peer DID Method Spec from the community. - Identify further issues with the Peer DID Method Spec. Conclusion The authors established a communication channel in the DIF slack and held a series of meetings. The issues introduced in this report (where still valid after significant changes to the Peer DID Method Spec ) will be created in the github repo where the method spec is being refined. The peer DID method has great promise. We feel that many of the interoperability concerns in the DID space may be addressed by wide adoption of a peer DID. We invite the SSI community to provide feedback on the Peer DID Method Spec reviewed here, and to participate in interoperability testing of implementations of the spec as they mature.","title":"Peer DID Method Specification Report"},{"location":"RWoT8/peer-DID-method-spec-report/#peer-did-method-specification-report","text":"","title":"Peer DID Method Specification Report"},{"location":"RWoT8/peer-DID-method-spec-report/#authors","text":"Brent Zundel, Timo Welde, Mike Varley, Marton Csernai","title":"Authors"},{"location":"RWoT8/peer-DID-method-spec-report/#contributors","text":"","title":"Contributors"},{"location":"RWoT8/peer-DID-method-spec-report/#abstract","text":"This paper consists of objectives, use cases and observations around a \"peer\" DID method, based off a draft specification submitted to RWOT8. The following abstract is from that draft specification, located here . \"This DID method spec conforms to the requirements in the DID specification currently published by the W3C Credentials Community Group. For more information about DIDs and DID method specifications, please see the DID Primer and DID Spec. \"This document defines a 'peer' DID Method that can be used independent of any source of truth external to the relationship in which it is used. The method is cheap, fast, scalable, and secure. It is suitable for most private relationships between people, organizations, and IoT things. DIDs associated with this method are also promotable to a more public context. That is, blockchains with different DID methods can graft some or all peer DIDs into their namespace(s) with no risk of accidental collision, and no loss of meaning. Peer DID will have a recognizable and consistent identity in all of them.\"","title":"Abstract"},{"location":"RWoT8/peer-DID-method-spec-report/#objectives","text":"Peer DID resolution does not require a Universal Resolver: documents are self-contained in a message protocol. Peer DID exchange is for the purpose of establishing secure communication, but Trust in the peers must be established at another level (in person, out of band, using Verifiable Credentials, using other attestations). Peer DIDs may be created on the fly for each new session between parties. This enables privacy and anonymity features. Peer DIDs may be persisted for subsequent sessions between the parties. This enables a persistent trust relationship between parties. Peer DID communication protocol is not bound to any specific ledger-based DID service or design model. (Someone who wishes to use a peer DID is not bound to any 'anchor,' such as a ledger). Peer DIDs may be interoperable with ledger-backed (anchored) DIDs; the peers group do not all need to be using peer DIDs (e.g. Alice wants to use a did:sov DID, and Bob wants to use a did:peer DID). Create an n-wise peer DID spec, of which one use case is pairwise DID exchange.","title":"Objectives"},{"location":"RWoT8/peer-DID-method-spec-report/#use-cases","text":"Two or more individuals can create DIDs \u201cwithout any overhead\u201d such as infrastructure, registry costs, time penalty, or even network requirement. Two service entities wish to communicate in an \"anonymous but trusted\" way for a data exchange transaction, but do not need this relationship persisted beyond the transaction lifetime. In a doctor/hospital/patient context these three entities may wish to establish trusted communication channels for delegating care or sharing information (securely) regarding the other parties (the hospital sharing a record with the doctor and the patient seeing the exchange has occurred).","title":"Use Cases"},{"location":"RWoT8/peer-DID-method-spec-report/#spec-review-observations","text":"","title":"Spec Review Observations"},{"location":"RWoT8/peer-DID-method-spec-report/#groups-section","text":"It should be made clear that if Alice and Bob are already connected (through peer DIDs), but wish to add another party, they should first create new peer DIDs with one another then invite Carol to that group. Removing participants from a Group is basically recreating the group without the person who is 'removed'.","title":"Groups Section"},{"location":"RWoT8/peer-DID-method-spec-report/#namestring-generation-keyfmtchar","text":"We understand the need for keyfmtchar, but it needs a definition, and an example of when to use it (i.e., when to make a \"2\") would be helpful.","title":"Namestring Generation: keyfmtchar"},{"location":"RWoT8/peer-DID-method-spec-report/#protocol-message-format-section","text":"Indy HIPE message protocol is referenced - what extensions are required (multiplexed encryption) and why? The observation here is that pure JWE may be better for adoption, so understanding the need for extensions would be helpful.","title":"Protocol: Message Format Section"},{"location":"RWoT8/peer-DID-method-spec-report/#comments-on-the-spec","text":"The language of the abstract is \"marketing speak\". We would suggest changing it to state just the intent. \"The method is cheap, fast, scalable, and secure\" -> \"The method is intended to be cheap, fast, scalable, and secure\". Section 2.1 links to a non-existing section \"cross-registration\" at the end. Section 2.3 could be better structured with subsections (e.g. for keyfmtstring and idstring ). Section 3.4 contains a lot of prose, which doesn't fit the structure of the rest of the document. Also it is not clear, what is meant by \"The significance of the error situation described above, ...\"","title":"Comments on the Spec"},{"location":"RWoT8/peer-DID-method-spec-report/#next-steps","text":"The next step is to form a working group and establish a regular cadence of meetings to continue working. The working group will work to: - Address the issues outlined in this document. - Continue to refine the objectives for peer DIDs. - Iteratively modify the draft Peer DID Method Spec to reflect the objectives. - Seek feedback on the draft Peer DID Method Spec from the community. - Identify further issues with the Peer DID Method Spec.","title":"Next Steps"},{"location":"RWoT8/peer-DID-method-spec-report/#conclusion","text":"The authors established a communication channel in the DIF slack and held a series of meetings. The issues introduced in this report (where still valid after significant changes to the Peer DID Method Spec ) will be created in the github repo where the method spec is being refined. The peer DID method has great promise. We feel that many of the interoperability concerns in the DID space may be addressed by wide adoption of a peer DID. We invite the SSI community to provide feedback on the Peer DID Method Spec reviewed here, and to participate in interoperability testing of implementations of the spec as they mature.","title":"Conclusion"}]}