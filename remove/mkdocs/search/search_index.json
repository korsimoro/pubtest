{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MkDocs PoC mkdocs is good for particularly well structured data. We can produce this data as part of our publication process. See Details for information about the pubtest repository See Home testing links -","title":"Home"},{"location":"#mkdocs-poc","text":"mkdocs is good for particularly well structured data. We can produce this data as part of our publication process. See Details for information about the pubtest repository See Home testing links -","title":"MkDocs PoC"},{"location":"details/","text":"Counter : AAA on pubtest/README.md Quickstart Purpose Git Configuration Quickstart git clone git@github.com:korsimoro/pubtest cd pubtest ./toolkit/setup.sh . ./toolkit/venv/bin/activate tool Purpose The purpose of this repository is to develop a publication technology with the following properties: community engagement integrate with external data inputs (like Forms, or chats) machine processable rich data normalize tabular data as sqlite files use sql queries to generate reports use graphql queries to generate reports use git commit history exploration to generate reports expose report data as yml, csv, and json for our own consumption via _data reference data promoting distribution of machine readable data to other sites about data and make the results of this analysis readily digestable for publication purposes Proof of Concept - recording the exploration of mission support technologies submodules - to access files from other repositories, linked to a specific point in their tracking history, which is available to us for inclusion in documentation mkdocs - expected useful for some structured data and ad-hoc data sources wordpress - it should be possible to do something with wordpress, but not sure what sqlitebiter - helps normalize tabular data around a sqlite format tool - our own shim, with whatever data transformation logic we want explore the use of git & github issues tracking to coordinate our work and advance our ideas to completion github pages for publication circleci integration for CI/CD pipeline use of submodules use of git commit history in publication Git Configuration CircleCI GitHub Pages branch: documentation branch: master CircleCI Integration https://circleci.com/gh/korsimoro/pubtest/tree/documentation commits to 'documentation' branch will trigger the workflow in \".circleci/config.yml\" The .circleci/config.yml file associates a command with a branch name, so we can run different commands depending upon updates to any branch. GitHub Pages Integration publication to 'master' branch will trigger jekyll run and publish the result. This is due to serving the repository via github pages. As a result we get a \"free, well maintained, powerful\" step that will process our 'master' branch, and in particular, the 'docs' directory and publish the result for global\\ visibility. branch: documentation the default branch is 'documentation'. when documentation is to be released, you should commit to this branch. That will trigger the circle-ci preparation step followed by the github pages publication step. branch: master ideally the 'master' branch will be controlled, and the state of master should always match the deployed website. This means we want to manage our updates to master as much as possible, and refrain, as much as possible, from committing directly to master. This effectively makes \"releasing the repo\" equivalent to \"publishing the documentation\", which is a nice parity to have for any project! PoC All of the PoC components can be ignored, but they demonstrate the sorts of tools we should consider within reach. mkdocs everything mentioning mkdocs can be safely ignored. We are leaving it in because it is working, and there are documentation subsets for which it might be very useful. It does not play any role in the main deployment. The mkdoc sandbox is visiable at /pubtest/mkdocs/index.html when deployed. tool's query system This has proven useful to me. It is equivalent to \"frozen views\" in classic rdbms, but the views are written out as CSV, JSON, and YML and locked to an audit trail via the supporting git repository. wordpress We could integrate with wordpress presentation of git commit logs Exposing the chain of provenance of information is of critical importance in the identity-tech universe. Since the data is ultimately published from git, via github pages, then it makes sense for the provenance chain to be rooted in the git commit history. This gives you 'tree-hashes' linked to an audit trail, which can, ultimately be linked to the ecosystem of DIDs and VCs. DIDs should be mappable to \"git repository states\" in a very decentralized manner. One could even imagine a did:git:* method ;) Key Directories docs/assets/mkdocs These are the markdown source files for mkdocs. docs/mkdocs This is what gets served vi /pubtest/mkdocs _data/sqlite This contains of the following structure /[db-name] database.sqlite [query].sql where the database.sqlite file a sqlite database. This can come from any source and - can be managed using standard SQL utilities - can be generated as a snapshot from googlesheets or other CSV data via sqlitebiter - can be pulled by scraping websites via sqlitebiter _data/[db-name]_[query].yml This will Toolkit tool sqlitebiter mkdocs tool (venv) pubtest> tool Usage: tool [OPTIONS] COMMAND [ARGS]... Simple utilities to massage data in the repo. Options: --dir TEXT Base Directory (Checkout) --help Show this message and exit. Commands: db-schema-table Dump schema db-schema-to-yaml Extract schema from sqlite and place as _data. db-to-yaml Extract data from sqlite and place as _data. Sqlitebiter ``` Usage: sqlitebiter [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. -o, --output-path PATH Output path of the SQLite database file. Defaults to 'out.sqlite'. -a, --append Append table(s) to existing database. --add-primary-key PRIMARY_KEY_NAME Add 'PRIMARY KEY AUTOINCREMENT' column with the specified name. --convert-config TEXT [experimental] Configurations for data conversion. The option can be used only for url subcommand. -i, --index INDEX_ATTR Comma separated attribute names to create indices. --no-type-inference All of the columns assume as TEXT data type in creating tables. --type-hint-header Use headers suffix as type hints. If there are type hints, converting columns by datatype corresponding with type hints. The following suffixes can be recognized as type hints (case insensitive): \"text\": TEXT datatype. \"integer\": INTEGER datatype. \"real\": REAL datatype. --replace-symbol TEXT Replace symbols in attributes. -v, --verbose --debug For debug print. -q, --quiet Suppress execution log messages. -h, --help Show this message and exit. Commands: completion A helper command to setup command completion. configure Configure the following application settings: (1) Default... file Convert tabular data within CSV/Excel/HTML/JSON/Jupyter... gs Convert a spreadsheet in Google Sheets to a SQLite database... url Scrape tabular data from a URL and convert data to a SQLite... ``` mkdocs ``` (venv) pubtest> mkdocs Usage: mkdocs [OPTIONS] COMMAND [ARGS]... MkDocs - Project documentation with Markdown. Options: -V, --version Show the version and exit. -q, --quiet Silence warnings -v, --verbose Enable verbose output -h, --help Show this message and exit. Commands: build Build the MkDocs documentation gh-deploy Deploy your documentation to GitHub Pages new Create a new MkDocs project serve Run the builtin development server ```","title":"Details"},{"location":"details/#counter-aaa-on-pubtestreadmemd","text":"Quickstart Purpose Git Configuration","title":"Counter : AAA on pubtest/README.md"},{"location":"details/#quickstart","text":"git clone git@github.com:korsimoro/pubtest cd pubtest ./toolkit/setup.sh . ./toolkit/venv/bin/activate tool","title":"Quickstart"},{"location":"details/#purpose","text":"The purpose of this repository is to develop a publication technology with the following properties:","title":"Purpose"},{"location":"details/#community-engagement","text":"integrate with external data inputs (like Forms, or chats)","title":"community engagement"},{"location":"details/#machine-processable-rich-data","text":"normalize tabular data as sqlite files use sql queries to generate reports use graphql queries to generate reports use git commit history exploration to generate reports expose report data as yml, csv, and json for our own consumption via _data reference data promoting distribution of machine readable data to other sites about data and make the results of this analysis readily digestable for publication purposes","title":"machine processable rich data"},{"location":"details/#proof-of-concept-recording-the-exploration-of-mission-support-technologies","text":"submodules - to access files from other repositories, linked to a specific point in their tracking history, which is available to us for inclusion in documentation mkdocs - expected useful for some structured data and ad-hoc data sources wordpress - it should be possible to do something with wordpress, but not sure what sqlitebiter - helps normalize tabular data around a sqlite format tool - our own shim, with whatever data transformation logic we want","title":"Proof of Concept - recording the exploration of mission support technologies"},{"location":"details/#explore-the-use-of-git-github","text":"issues tracking to coordinate our work and advance our ideas to completion github pages for publication circleci integration for CI/CD pipeline use of submodules use of git commit history in publication","title":"explore the use of git &amp; github"},{"location":"details/#git-configuration","text":"CircleCI GitHub Pages branch: documentation branch: master","title":"Git Configuration"},{"location":"details/#circleci-integration","text":"https://circleci.com/gh/korsimoro/pubtest/tree/documentation commits to 'documentation' branch will trigger the workflow in \".circleci/config.yml\" The .circleci/config.yml file associates a command with a branch name, so we can run different commands depending upon updates to any branch.","title":"CircleCI Integration"},{"location":"details/#github-pages-integration","text":"publication to 'master' branch will trigger jekyll run and publish the result. This is due to serving the repository via github pages. As a result we get a \"free, well maintained, powerful\" step that will process our 'master' branch, and in particular, the 'docs' directory and publish the result for global\\ visibility.","title":"GitHub Pages Integration"},{"location":"details/#branch-documentation","text":"the default branch is 'documentation'. when documentation is to be released, you should commit to this branch. That will trigger the circle-ci preparation step followed by the github pages publication step.","title":"branch: documentation"},{"location":"details/#branch-master","text":"ideally the 'master' branch will be controlled, and the state of master should always match the deployed website. This means we want to manage our updates to master as much as possible, and refrain, as much as possible, from committing directly to master. This effectively makes \"releasing the repo\" equivalent to \"publishing the documentation\", which is a nice parity to have for any project!","title":"branch: master"},{"location":"details/#poc","text":"All of the PoC components can be ignored, but they demonstrate the sorts of tools we should consider within reach.","title":"PoC"},{"location":"details/#mkdocs","text":"everything mentioning mkdocs can be safely ignored. We are leaving it in because it is working, and there are documentation subsets for which it might be very useful. It does not play any role in the main deployment. The mkdoc sandbox is visiable at /pubtest/mkdocs/index.html when deployed.","title":"mkdocs"},{"location":"details/#tools-query-system","text":"This has proven useful to me. It is equivalent to \"frozen views\" in classic rdbms, but the views are written out as CSV, JSON, and YML and locked to an audit trail via the supporting git repository.","title":"tool's query system"},{"location":"details/#wordpress","text":"We could integrate with wordpress","title":"wordpress"},{"location":"details/#presentation-of-git-commit-logs","text":"Exposing the chain of provenance of information is of critical importance in the identity-tech universe. Since the data is ultimately published from git, via github pages, then it makes sense for the provenance chain to be rooted in the git commit history. This gives you 'tree-hashes' linked to an audit trail, which can, ultimately be linked to the ecosystem of DIDs and VCs. DIDs should be mappable to \"git repository states\" in a very decentralized manner. One could even imagine a did:git:* method ;)","title":"presentation of git commit logs"},{"location":"details/#key-directories","text":"","title":"Key Directories"},{"location":"details/#docsassetsmkdocs","text":"These are the markdown source files for mkdocs.","title":"docs/assets/mkdocs"},{"location":"details/#docsmkdocs","text":"This is what gets served vi /pubtest/mkdocs","title":"docs/mkdocs"},{"location":"details/#95datasqlite","text":"This contains of the following structure /[db-name] database.sqlite [query].sql where the database.sqlite file a sqlite database. This can come from any source and - can be managed using standard SQL utilities - can be generated as a snapshot from googlesheets or other CSV data via sqlitebiter - can be pulled by scraping websites via sqlitebiter","title":"_data/sqlite"},{"location":"details/#95datadb-name95queryyml","text":"This will","title":"_data/[db-name]_[query].yml"},{"location":"details/#toolkit","text":"tool sqlitebiter mkdocs","title":"Toolkit"},{"location":"details/#tool","text":"(venv) pubtest> tool Usage: tool [OPTIONS] COMMAND [ARGS]... Simple utilities to massage data in the repo. Options: --dir TEXT Base Directory (Checkout) --help Show this message and exit. Commands: db-schema-table Dump schema db-schema-to-yaml Extract schema from sqlite and place as _data. db-to-yaml Extract data from sqlite and place as _data.","title":"tool"},{"location":"details/#sqlitebiter","text":"``` Usage: sqlitebiter [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. -o, --output-path PATH Output path of the SQLite database file. Defaults to 'out.sqlite'. -a, --append Append table(s) to existing database. --add-primary-key PRIMARY_KEY_NAME Add 'PRIMARY KEY AUTOINCREMENT' column with the specified name. --convert-config TEXT [experimental] Configurations for data conversion. The option can be used only for url subcommand. -i, --index INDEX_ATTR Comma separated attribute names to create indices. --no-type-inference All of the columns assume as TEXT data type in creating tables. --type-hint-header Use headers suffix as type hints. If there are type hints, converting columns by datatype corresponding with type hints. The following suffixes can be recognized as type hints (case insensitive): \"text\": TEXT datatype. \"integer\": INTEGER datatype. \"real\": REAL datatype. --replace-symbol TEXT Replace symbols in attributes. -v, --verbose --debug For debug print. -q, --quiet Suppress execution log messages. -h, --help Show this message and exit. Commands: completion A helper command to setup command completion. configure Configure the following application settings: (1) Default... file Convert tabular data within CSV/Excel/HTML/JSON/Jupyter... gs Convert a spreadsheet in Google Sheets to a SQLite database... url Scrape tabular data from a URL and convert data to a SQLite... ```","title":"Sqlitebiter"},{"location":"details/#mkdocs_1","text":"``` (venv) pubtest> mkdocs Usage: mkdocs [OPTIONS] COMMAND [ARGS]... MkDocs - Project documentation with Markdown. Options: -V, --version Show the version and exit. -q, --quiet Silence warnings -v, --verbose Enable verbose output -h, --help Show this message and exit. Commands: build Build the MkDocs documentation gh-deploy Deploy your documentation to GitHub Pages new Create a new MkDocs project serve Run the builtin development server ```","title":"mkdocs"},{"location":"rwot5/topics-and-advance-readings/","text":"If you will be attending Rebooting the Web of Trust Fall 2017 in Boston, Massachusetts, please upload your topic papers and advanced readings to this directory with a pull request (or, if you don't know how, post an issue). Please see the Web of Trust Info website for more information about our community, including upcoming events. Topics & Advance Readings In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? The following advanced readings have been prepared as primers, intended to give you a foundation in some of the topics that may be discussed at the design workshop: A Primer on Functional Identity by Joe Andrieu A Primer on Self-Sovereign identity by Christopher Allen & Shannon Appelcline DID Primer by Drummond Reed & Manu Sporny A Verifiable Claims Primer by Manu Sporny Here are the rest of the advance readings to date: #RebootingWebOfTrust User Story & Tech Concept by Christopher Allen ActivityPub: from decentralized to distributed social networks by Christopher Lemmer Webber ( PDF version ) Architectural Layering for Decentralized Identification by Drummond Reed BFTKV: Byzantine Fault Tolerant Web of Trust based Key-Value Storage by Ercan Ozturk BFTKV DID Method Specification by Ercan Ozturk Biometric transaction signing on blockchain by John Callahan & Virgil Tornoreanu Blockchain Based Digital Signatures: Admissibility and Enforceability by Dazza Greenwood BTCR DIDs and DDOs by Kim Hamilton Duffy Credential Handler API by Dave Longley and Manu Sporny Data Minimization and Selective Disclosure by Lionel Wolberger Decentralized Identifier Tooling by Dave Longley & Manu Sporny DID for the 3D Web by Alberto Elias DID Tooling by Manu Sporny and Matt Collier First XDI Link Contract between \"btcr\" DID and \"sov\" DID by Markus Sabadello Framework for the Comparison of Identity Systems by Kyle Den Hartog HIE of One Loop: A Patient-Controlled Independent Health Record by Adrian Gropper The Horcrux protocol: Biometric credentials as high-privacy verifiable claims by John Callahan & Asem Othman Hub Asset Access Control System - Intent by Daniel Buchner Hypercerts: Blockcerts Revocation Improvements by Jo\u00e3o Santos and Kim Hamilton Duffy Identifying stakeholders' challenges in the digital economy by Irene Hernandez Open Badges (and Blockcerts) as Verifiable Claims by Kim Hamilton Duffy and Nate Otto Owned vs. Unowned Claims and Self-Sovereign Identity by Natalie Smolenski Recommendations for Decentralized Key Management Systems by Michael Lodder Trust Objects: Enabling Advanced Reputation Services on the Web of Trust by Moses Ma & Dr. Rutu Mulka Original Proposal by Moses Ma & Dr. Rutu Mulka Veres One DID Method Specification by Manu Sporny, Dave Longley & Matt Collier Verifiable Claims of Impact by Cedric Franz & Dr. Shaun Conway Visa? Really? by Kaliya Identity Woman Data Minimization and Selective Disclosure by Lionel Wolberger HIE of One Loop: A Patient-Controlled Independent Health Record by Adrian Gropper, MD and the Loop Project Team","title":"Home"},{"location":"rwot5/topics-and-advance-readings/#topics-advance-readings","text":"In advance of the design workshop, all participants produced a one-or-two page topic paper to be shared with the other attendees on either: A specific problem that they wanted to solve with a web-of-trust solution, and why current solutions (PGP or CA-based PKI) can't address the problem? A specific solution related to the web-of-trust that you'd like others to use or contribute to? The following advanced readings have been prepared as primers, intended to give you a foundation in some of the topics that may be discussed at the design workshop: A Primer on Functional Identity by Joe Andrieu A Primer on Self-Sovereign identity by Christopher Allen & Shannon Appelcline DID Primer by Drummond Reed & Manu Sporny A Verifiable Claims Primer by Manu Sporny Here are the rest of the advance readings to date: #RebootingWebOfTrust User Story & Tech Concept by Christopher Allen ActivityPub: from decentralized to distributed social networks by Christopher Lemmer Webber ( PDF version ) Architectural Layering for Decentralized Identification by Drummond Reed BFTKV: Byzantine Fault Tolerant Web of Trust based Key-Value Storage by Ercan Ozturk BFTKV DID Method Specification by Ercan Ozturk Biometric transaction signing on blockchain by John Callahan & Virgil Tornoreanu Blockchain Based Digital Signatures: Admissibility and Enforceability by Dazza Greenwood BTCR DIDs and DDOs by Kim Hamilton Duffy Credential Handler API by Dave Longley and Manu Sporny Data Minimization and Selective Disclosure by Lionel Wolberger Decentralized Identifier Tooling by Dave Longley & Manu Sporny DID for the 3D Web by Alberto Elias DID Tooling by Manu Sporny and Matt Collier First XDI Link Contract between \"btcr\" DID and \"sov\" DID by Markus Sabadello Framework for the Comparison of Identity Systems by Kyle Den Hartog HIE of One Loop: A Patient-Controlled Independent Health Record by Adrian Gropper The Horcrux protocol: Biometric credentials as high-privacy verifiable claims by John Callahan & Asem Othman Hub Asset Access Control System - Intent by Daniel Buchner Hypercerts: Blockcerts Revocation Improvements by Jo\u00e3o Santos and Kim Hamilton Duffy Identifying stakeholders' challenges in the digital economy by Irene Hernandez Open Badges (and Blockcerts) as Verifiable Claims by Kim Hamilton Duffy and Nate Otto Owned vs. Unowned Claims and Self-Sovereign Identity by Natalie Smolenski Recommendations for Decentralized Key Management Systems by Michael Lodder Trust Objects: Enabling Advanced Reputation Services on the Web of Trust by Moses Ma & Dr. Rutu Mulka Original Proposal by Moses Ma & Dr. Rutu Mulka Veres One DID Method Specification by Manu Sporny, Dave Longley & Matt Collier Verifiable Claims of Impact by Cedric Franz & Dr. Shaun Conway Visa? Really? by Kaliya Identity Woman Data Minimization and Selective Disclosure by Lionel Wolberger HIE of One Loop: A Patient-Controlled Independent Health Record by Adrian Gropper, MD and the Loop Project Team","title":"Topics &amp; Advance Readings"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/","text":"Architectural Layering for Decentralized Identification A submission to Rebooting the Web of Trust #5\\ 2017-09-30\\ Drummond Reed, Co-Chair, DIF Identifiers, Names, and Discovery Working Group Introduction This document explains why, in building infrastructure for decentralized identification, semantic identification should be layered over decentralized identifiers (DIDs). It also proposes four key requirements for this semantic identification layer. The DID Layer DIDs (decentralized identifiers) solve a fundamental problem in digital identity by providing a completely decentralized way to identify a resource. Following is the abstract of the DID Data Model and Generic Syntax spec : DIDs (decentralized identifiers) are a new type of identifier intended for verifiable digital identity that is \u201cself-sovereign\u201d, i.e, fully under the control of the identity owner and not dependent on a centralized registry, identity provider, or certificate authority. DIDs resolve to DID documents \u2014simple JSON documents that contain all the metadata needed to prove ownership and control of a DID. Specifically, a DID document contains a set of key descriptions \u2014 machine-readable descriptions of the identity owner\u2019s public keys\u2014and a set of service endpoints \u2014resource pointers necessary to initiate trusted interactions with the identity owner. Each DID uses a specific DID method , defined in a separate DID method specification , to define how the DID is registered, resolved, updated, and revoked on a specific distributed ledger or network. Section 1.3 of the specification explains the specific technical motivations for DIDs: The growing need for decentralized identity has produced three specific requirements for a new type of URI that fits within the URI/URL/URN architecture, albeit in a less traditional way: A URI that is persistent like a URN yet can be resolved or de-referenced to locate a resource like a URL. In essence, a DID is a URI that serves both functions. A URI that does not require a centralized authority to register, resolve, update, or revoke. The overwhelming majority of URIs today are based on DNS names or IP addresses that depend on centralized authorities for registration and ultimate control. DIDs can be created and managed without any such authority. A URI whose ownership and associated metadata, including public keys, can be cryptographically verified. Control of DIDs and DID documents leverages the same public/private key cryptography as distributed ledgers. The Semantic Identification Layer While DIDs provide a machine-friendly layer of decentralized resource identification similar to the IP layer of addressing on the Internet, they do not solve the problem of human-friendly semantic identification, i.e., how to map from the semantic identifiers that humans use to identify a resource\u2014which may change over time\u2014to a machine-friendly persistent DID. From section 1.4 of the DID spec: DIDs achieve global uniqueness without the need for a central registration authority. This comes, however, at the cost of human memorability. The algorithms capable of generating globally unique identifiers automatically produce random strings of characters that have no human meaning. This leads to the axiom about identifiers known as Zooko\u2019s Triangle : \u201chuman-meaningful, decentralized, secure\u2014pick any two\u201d. There are of course many use cases where it is desirable to discover a DID starting from a human-friendly identifier\u2014a natural language name, a domain name, or a conventional address for a DID owner, such as a mobile telephone number, email address, Twitter handle, blog URL, etc. However, the problem of mapping human-friendly identifiers to DIDs (and doing so in a way that can be verified and trusted) is out-of-scope for this specification. Solutions to this problem (and there are many) should be defined in separate specifications that reference this specification. It is strongly recommended that such specifications carefully consider: (a) the numerous security attacks based on deceiving users about the true human-friendly identifier for a target entity, and (b) the privacy consequences of using human-friendly identifiers that are inherently correlatable, especially if they are globally unique. Requirements of the Semantic Identification Layer Following are the key proposed requirements for the semantic identification layer. #1: The Semantic Identification Layer MUST Map to a DID This requirement is paramount due to a fundamental security requirement: the resource identified by an identifier MUST NOT change over time. The OpenID community learned this lesson very painfully when it did not realize that domain names in the URLs used as OpenID identifiers permitted these URLs to be reassigned to new identity owners. For example, the URL: https://janedoe.com/ is controlled by the domain owner of \u201cjanedoe.com\u201d. If this domain name is reassigned, either directly or maliciously, then it represents a different identity owner, which means all relying parties who have granted access rights to the original identity owner are now vulnerable. The OpenID authors subsequently changed the OpenID specification to explicitly require OpenID providers to use non-reassignable URLs by appending a unique fragment to the URL. However this solution was acknowledged to be weak because the owner of the domain name for a URL controls the fragment space, not the OpenID provider. The OpenID community has never adequately solved what came to be known as the \u201cOpenID recycling problem\u201d. Furthermore, the OpenID recycling problem\u2014or more generically the identifier reassignment problem\u2014is inherent at the semantic identification layer because of semantic drift . The human brain is constantly remapping semantic identifiers and cannot be constrained in the same way machine code can. Therefore, to avoid reproducing the OpenID recycling problem and the attendant security hole it introduces, any solution for semantic identification of a resource MUST map to a DID for the resource. Only a DID can offer the structural characteristics of non-reassignability required of persistent identifiers. (Note that even a DID cannot prevent reassignment of ownership by the identity owner via transference or reassignment of the private keys. However DID architecture prevents that reassignment from being outside of the control of the identity owner.) #2: The Semantic Identification Layer MUST Allow Many-to-One Mapping Another fact of human semantic identification is that humans assign multiple identifiers to same resource. Even a single human being often responds to many names\u2014for example, an \u201cElizabeth\u201d may also be identified as \u201cLiz\u201d, \u201cBeth\u201d, \u201cLisa\u201d, \u201cLiza\u201d, etc. The same goes for organizational entities\u2014for example a single corporation may have multiple DBAs that \u201cresolve\u201d to the same unique business identifier (or LEI\u2014legal entity identifier ). So at the semantic identification layer it MUST be possible to assign multiple semantic identifiers to the same DID. #3: The Semantic Identification Layer MUST Support Web Resource Mapping Web architecture inherently enables the mapping of arbitrary strings to URIs. Typically those URIs are human-readable (or semi-human readable) URLs. However that is often not the case. For example, the Google doc where this paper was originally drafted has the following distinctly un-human-friendly URL: https://docs.google.com/document/d/12suJL5sX3CsNex_mMhUW7A2mAmS50HjAaUUvq8O52-I/ This is a classic example of using a machine-friendly identifier for persistence. The semantic mapping necessary for a human to recognize the document is provided by a link, e.g.: Architectural Layering for Decentralized Identification At a minimum, this form of semantic mapping MUST also be supported for DIDs. This requires either or both of two mechanisms: DID Web proxy resolvers. These are web servers that accept a URL containing a DID and return the DID document. Almost all new identifier formats require Web proxy resolvers\u2014for example this is still the way most DOIs ( digital object identifiers ) are resolved today. Native DID resolvers. These are URI processors built into the browser or OS that handle DID resolution natively just as they currently handle DNS name resolution natively. Note that implementing a universal resolver for DIDs that supports all popular DID methods is a core focus of the Decentralized Identity Foundation . #4: The Semantic Identification Layer SHOULD Support Market-Driven Mapping Solutions While the traditional approach to semantic identification has been a \u201cname service\u201d, the introduction of a layer for persistent, cryptographically-verifiable decentralized identifiers enables a semantic identification layer to be \u201cliberated\u201d from the traditional constraints of a semantic name service. For example, a semantic identification layer over DIDs could support: Global names. These would be the DID infrastructure equivalent of domain names. Local names. This \u201cname service\u201d is not globally unique, but locally-unique using P2P connections. See Christopher Allen\u2019s Rebooting the Web of Trust paper on linked local names . Structured directory services. These are services similar to LinkedIn or corporate LDAP services that permit queries by attributes. Unstructured web search. This is simply applying the same Web document spidering/indexing (and even PageRank-style reputation) to locating a link to the DID for the target entity. Note that verification of the actual ownership of the DID by this entity is out-of-scope for the DID specification but directly in scope for the W3C Verifiable Claims Working Group .","title":"Architectural Layering for Decentralized Identification"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/#architectural-layering-for-decentralized-identification","text":"A submission to Rebooting the Web of Trust #5\\ 2017-09-30\\ Drummond Reed, Co-Chair, DIF Identifiers, Names, and Discovery Working Group","title":"Architectural Layering for Decentralized Identification"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/#introduction","text":"This document explains why, in building infrastructure for decentralized identification, semantic identification should be layered over decentralized identifiers (DIDs). It also proposes four key requirements for this semantic identification layer.","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/#the-did-layer","text":"DIDs (decentralized identifiers) solve a fundamental problem in digital identity by providing a completely decentralized way to identify a resource. Following is the abstract of the DID Data Model and Generic Syntax spec : DIDs (decentralized identifiers) are a new type of identifier intended for verifiable digital identity that is \u201cself-sovereign\u201d, i.e, fully under the control of the identity owner and not dependent on a centralized registry, identity provider, or certificate authority. DIDs resolve to DID documents \u2014simple JSON documents that contain all the metadata needed to prove ownership and control of a DID. Specifically, a DID document contains a set of key descriptions \u2014 machine-readable descriptions of the identity owner\u2019s public keys\u2014and a set of service endpoints \u2014resource pointers necessary to initiate trusted interactions with the identity owner. Each DID uses a specific DID method , defined in a separate DID method specification , to define how the DID is registered, resolved, updated, and revoked on a specific distributed ledger or network. Section 1.3 of the specification explains the specific technical motivations for DIDs: The growing need for decentralized identity has produced three specific requirements for a new type of URI that fits within the URI/URL/URN architecture, albeit in a less traditional way: A URI that is persistent like a URN yet can be resolved or de-referenced to locate a resource like a URL. In essence, a DID is a URI that serves both functions. A URI that does not require a centralized authority to register, resolve, update, or revoke. The overwhelming majority of URIs today are based on DNS names or IP addresses that depend on centralized authorities for registration and ultimate control. DIDs can be created and managed without any such authority. A URI whose ownership and associated metadata, including public keys, can be cryptographically verified. Control of DIDs and DID documents leverages the same public/private key cryptography as distributed ledgers.","title":"The DID Layer"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/#the-semantic-identification-layer","text":"While DIDs provide a machine-friendly layer of decentralized resource identification similar to the IP layer of addressing on the Internet, they do not solve the problem of human-friendly semantic identification, i.e., how to map from the semantic identifiers that humans use to identify a resource\u2014which may change over time\u2014to a machine-friendly persistent DID. From section 1.4 of the DID spec: DIDs achieve global uniqueness without the need for a central registration authority. This comes, however, at the cost of human memorability. The algorithms capable of generating globally unique identifiers automatically produce random strings of characters that have no human meaning. This leads to the axiom about identifiers known as Zooko\u2019s Triangle : \u201chuman-meaningful, decentralized, secure\u2014pick any two\u201d. There are of course many use cases where it is desirable to discover a DID starting from a human-friendly identifier\u2014a natural language name, a domain name, or a conventional address for a DID owner, such as a mobile telephone number, email address, Twitter handle, blog URL, etc. However, the problem of mapping human-friendly identifiers to DIDs (and doing so in a way that can be verified and trusted) is out-of-scope for this specification. Solutions to this problem (and there are many) should be defined in separate specifications that reference this specification. It is strongly recommended that such specifications carefully consider: (a) the numerous security attacks based on deceiving users about the true human-friendly identifier for a target entity, and (b) the privacy consequences of using human-friendly identifiers that are inherently correlatable, especially if they are globally unique.","title":"The Semantic Identification Layer"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/#requirements-of-the-semantic-identification-layer","text":"Following are the key proposed requirements for the semantic identification layer.","title":"Requirements of the Semantic Identification Layer"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/#351-the-semantic-identification-layer-must-map-to-a-did","text":"This requirement is paramount due to a fundamental security requirement: the resource identified by an identifier MUST NOT change over time. The OpenID community learned this lesson very painfully when it did not realize that domain names in the URLs used as OpenID identifiers permitted these URLs to be reassigned to new identity owners. For example, the URL: https://janedoe.com/ is controlled by the domain owner of \u201cjanedoe.com\u201d. If this domain name is reassigned, either directly or maliciously, then it represents a different identity owner, which means all relying parties who have granted access rights to the original identity owner are now vulnerable. The OpenID authors subsequently changed the OpenID specification to explicitly require OpenID providers to use non-reassignable URLs by appending a unique fragment to the URL. However this solution was acknowledged to be weak because the owner of the domain name for a URL controls the fragment space, not the OpenID provider. The OpenID community has never adequately solved what came to be known as the \u201cOpenID recycling problem\u201d. Furthermore, the OpenID recycling problem\u2014or more generically the identifier reassignment problem\u2014is inherent at the semantic identification layer because of semantic drift . The human brain is constantly remapping semantic identifiers and cannot be constrained in the same way machine code can. Therefore, to avoid reproducing the OpenID recycling problem and the attendant security hole it introduces, any solution for semantic identification of a resource MUST map to a DID for the resource. Only a DID can offer the structural characteristics of non-reassignability required of persistent identifiers. (Note that even a DID cannot prevent reassignment of ownership by the identity owner via transference or reassignment of the private keys. However DID architecture prevents that reassignment from being outside of the control of the identity owner.)","title":"#1: The Semantic Identification Layer MUST Map to a DID"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/#352-the-semantic-identification-layer-must-allow-many-to-one-mapping","text":"Another fact of human semantic identification is that humans assign multiple identifiers to same resource. Even a single human being often responds to many names\u2014for example, an \u201cElizabeth\u201d may also be identified as \u201cLiz\u201d, \u201cBeth\u201d, \u201cLisa\u201d, \u201cLiza\u201d, etc. The same goes for organizational entities\u2014for example a single corporation may have multiple DBAs that \u201cresolve\u201d to the same unique business identifier (or LEI\u2014legal entity identifier ). So at the semantic identification layer it MUST be possible to assign multiple semantic identifiers to the same DID.","title":"#2: The Semantic Identification Layer MUST Allow Many-to-One Mapping"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/#353-the-semantic-identification-layer-must-support-web-resource-mapping","text":"Web architecture inherently enables the mapping of arbitrary strings to URIs. Typically those URIs are human-readable (or semi-human readable) URLs. However that is often not the case. For example, the Google doc where this paper was originally drafted has the following distinctly un-human-friendly URL: https://docs.google.com/document/d/12suJL5sX3CsNex_mMhUW7A2mAmS50HjAaUUvq8O52-I/ This is a classic example of using a machine-friendly identifier for persistence. The semantic mapping necessary for a human to recognize the document is provided by a link, e.g.: Architectural Layering for Decentralized Identification At a minimum, this form of semantic mapping MUST also be supported for DIDs. This requires either or both of two mechanisms: DID Web proxy resolvers. These are web servers that accept a URL containing a DID and return the DID document. Almost all new identifier formats require Web proxy resolvers\u2014for example this is still the way most DOIs ( digital object identifiers ) are resolved today. Native DID resolvers. These are URI processors built into the browser or OS that handle DID resolution natively just as they currently handle DNS name resolution natively. Note that implementing a universal resolver for DIDs that supports all popular DID methods is a core focus of the Decentralized Identity Foundation .","title":"#3: The Semantic Identification Layer MUST Support Web Resource Mapping"},{"location":"rwot5/topics-and-advance-readings/Architectural-Layering-for-Decentralized-Identification/#354-the-semantic-identification-layer-should-support-market-driven-mapping-solutions","text":"While the traditional approach to semantic identification has been a \u201cname service\u201d, the introduction of a layer for persistent, cryptographically-verifiable decentralized identifiers enables a semantic identification layer to be \u201cliberated\u201d from the traditional constraints of a semantic name service. For example, a semantic identification layer over DIDs could support: Global names. These would be the DID infrastructure equivalent of domain names. Local names. This \u201cname service\u201d is not globally unique, but locally-unique using P2P connections. See Christopher Allen\u2019s Rebooting the Web of Trust paper on linked local names . Structured directory services. These are services similar to LinkedIn or corporate LDAP services that permit queries by attributes. Unstructured web search. This is simply applying the same Web document spidering/indexing (and even PageRank-style reputation) to locating a link to the DID for the target entity. Note that verification of the actual ownership of the DID by this entity is out-of-scope for the DID specification but directly in scope for the W3C Verifiable Claims Working Group .","title":"#4: The Semantic Identification Layer SHOULD Support Market-Driven Mapping Solutions"},{"location":"rwot5/topics-and-advance-readings/Biometric-credentials-as-high-privacy-verifiable-claims/","text":"The Horcrux protocol: Biometric credentials as high-privacy verifiable claims John Callahan (Veridium IP Ltd), Asem Othman (Veridium IP Ltd) Private keys locked by biometrics on mobile devices provide not only ways for users to authenticate themselves, but also to manage important assets tied to the corresponding public keys used in various contexts including blockchains (NOTE: https://www.w3.org/2016/04/blockchain-workshop/interest/blocko.html), (NOTE: https://www.slideshare.net/FIDOAlliance/fido-authentication-blockchain). Biometric authentication is increasingly being used on mobile devices to secure private keys and other sensitive personally identifiable information (PII) in some form of trusted platform module (TPM). While on-device biometrics (NOTE: Gartner refers to on-device biometrics as \"local\" biometrics) have advantages for service providers (who want to avoid risks associated with PII loss) and users (who feel they have control of their PII if it is locked on their device), it also has important disadvantages for both users and providers, namely presentation attacks, device attacks, loss of device, and theft. Revocation and recovery of associated credentials, accounts and other assets previously tied to on-device keys can be complex, error prone and subject to social engineering. To avoid these problems and comply with identity proofing requirements like know-your-customer (KYC) and anti-money laundering (AML) regulations, many financial institutions are opting for server-side biometric authentication solutions. A recent paper (NOTE: Giulio Lovisotto, et al, Mobile Biometrics in Financial Services: A Five Factor Framework, CS-RR-17-03, University of Oxford Computer Science Department.) favorably compares two biometric authentication protocols, FIDO UAF and IEEE 2410 BOPS, as the leading methods for biometric authentication in financial services. While these protocols focus on defense against scalable attacks, they rely on device-specific TPM for protection and processing of biometric templates. The IEEE 2410 BOPS protocol is extensible to a combination of on-device (FIDO UAF compatible), server-side or a multi-distribution model that utilizes a secret scheme. Indeed, the standard allows for off-device biometric credentials under user control. The device\u2019s local TPM is only one option (though dominant at the moment) for persisting biometric credentials and associated key(s). We propose implementation of a BOPS-compatible biometric credential storage option using the high-privacy verifiable claims (NOTE: https://blog.tokenize.io/high-privacy-verifiable-claims-4e556af28fe2) format. Credentials would be stored off-chain as as a DID Descriptor Object (NOTE: Data Model and Syntaxes for Decentralized Identifiers (DIDs) - https://w3c-ccg.github.io/did-spec/) (DDO) with its referent decentralized identifier (DID) stored on a blockchain. Use of DDOs for biometric templates has already been proposed (NOTE: Stephen Wilson, Safeguarding the pedigree of personal attributes, http://lockstep.com.au/blog/2014/09/01/pedigree-of-ids), but to further protect the biometric data, we propose the use of the existing BOPS multi-distribution model that utilizes a secret scheme to divide the templates into n \u2265 2 shares using visual cryptography (NOTE: Ross, Arun and Asem Othman, Visual Cryptography for Biometric Privacy, IEEE Transactions on Information Forensics and Security, Volume: 6, Issue: 1, March 2011) as specified in IEEE 2410-2015 and IEEE 2410-2016. Multiple shares (and potentially redundant shares) could be spread across alternate off-chain storage (like IPFS, Dropbox, Storj, etc.) to improve availability and security. The PII have been encrypted to generate these shares in such a way that the PII can be revealed only when the predefined number of shares are simultaneously available; further, the individual Shares do not reveal any information about the original PII. We believe that a straightforward implementation of high-privacy verifiable claims for biometric credentials could be integrated with Blockstack authentication and incorporate evolving decentralized key management systems (NOTE: Rebooting Web of Trust, D. Reed, DKMS: Decentralized Key Management Systems (Spring 2017)), (NOTE: Decentralized Key Management using Blockchain (see https://www.sbir.gov/sbirsearch/detail/1302463)) (DKMS). Rather than reinvent the wheel, we seek to incorporate biometric credentials as securely as possible within an ecosystem of open standards while restoring privacy and consumer control even within server-side scenarios, particularly use of social and biometric revocation and recovery mechanisms. The use of biometrics will continue to grow and be used for authentication, asset management and transaction records. The horse is out-of-the-barn: systems like Aadhaar, IAFIS, etc. already use server-side solutions and do not advance self-sovereign identity (SSI), i.e., they hold personal data (e.g., biometrics) in a centralized store under a central authority. Will each Internet-of-Things (IOT) require individual biometric enrollment, a TPM, and supporting sensors? Surely, these devices will rely on server-side biometric solutions for convenience and scale, but likely with poorly designed one-off, hand-rolled solutions with critical vulnerabilities. The original vision of ubiquitous computing was that mobile devices could be used and borrowed like a ball-point pen and not \"personal\" like the first computers. Finally, as blockchains allow previously \u201cunbanked\u201d persons sovereignty over their assets, we will need device-independent means to enroll, verify, and authenticate payments using biometrics in addition to or an alternative to passwords that can be forgotten, paper wallets and tokens that can be lost or stolen. We would like to find people to explore these ideas in order to determine how to get this tech to market in the next year. Our current products, VeridiumID and VeridiumAD, provide biometric transaction signing, end-to-end compliance with IEEE 2410-2016 including mobile and server-based secure storage of credentials, with FIDO UAF and with existing products and standards like Active Directory, Citrix, SAML 2.0 and OpenID Connect. Continuing the spirit of our open standards work, we seek to integrate IEEE 2410 BOPS with existing identity standards like DIDs/DDOs via the Verifiable Claims work, DKMS, Open Badges, Blockstack auth, WebAuthN, and other evolving efforts.","title":"The Horcrux protocol: Biometric credentials as high-privacy verifiable claims"},{"location":"rwot5/topics-and-advance-readings/Biometric-credentials-as-high-privacy-verifiable-claims/#the-horcrux-protocol-biometric-credentials-as-high-privacy-verifiable-claims","text":"","title":"The Horcrux protocol: Biometric credentials as high-privacy verifiable claims"},{"location":"rwot5/topics-and-advance-readings/Biometric-credentials-as-high-privacy-verifiable-claims/#john-callahan-veridium-ip-ltd-asem-othman-veridium-ip-ltd","text":"Private keys locked by biometrics on mobile devices provide not only ways for users to authenticate themselves, but also to manage important assets tied to the corresponding public keys used in various contexts including blockchains (NOTE: https://www.w3.org/2016/04/blockchain-workshop/interest/blocko.html), (NOTE: https://www.slideshare.net/FIDOAlliance/fido-authentication-blockchain). Biometric authentication is increasingly being used on mobile devices to secure private keys and other sensitive personally identifiable information (PII) in some form of trusted platform module (TPM). While on-device biometrics (NOTE: Gartner refers to on-device biometrics as \"local\" biometrics) have advantages for service providers (who want to avoid risks associated with PII loss) and users (who feel they have control of their PII if it is locked on their device), it also has important disadvantages for both users and providers, namely presentation attacks, device attacks, loss of device, and theft. Revocation and recovery of associated credentials, accounts and other assets previously tied to on-device keys can be complex, error prone and subject to social engineering. To avoid these problems and comply with identity proofing requirements like know-your-customer (KYC) and anti-money laundering (AML) regulations, many financial institutions are opting for server-side biometric authentication solutions. A recent paper (NOTE: Giulio Lovisotto, et al, Mobile Biometrics in Financial Services: A Five Factor Framework, CS-RR-17-03, University of Oxford Computer Science Department.) favorably compares two biometric authentication protocols, FIDO UAF and IEEE 2410 BOPS, as the leading methods for biometric authentication in financial services. While these protocols focus on defense against scalable attacks, they rely on device-specific TPM for protection and processing of biometric templates. The IEEE 2410 BOPS protocol is extensible to a combination of on-device (FIDO UAF compatible), server-side or a multi-distribution model that utilizes a secret scheme. Indeed, the standard allows for off-device biometric credentials under user control. The device\u2019s local TPM is only one option (though dominant at the moment) for persisting biometric credentials and associated key(s). We propose implementation of a BOPS-compatible biometric credential storage option using the high-privacy verifiable claims (NOTE: https://blog.tokenize.io/high-privacy-verifiable-claims-4e556af28fe2) format. Credentials would be stored off-chain as as a DID Descriptor Object (NOTE: Data Model and Syntaxes for Decentralized Identifiers (DIDs) - https://w3c-ccg.github.io/did-spec/) (DDO) with its referent decentralized identifier (DID) stored on a blockchain. Use of DDOs for biometric templates has already been proposed (NOTE: Stephen Wilson, Safeguarding the pedigree of personal attributes, http://lockstep.com.au/blog/2014/09/01/pedigree-of-ids), but to further protect the biometric data, we propose the use of the existing BOPS multi-distribution model that utilizes a secret scheme to divide the templates into n \u2265 2 shares using visual cryptography (NOTE: Ross, Arun and Asem Othman, Visual Cryptography for Biometric Privacy, IEEE Transactions on Information Forensics and Security, Volume: 6, Issue: 1, March 2011) as specified in IEEE 2410-2015 and IEEE 2410-2016. Multiple shares (and potentially redundant shares) could be spread across alternate off-chain storage (like IPFS, Dropbox, Storj, etc.) to improve availability and security. The PII have been encrypted to generate these shares in such a way that the PII can be revealed only when the predefined number of shares are simultaneously available; further, the individual Shares do not reveal any information about the original PII. We believe that a straightforward implementation of high-privacy verifiable claims for biometric credentials could be integrated with Blockstack authentication and incorporate evolving decentralized key management systems (NOTE: Rebooting Web of Trust, D. Reed, DKMS: Decentralized Key Management Systems (Spring 2017)), (NOTE: Decentralized Key Management using Blockchain (see https://www.sbir.gov/sbirsearch/detail/1302463)) (DKMS). Rather than reinvent the wheel, we seek to incorporate biometric credentials as securely as possible within an ecosystem of open standards while restoring privacy and consumer control even within server-side scenarios, particularly use of social and biometric revocation and recovery mechanisms. The use of biometrics will continue to grow and be used for authentication, asset management and transaction records. The horse is out-of-the-barn: systems like Aadhaar, IAFIS, etc. already use server-side solutions and do not advance self-sovereign identity (SSI), i.e., they hold personal data (e.g., biometrics) in a centralized store under a central authority. Will each Internet-of-Things (IOT) require individual biometric enrollment, a TPM, and supporting sensors? Surely, these devices will rely on server-side biometric solutions for convenience and scale, but likely with poorly designed one-off, hand-rolled solutions with critical vulnerabilities. The original vision of ubiquitous computing was that mobile devices could be used and borrowed like a ball-point pen and not \"personal\" like the first computers. Finally, as blockchains allow previously \u201cunbanked\u201d persons sovereignty over their assets, we will need device-independent means to enroll, verify, and authenticate payments using biometrics in addition to or an alternative to passwords that can be forgotten, paper wallets and tokens that can be lost or stolen. We would like to find people to explore these ideas in order to determine how to get this tech to market in the next year. Our current products, VeridiumID and VeridiumAD, provide biometric transaction signing, end-to-end compliance with IEEE 2410-2016 including mobile and server-based secure storage of credentials, with FIDO UAF and with existing products and standards like Active Directory, Citrix, SAML 2.0 and OpenID Connect. Continuing the spirit of our open standards work, we seek to integrate IEEE 2410 BOPS with existing identity standards like DIDs/DDOs via the Verifiable Claims work, DKMS, Open Badges, Blockstack auth, WebAuthN, and other evolving efforts.","title":"John Callahan (Veridium IP Ltd), Asem Othman (Veridium IP Ltd)"},{"location":"rwot5/topics-and-advance-readings/Biometric-transaction-signing-on-blockchain/","text":"Biometric transaction signing on blockchain John Callahan (Veridium IP Ltd), Virgil Tornoreanu (Veridium IP Ltd) The use of strong customer authentication (SCA) is a critical requirement for collecting explicit consent under the new PSD2 regulations, but equally important are GDPR regulations like the \"right to be forgotten\" that require banks to purge their records of any data associated with an account at a former customer\u2019s request. These regulations are designed to empower individuals with control over their data held by institutions, but technical, legal and policy difficulties are delaying programs needed to implement such policies. Delay means that new financial services providers like AISPs (NOTE: Account Information Service Providers (see https://www.w3.org/Payments/IG/wiki/PSD2#AISP)) and PISPs (NOTE: Payment Initiation Service Providers (see https://www.w3.org/Payments/IG/wiki/PSD2#PISP)) that require consent to access personal information will avoid postpone important innovations to avoid incumbent risks. Strong customer authentication (SCA) means that 2 out of 3 methods for authenticated consent must be employed on high-risk financial transactions (any single payment or transfer over 50 Euro or cumulatively (or 5 consecutive transactions) over 150 Euro (NOTE: Regulatory Technical Standards (RTS) on strong customer authentication and secure communication under PSD2, Chapter 2, Article 11 - Contactless payments at point of sale)). The three methods (NOTE: Regulatory Technical Standards (RTS) on strong customer authentication and secure communication under PSD2, Chapter 2, Article 4 pursuant Article 97(1) of Directive (EU) 2015/2366) are: Knowledge: something only the user knows (such as a password). Ownership: something only the user possesses (such as a chip card). Inherence: something the user personally or physically has (such as a fingerprint). These methods must not be connected and each transaction must be distinct (e.g., unique and non-sequential transaction identifiers). In addition, biometric authentication uniquely offers the possibility for non-repudiation and favored by some institutions for consent because it is difficult to contest legally. In the case of passwords or token, the customer could deny approving the payment due to the theft of password or token. Such cases are not theoretical (NOTE: https://www.finextra.com/blogposting/14068/taking-bold-steps-to-protect-high-value-trading). In addition to covering billions in fraud costs, financial institutions face severe fines under GDPR (NOTE: Organizations can be fined up to 4% of annual global turnover for breaching GDPR or \u20ac20 Million) for failure to \"most serious infringements e.g. not having sufficient customer consent to process data\" and other violations (NOTE: http://www.eugdpr.org/gdpr-faqs.html). The IEEE 2410-2016 BOPS protocol extends IEEE 2410-2015 in order to be PSD2 compliant by incorporating the explicit consent message, beneficiary and amount in any authentication request as an option for authentication, signing and escalated transaction calls. For a given payment transaction, the customer is prompted for biometric authentication on the mobile device with a message indicating the reason, beneficiary and amount. If successful, the transaction is signed with an associated private unlocked by biometric authentication, transmitted to and recorded in server logs that can be audited, archived and purged as per records management regulations. The transaction is \"biometrically signed\" by transitive association of the key associated with the biometric. As an alternative storage option, we propose use of off-chain DID Descriptor Object (NOTE: Data Model and Syntaxes for Decentralized Identifiers (DIDs) - https://w3c-ccg.github.io/did-spec/) (DDOs) with referent decentralized identifier (DIDs) on blockchains for recording of PSD2 compliant transactions. Specifically, a biometrically signed transaction would exist as a high-privacy verifiable claim (NOTE: https://blog.tokenize.io/high-privacy-verifiable-claims-4e556af28fe2). Several issues must be addressed to protect user privacy, allow for regulatory compliance and auditing, records purging via revocation, and recovery in case of loss of credentials. We envision several options for institutions and their customers based on security \"profiles\" that allow customers to balance privacy with convenience including: Storage of encrypted transactions off-chain in user owned media (IPFS, Dropbox, Storj, etc.) that gives a customer sovereign control, but restricts the type of services based on availability Storage of encrypted transactions off-chain in publicly accessible storage at the institutions choice using customer\u2019s public key(s). Subsequent audits could be done only with customer\u2019s authorization. Storage of encrypted transactions off-chain in public or private accessible storage at the institutions choice using customer\u2019s private key(s). Subsequent audits could be done by institution or customer. Our approach to blockchain-based biometrically signed transactions promotes self-sovereign identity by allowing customers to control their record data. This benefits both the customer and financial institutions themselves by reducing the risk of lost, costs of compliance, and improve secure access to data by the new AISP and PISP providers without resort to centralized services (like the ASPSPs). Projects like the Open Banking Project (OBP) could adapt their APIs to accommodate blockchain-based methods using serverless methods for payment processing. Any blockchain-based transaction recording scheme should also integrate with digital identity providers and standards, related authentication methods (e.g., Blockstack auth, WebAuthN/FIDO 2.0) and decentralized key management systems (DKMS) (NOTE: Rebooting Web of Trust, D. Reed, DKMS: Decentralized Key Management Systems (Spring 2017)), (NOTE: Decentralized Key Management using Blockchain (see https://www.sbir.gov/sbirsearch/detail/1302463)). We would like to find people to explore these ideas in order to determine how to get this tech to market in the next year. Our current products, VeridiumID and VeridiumAD, provide biometric transaction signing, end-to-end compliance with IEEE 2410-2016 including mobile and server-based secure storage of credentials, with FIDO UAF and with existing products and standards like Active Directory, Citrix, SAML 2.0 and OpenID Connect. Continuing the spirit of our open standards work, we seek to integrate IEEE 2410 BOPS with existing identity standards like DIDs/DDOs via the Verifiable Claims work, DKMS, Open Badges, Blockstack auth, WebAuthN, and other evolving efforts.","title":"Biometric transaction signing on blockchain"},{"location":"rwot5/topics-and-advance-readings/Biometric-transaction-signing-on-blockchain/#biometric-transaction-signing-on-blockchain","text":"","title":"Biometric transaction signing on blockchain"},{"location":"rwot5/topics-and-advance-readings/Biometric-transaction-signing-on-blockchain/#john-callahan-veridium-ip-ltd-virgil-tornoreanu-veridium-ip-ltd","text":"The use of strong customer authentication (SCA) is a critical requirement for collecting explicit consent under the new PSD2 regulations, but equally important are GDPR regulations like the \"right to be forgotten\" that require banks to purge their records of any data associated with an account at a former customer\u2019s request. These regulations are designed to empower individuals with control over their data held by institutions, but technical, legal and policy difficulties are delaying programs needed to implement such policies. Delay means that new financial services providers like AISPs (NOTE: Account Information Service Providers (see https://www.w3.org/Payments/IG/wiki/PSD2#AISP)) and PISPs (NOTE: Payment Initiation Service Providers (see https://www.w3.org/Payments/IG/wiki/PSD2#PISP)) that require consent to access personal information will avoid postpone important innovations to avoid incumbent risks. Strong customer authentication (SCA) means that 2 out of 3 methods for authenticated consent must be employed on high-risk financial transactions (any single payment or transfer over 50 Euro or cumulatively (or 5 consecutive transactions) over 150 Euro (NOTE: Regulatory Technical Standards (RTS) on strong customer authentication and secure communication under PSD2, Chapter 2, Article 11 - Contactless payments at point of sale)). The three methods (NOTE: Regulatory Technical Standards (RTS) on strong customer authentication and secure communication under PSD2, Chapter 2, Article 4 pursuant Article 97(1) of Directive (EU) 2015/2366) are: Knowledge: something only the user knows (such as a password). Ownership: something only the user possesses (such as a chip card). Inherence: something the user personally or physically has (such as a fingerprint). These methods must not be connected and each transaction must be distinct (e.g., unique and non-sequential transaction identifiers). In addition, biometric authentication uniquely offers the possibility for non-repudiation and favored by some institutions for consent because it is difficult to contest legally. In the case of passwords or token, the customer could deny approving the payment due to the theft of password or token. Such cases are not theoretical (NOTE: https://www.finextra.com/blogposting/14068/taking-bold-steps-to-protect-high-value-trading). In addition to covering billions in fraud costs, financial institutions face severe fines under GDPR (NOTE: Organizations can be fined up to 4% of annual global turnover for breaching GDPR or \u20ac20 Million) for failure to \"most serious infringements e.g. not having sufficient customer consent to process data\" and other violations (NOTE: http://www.eugdpr.org/gdpr-faqs.html). The IEEE 2410-2016 BOPS protocol extends IEEE 2410-2015 in order to be PSD2 compliant by incorporating the explicit consent message, beneficiary and amount in any authentication request as an option for authentication, signing and escalated transaction calls. For a given payment transaction, the customer is prompted for biometric authentication on the mobile device with a message indicating the reason, beneficiary and amount. If successful, the transaction is signed with an associated private unlocked by biometric authentication, transmitted to and recorded in server logs that can be audited, archived and purged as per records management regulations. The transaction is \"biometrically signed\" by transitive association of the key associated with the biometric. As an alternative storage option, we propose use of off-chain DID Descriptor Object (NOTE: Data Model and Syntaxes for Decentralized Identifiers (DIDs) - https://w3c-ccg.github.io/did-spec/) (DDOs) with referent decentralized identifier (DIDs) on blockchains for recording of PSD2 compliant transactions. Specifically, a biometrically signed transaction would exist as a high-privacy verifiable claim (NOTE: https://blog.tokenize.io/high-privacy-verifiable-claims-4e556af28fe2). Several issues must be addressed to protect user privacy, allow for regulatory compliance and auditing, records purging via revocation, and recovery in case of loss of credentials. We envision several options for institutions and their customers based on security \"profiles\" that allow customers to balance privacy with convenience including: Storage of encrypted transactions off-chain in user owned media (IPFS, Dropbox, Storj, etc.) that gives a customer sovereign control, but restricts the type of services based on availability Storage of encrypted transactions off-chain in publicly accessible storage at the institutions choice using customer\u2019s public key(s). Subsequent audits could be done only with customer\u2019s authorization. Storage of encrypted transactions off-chain in public or private accessible storage at the institutions choice using customer\u2019s private key(s). Subsequent audits could be done by institution or customer. Our approach to blockchain-based biometrically signed transactions promotes self-sovereign identity by allowing customers to control their record data. This benefits both the customer and financial institutions themselves by reducing the risk of lost, costs of compliance, and improve secure access to data by the new AISP and PISP providers without resort to centralized services (like the ASPSPs). Projects like the Open Banking Project (OBP) could adapt their APIs to accommodate blockchain-based methods using serverless methods for payment processing. Any blockchain-based transaction recording scheme should also integrate with digital identity providers and standards, related authentication methods (e.g., Blockstack auth, WebAuthN/FIDO 2.0) and decentralized key management systems (DKMS) (NOTE: Rebooting Web of Trust, D. Reed, DKMS: Decentralized Key Management Systems (Spring 2017)), (NOTE: Decentralized Key Management using Blockchain (see https://www.sbir.gov/sbirsearch/detail/1302463)). We would like to find people to explore these ideas in order to determine how to get this tech to market in the next year. Our current products, VeridiumID and VeridiumAD, provide biometric transaction signing, end-to-end compliance with IEEE 2410-2016 including mobile and server-based secure storage of credentials, with FIDO UAF and with existing products and standards like Active Directory, Citrix, SAML 2.0 and OpenID Connect. Continuing the spirit of our open standards work, we seek to integrate IEEE 2410 BOPS with existing identity standards like DIDs/DDOs via the Verifiable Claims work, DKMS, Open Badges, Blockstack auth, WebAuthN, and other evolving efforts.","title":"John Callahan (Veridium IP Ltd), Virgil Tornoreanu (Veridium IP Ltd)"},{"location":"rwot5/topics-and-advance-readings/Blockchain-Based-Digital-Signatures--Admissibility-and-Enforceability/","text":"Blockchain Based Digital Signatures: Admissibility and Enforceability The authoritative version of this file can be found at: https://github.com/mitmedialab/MITLegalForum/blob/master/topics-and-advance-readings/Blockchain-Based-Digital-Signatures--Admissibility-and-Enforceability.md Members of Massachusetts Legal Hackers and the law.MIT.edu research team have been pursuing a unique approach to blockchain based cryptographic signature rapid development. To the extent this novel approach may provide a usable and repeatable method for testing the capability of relevant technology to provide predictable legal results, it may be on interest beyond academic, research and innovation hacking communities to a broader set of stakeholders. Rather than starting with the idea of building out some technology because it is cool and innovative or attempting to catch the wave of a specific business opportunity this prototype effort is starting with the assumption that the technology is being used and focused on identifying whether or the extent to which it achieves predictable legal results. Gaps, conflicts and other suboptimal or anomalous legal results identified become inputs for new or different design and development goals in a way that can be traceably enginered to enable success criteria to be tested. Background and Context The following blog post reflects the overall concept and ideas propelling this research project of the Human Dynamics group at the MIT Media Lab: https://law.mit.edu/blog/core-identity-blockchain-project . This project is part of a broader research and development inquiry into the potential definition, scope, application and dynamics of computational law and legla processes. The following Google Doc includes a draft of the Massachusetts Legal Hackers notes on how to conduct a relevant \"witness examination\" to model and evaluate the evidentiary aspects of this project and reflects other ways this novel process of \"legal hacking\" are being planned: https://docs.google.com/document/d/1OT8sxwOqO3F7QCTlqaCAhIjMKwbXNEKB40xluQ1DOco/pub Aims and Goals The approach of this rapid prototype style research and development project is intended to identity testable and engineerable legal requirements or other priorities as part of a standard, agile general design and build process. We expect and hope this exploratory prototyping method will generate constructive and valuable feedback and other inputs to the blockchain digital signature software tool and practical innovation design and build process. The intention is that downstream implications of design-phase decisions will become visible and understood through the visceral experience of role playing litigation and that the crucible of evidentiary and other procedural rules can provide a clear and beneficial source of requirements, constraints and other priority capabilities. These requirements, constraints and design priorities, if correctly and completely derived, can become a buildable set of blueprints needed for refactoring the aglaw and engineering the legal processes needed to successfully execute and rely upon legal signatures in the digital, data-driven and algorithmic age. Specifically, the scope of this research and development project starts with the assumption that the users of the technology have ended up in litigation and works backwards from that scenario. The idea is to test the legal sufficiency of blockchain based digital signatures for purposes of executing standard commercial contracts and also to explore whether the software implementation and manner of usage are likely to produce predictable legal results. By probing the jurisprudential event horizon in this way, we hope to reveal some combination of confirmation of our implementation path and probably also some unexpected hitches and snags along the way. To define and describe a legally valid and verifiable scenario this approach borrows from and bencharks to standard types of \"legal fact patterns\" relied upon for law school tests, for bar exams and for mock trial competitions. In so doing, the all-to-common lawyerly tendency to avoid or deflect agreement upon clear, definite and dispositive criteria with slippery reactions such as \"it depends\" or \"the analysis is fact specific\" can be directly addressed by crafting each scenario to supply precisely those facts upon which predicting the legal outcomes depend. Emerging Questions and Issues Important questions and issues have already started to take form as we run through our draft \"witness examination\" scenario. Among other questions, we have discussed whether: the parties have access to anybody who is qualified and has standing to lay a foundation for admissibility through first hand or expert testimony the cryptographic and other verifiable data is correctly aligned to the specific activity and records necessary to legally attribute the technical signature processes to a given legal party (ie: can we prove which person in fact executed the digital signature?) the purported signatory in fact had the requisite legal intent to sign (ie to authenticate, be bound by, assent to, attest, acknowledge, agree, authorize, etc) the contract or other record in question the right evidence was created and properly maintained sufficiently to prove a valid chain of custody occurred_ ...and many more hitches, snags and snafus! Scope and Context The longer-term approach to honing valid and verifiable legal scenarios would presumably rely upon carefully, thorougly and openly publishing each version of draft scenarios (including technical use cases and legal fact patterns) hand-of-glove with each version of the technnology implementations, workflows and other surrounding processes for expert evaluation and broad feedback. The purpose of each published version of a scenario would be to come ever closer to rendering the law and legal processes underlying technology as something that is also engineerable as part of the design of the overall systems, methods and processes within which the technology exists and operates. Even as banking, transportation, energy and other sectors have become or are becoming engineerable as digital systems, a driver propelling this project is to identify one or more verifiable and reusable approaches enabling legal dimensions of technology to be designed, tested, deplployed and upgraded as part of the business, public sector and social systems that exist increasingly within digital, networked environments. It is postulated that currently external and subjective yet important and knowable legal requirements and constraints can be explicitly incorporated into a broader, standard holistic technnology design, testing and deployment of technology and the processes or systems surrounding the technology. One of the key approaches of this project is to develop and use a method for composing testable scenarios in a way that relects and tracks exactly the level of abstraction and precision needed for a legal fact pattern to enable expected and \"correct\" legal analysis and legal conclusions. It is assumed that a sound starting point for establishing the needed level of abstraction and precision can be found by benchmarking the same legal fact patterns used to test the capability of a person to graduate from an accredited law school, to win a litigation competition and to earn a license for the practice of law, namely: law school tests, bar exams and mock trial fact patterns. This is literally a process of \"evidence-based\" requirements gathering. Interested to follow along from afar? Want to grab an oar and help us row? This activity is being pursued as a participatory collaborative event series that is free and open to the public. To participate and contribute, start by joining Massachusetts Legal Hackers and RSVP to the next meetup event related to this project right here: https://www.meetup.com/Massachusetts-Legal-Hackers and get updates/announcements on the broader research from MIT by signing up to our email list right here: law.MIT.edu/contact . The above file is offered as a topic or advance read for Rebooting the Web of Trust in Boston, October 2017.","title":"Blockchain Based Digital Signatures: Admissibility and Enforceability"},{"location":"rwot5/topics-and-advance-readings/Blockchain-Based-Digital-Signatures--Admissibility-and-Enforceability/#blockchain-based-digital-signatures-admissibility-and-enforceability","text":"The authoritative version of this file can be found at: https://github.com/mitmedialab/MITLegalForum/blob/master/topics-and-advance-readings/Blockchain-Based-Digital-Signatures--Admissibility-and-Enforceability.md Members of Massachusetts Legal Hackers and the law.MIT.edu research team have been pursuing a unique approach to blockchain based cryptographic signature rapid development. To the extent this novel approach may provide a usable and repeatable method for testing the capability of relevant technology to provide predictable legal results, it may be on interest beyond academic, research and innovation hacking communities to a broader set of stakeholders. Rather than starting with the idea of building out some technology because it is cool and innovative or attempting to catch the wave of a specific business opportunity this prototype effort is starting with the assumption that the technology is being used and focused on identifying whether or the extent to which it achieves predictable legal results. Gaps, conflicts and other suboptimal or anomalous legal results identified become inputs for new or different design and development goals in a way that can be traceably enginered to enable success criteria to be tested.","title":"Blockchain Based Digital Signatures: Admissibility and Enforceability"},{"location":"rwot5/topics-and-advance-readings/Blockchain-Based-Digital-Signatures--Admissibility-and-Enforceability/#background-and-context","text":"The following blog post reflects the overall concept and ideas propelling this research project of the Human Dynamics group at the MIT Media Lab: https://law.mit.edu/blog/core-identity-blockchain-project . This project is part of a broader research and development inquiry into the potential definition, scope, application and dynamics of computational law and legla processes. The following Google Doc includes a draft of the Massachusetts Legal Hackers notes on how to conduct a relevant \"witness examination\" to model and evaluate the evidentiary aspects of this project and reflects other ways this novel process of \"legal hacking\" are being planned: https://docs.google.com/document/d/1OT8sxwOqO3F7QCTlqaCAhIjMKwbXNEKB40xluQ1DOco/pub","title":"Background and Context"},{"location":"rwot5/topics-and-advance-readings/Blockchain-Based-Digital-Signatures--Admissibility-and-Enforceability/#aims-and-goals","text":"The approach of this rapid prototype style research and development project is intended to identity testable and engineerable legal requirements or other priorities as part of a standard, agile general design and build process. We expect and hope this exploratory prototyping method will generate constructive and valuable feedback and other inputs to the blockchain digital signature software tool and practical innovation design and build process. The intention is that downstream implications of design-phase decisions will become visible and understood through the visceral experience of role playing litigation and that the crucible of evidentiary and other procedural rules can provide a clear and beneficial source of requirements, constraints and other priority capabilities. These requirements, constraints and design priorities, if correctly and completely derived, can become a buildable set of blueprints needed for refactoring the aglaw and engineering the legal processes needed to successfully execute and rely upon legal signatures in the digital, data-driven and algorithmic age. Specifically, the scope of this research and development project starts with the assumption that the users of the technology have ended up in litigation and works backwards from that scenario. The idea is to test the legal sufficiency of blockchain based digital signatures for purposes of executing standard commercial contracts and also to explore whether the software implementation and manner of usage are likely to produce predictable legal results. By probing the jurisprudential event horizon in this way, we hope to reveal some combination of confirmation of our implementation path and probably also some unexpected hitches and snags along the way. To define and describe a legally valid and verifiable scenario this approach borrows from and bencharks to standard types of \"legal fact patterns\" relied upon for law school tests, for bar exams and for mock trial competitions. In so doing, the all-to-common lawyerly tendency to avoid or deflect agreement upon clear, definite and dispositive criteria with slippery reactions such as \"it depends\" or \"the analysis is fact specific\" can be directly addressed by crafting each scenario to supply precisely those facts upon which predicting the legal outcomes depend.","title":"Aims and Goals"},{"location":"rwot5/topics-and-advance-readings/Blockchain-Based-Digital-Signatures--Admissibility-and-Enforceability/#emerging-questions-and-issues","text":"Important questions and issues have already started to take form as we run through our draft \"witness examination\" scenario. Among other questions, we have discussed whether: the parties have access to anybody who is qualified and has standing to lay a foundation for admissibility through first hand or expert testimony the cryptographic and other verifiable data is correctly aligned to the specific activity and records necessary to legally attribute the technical signature processes to a given legal party (ie: can we prove which person in fact executed the digital signature?) the purported signatory in fact had the requisite legal intent to sign (ie to authenticate, be bound by, assent to, attest, acknowledge, agree, authorize, etc) the contract or other record in question the right evidence was created and properly maintained sufficiently to prove a valid chain of custody occurred_ ...and many more hitches, snags and snafus!","title":"Emerging Questions and Issues"},{"location":"rwot5/topics-and-advance-readings/Blockchain-Based-Digital-Signatures--Admissibility-and-Enforceability/#scope-and-context","text":"The longer-term approach to honing valid and verifiable legal scenarios would presumably rely upon carefully, thorougly and openly publishing each version of draft scenarios (including technical use cases and legal fact patterns) hand-of-glove with each version of the technnology implementations, workflows and other surrounding processes for expert evaluation and broad feedback. The purpose of each published version of a scenario would be to come ever closer to rendering the law and legal processes underlying technology as something that is also engineerable as part of the design of the overall systems, methods and processes within which the technology exists and operates. Even as banking, transportation, energy and other sectors have become or are becoming engineerable as digital systems, a driver propelling this project is to identify one or more verifiable and reusable approaches enabling legal dimensions of technology to be designed, tested, deplployed and upgraded as part of the business, public sector and social systems that exist increasingly within digital, networked environments. It is postulated that currently external and subjective yet important and knowable legal requirements and constraints can be explicitly incorporated into a broader, standard holistic technnology design, testing and deployment of technology and the processes or systems surrounding the technology. One of the key approaches of this project is to develop and use a method for composing testable scenarios in a way that relects and tracks exactly the level of abstraction and precision needed for a legal fact pattern to enable expected and \"correct\" legal analysis and legal conclusions. It is assumed that a sound starting point for establishing the needed level of abstraction and precision can be found by benchmarking the same legal fact patterns used to test the capability of a person to graduate from an accredited law school, to win a litigation competition and to earn a license for the practice of law, namely: law school tests, bar exams and mock trial fact patterns.","title":"Scope and Context"},{"location":"rwot5/topics-and-advance-readings/Blockchain-Based-Digital-Signatures--Admissibility-and-Enforceability/#this-is-literally-a-process-of-evidence-based-requirements-gathering","text":"Interested to follow along from afar? Want to grab an oar and help us row? This activity is being pursued as a participatory collaborative event series that is free and open to the public. To participate and contribute, start by joining Massachusetts Legal Hackers and RSVP to the next meetup event related to this project right here: https://www.meetup.com/Massachusetts-Legal-Hackers and get updates/announcements on the broader research from MIT by signing up to our email list right here: law.MIT.edu/contact . The above file is offered as a topic or advance read for Rebooting the Web of Trust in Boston, October 2017.","title":"This is literally a process of \"evidence-based\" requirements gathering."},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/","text":"Hub Asset Access Control System - Intent Back to Explainer Intent This document is a draft proposal intended to be a starting point for the specification of a robust, forward-compatible access control system to serve as a reference implementation of the DIF Hub protocol. We intend to work with other participants to assess the direction, security, and permissioning scheme of the identity capabilities specification drafts to generate a code-ready draft that can be implemented as an easily portable and reusable access control module for projects employing DID-based authentication and/or the DIF Hub protocol. Motivation Per the hub specification, there is a need for a flexible, expressive, and simple permissioning system to represent authority over files stored by a hub. The goal of this proposal is to present an access control system which: - Allows permissions to be delegated to DIDs - Supports multiple permission levels for differential access - Allows independent permissioning of core endpoints and individual extensions Permission Levels Hubs support basic data manipulation operations which follow RESTful design principles as well as extensions which can expose a REST or RPC-style interface. To handle both styles, this proposal recommends the CRUDX permission set: - C: Create - R: Read - U: Update - D: Delete - X: Execute The goal is to provide intuitive permission levels for common operations ( CRUD for RESTful interfaces) as well as flexible permissioning for complex or aberrant operations ( X for extensions that manage their own access controls or expose an RPC interface, e.g. via an action body parameter). CRUDX Specification The CRUDX specification string follows the UNIX file system model: each specification is a bit-array of length 5 where each element of the array represents the corresponding permission level and can be set to 1 (allowed), 0 (not allowed). This can be displayed as a 5-character string where each character is either the first letter of the permission level's name (allowed) or a hyphen (not allowed); hyphens can optionally be omitted (e.g. C--DX == CDX . Since the representation is an ordered list, a permission can also be specified as an unsigned, 5-bit integer (e.g. C--DX == 25 ). Examples Description String Integer Full Permissions CRUDX 31 Null Permissions ----- 0 Read Only -R--- 2 Read & Execute -R--X 18 NOTE: this proposal could be confusing because the integer values of each permission level do NOT match the UNIX levels; they are not even ordered from least to most dangerous. DID (Permission Recipients/Grantees) Following the higher-level architecture of Hubs, the permission system uses DIDs as the primary mechanism for identifying recipients. It might also support raw cryptographic keys, but a better idea would be to allow embedded DDOs which can represent raw keys for whatever cryptosystems are supported by the DID spec. Below is a json array of valid identifiers: TODO: I'm not sure if there's a simple and valid DDO that just exposes a key without addl rigmarole [ \"did:sov:dan.id\", { \"@context\": \"https://example.org/did/v1\", \"owner\": [ { \"type\": [\"CryptographicKey\", \"EdDsaPublicKey\"], \"curve\": \"ed25519\", \"publicKeyBase64\": \"lji9qTtkCydxtez/bt1zdLxVMMbz4SzWvlqgOBmURoM=\" } ] } ] PermissionSpecification Permissions are represented via a series of PermissionSpecification objects. A root PermissionSpecification is stored at: /.well-known/identity/:id/permissions . See Inheritance for more on All PermissionSpecifications adhere to the following schema: Field Type Description Required Examples did DID | DDO | glob This field specifies the entity who is granted the permissions specified by this PermissionSpecification object TRUE \"did:sov:dan.id\" {\"@context\": \"https://example.org/did/v1\", \"owner\": [{ \"type\": [\"CryptographicKey\", \"EdDsaPublicKey\"], \"curve\": \"ed25519\", \"publicKeyBase64\": \"lji9qTtkCydxtez/bt1zdLxVMMbz4SzWvlqgOBmURoM=\"}]} path DID-reference | glob The path field contains an absolute or relative DID reference that identifies a path, data asset, or endpoint. Pattern matching is supported per the Patterns section TRUE * , ./stores , did:sov:dan.id/collections/photos object_filters object The object_filters field allows for the selection of data assets using the metadata in the control object which wraps each path and asset. This is (currently) a flat key-value mapping with no pattern-matching support. FALSE To allow access to objects created by a given identity: { \"author\": \"did:btcr:123\" } argument_filters object The argument_filters field allows for selection of data assets using the properties of the incoming request which initiated the permission validation. FALSE To allow access to execution of an extension's invokeRPC method: { \"action\": \"invokeRPC\" } allow CRUDX specification The permissions granted to the identities specified in identifiers TRUE CR--X , CRUDX , 19 (CR--X) , 31 (CRUDX) deny CRUDX specification The permissions denied to the identities specified in identifiers . NOTE: this is field is not yet part of the specification and is being reserved for possible future inclusion. FALSE CR--X , CRUDX , 19 (CR--X) , 31 (CRUDX) ext object The ext field is reserved for additional fields that are use to configure specific routes or extensions. FALSE The /stores endpoint supports min_size and max_size to configure how much storage an identity is permitted to keep in its store: { \"min_size\": 1, \"max_size\": 100 } Example GET /.identity/:id_owner_did/permissions?path=collections [{ \"did\": \"did:btcr:123\", \"path\": \"hl7.org:fhir/*\", \"object_filters\": { \"author\": \"did:btcr:123\" }, \"argument_filters\": { \"action\": \"create\" }, \"allow\": \"CR---\", \"ext\": { \"callbacks\": [ { \"event\": \"created\", \"uri\": URI, \"headers\": { \"X-MY-DOPE-HEADER\": 44 } } ] } }, ...] Paths The path is the main unit of granularity for permissions. Each PermissionSpecification has a path field which supports glob pattern matching, and an optional parent path query parameter can be specified in the the get request to the /permissions endpoint. Paths in the body of a PermissionSpecification may be either full DID-references or relative DID-paths . Relative paths are relative to the location of the PermissionSpecification in which they are used, e.g. GET /.identity/:id_owner_did/permissions [{ \"did\": \"did:btcr:123\", \"path\": \"collections/hl7.org:fhir/*\", \"allow\": \"CRU--\" }] Paths in the query parameter are always relative to the root of the identity owner's DID are being requested, e.g. GET /.identity/:id_owner_did/permissions?path=stores&did=btcr:123 [{ \"did\": \"did:btcr:123\", \"path\": \"did:btcr:123/*\", \"allow\": \"CRUD-\" }] If no path is specified in the query string, the implied path is also the root of the identity owner's DID. Patterns There is a clear need to support specifications that apply to a set of resources, rather than a single path (directory) or resource (data asset). To support this behavior we have to choose a solution that balances expressiveness, comprehensibility, and safety. The naive solution is to employ regular expressions. While regex are extremely powerful, they are also error prone and difficult to audit, so instead we elect to use the less powerful solution, but safer solution: UNIX-like glob matching, which provides a subset of regex functionality, particularly ? (single-character wildcard) and * (many-character wildcard). Patterns (currently) may only be used in the path field of the PermissionSpecification object. Inheritance / Field-level Permissions The deny field of the PermissionSpecification is reserved for future use to implement field-level permissioning, i.e. allowing access to a data asset, but resistrictng access to specific fields. This concept is only sensible in the mode where the hub has access to the data asset itself (where it is not encrypted with a client-side encryption key). This example will grant did:btrc:123 access to read the identity owner's profile document, with the github-handle field omitted : [{ \"did\": \"did:btcr:123\", \"path\": \"profile\", \"allow\": \"-R---\" }, { \"did\": \"did:btcr:123\", \"path\": \"profile#github-handle\", \"deny\": \"-R---\" }] Content-Type Selection (Tags/Querying) It is useful to be able to select resources based on metadata apart from their path. Examples include selecting all resources: - created by a certain identity - updated at a certain time - adhering to a certain schema, or - tagged with a certain label To achieve this, all data assets are wrapped with a control object that contains a finite set ( TODO: define the set ). These fields are exposed via the object_filters field in the PermissionSpecification which does not support any pattern matching, only direct comparison. This example will grant did:btcr:123 access to only the assets in the hl7.org:fhir collection that were created by did:btcr:123 : [{ \"did\": \"did:btcr:123\", \"path\": \"hl7.org:fhir/*\", \"object_filters\": { \"author\": \"did:btcr:123\" } }] Callbacks In order for applications to operate with hubs reactively, there is a need to expose a mechanism for requesting notifications from the hub when data assets are created/updated/executed/etc. Since this mechanism would essentially enable autonomous operation of the hub, it must be explicitly permissioned. We leverage the ext block to add support for callback permissioning. CallbackSpecification Callback permissions are represented via a series of CallbackSpecification objects, which adhere to the following schema: WIP! Field Type Description Required Examples events array\\ The event field specifies when a callback should be launched. (Should this just be a normal CRUDX value?) TRUE [\"created\", \"read\", \"updated\", \"deleted\", \"executed\"] uri URI The uri field specifies who should receive the callback notification. This field and the did field are mutually exclusive. FALSE https://someuri.co/callback_endpoint did DID or DDO The did field specifies who should received. A hub for the receiving DID will be resolved and that will receive the notification. This field and the URI field are mutually exclusive. FALSE did:btcr:123 headers object The headers field contains any custom headers the receiver wants to be included on outgoing callback notification requests. FALSE { \"Content-Type\": \"application/Person.json\" } [{ \"did\": \"did:btcr:123\", \"path\": \"hl7.org:fhir/*\", \"ext\": { \"callbacks\": [{ \"events\": [\"created\"], \"uri\": URI, \"headers\": {\"X-MY-DOPE-HEADER\": 44} }] } }] TODO: define the callback request interface.","title":"Hub Asset Access Control System - Intent"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#hub-asset-access-control-system-intent","text":"Back to Explainer","title":"Hub Asset Access Control System - Intent"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#intent","text":"This document is a draft proposal intended to be a starting point for the specification of a robust, forward-compatible access control system to serve as a reference implementation of the DIF Hub protocol. We intend to work with other participants to assess the direction, security, and permissioning scheme of the identity capabilities specification drafts to generate a code-ready draft that can be implemented as an easily portable and reusable access control module for projects employing DID-based authentication and/or the DIF Hub protocol.","title":"Intent"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#motivation","text":"Per the hub specification, there is a need for a flexible, expressive, and simple permissioning system to represent authority over files stored by a hub. The goal of this proposal is to present an access control system which: - Allows permissions to be delegated to DIDs - Supports multiple permission levels for differential access - Allows independent permissioning of core endpoints and individual extensions","title":"Motivation"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#permission-levels","text":"Hubs support basic data manipulation operations which follow RESTful design principles as well as extensions which can expose a REST or RPC-style interface. To handle both styles, this proposal recommends the CRUDX permission set: - C: Create - R: Read - U: Update - D: Delete - X: Execute The goal is to provide intuitive permission levels for common operations ( CRUD for RESTful interfaces) as well as flexible permissioning for complex or aberrant operations ( X for extensions that manage their own access controls or expose an RPC interface, e.g. via an action body parameter).","title":"Permission Levels"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#crudx-specification","text":"The CRUDX specification string follows the UNIX file system model: each specification is a bit-array of length 5 where each element of the array represents the corresponding permission level and can be set to 1 (allowed), 0 (not allowed). This can be displayed as a 5-character string where each character is either the first letter of the permission level's name (allowed) or a hyphen (not allowed); hyphens can optionally be omitted (e.g. C--DX == CDX . Since the representation is an ordered list, a permission can also be specified as an unsigned, 5-bit integer (e.g. C--DX == 25 ).","title":"CRUDX Specification"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#examples","text":"Description String Integer Full Permissions CRUDX 31 Null Permissions ----- 0 Read Only -R--- 2 Read & Execute -R--X 18 NOTE: this proposal could be confusing because the integer values of each permission level do NOT match the UNIX levels; they are not even ordered from least to most dangerous.","title":"Examples"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#did-permission-recipientsgrantees","text":"Following the higher-level architecture of Hubs, the permission system uses DIDs as the primary mechanism for identifying recipients. It might also support raw cryptographic keys, but a better idea would be to allow embedded DDOs which can represent raw keys for whatever cryptosystems are supported by the DID spec. Below is a json array of valid identifiers: TODO: I'm not sure if there's a simple and valid DDO that just exposes a key without addl rigmarole [ \"did:sov:dan.id\", { \"@context\": \"https://example.org/did/v1\", \"owner\": [ { \"type\": [\"CryptographicKey\", \"EdDsaPublicKey\"], \"curve\": \"ed25519\", \"publicKeyBase64\": \"lji9qTtkCydxtez/bt1zdLxVMMbz4SzWvlqgOBmURoM=\" } ] } ]","title":"DID (Permission Recipients/Grantees)"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#permissionspecification","text":"Permissions are represented via a series of PermissionSpecification objects. A root PermissionSpecification is stored at: /.well-known/identity/:id/permissions . See Inheritance for more on All PermissionSpecifications adhere to the following schema: Field Type Description Required Examples did DID | DDO | glob This field specifies the entity who is granted the permissions specified by this PermissionSpecification object TRUE \"did:sov:dan.id\" {\"@context\": \"https://example.org/did/v1\", \"owner\": [{ \"type\": [\"CryptographicKey\", \"EdDsaPublicKey\"], \"curve\": \"ed25519\", \"publicKeyBase64\": \"lji9qTtkCydxtez/bt1zdLxVMMbz4SzWvlqgOBmURoM=\"}]} path DID-reference | glob The path field contains an absolute or relative DID reference that identifies a path, data asset, or endpoint. Pattern matching is supported per the Patterns section TRUE * , ./stores , did:sov:dan.id/collections/photos object_filters object The object_filters field allows for the selection of data assets using the metadata in the control object which wraps each path and asset. This is (currently) a flat key-value mapping with no pattern-matching support. FALSE To allow access to objects created by a given identity: { \"author\": \"did:btcr:123\" } argument_filters object The argument_filters field allows for selection of data assets using the properties of the incoming request which initiated the permission validation. FALSE To allow access to execution of an extension's invokeRPC method: { \"action\": \"invokeRPC\" } allow CRUDX specification The permissions granted to the identities specified in identifiers TRUE CR--X , CRUDX , 19 (CR--X) , 31 (CRUDX) deny CRUDX specification The permissions denied to the identities specified in identifiers . NOTE: this is field is not yet part of the specification and is being reserved for possible future inclusion. FALSE CR--X , CRUDX , 19 (CR--X) , 31 (CRUDX) ext object The ext field is reserved for additional fields that are use to configure specific routes or extensions. FALSE The /stores endpoint supports min_size and max_size to configure how much storage an identity is permitted to keep in its store: { \"min_size\": 1, \"max_size\": 100 }","title":"PermissionSpecification"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#example","text":"GET /.identity/:id_owner_did/permissions?path=collections [{ \"did\": \"did:btcr:123\", \"path\": \"hl7.org:fhir/*\", \"object_filters\": { \"author\": \"did:btcr:123\" }, \"argument_filters\": { \"action\": \"create\" }, \"allow\": \"CR---\", \"ext\": { \"callbacks\": [ { \"event\": \"created\", \"uri\": URI, \"headers\": { \"X-MY-DOPE-HEADER\": 44 } } ] } }, ...]","title":"Example"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#paths","text":"The path is the main unit of granularity for permissions. Each PermissionSpecification has a path field which supports glob pattern matching, and an optional parent path query parameter can be specified in the the get request to the /permissions endpoint. Paths in the body of a PermissionSpecification may be either full DID-references or relative DID-paths . Relative paths are relative to the location of the PermissionSpecification in which they are used, e.g. GET /.identity/:id_owner_did/permissions [{ \"did\": \"did:btcr:123\", \"path\": \"collections/hl7.org:fhir/*\", \"allow\": \"CRU--\" }] Paths in the query parameter are always relative to the root of the identity owner's DID are being requested, e.g. GET /.identity/:id_owner_did/permissions?path=stores&did=btcr:123 [{ \"did\": \"did:btcr:123\", \"path\": \"did:btcr:123/*\", \"allow\": \"CRUD-\" }] If no path is specified in the query string, the implied path is also the root of the identity owner's DID.","title":"Paths"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#patterns","text":"There is a clear need to support specifications that apply to a set of resources, rather than a single path (directory) or resource (data asset). To support this behavior we have to choose a solution that balances expressiveness, comprehensibility, and safety. The naive solution is to employ regular expressions. While regex are extremely powerful, they are also error prone and difficult to audit, so instead we elect to use the less powerful solution, but safer solution: UNIX-like glob matching, which provides a subset of regex functionality, particularly ? (single-character wildcard) and * (many-character wildcard). Patterns (currently) may only be used in the path field of the PermissionSpecification object.","title":"Patterns"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#inheritance-field-level-permissions","text":"The deny field of the PermissionSpecification is reserved for future use to implement field-level permissioning, i.e. allowing access to a data asset, but resistrictng access to specific fields. This concept is only sensible in the mode where the hub has access to the data asset itself (where it is not encrypted with a client-side encryption key). This example will grant did:btrc:123 access to read the identity owner's profile document, with the github-handle field omitted : [{ \"did\": \"did:btcr:123\", \"path\": \"profile\", \"allow\": \"-R---\" }, { \"did\": \"did:btcr:123\", \"path\": \"profile#github-handle\", \"deny\": \"-R---\" }]","title":"Inheritance / Field-level Permissions"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#content-type-selection-tagsquerying","text":"It is useful to be able to select resources based on metadata apart from their path. Examples include selecting all resources: - created by a certain identity - updated at a certain time - adhering to a certain schema, or - tagged with a certain label To achieve this, all data assets are wrapped with a control object that contains a finite set ( TODO: define the set ). These fields are exposed via the object_filters field in the PermissionSpecification which does not support any pattern matching, only direct comparison. This example will grant did:btcr:123 access to only the assets in the hl7.org:fhir collection that were created by did:btcr:123 : [{ \"did\": \"did:btcr:123\", \"path\": \"hl7.org:fhir/*\", \"object_filters\": { \"author\": \"did:btcr:123\" } }]","title":"Content-Type Selection (Tags/Querying)"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#callbacks","text":"In order for applications to operate with hubs reactively, there is a need to expose a mechanism for requesting notifications from the hub when data assets are created/updated/executed/etc. Since this mechanism would essentially enable autonomous operation of the hub, it must be explicitly permissioned. We leverage the ext block to add support for callback permissioning.","title":"Callbacks"},{"location":"rwot5/topics-and-advance-readings/DIF-Hub-Permissions-RWOT-Intent/#callbackspecification","text":"Callback permissions are represented via a series of CallbackSpecification objects, which adhere to the following schema: WIP! Field Type Description Required Examples events array\\ The event field specifies when a callback should be launched. (Should this just be a normal CRUDX value?) TRUE [\"created\", \"read\", \"updated\", \"deleted\", \"executed\"] uri URI The uri field specifies who should receive the callback notification. This field and the did field are mutually exclusive. FALSE https://someuri.co/callback_endpoint did DID or DDO The did field specifies who should received. A hub for the receiving DID will be resolved and that will receive the notification. This field and the URI field are mutually exclusive. FALSE did:btcr:123 headers object The headers field contains any custom headers the receiver wants to be included on outgoing callback notification requests. FALSE { \"Content-Type\": \"application/Person.json\" } [{ \"did\": \"did:btcr:123\", \"path\": \"hl7.org:fhir/*\", \"ext\": { \"callbacks\": [{ \"events\": [\"created\"], \"uri\": URI, \"headers\": {\"X-MY-DOPE-HEADER\": 44} }] } }] TODO: define the callback request interface.","title":"CallbackSpecification"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/","text":"Engineering Privacy for Verified Credentials: In Which We Describe Data Minimization, Selective Disclosure and Progressive Trust ============================== Contributors: Lionel Wolberger, Platin.io Brent Zundel, Evernym/Sovrin Zachary Larson, Independent Irene Hernandez, Independent Katryna Dow, Meeco DEPRECATED THIS VERSION IS DEPRECATED: THE PAPER IS PROMOTED TO A CREDENTIALS COMMUNITY GROUP REPORT SEE https://github.com/w3c-ccg/data-minimization Introduction We often share information on the World Wide Web, though some of it is private. The W3C Credentials Community Group focuses on how privacy can be enhanced when attributes are shared electronically. In the course of our work, we have identified three related but distinct privacy enhancing strategies: \"data minimization,\" \"selective disclosure,\" and \"progressive trust.\" These enhancements are enabled with cryptography. The goal of this paper is to enable decision makers, particularly non-technical ones, to gain a nuanced grasp of these enhancements along with some idea of how their enablers work. We describe them below in plain English, but with some rigor. This knowledge will enable readers of this paper to be better able to know when they need privacy enhancements, to select the type of enhancement needed, to assess techniques that enable those enhancements, and to adopt the correct enhancement for the correct use case. Three Examples Three examples of how people would like their privacy preserved in the process of sharing credentials help to illuminate these three techniques. Diego attempts to use an online service and is asked to share his location in order to prove his geolocation. Diego hesitates, since the service doesn't need his location everyday, everywhere. He knows that the service may share this information with other parties without meaningful consent on his part. Thoughts pass through his mind: What location data does the service actually need? What will it read in future? Is there a way for him to share his location just this once, or to only share an approximate location? Selena hands her driver's license to a bouncer to prove she is of drinking age. As he looks it over, she sees him inspecting her date of birth and home address. He only needs to know that she is over 21. Is there a way to disclose that she is indeed old enough without revealing her actual age, along with her home address and city of residence as well? Proctor , negotiating with a real estate agent to purchase a home, reveals a letter from his bank stating his credit limit. He wanted to reveal its approximate amount only, but the agent insisted on verifying that the letter was authentic. Proctor feels the agent now has the upper hand in the negotiation, as the letter reveals more than just its authenticity. Could he have revealed only an approximate amount and reveal more details as the negotiations progress? Each story features information that is verifiable: a home address, age, or credit limit. We call such information a credential, and a detail of a credential we call an attribute. We have three strategies for enhancing the privacy of digitally shared credential attributes, and each story highlights one. Diego's story highlights the need for \"data minimization,\" Selena's for \"selective disclosure,\" and Proctor's for \"progressive trust.\" Let's examine each one in detail before discussing enablers. Privacy Enhancements We propose the following three privacy enhancements. (Sources used to curate these definitions are listed in Appendix A.) Data Minimization Data minimization is the act of limiting the amount of shared data strictly to the minimum necessary in order to successfully accomplish a task or goal. There are three types of minimization: * Content minimization \u2013 the amount of data should be strictly the minimum necessary. * Temporal minimization \u2013 the data should be stored by the receiver strictly for the minimum amount of time necessary to execute the task. * Scope minimization \u2013 the data should only be used for the strict purpose of the active task. Data minimization is enacted primarily by policy decisions made by stakeholders in the credentials ecosystem: * Credential issuers ensure that credentials may be presented in such a way as to enable data minimization. This may require issuing multiple, related, granular sub-credentials. * Credential inspectors establish in advance policies regarding the data they will examine: * what is the minimum data necessary to accomplish the task or goal? * what is the minimum time the data can be stored to execute the task? * what processes ensure that the data is applied only to the task at hand and does not, by a process of scope creep, become applied to other tasks or goals? Data minimization policies impact selective disclosure, the next privacy enhancement. Selective disclosure Selective disclosure is the ability of an individual to granularly decide what information to share. Stakeholders in the credentials ecosystem enable selective disclosure capabilities in the following ways: * Credential issuers format the credential and its attributes in such a way as to enable selective disclosure. As with the strategy of data minimization, they may issue multiple, related, granular sub-credentials. Each attribute and the overall credential may be formatted to support cryptography, a capability described in more detail below. * Credential inspectors ensure the request is framed in such a way as to enable selective disclosure, using the cryptographic tools required. Once data minimization policies and selective disclosure are in place, the third and last enhancement can be applied. Progressive Trust Progressive trust is the ability of an individual to gradually increase the amount of relevant data revealed as trust is built or value generated. To enable progressive trust capabilities, stakeholders in the credentials ecosystem act in the following ways: * Issuers format the credential(s) in such a way as to enable progressive trust. This may require issuing multiple, related, atomic sub-credentials. It also may require formatting the credential to support mathematical queries and cryptographic proofs. Finally, the issuer's data model may express how the various sub-credentials are related in a scenario involving progressive trust. * Inspectors ensure that requests are framed in such a way as to enable progressive trust. They structure the communication in order to to gradually escalate credential requests in order to enable a subject to progressively trust the inspector more and more, revealing the minimum data necessary to accomplish each step of the task or goal and revealing more and more as the mutual communication progresses. Crypto Enablers Implementing privacy enhancements depends on organizational decisions. Determination of the data needed, with an eye towards data minimization, along with a clear model of how data is used over the lifecycle of engagement, goes a long way towards enabling progressive trust. However, policies are not enough. When enhancing privacy online, some data parts must be revealed while others remain concealed. Concealment is achieved mostly by the art of cryptography, from the greek word \"kryptos,\" meaning hidden, like in a crypt. Crypto (a short word we will use for cryptography) enables us to achieve our goal by means of three primary enablers: having a secret, having a difficult mathematical task, and having zero-knowledge enablers. The children's \"Where's Waldo?\" illustrated book series helps us to understand these three enablers. In these books a distinctively dressed man appears only once on each page, wearing a striped hat. Readers are asked to scour the page and locate him. We can understand the three enablers by examining Where's Waldo one step at a time. A Secret : For the new reader, Waldo's location is a secret. The illustrator knows it, and the reader doesn't. The reader is encouraged to search the page and find Waldo, but that is a difficult task. Some readers give up and ask someone who has already found Waldo to show them his location. In essence, they are asking another reader to reveal the secret. Once found, a reader could keep the information secret by circling Waldo in red and storing the book in a safe. This amounts to storing the secret for future use. Secrets are essential to crypto. They are usually called keys, and they must be managed carefully. A Difficult Task : Waldo is difficult to find on the page. The reader has to search everywhere and mistakenly identify many Waldo look-alike characters before reaching a satisfactory conclusion and finding him. Yet when he is finally discovered, or someone points Waldo out, it's easy to see where he is. That's why it's a fun task. This difference between the difficulty of conducting the task and the ease of verifying the task lies at the heart of cryptographic enablers. A Zero Knowledge Enabler : Can you prove you found Waldo without revealing the secret of his actual location on the page? There is a simple way to do so. Take a rectangular piece of white cardboard that is much larger than the book. Cut a hole exactly fitting Waldo to reveal his silhouette only, nothing else. You can now show Waldo to anyone, peeking out of the cardboard. Yet the cardboard is wide and opaque, hiding the book thoroughly, so a verifier has no idea where Waldo is on the page. The puzzle was solved and someone verified the achievement, without revealing any knowledge of how to solve the puzzle. The secret is still safe, the task still just as difficult as before. Where's Waldo books are drawings, while crypto is built from mathematical equations, basically puzzles based on numbers. We provide the interested reader with a layman's overview in Appendix B. Three Solutions We now return to our opening examples, apply the privacy preserving strategies and enablers described, and describe the improved outcomes. The online service that Diego uses does an internal policy review and realizes (a) it only needs a location when a user signs up for an account, and (b) it does not need an exact address, only the county district. It changes its interface to request a Verifiable Credential for Diego's location. Diego's system creates this credential for him, which can be inspected to reveal the county district. The crypto to enable this would be similar to that described in Appendix C. With this data minimization, the online service has less risk of violating data protection rules, is less a target for hacking, and has lower overall costs, while at the same time preserving Diego's privacy. The bar seeking to verify Selena 's age uses selective disclosure as built into the Verifiable Claims system. Selena will no longer share her date of birth. Instead, Selena creates a secret that we harness to craft a crypto-formatted credential. This crypto makes it easy to verify her age, but difficult to determine her exact date of birth. The bouncer's system can perform a zero-knowledge proof to determine the credential is valid and that Selena is older than twenty-one, without revealing her birthday or her secret. The bouncer sees she is over twenty-one without seeing her date of birth, residence address, or any other unnecessary information. In Appendix C we show the process step-by-step. The real estate agency working with Proctor implements a data model specifying what is required at each step of the real estate negotiation. The first step requires only proof of being an account holder in good standing at a known bank, so Proctor does not have to reveal the detailed letter at this point. As their negotiation continues, Proctor reveals more and more information as required. Some steps of the process may share Verifiable Claims encoded with crypto. Summary The World Wide Web accelerates the sharing of credentials and other digital interactions, and many regulations have been passed and strategies proposed to protect privacy, some of which require cryptography. To align terminology, the World Wide Web Credentials Community Group has found three related but distinct privacy enhancing strategies that create a useful rubric for discussing the challenges and arriving at solutions. We share the examples of Diego, Selena, and Proctor and propose \"data minimization,\" \"selective disclosure,\" and \"progressive trust,\" with accompanying crypto protocols as useful semantics for accelerating the adoption of digital interaction while protecting privacy. Appendix A: Definition Sources This section contains definitions we curated, based on research and oral interviews, to create the definitions of data minimization, selective disclosure and progressive trust. Data Minimization Definitions of data minimization that we considered in the formation of our definition above. GDPR Rec.39; Art.5(1)(c) definition: \u201cThe personal data should be adequate, relevant and limited to what is necessary for the purposes for which they are processed. This requires, in particular, ensuring that the period for which the personal data are stored is limited to a strict minimum. Personal data should be processed only if the purpose of the processing could not reasonably be fulfilled by other means.\u201d Reducing your overall footprint of data outside of your control. Can be accomplished by using selective disclosure. Adequate, relevant and non-excessive. Reducing the amount of data you are sending in a payload to only the one ... needed. That prevents leakage of confidential information. Providing people with the information they need without revealing non necessary info. If I need to prove if I am old enough without revealing an actual birthdate. Best practices \u2013 your should deeply inspect your use case and come to a conclusion as to what is the minimum data you need to accomplish your goal. Don\u2019t be greedy. Data has proven to be toxic. Mathematical \u2013 finding a way to express the data that you wish as an equation related to the data you have. Minimizing the amount of data to achieve your goal or communicate what you need to. Designing systems to operate efficiently in order to maximize privacy. Choosing to only share the minimal amount of data about yourself or something during an interaction. Trying to keep the amount of info that is being disclosed as limited as possible to the requirements of the vulnerability. The minimum is what ... will lead them to move to action. Always happens in a context: a relationship where the two parties are considering interacting in some way. Sending only the signals that I want to send and that are needed by the other party, hem to interact with me in a particular The least amount of data needed for a system to function Collecting the least amount of date for the highest outcome. Also known as minimal disclosure, data minimization is the principle of using the least amount of data to accomplish a transaction. This is incumbent on all three parties in an exchange. The holder should attempt to share the minimum. The issuer needs to create attributes designed for composition and minimal use, as opposed to monolithic credentials with all the data. The verifier needs to ask only for what they need. The motivation to minimize data is that unneeded data is potentially \u201ctoxic.\u201d Selective Disclosure Definitions of selective disclosure that we considered in the formation of our definition above. Ability to decide what info you give and how it can be used. Smart disclosure, allows [selecting] what information to give based on logic. Blind search. You can decide who gets to see what. Means by which we achieve data minimization. Form of policies. Ability to mask attributes that you do not have to share. Relates to mathematical definition \u2013 the computational ability to reveal only parts of your data profile. Act of communicating or revealing only what you intend to, and not any peripheric data Having granular control over the ways in which data is shared Is a pattern for user interfaces allowing people to choose what to share about them during an interaction Method for achieving data minimization where only certain signals are being shared and there is control of who it is being shared with. That control is never perfect. The communication channel matters. An entity having granular control on what\u2019s revealed. The individual having the freedom to decide what to share, or the acquirer using data minimization approach requesting the minimum amount amount of data for the maximum impact Progressive Trust Definitions of progressive trust that we considered in the formation of our definition above. Note that we included definitions of progressive trust and progressive disclosure as well. Procedure for increasing revelation of relevant data as the communication proceeds. As we continue to communicate we decide to reveal more information. It becomes more generous as trust builds. Being able to reveal more data as you need to given certain conditions Information is disclosed as needed when needed. You can choose to increase the amount of data you disclose over time as needed. Taking as little vulnerability as possible at the beginning, then gaining information and becoming willing to take on additional vulnerability by revealing more information. Trust is built through step by step interactions where we start making ourselves vulnerable in a very small way and we observe how this works out. Based on results we consider making ourselves further vulnerable or not. It is about increasing levels of familiarity and prediction making (I am better able to predict your behavior). Releasing information as needed Escalation of the previous steps (data minimization, selective disclosure) in line with the value increasing. Purpose binding is the auditable use of data, so I can audit the use of my data and determine that it was used for the purposes declared. Progressive trust is the feeling of assurance and safety that develops over time, based on a history of data used only for its bound purposes, and so based on this feeling a data holder will be ready to share more data or other data, if at some point in the relationship this other data is requested. Trust is required when you depend on the actions of someone who you can't control. Appendix B: Basic Crypto Concepts This appendix describes basic cryptographic concepts critical to the privacy preserving engineering of credential attributes. For readability, we use the short word, \"crypto.\" Overview Crypto is a huge field with highly specialized jargon, too much to cover here. But non-specialists would benefit from some understanding of relevant crypto in order to make informed decisions. We begin with a brief overview of several concepts from number theory that serve as a foundation for all crypto used in this process. This is a curated list of topics progressing from the simple to the more complex. Notice how ideas are re-used and layered as you read on. Number Theory Number theory refers to the study of the behavior of integer numbers such as one, three, or two hundred. The following are behaviors of these numbers that make them useful for crypto: Prime or not : Some numbers are only divisible by themselves and one. These are called 'prime.' One-way function : A numerical function that takes a publicly known number, and without any secret information, computes a value. Like a one-way street, the computation goes only one direction. Given the computed value, it is hard to find the publicly known number. Clock arithmetic , aka modular arithmetic: a system of arithmetic where numbers \"wrap around\" upon reaching a certain value. A familiar use is in the ordinary clock, in which our day is divided into twelve-hour periods. If the time is seven o'clock now, then ten hours later it will be five o'clock, though a military man might say seventeen hundred hours. Even he will say that ten hours later it is three o'clock, and not twenty-seven hundred hour, because his clock time \"wraps around\" every twenty-four hours (as opposed to twelve). Groups and Finite Fields : Some subsets of all integers form a \"group\" that behaves in ways very useful to the performance of cryptography. In extremely simplified terms, a group is a self-sufficient set of integers, where any possible manipulation returns an answer from within the group. Finite fields are types of groups that satisfy certain demanding properties. Notice how this resembles clock arithmetic, where the same numbers are used over and over again. Discrete Logarithms : A discrete logarithm is a property for numbers in a group. Since there is no efficient method for computing discrete logarithms, they form a \"difficult problem\" and so are very useful in cryptography. (The logarithm log(b) of a is an exponent x such that b^x ( b raised to the x exponent) = a .) Quadratic Residues : Quadratic refers to 'squared' numbers, a number raised to the second exponential power. Quadratic residues are a useful property of squared numbers as they behave in modular arithmetic. Primary Objectives The curious behavior of numbers is exploited to achieve four primary crypto objectives. Confidentiality : a hidden part of a credential cannot be understood by anyone for whom it is unintended. Often called \"privacy,\" we avoid that word here since it can mean many things in addition to confidentiality. Authentication : the identity of information shared can be validated as authentic. Integrity : the revealed part of the credential cannot be altered without such alteration being detected. Also known as validity, fidelity or verifiability. Non-repudiation : aka non-deniability, a credential's creator cannot deny at a later stage his or her involvement. Ten Crypto Concepts Over the decades hundreds if not thousands of crypto protocols, processes, algorithms and protocols have been innovated to achieve these objectives, by cobbling together the above six behaviors in different ways. We present here a brief tour of the ten most significant ones in our field of verifiable credentials: PKI or \"public and private keys\": a system that lies at the heart of most relevant crypto since a publicly shared digital asset can be locked to, or encumbered by, a private key that is kept secret. Only the person with knoweldge of that private key can, with the right software, unlock that asset. This enables a broad range of activities such as \"signature,\" \"authentication,\" and \"certificate validation.\" In general we call these activities PKI, a public key infrastructure. Signature : a signature in this context is a use of PKI. A valid signature gives a recipient \"authentication\", confidence that the message was created by a known sender; \"non-repudiation,\" that the sender cannot deny having sent the message; and \"integrity,\" that the message was not altered in transit. Signatures enable every cryptographic objective except for confidentiality. There are many types of signature schemes in use including Digital Signature Algorithm (DSA), Camenisch-Lysyanskaya (CL) signatures, and Boneh\u2013Lynn\u2013Shacham (BLS) signatures. Key exchange : exchange methods enable two parties that have no prior knowledge of each other to jointly establish a shared secret key over an insecure channel. This key can then be used to encrypt subsequent communications using a symmetric key cipher. Diffie\u2013Hellman is a common example. Elliptic-curve cryptography (ECC): an approach to PKI based on the numeric structure of elliptic curves over finite fields. ECC is useful as it requires smaller keys compared to non-ECC cryptography. There are many variants of ECC used including Edwards-curve Digital Signature Algorithm (EdDSA). Hash or message digest : one-way functions, such as SHA-256. A set of many one-way functions may be applied to a tree of data to form a Merkle Tree (or trie). Zero-Knowledge (ZK): zero knowledge is defined above loosely as a set of practices where some data is revealed while other parts are kept secret. Many ZK methods are used in cryptography incuding Fiat Shamir, Proof of knowledge of discrete logarithms, ZK Snarks, and ZK Starks. Accumulators : a form of ZK, a cryptographic accumulator is a one-way membership function that answers a query as to whether a potential candidate is a member of a set without revealing the individual members of the set. Similar to a one-way hash function, cryptographic accumulators generate a fixed-size digest representing an arbitrarily large set of values. Some further provide a fixed-size witness for any value of the set, which can be used together with the accumulated digest to verify its membership in the set. Commitment : a cryptographic commitment, which allows one to commit to a chosen value (or chosen statement) while keeping it hidden to others, with the ability to reveal the committed value later. Witness : a term that has different applications in cryptography. In this paper, a witness is a value used in a cryptographic accumulator. In Bitcoin the unlocking signature is called the \"witness data.\" Quantum Computing and Cryptography: as quantum computing is developed, it poses a threat to the difficulty of puzzles. For example, they are likely to be much faster at determining if a number is truly prime or not. Appendix C: Drinking Age Credential Implementation The birthday of an individual is formatted into a verifiable credential, which can be inspected to reveal the age of the credential holder without revealing their birthdate. The flow described here is based on the developing Verifiable Claims standard of the W3C Credentials Community Group. It uses cryptography developed by Jan Camenisch, as implemented by Sovrin. This is a work in progress. Note that other types of crypto could be applied to achieve the same privacy preserving goals. Communication Flow The flow below may be copies and pasted into the Web Sequence Diagram webpage to generate a flow diagram. title Verifiable credential using Selective Disclosure participant Valid Time Oracle participant Janet participant ID Provider participant Ledger participant Bar note over Janet:Prover note over Bar:Validator note over Janet,Bar: Preparation and Setup note right of ID Provider:Infrastructure ID Provider->Ledger: Define Schema (Name, Birthdate, Address) ID Provider->Ledger: credential Definition (Pub Key, etc.) ID Provider->ID Provider: Generate Prv Key for this credential ID Provider->Ledger:Revocation Registry note left of Bar: Prepare to accept credentials Bar->Bar:Install Agent Bar->Ledger: Check schema note over Janet,Bar: Begin Use Case Janet->ID Provider: Request ID ID Provider-->Janet: ID will be issued as a digital credential note right of Janet: Prepare to receive credentials Janet->Janet: Install Agent Janet->Janet: Prv Key Generate, Store Janet->Ledger:Check Schema Ledger->Janet:credential Definition Janet-->ID Provider:Proof of Name, Birthdate, Address Janet->ID Provider: Blinded secret ID Provider->Janet: credential Janet->Janet: Validate credential against credential Def note over Janet,Bar: Janet goes to the bar note left of Bar: Can Janet Enter? Bar->Janet: Request Proof of Age Janet->Valid Time Oracle: Get time Valid Time Oracle->Janet: Time credential Janet->Janet:Generate Proof (This person is over 21) Janet->Bar: Provide Proof Bar->Bar: Evaluate proof Bar->Ledger: Verify on Ledger Ledger->Bar: Verification Bar->Janet: Come in note left of Bar: Invite to club Bar->Janet: Join loyalty club? (requires valid postal code) Janet->Janet:Generate Proof (postal code) Janet->Bar: Provide Proof Bar->Bar: Evaluate proof Bar->Ledger: Verify on Ledger Ledger->Bar: Verification Bar->Janet: Have Loyalty Card Crypto Details Below are some of the detailed mathematics involved in issuing a verifiable credential as implemented by Sovrin, a non-profit organization dedicated to managing a decentralized, public network for the purposes of self-sovereign identity. Issuer Setup The following setup is a necessary precursor to issuing a privacy-preserving credential. Compute Perform the mathematical calculations required to curate the essential ingredients of the operations we are about to perform. Some of these results, like the private keys, are very sensitive and must be kept secret by the credential holder; others are to be shared. Random \ud835\udcf9', \ud835\udcfa', 1024-bit prime numbers, such that \ud835\udcf9 = 2\ud835\udcf9' + 1 and \ud835\udcfa = 2\ud835\udcfa' + 1 are both 1024-bit prime numbers. \ud835\udcf7 = \ud835\udcf9\ud835\udcfa. Random quadratic residue: \ud835\udce2 mod \ud835\udcf7 Random \ud835\udce7 \ud835\udce9 , \ud835\udce7 \ud835\udce11 , . . . , \ud835\udce7 \ud835\udce1\ud835\udcf5 \u2208 [2: \ud835\udcf9'\ud835\udcfa' - 1], where \ud835\udcf5 is the number of attributes in the credential. \ud835\udce9 = \ud835\udce2 \ud835\udce7\ud835\udce9 mod \ud835\udcf7 \ud835\udce1 \ud835\udcf2 = \ud835\udce2 \ud835\udce7\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 Issuer private key \ud835\udcfc\ud835\udcf4 \ud835\udcec = \ud835\udcf9'\ud835\udcfa' Issuer public key \ud835\udcf9\ud835\udcf4 \ud835\udcec = {\ud835\udcf7, \ud835\udce2, \ud835\udce9, \ud835\udce1 1 , . . . , \ud835\udce1 \ud835\udcf5 } Proof of Correctness As a result of the above computations, we then curate the following. This proof, along with the public keys, is the computational algorithm that will be used to validate the credential. Random \ud835\udce7' \ud835\udce9 , \ud835\udce7' \ud835\udce11 , . . . , \ud835\udce7' \ud835\udce1\ud835\udcf5 \u2208 [2: \ud835\udcf9'\ud835\udcfa' - 1] \ud835\udce9' = \ud835\udce2 \ud835\udce7'\ud835\udce9 mod \ud835\udcf7 \ud835\udce1' \ud835\udcf2 = \ud835\udce2 \ud835\udce7'\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 \ud835\udcec = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce9 || \ud835\udce1 1 || . . . || \ud835\udce1 \ud835\udcf5 || \ud835\udce9' || \ud835\udce1' 1 || . . . || \ud835\udce1' \ud835\udcf5 ) \ud835\udce7'' \ud835\udce9 = \ud835\udce7' \ud835\udce9 + \ud835\udcec \ud835\udce7 \ud835\udce9 \ud835\udce7'' \ud835\udce1\ud835\udcf2 = \ud835\udce7' \ud835\udce1\ud835\udcf2 + \ud835\udcec \ud835\udce7 \ud835\udce1\ud835\udcf2 , 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 The Cred Def is comprised of the public key and the proof of correctness; this is published to the distributed ledger. Issuing a Credential With setup complete, we can now issue the credential in a privacy-preserving manner. For Each Credential For each credential issued, perform the following operations. Issuer Computes A cryptographic accumulator is constructed in order to enable zero-knowledge queries further on. It is a one-way membership function, including the claim in the membership set. The operation can then answers a query as to whether a potential candidate is a member of a set without revealing the individual members of the set. \ud835\udcd0 \ud835\udcf2 = accumulator index \ud835\udce4 \ud835\udcf2 = user index \ud835\udcf6 2 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udcd0 \ud835\udcf2 || \ud835\udce4 \ud835\udcf2 ) 256-bit integer representations of each of the attributes: \ud835\udcf6 3 , . . . , \ud835\udcf6 \ud835\udcf5 \ud835\udcf7 0 = nonce Issuer Sends \ud835\udcf7 0 to Prover This nonce is provided to the Prover for calculation of the Prover's proof of correctness. Prover Receives \ud835\udcf7 0 and Computes the Following The prover aggregates and prepares public keys for use in validating the signatures. The prover also commits to a chosen value while keeping it temporarily hidden, making the calculation binding. Retrieves Issuer\u2019s public key \ud835\udcf9\ud835\udcf4 \ud835\udcec Retrieves Issuer\u2019s proof of correctness Generates: \ud835\udcf6 1 = pedersen commitment of claim link secret Random \ud835\udcff', \ud835\udcff'', \ud835\udcf6' 1 \ud835\udcf7 1 = nonce Prover Verifies the Issuer\u2019s Proof of Correctness \ud835\udce9^ = \ud835\udce9 \ud835\udcec \ud835\udce2 \ud835\udce7''\ud835\udce9 mod \ud835\udcf7 \ud835\udce1^ \ud835\udcf2 = \ud835\udce1 \ud835\udcf2 \ud835\udcec \ud835\udce2 \ud835\udce7''\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 Verifies \ud835\udcec = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce9 || \ud835\udce1 1 || . . . || \ud835\udce1 \ud835\udcf5 || \ud835\udce9^ || \ud835\udce1^ 1 || . . . || \ud835\udce1^ \ud835\udcf5 ) Prover Computes \ud835\udce4 = \ud835\udce2 \ud835\udcff\u2019 \ud835\udce1 1 \ud835\udcf61 mod \ud835\udcf7 \ud835\udce4\u2019 = \ud835\udce2 \ud835\udcff\u2019\u2019 \ud835\udce1 1 \ud835\udcf6\u20191 mod \ud835\udcf7 \ud835\udcec\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce4 || \ud835\udce4\u2019 || \ud835\udcf7 0 ) \ud835\udcff^ = \ud835\udcff\u2019\u2019 + \ud835\udcec\u2019\ud835\udcff\u2019 \ud835\udcf6^ 1 = \ud835\udcf6\u2019 1 + \ud835\udcec\u2019\ud835\udcf6 1 Prover Sends \ud835\udcdf = { \ud835\udce4, \ud835\udcec\u2019, \ud835\udcff^, \ud835\udcf6^ 1 , \ud835\udcf7 1 } to the Issuer Issuer Verifies Prover Setup Computes \ud835\udce4^ = \ud835\udce4 -\ud835\udcec \ud835\udce2 \ud835\udcff^ \ud835\udce1 1 \ud835\udcf6^ 1 mod \ud835\udcf7 Verifies \ud835\udcec\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce4 || \ud835\udce4^ || \ud835\udcf7 0 ) Issuer Signs the Credential by Computing the Following \ud835\udce0 = \ud835\udce9 / (\ud835\udce4\ud835\udce2 \ud835\udcff* \ud835\udce1 2 \ud835\udcf62 \ud835\udce1 3 \ud835\udcf63 \u00b7\u00b7\u00b7 \ud835\udce1 \ud835\udcf5 \ud835\udcf6\ud835\udcf5 ) mod \ud835\udcf7 \ud835\udced = \ud835\udcee -1 mod \ud835\udcf9\u2019\ud835\udcfa\u2019 \ud835\udcd0 = \ud835\udce0 \ud835\udced mod \ud835\udcf7 \ud835\udcd0\u2019 = \ud835\udce0 \ud835\udcfb mod \ud835\udcf7 \ud835\udcec\u2019\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 (\ud835\udce0 || \ud835\udcd0 || \ud835\udcd0\u2019|| \ud835\udcf7 1 ) \ud835\udcfc \ud835\udcee = (\ud835\udcfb - \ud835\udcec\u2019\u2019\ud835\udcee -1 ) mod \ud835\udcf9\u2019\ud835\udcfa\u2019 Issuer Sends \ud835\udcde = {\ud835\udcd0, \ud835\udcee, \ud835\udcff*, \ud835\udcfc \ud835\udcee , \ud835\udcec\u2019\u2019, \ud835\udcf6 2 , . . . , \ud835\udcf6 \ud835\udcf5 } to the Prover Prover Receives \ud835\udcde and Does the Following Prover Computes \ud835\udcff = \ud835\udcff\u2019 + \ud835\udcff* \ud835\udce0\u2019 = \ud835\udce9 / (\ud835\udce2 \ud835\udcff \ud835\udce1 2 \ud835\udcf62 \ud835\udce1 3 \ud835\udcf63 \u00b7\u00b7\u00b7 \ud835\udce1 \ud835\udcf5 \ud835\udcf6\ud835\udcf5 ) mod \ud835\udcf7 \ud835\udced\u2019 = \ud835\udcec\u2019\u2019 + \ud835\udcfc \ud835\udcee \ud835\udcee \ud835\udcd0^ = \ud835\udcd0 \ud835\udced\u2019 \ud835\udce2 \ud835\udcff\u2019\ud835\udcfc\ud835\udcee mod \ud835\udcf7 Prover Verifies \ud835\udcee is prime and 2 596 \u2264 \ud835\udcee \u2264 2 596 + 2 119 \ud835\udce0\u2019 = \ud835\udcd0 \ud835\udcee mod \ud835\udcf7 \ud835\udcec\u2019\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 (\ud835\udce0\u2019 || \ud835\udcd0 || \ud835\udcd0^ || \ud835\udcf7 1 ) Prover Stores Primary Claim ({\ud835\udcf6 1 , . . . , \ud835\udcf6 \ud835\udcf5 }, \ud835\udcd0, \ud835\udcee, \ud835\udcff) For Additional Information The crypto used here is originally from the Identity Mixer. Link: (https://www.zurich.ibm.com/identity_mixer/) The Sovrin team shares additional information and working code at the following links. * Verifiable Credentials Code * Verifiable Credentials Example Usage in Python References: W3C Credentials Community Group Home Page Data Minimization and Selective Disclosure Repo Camenisch, Lysyanskaya. An Efficient System for Non-transferable Anonymous Credentials with Optional Anonymity Revocation Pfitzmann, Hansen. 2010. A terminology for talking about privacy by data minimization: Anonymity, Unlinkability, Undetectability, Unobservability, Pseudonymity, and Identity Management Cooper, Tschofenig, Aboba, Peterson, Morris, Hansen, Smith, Janet. 2013. RFC6973 . The draft can also be helpful, \"This document focuses on introducing terms used to describe privacy properties that support data minimization.\" Hansen, Tschofenig, Smith, Cooper. 2012 Privacy Terminology and Concepts. Network Working Group Internet-Draft Expires: September 13, 2012 Longley, Sporny. Redaction Signature Suite 2016. 26 June 2017. Draft Community Group Report \"This specification describes the Redaction Signature Suite created in 2016 for the Linked Data Signatures specification. It enables a sender to redact information in a message without invalidating the digital signature.\"","title":"Data minimization and selective disclosure"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#deprecated","text":"THIS VERSION IS DEPRECATED: THE PAPER IS PROMOTED TO A CREDENTIALS COMMUNITY GROUP REPORT SEE https://github.com/w3c-ccg/data-minimization","title":"DEPRECATED"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#introduction","text":"We often share information on the World Wide Web, though some of it is private. The W3C Credentials Community Group focuses on how privacy can be enhanced when attributes are shared electronically. In the course of our work, we have identified three related but distinct privacy enhancing strategies: \"data minimization,\" \"selective disclosure,\" and \"progressive trust.\" These enhancements are enabled with cryptography. The goal of this paper is to enable decision makers, particularly non-technical ones, to gain a nuanced grasp of these enhancements along with some idea of how their enablers work. We describe them below in plain English, but with some rigor. This knowledge will enable readers of this paper to be better able to know when they need privacy enhancements, to select the type of enhancement needed, to assess techniques that enable those enhancements, and to adopt the correct enhancement for the correct use case.","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#three-examples","text":"Three examples of how people would like their privacy preserved in the process of sharing credentials help to illuminate these three techniques. Diego attempts to use an online service and is asked to share his location in order to prove his geolocation. Diego hesitates, since the service doesn't need his location everyday, everywhere. He knows that the service may share this information with other parties without meaningful consent on his part. Thoughts pass through his mind: What location data does the service actually need? What will it read in future? Is there a way for him to share his location just this once, or to only share an approximate location? Selena hands her driver's license to a bouncer to prove she is of drinking age. As he looks it over, she sees him inspecting her date of birth and home address. He only needs to know that she is over 21. Is there a way to disclose that she is indeed old enough without revealing her actual age, along with her home address and city of residence as well? Proctor , negotiating with a real estate agent to purchase a home, reveals a letter from his bank stating his credit limit. He wanted to reveal its approximate amount only, but the agent insisted on verifying that the letter was authentic. Proctor feels the agent now has the upper hand in the negotiation, as the letter reveals more than just its authenticity. Could he have revealed only an approximate amount and reveal more details as the negotiations progress? Each story features information that is verifiable: a home address, age, or credit limit. We call such information a credential, and a detail of a credential we call an attribute. We have three strategies for enhancing the privacy of digitally shared credential attributes, and each story highlights one. Diego's story highlights the need for \"data minimization,\" Selena's for \"selective disclosure,\" and Proctor's for \"progressive trust.\" Let's examine each one in detail before discussing enablers.","title":"Three Examples"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#privacy-enhancements","text":"We propose the following three privacy enhancements. (Sources used to curate these definitions are listed in Appendix A.)","title":"Privacy Enhancements"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#data-minimization","text":"Data minimization is the act of limiting the amount of shared data strictly to the minimum necessary in order to successfully accomplish a task or goal. There are three types of minimization: * Content minimization \u2013 the amount of data should be strictly the minimum necessary. * Temporal minimization \u2013 the data should be stored by the receiver strictly for the minimum amount of time necessary to execute the task. * Scope minimization \u2013 the data should only be used for the strict purpose of the active task. Data minimization is enacted primarily by policy decisions made by stakeholders in the credentials ecosystem: * Credential issuers ensure that credentials may be presented in such a way as to enable data minimization. This may require issuing multiple, related, granular sub-credentials. * Credential inspectors establish in advance policies regarding the data they will examine: * what is the minimum data necessary to accomplish the task or goal? * what is the minimum time the data can be stored to execute the task? * what processes ensure that the data is applied only to the task at hand and does not, by a process of scope creep, become applied to other tasks or goals? Data minimization policies impact selective disclosure, the next privacy enhancement.","title":"Data Minimization"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#selective-disclosure","text":"Selective disclosure is the ability of an individual to granularly decide what information to share. Stakeholders in the credentials ecosystem enable selective disclosure capabilities in the following ways: * Credential issuers format the credential and its attributes in such a way as to enable selective disclosure. As with the strategy of data minimization, they may issue multiple, related, granular sub-credentials. Each attribute and the overall credential may be formatted to support cryptography, a capability described in more detail below. * Credential inspectors ensure the request is framed in such a way as to enable selective disclosure, using the cryptographic tools required. Once data minimization policies and selective disclosure are in place, the third and last enhancement can be applied.","title":"Selective disclosure"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#progressive-trust","text":"Progressive trust is the ability of an individual to gradually increase the amount of relevant data revealed as trust is built or value generated. To enable progressive trust capabilities, stakeholders in the credentials ecosystem act in the following ways: * Issuers format the credential(s) in such a way as to enable progressive trust. This may require issuing multiple, related, atomic sub-credentials. It also may require formatting the credential to support mathematical queries and cryptographic proofs. Finally, the issuer's data model may express how the various sub-credentials are related in a scenario involving progressive trust. * Inspectors ensure that requests are framed in such a way as to enable progressive trust. They structure the communication in order to to gradually escalate credential requests in order to enable a subject to progressively trust the inspector more and more, revealing the minimum data necessary to accomplish each step of the task or goal and revealing more and more as the mutual communication progresses.","title":"Progressive Trust"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#crypto-enablers","text":"Implementing privacy enhancements depends on organizational decisions. Determination of the data needed, with an eye towards data minimization, along with a clear model of how data is used over the lifecycle of engagement, goes a long way towards enabling progressive trust. However, policies are not enough. When enhancing privacy online, some data parts must be revealed while others remain concealed. Concealment is achieved mostly by the art of cryptography, from the greek word \"kryptos,\" meaning hidden, like in a crypt. Crypto (a short word we will use for cryptography) enables us to achieve our goal by means of three primary enablers: having a secret, having a difficult mathematical task, and having zero-knowledge enablers. The children's \"Where's Waldo?\" illustrated book series helps us to understand these three enablers. In these books a distinctively dressed man appears only once on each page, wearing a striped hat. Readers are asked to scour the page and locate him. We can understand the three enablers by examining Where's Waldo one step at a time. A Secret : For the new reader, Waldo's location is a secret. The illustrator knows it, and the reader doesn't. The reader is encouraged to search the page and find Waldo, but that is a difficult task. Some readers give up and ask someone who has already found Waldo to show them his location. In essence, they are asking another reader to reveal the secret. Once found, a reader could keep the information secret by circling Waldo in red and storing the book in a safe. This amounts to storing the secret for future use. Secrets are essential to crypto. They are usually called keys, and they must be managed carefully. A Difficult Task : Waldo is difficult to find on the page. The reader has to search everywhere and mistakenly identify many Waldo look-alike characters before reaching a satisfactory conclusion and finding him. Yet when he is finally discovered, or someone points Waldo out, it's easy to see where he is. That's why it's a fun task. This difference between the difficulty of conducting the task and the ease of verifying the task lies at the heart of cryptographic enablers. A Zero Knowledge Enabler : Can you prove you found Waldo without revealing the secret of his actual location on the page? There is a simple way to do so. Take a rectangular piece of white cardboard that is much larger than the book. Cut a hole exactly fitting Waldo to reveal his silhouette only, nothing else. You can now show Waldo to anyone, peeking out of the cardboard. Yet the cardboard is wide and opaque, hiding the book thoroughly, so a verifier has no idea where Waldo is on the page. The puzzle was solved and someone verified the achievement, without revealing any knowledge of how to solve the puzzle. The secret is still safe, the task still just as difficult as before. Where's Waldo books are drawings, while crypto is built from mathematical equations, basically puzzles based on numbers. We provide the interested reader with a layman's overview in Appendix B.","title":"Crypto Enablers"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#three-solutions","text":"We now return to our opening examples, apply the privacy preserving strategies and enablers described, and describe the improved outcomes. The online service that Diego uses does an internal policy review and realizes (a) it only needs a location when a user signs up for an account, and (b) it does not need an exact address, only the county district. It changes its interface to request a Verifiable Credential for Diego's location. Diego's system creates this credential for him, which can be inspected to reveal the county district. The crypto to enable this would be similar to that described in Appendix C. With this data minimization, the online service has less risk of violating data protection rules, is less a target for hacking, and has lower overall costs, while at the same time preserving Diego's privacy. The bar seeking to verify Selena 's age uses selective disclosure as built into the Verifiable Claims system. Selena will no longer share her date of birth. Instead, Selena creates a secret that we harness to craft a crypto-formatted credential. This crypto makes it easy to verify her age, but difficult to determine her exact date of birth. The bouncer's system can perform a zero-knowledge proof to determine the credential is valid and that Selena is older than twenty-one, without revealing her birthday or her secret. The bouncer sees she is over twenty-one without seeing her date of birth, residence address, or any other unnecessary information. In Appendix C we show the process step-by-step. The real estate agency working with Proctor implements a data model specifying what is required at each step of the real estate negotiation. The first step requires only proof of being an account holder in good standing at a known bank, so Proctor does not have to reveal the detailed letter at this point. As their negotiation continues, Proctor reveals more and more information as required. Some steps of the process may share Verifiable Claims encoded with crypto.","title":"Three Solutions"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#summary","text":"The World Wide Web accelerates the sharing of credentials and other digital interactions, and many regulations have been passed and strategies proposed to protect privacy, some of which require cryptography. To align terminology, the World Wide Web Credentials Community Group has found three related but distinct privacy enhancing strategies that create a useful rubric for discussing the challenges and arriving at solutions. We share the examples of Diego, Selena, and Proctor and propose \"data minimization,\" \"selective disclosure,\" and \"progressive trust,\" with accompanying crypto protocols as useful semantics for accelerating the adoption of digital interaction while protecting privacy.","title":"Summary"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#appendix-a-definition-sources","text":"This section contains definitions we curated, based on research and oral interviews, to create the definitions of data minimization, selective disclosure and progressive trust.","title":"Appendix A: Definition Sources"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#data-minimization_1","text":"Definitions of data minimization that we considered in the formation of our definition above. GDPR Rec.39; Art.5(1)(c) definition: \u201cThe personal data should be adequate, relevant and limited to what is necessary for the purposes for which they are processed. This requires, in particular, ensuring that the period for which the personal data are stored is limited to a strict minimum. Personal data should be processed only if the purpose of the processing could not reasonably be fulfilled by other means.\u201d Reducing your overall footprint of data outside of your control. Can be accomplished by using selective disclosure. Adequate, relevant and non-excessive. Reducing the amount of data you are sending in a payload to only the one ... needed. That prevents leakage of confidential information. Providing people with the information they need without revealing non necessary info. If I need to prove if I am old enough without revealing an actual birthdate. Best practices \u2013 your should deeply inspect your use case and come to a conclusion as to what is the minimum data you need to accomplish your goal. Don\u2019t be greedy. Data has proven to be toxic. Mathematical \u2013 finding a way to express the data that you wish as an equation related to the data you have. Minimizing the amount of data to achieve your goal or communicate what you need to. Designing systems to operate efficiently in order to maximize privacy. Choosing to only share the minimal amount of data about yourself or something during an interaction. Trying to keep the amount of info that is being disclosed as limited as possible to the requirements of the vulnerability. The minimum is what ... will lead them to move to action. Always happens in a context: a relationship where the two parties are considering interacting in some way. Sending only the signals that I want to send and that are needed by the other party, hem to interact with me in a particular The least amount of data needed for a system to function Collecting the least amount of date for the highest outcome. Also known as minimal disclosure, data minimization is the principle of using the least amount of data to accomplish a transaction. This is incumbent on all three parties in an exchange. The holder should attempt to share the minimum. The issuer needs to create attributes designed for composition and minimal use, as opposed to monolithic credentials with all the data. The verifier needs to ask only for what they need. The motivation to minimize data is that unneeded data is potentially \u201ctoxic.\u201d","title":"Data Minimization"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#selective-disclosure_1","text":"Definitions of selective disclosure that we considered in the formation of our definition above. Ability to decide what info you give and how it can be used. Smart disclosure, allows [selecting] what information to give based on logic. Blind search. You can decide who gets to see what. Means by which we achieve data minimization. Form of policies. Ability to mask attributes that you do not have to share. Relates to mathematical definition \u2013 the computational ability to reveal only parts of your data profile. Act of communicating or revealing only what you intend to, and not any peripheric data Having granular control over the ways in which data is shared Is a pattern for user interfaces allowing people to choose what to share about them during an interaction Method for achieving data minimization where only certain signals are being shared and there is control of who it is being shared with. That control is never perfect. The communication channel matters. An entity having granular control on what\u2019s revealed. The individual having the freedom to decide what to share, or the acquirer using data minimization approach requesting the minimum amount amount of data for the maximum impact","title":"Selective Disclosure"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#progressive-trust_1","text":"Definitions of progressive trust that we considered in the formation of our definition above. Note that we included definitions of progressive trust and progressive disclosure as well. Procedure for increasing revelation of relevant data as the communication proceeds. As we continue to communicate we decide to reveal more information. It becomes more generous as trust builds. Being able to reveal more data as you need to given certain conditions Information is disclosed as needed when needed. You can choose to increase the amount of data you disclose over time as needed. Taking as little vulnerability as possible at the beginning, then gaining information and becoming willing to take on additional vulnerability by revealing more information. Trust is built through step by step interactions where we start making ourselves vulnerable in a very small way and we observe how this works out. Based on results we consider making ourselves further vulnerable or not. It is about increasing levels of familiarity and prediction making (I am better able to predict your behavior). Releasing information as needed Escalation of the previous steps (data minimization, selective disclosure) in line with the value increasing. Purpose binding is the auditable use of data, so I can audit the use of my data and determine that it was used for the purposes declared. Progressive trust is the feeling of assurance and safety that develops over time, based on a history of data used only for its bound purposes, and so based on this feeling a data holder will be ready to share more data or other data, if at some point in the relationship this other data is requested. Trust is required when you depend on the actions of someone who you can't control.","title":"Progressive Trust"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#appendix-b-basic-crypto-concepts","text":"This appendix describes basic cryptographic concepts critical to the privacy preserving engineering of credential attributes. For readability, we use the short word, \"crypto.\"","title":"Appendix B: Basic Crypto Concepts"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#overview","text":"Crypto is a huge field with highly specialized jargon, too much to cover here. But non-specialists would benefit from some understanding of relevant crypto in order to make informed decisions. We begin with a brief overview of several concepts from number theory that serve as a foundation for all crypto used in this process. This is a curated list of topics progressing from the simple to the more complex. Notice how ideas are re-used and layered as you read on.","title":"Overview"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#number-theory","text":"Number theory refers to the study of the behavior of integer numbers such as one, three, or two hundred. The following are behaviors of these numbers that make them useful for crypto: Prime or not : Some numbers are only divisible by themselves and one. These are called 'prime.' One-way function : A numerical function that takes a publicly known number, and without any secret information, computes a value. Like a one-way street, the computation goes only one direction. Given the computed value, it is hard to find the publicly known number. Clock arithmetic , aka modular arithmetic: a system of arithmetic where numbers \"wrap around\" upon reaching a certain value. A familiar use is in the ordinary clock, in which our day is divided into twelve-hour periods. If the time is seven o'clock now, then ten hours later it will be five o'clock, though a military man might say seventeen hundred hours. Even he will say that ten hours later it is three o'clock, and not twenty-seven hundred hour, because his clock time \"wraps around\" every twenty-four hours (as opposed to twelve). Groups and Finite Fields : Some subsets of all integers form a \"group\" that behaves in ways very useful to the performance of cryptography. In extremely simplified terms, a group is a self-sufficient set of integers, where any possible manipulation returns an answer from within the group. Finite fields are types of groups that satisfy certain demanding properties. Notice how this resembles clock arithmetic, where the same numbers are used over and over again. Discrete Logarithms : A discrete logarithm is a property for numbers in a group. Since there is no efficient method for computing discrete logarithms, they form a \"difficult problem\" and so are very useful in cryptography. (The logarithm log(b) of a is an exponent x such that b^x ( b raised to the x exponent) = a .) Quadratic Residues : Quadratic refers to 'squared' numbers, a number raised to the second exponential power. Quadratic residues are a useful property of squared numbers as they behave in modular arithmetic.","title":"Number Theory"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#primary-objectives","text":"The curious behavior of numbers is exploited to achieve four primary crypto objectives. Confidentiality : a hidden part of a credential cannot be understood by anyone for whom it is unintended. Often called \"privacy,\" we avoid that word here since it can mean many things in addition to confidentiality. Authentication : the identity of information shared can be validated as authentic. Integrity : the revealed part of the credential cannot be altered without such alteration being detected. Also known as validity, fidelity or verifiability. Non-repudiation : aka non-deniability, a credential's creator cannot deny at a later stage his or her involvement.","title":"Primary Objectives"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#ten-crypto-concepts","text":"Over the decades hundreds if not thousands of crypto protocols, processes, algorithms and protocols have been innovated to achieve these objectives, by cobbling together the above six behaviors in different ways. We present here a brief tour of the ten most significant ones in our field of verifiable credentials: PKI or \"public and private keys\": a system that lies at the heart of most relevant crypto since a publicly shared digital asset can be locked to, or encumbered by, a private key that is kept secret. Only the person with knoweldge of that private key can, with the right software, unlock that asset. This enables a broad range of activities such as \"signature,\" \"authentication,\" and \"certificate validation.\" In general we call these activities PKI, a public key infrastructure. Signature : a signature in this context is a use of PKI. A valid signature gives a recipient \"authentication\", confidence that the message was created by a known sender; \"non-repudiation,\" that the sender cannot deny having sent the message; and \"integrity,\" that the message was not altered in transit. Signatures enable every cryptographic objective except for confidentiality. There are many types of signature schemes in use including Digital Signature Algorithm (DSA), Camenisch-Lysyanskaya (CL) signatures, and Boneh\u2013Lynn\u2013Shacham (BLS) signatures. Key exchange : exchange methods enable two parties that have no prior knowledge of each other to jointly establish a shared secret key over an insecure channel. This key can then be used to encrypt subsequent communications using a symmetric key cipher. Diffie\u2013Hellman is a common example. Elliptic-curve cryptography (ECC): an approach to PKI based on the numeric structure of elliptic curves over finite fields. ECC is useful as it requires smaller keys compared to non-ECC cryptography. There are many variants of ECC used including Edwards-curve Digital Signature Algorithm (EdDSA). Hash or message digest : one-way functions, such as SHA-256. A set of many one-way functions may be applied to a tree of data to form a Merkle Tree (or trie). Zero-Knowledge (ZK): zero knowledge is defined above loosely as a set of practices where some data is revealed while other parts are kept secret. Many ZK methods are used in cryptography incuding Fiat Shamir, Proof of knowledge of discrete logarithms, ZK Snarks, and ZK Starks. Accumulators : a form of ZK, a cryptographic accumulator is a one-way membership function that answers a query as to whether a potential candidate is a member of a set without revealing the individual members of the set. Similar to a one-way hash function, cryptographic accumulators generate a fixed-size digest representing an arbitrarily large set of values. Some further provide a fixed-size witness for any value of the set, which can be used together with the accumulated digest to verify its membership in the set. Commitment : a cryptographic commitment, which allows one to commit to a chosen value (or chosen statement) while keeping it hidden to others, with the ability to reveal the committed value later. Witness : a term that has different applications in cryptography. In this paper, a witness is a value used in a cryptographic accumulator. In Bitcoin the unlocking signature is called the \"witness data.\" Quantum Computing and Cryptography: as quantum computing is developed, it poses a threat to the difficulty of puzzles. For example, they are likely to be much faster at determining if a number is truly prime or not.","title":"Ten Crypto Concepts"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#appendix-c-drinking-age-credential-implementation","text":"The birthday of an individual is formatted into a verifiable credential, which can be inspected to reveal the age of the credential holder without revealing their birthdate. The flow described here is based on the developing Verifiable Claims standard of the W3C Credentials Community Group. It uses cryptography developed by Jan Camenisch, as implemented by Sovrin. This is a work in progress. Note that other types of crypto could be applied to achieve the same privacy preserving goals.","title":"Appendix C: Drinking Age Credential Implementation"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#communication-flow","text":"The flow below may be copies and pasted into the Web Sequence Diagram webpage to generate a flow diagram. title Verifiable credential using Selective Disclosure participant Valid Time Oracle participant Janet participant ID Provider participant Ledger participant Bar note over Janet:Prover note over Bar:Validator note over Janet,Bar: Preparation and Setup note right of ID Provider:Infrastructure ID Provider->Ledger: Define Schema (Name, Birthdate, Address) ID Provider->Ledger: credential Definition (Pub Key, etc.) ID Provider->ID Provider: Generate Prv Key for this credential ID Provider->Ledger:Revocation Registry note left of Bar: Prepare to accept credentials Bar->Bar:Install Agent Bar->Ledger: Check schema note over Janet,Bar: Begin Use Case Janet->ID Provider: Request ID ID Provider-->Janet: ID will be issued as a digital credential note right of Janet: Prepare to receive credentials Janet->Janet: Install Agent Janet->Janet: Prv Key Generate, Store Janet->Ledger:Check Schema Ledger->Janet:credential Definition Janet-->ID Provider:Proof of Name, Birthdate, Address Janet->ID Provider: Blinded secret ID Provider->Janet: credential Janet->Janet: Validate credential against credential Def note over Janet,Bar: Janet goes to the bar note left of Bar: Can Janet Enter? Bar->Janet: Request Proof of Age Janet->Valid Time Oracle: Get time Valid Time Oracle->Janet: Time credential Janet->Janet:Generate Proof (This person is over 21) Janet->Bar: Provide Proof Bar->Bar: Evaluate proof Bar->Ledger: Verify on Ledger Ledger->Bar: Verification Bar->Janet: Come in note left of Bar: Invite to club Bar->Janet: Join loyalty club? (requires valid postal code) Janet->Janet:Generate Proof (postal code) Janet->Bar: Provide Proof Bar->Bar: Evaluate proof Bar->Ledger: Verify on Ledger Ledger->Bar: Verification Bar->Janet: Have Loyalty Card","title":"Communication Flow"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#crypto-details","text":"Below are some of the detailed mathematics involved in issuing a verifiable credential as implemented by Sovrin, a non-profit organization dedicated to managing a decentralized, public network for the purposes of self-sovereign identity.","title":"Crypto Details"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#issuer-setup","text":"The following setup is a necessary precursor to issuing a privacy-preserving credential.","title":"Issuer Setup"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#compute","text":"Perform the mathematical calculations required to curate the essential ingredients of the operations we are about to perform. Some of these results, like the private keys, are very sensitive and must be kept secret by the credential holder; others are to be shared. Random \ud835\udcf9', \ud835\udcfa', 1024-bit prime numbers, such that \ud835\udcf9 = 2\ud835\udcf9' + 1 and \ud835\udcfa = 2\ud835\udcfa' + 1 are both 1024-bit prime numbers. \ud835\udcf7 = \ud835\udcf9\ud835\udcfa. Random quadratic residue: \ud835\udce2 mod \ud835\udcf7 Random \ud835\udce7 \ud835\udce9 , \ud835\udce7 \ud835\udce11 , . . . , \ud835\udce7 \ud835\udce1\ud835\udcf5 \u2208 [2: \ud835\udcf9'\ud835\udcfa' - 1], where \ud835\udcf5 is the number of attributes in the credential. \ud835\udce9 = \ud835\udce2 \ud835\udce7\ud835\udce9 mod \ud835\udcf7 \ud835\udce1 \ud835\udcf2 = \ud835\udce2 \ud835\udce7\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 Issuer private key \ud835\udcfc\ud835\udcf4 \ud835\udcec = \ud835\udcf9'\ud835\udcfa' Issuer public key \ud835\udcf9\ud835\udcf4 \ud835\udcec = {\ud835\udcf7, \ud835\udce2, \ud835\udce9, \ud835\udce1 1 , . . . , \ud835\udce1 \ud835\udcf5 }","title":"Compute"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#proof-of-correctness","text":"As a result of the above computations, we then curate the following. This proof, along with the public keys, is the computational algorithm that will be used to validate the credential. Random \ud835\udce7' \ud835\udce9 , \ud835\udce7' \ud835\udce11 , . . . , \ud835\udce7' \ud835\udce1\ud835\udcf5 \u2208 [2: \ud835\udcf9'\ud835\udcfa' - 1] \ud835\udce9' = \ud835\udce2 \ud835\udce7'\ud835\udce9 mod \ud835\udcf7 \ud835\udce1' \ud835\udcf2 = \ud835\udce2 \ud835\udce7'\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 \ud835\udcec = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce9 || \ud835\udce1 1 || . . . || \ud835\udce1 \ud835\udcf5 || \ud835\udce9' || \ud835\udce1' 1 || . . . || \ud835\udce1' \ud835\udcf5 ) \ud835\udce7'' \ud835\udce9 = \ud835\udce7' \ud835\udce9 + \ud835\udcec \ud835\udce7 \ud835\udce9 \ud835\udce7'' \ud835\udce1\ud835\udcf2 = \ud835\udce7' \ud835\udce1\ud835\udcf2 + \ud835\udcec \ud835\udce7 \ud835\udce1\ud835\udcf2 , 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 The Cred Def is comprised of the public key and the proof of correctness; this is published to the distributed ledger.","title":"Proof of Correctness"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#issuing-a-credential","text":"With setup complete, we can now issue the credential in a privacy-preserving manner.","title":"Issuing a Credential"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#for-each-credential","text":"For each credential issued, perform the following operations.","title":"For Each Credential"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#issuer-computes","text":"A cryptographic accumulator is constructed in order to enable zero-knowledge queries further on. It is a one-way membership function, including the claim in the membership set. The operation can then answers a query as to whether a potential candidate is a member of a set without revealing the individual members of the set. \ud835\udcd0 \ud835\udcf2 = accumulator index \ud835\udce4 \ud835\udcf2 = user index \ud835\udcf6 2 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udcd0 \ud835\udcf2 || \ud835\udce4 \ud835\udcf2 ) 256-bit integer representations of each of the attributes: \ud835\udcf6 3 , . . . , \ud835\udcf6 \ud835\udcf5 \ud835\udcf7 0 = nonce","title":"Issuer Computes"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#issuer-sends-n0-to-prover","text":"This nonce is provided to the Prover for calculation of the Prover's proof of correctness.","title":"Issuer Sends \ud835\udcf70 to Prover"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#prover-receives-n0-and-computes-the-following","text":"The prover aggregates and prepares public keys for use in validating the signatures. The prover also commits to a chosen value while keeping it temporarily hidden, making the calculation binding. Retrieves Issuer\u2019s public key \ud835\udcf9\ud835\udcf4 \ud835\udcec Retrieves Issuer\u2019s proof of correctness Generates: \ud835\udcf6 1 = pedersen commitment of claim link secret Random \ud835\udcff', \ud835\udcff'', \ud835\udcf6' 1 \ud835\udcf7 1 = nonce","title":"Prover Receives \ud835\udcf70 and Computes the Following"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#prover-verifies-the-issuers-proof-of-correctness","text":"\ud835\udce9^ = \ud835\udce9 \ud835\udcec \ud835\udce2 \ud835\udce7''\ud835\udce9 mod \ud835\udcf7 \ud835\udce1^ \ud835\udcf2 = \ud835\udce1 \ud835\udcf2 \ud835\udcec \ud835\udce2 \ud835\udce7''\ud835\udce1\ud835\udcf2 mod \ud835\udcf7, 1 \u2264 \ud835\udcf2 \u2264 \ud835\udcf5 Verifies \ud835\udcec = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce9 || \ud835\udce1 1 || . . . || \ud835\udce1 \ud835\udcf5 || \ud835\udce9^ || \ud835\udce1^ 1 || . . . || \ud835\udce1^ \ud835\udcf5 )","title":"Prover Verifies the Issuer\u2019s Proof of Correctness"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#prover-computes","text":"\ud835\udce4 = \ud835\udce2 \ud835\udcff\u2019 \ud835\udce1 1 \ud835\udcf61 mod \ud835\udcf7 \ud835\udce4\u2019 = \ud835\udce2 \ud835\udcff\u2019\u2019 \ud835\udce1 1 \ud835\udcf6\u20191 mod \ud835\udcf7 \ud835\udcec\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce4 || \ud835\udce4\u2019 || \ud835\udcf7 0 ) \ud835\udcff^ = \ud835\udcff\u2019\u2019 + \ud835\udcec\u2019\ud835\udcff\u2019 \ud835\udcf6^ 1 = \ud835\udcf6\u2019 1 + \ud835\udcec\u2019\ud835\udcf6 1","title":"Prover Computes"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#prover-sends-p-u-c-v-m1-n1-to-the-issuer","text":"","title":"Prover Sends \ud835\udcdf = { \ud835\udce4, \ud835\udcec\u2019, \ud835\udcff^, \ud835\udcf6^1, \ud835\udcf71 } to the Issuer"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#issuer-verifies-prover-setup","text":"Computes \ud835\udce4^ = \ud835\udce4 -\ud835\udcec \ud835\udce2 \ud835\udcff^ \ud835\udce1 1 \ud835\udcf6^ 1 mod \ud835\udcf7 Verifies \ud835\udcec\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 ( \ud835\udce4 || \ud835\udce4^ || \ud835\udcf7 0 )","title":"Issuer Verifies Prover Setup"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#issuer-signs-the-credential-by-computing-the-following","text":"\ud835\udce0 = \ud835\udce9 / (\ud835\udce4\ud835\udce2 \ud835\udcff* \ud835\udce1 2 \ud835\udcf62 \ud835\udce1 3 \ud835\udcf63 \u00b7\u00b7\u00b7 \ud835\udce1 \ud835\udcf5 \ud835\udcf6\ud835\udcf5 ) mod \ud835\udcf7 \ud835\udced = \ud835\udcee -1 mod \ud835\udcf9\u2019\ud835\udcfa\u2019 \ud835\udcd0 = \ud835\udce0 \ud835\udced mod \ud835\udcf7 \ud835\udcd0\u2019 = \ud835\udce0 \ud835\udcfb mod \ud835\udcf7 \ud835\udcec\u2019\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 (\ud835\udce0 || \ud835\udcd0 || \ud835\udcd0\u2019|| \ud835\udcf7 1 ) \ud835\udcfc \ud835\udcee = (\ud835\udcfb - \ud835\udcec\u2019\u2019\ud835\udcee -1 ) mod \ud835\udcf9\u2019\ud835\udcfa\u2019","title":"Issuer Signs the Credential by Computing the Following"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#issuer-sends-o-a-e-v-se-c-m2-ml-to-the-prover","text":"","title":"Issuer Sends \ud835\udcde = {\ud835\udcd0, \ud835\udcee, \ud835\udcff*, \ud835\udcfc\ud835\udcee, \ud835\udcec\u2019\u2019, \ud835\udcf62, . . . , \ud835\udcf6\ud835\udcf5 } to the Prover"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#prover-receives-o-and-does-the-following","text":"","title":"Prover Receives \ud835\udcde and Does the Following"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#prover-computes_1","text":"\ud835\udcff = \ud835\udcff\u2019 + \ud835\udcff* \ud835\udce0\u2019 = \ud835\udce9 / (\ud835\udce2 \ud835\udcff \ud835\udce1 2 \ud835\udcf62 \ud835\udce1 3 \ud835\udcf63 \u00b7\u00b7\u00b7 \ud835\udce1 \ud835\udcf5 \ud835\udcf6\ud835\udcf5 ) mod \ud835\udcf7 \ud835\udced\u2019 = \ud835\udcec\u2019\u2019 + \ud835\udcfc \ud835\udcee \ud835\udcee \ud835\udcd0^ = \ud835\udcd0 \ud835\udced\u2019 \ud835\udce2 \ud835\udcff\u2019\ud835\udcfc\ud835\udcee mod \ud835\udcf7","title":"Prover Computes"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#prover-verifies","text":"\ud835\udcee is prime and 2 596 \u2264 \ud835\udcee \u2264 2 596 + 2 119 \ud835\udce0\u2019 = \ud835\udcd0 \ud835\udcee mod \ud835\udcf7 \ud835\udcec\u2019\u2019 = \ud835\udcd7\ud835\udcea\ud835\udcfc\ud835\udcf1 (\ud835\udce0\u2019 || \ud835\udcd0 || \ud835\udcd0^ || \ud835\udcf7 1 )","title":"Prover Verifies"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#prover-stores-primary-claim-m1-ml-a-e-v","text":"","title":"Prover Stores Primary Claim ({\ud835\udcf61, . . . , \ud835\udcf6\ud835\udcf5}, \ud835\udcd0, \ud835\udcee, \ud835\udcff)"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#for-additional-information","text":"The crypto used here is originally from the Identity Mixer. Link: (https://www.zurich.ibm.com/identity_mixer/) The Sovrin team shares additional information and working code at the following links. * Verifiable Credentials Code * Verifiable Credentials Example Usage in Python","title":"For Additional Information"},{"location":"rwot5/topics-and-advance-readings/Data-minimization-and-selective-disclosure/#references","text":"W3C Credentials Community Group Home Page Data Minimization and Selective Disclosure Repo Camenisch, Lysyanskaya. An Efficient System for Non-transferable Anonymous Credentials with Optional Anonymity Revocation Pfitzmann, Hansen. 2010. A terminology for talking about privacy by data minimization: Anonymity, Unlinkability, Undetectability, Unobservability, Pseudonymity, and Identity Management Cooper, Tschofenig, Aboba, Peterson, Morris, Hansen, Smith, Janet. 2013. RFC6973 . The draft can also be helpful, \"This document focuses on introducing terms used to describe privacy properties that support data minimization.\" Hansen, Tschofenig, Smith, Cooper. 2012 Privacy Terminology and Concepts. Network Working Group Internet-Draft Expires: September 13, 2012 Longley, Sporny. Redaction Signature Suite 2016. 26 June 2017. Draft Community Group Report \"This specification describes the Redaction Signature Suite created in 2016 for the Linked Data Signatures specification. It enables a sender to redact information in a message without invalidating the digital signature.\"","title":"References:"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/","text":"Author: Kyle Den Hartog Title: Framework for the Comparison of Identity Systems Date: 2017-05-12 Abstract: This paper intends to address the different types of identity systems and develop a method of comparing the identity systems. It is the goal of this paper to first identify the types of identity systems that have been created in the past as well as newly emerging identity systems. These identity systems can be used in many ways, but they often have many similarities allowing for the comparison of these identity systems. Outlined in this paper is a framework to make these comparisons by relying upon 4 categories that are apart of identity systems. They are verifiability, system architecture, accessibility, and security. Introduction: Throughout history, identity has come to mean many different things. An identity can be representative of a trait or many traits with which a person identifies with. It can also take on a third-party view in which other people choose traits to describe and identify another person other than themselves. In general, we will describe an identity as a collection of traits or properties used to describe a human or a group of humans to uniquely categorize a subset of a population. It should also be noted that an identity can also be used to describe things, however this paper will focus on human identities to build a framework for analyzing human identity systems. As new technologies have developed the methods of used to build systems of verifiable identity have changed. One of the earliest systems that predates National ID card systems, was an implementation put in place by Napoleon to streamline the capabilities of a centralized government. 1 As centralized governments progressed in their needs to identify their citizens they improved national ID systems eventually moving to a national ID card system. The ottoman empire was one of earliest adopters of a national ID card system launching it in 1844. 1 However, it wasn\u2019t until post World War II era that national ID card systems became a commonality between many countries. 1 Through the modernization of the computer, internet, and new sensors, we have seen many new systems put in place. With the usage of these new technologies, a different realm of identity exists built upon digital platforms. With these digital identities, humans can now maintain various pseudonymous identities. Pseudonymous identities such as email accounts, social media accounts, and even the emergence of fully anonymous identities have surged in recent years. One of the key uses today for identity systems, is to authenticate users through a variety of means. The three main ones are \u201cWhat you know\u201d (Passwords and Pins), \u201cWhat you have\u201d (Bluetooth key, NFC key, cell phones), and \u201cwhat you are\u201d (biometrics like fingerprints). Within the space of digital identities, we have seen organizational structures change. The main changes that have occurred is the shift away from centralized identity system architectures. Identity systems have shifted towards federated identity systems, delegated identity systems, and even some self-sovereign identity systems. 5 The key difference between these identity systems is who creates the identity and how the identity is validated. Traditionally with central identity systems, governments have been the ones to create and validate the identity, creating a low necessity for external verification. When federated identity systems emerged, digital identities begun to place more emphasis on cryptography as a means of validation relying upon protocols agreed upon by many parties. The primary purpose federated identity systems emerged was to reduce the number of identities a user may need when using the internet. With these new Federated identity systems, users could maintain fewer accounts while gaining access to more services on the internet. Similarly, delegated identity systems have emerged in recent years which operate very similarly to how federated identities are validated through cryptographic logic. The key difference between federated identities and delegated identities is that identities in a delegated system are created by a single party who then validates the identity for other service access. 2 This in turn pushes back towards centralization architecture of identity systems, which is much different from self-sovereign identities. With self-sovereign identity systems, a paradigm shift has occurred in how the identity system creates, validates, and verifies an identity. In a self-sovereign identity system, users now no longer need another party to create an identity, but rather create the identity themselves. 7 Another significant change is in the way a user\u2019s identity is accepted. In all the other types of identity systems, identities we\u2019re indirectly validated because they were created by a trusted 3rd party. However, in self-sovereign identity systems, identities are validated and trusted each time the identity is used. This happens because each time the identity is used it is verified by a new party among the system, which over time validates and gives more credibility to the identity. As such, self-sovereign identity systems create a system where trust is built up over time rather than being instantly granted. One of the more concerning issues that has arisen with digital identities is the issue of Sybil attacks in identity systems. 4 Sybil attacks occur because it is difficult to verify that a user has a single digital identity without a method of physical verification. This however is not always possible, which is where new projects have emerged to resolve this issue using video chats. 6 Benefits: In this paper, there are 4 main categories that we use to compare identity systems. They are verifiability, system architecture, accessibility, and security. Using these categories, the focus is to establish a framework that can be used to compare identity systems in a standardized way. These categories were selected as way to cover a broad basis of properties that emerged from many different identity systems. Throughout this paper, a scale is used to identify the ability of a system to meet the requirements. The scale will be as follows, high will be for an identity system that meets the criteria entirely. It is intended to be representative of an ideal identity system. Medium will be used to represent an identity system that meets some of these needs of an identity system or meets all the criteria, but creates friction in doing so. Lastly, we will use a low to indicate that the system does not meet the criteria in any capacity. This will allow for a broader range of identity systems to be analyzed. Verifiability Verifiability serves in important role in an identity system, particularly digital identity systems. Verification is one of the integral features of an identity system and must be quick, efficient, and decisive in its ability to verify a legitimate identity versus a falsified identity. The two subcategories of verifiability is Trusted and Verification method. Trusted source: The purpose in having a trusted source in an identity system is important to being able to guarantee the validity of the system. If falsified identities can be created, it degrades the integrity of all other identities in the system. As such the criteria for trusted source is as follows. * High: A user can independently determine its ability to trust another user * Medium: A user requires a trust link to verify another user * Low: A user cannot trust another user in the system The rationality of setting high as any node being able to independently verify another user is that it allows for the greatest level of ease as well as meeting the guarantee of verification. Medium is more representative of a centralized system which requires barriers of entry to verify another user. Last, a low is intended to be given to systems that have no trust and as such the identity system can be falsified. Verification method: The verification method criteria are intended to look specifically at how the verification of the data is conducted. This is important to be able to identify the level of ease a false identity would have in being able to pass as a legitimate identity. The ranks for verification methods are: * High: Mathematical proof * Medium: provides reasonable certainty of verification * Low: Verification doesn\u2019t provide certainty The rationality of setting high as a mathematical proof is that the system provides a perfect guarantee of the verification of the identity. With a perfect guarantee of the identity we can be certain the user is legitimate if the source of the identity didn\u2019t cheat the system. For medium, only reasonable certainty is needed to verify an identity of a system. Reasonable is subjective, so to better define reasonable, it shall be considered a system in which cheating the verification method requires an excessive amount of time to cheat once, or requires an excessive amount of resources to cheat the system once, or it is deemed highly economically inefficient to cheat the system once. The purpose in stating once is of importance because if it requires an excess once to cheat the system many times, the criteria of reasonable is not met. With Low, no verification is provided which makes the identity system weak in terms of its purpose. System Architecture: System architecture addresses the ways in which the system is architected. This is an important factor in any system and can be the difference between a successful and a failed identity system. Organizational structures: Organizational structure is an important to be considered in an identity system because it allows for subsets of a population larger than a single user to emerge. As such an identity system benefits immensely from the formation of organizational structures, otherwise known as groups, within the system. The ranking for organization structures is: * High: Organizational Structures are integrated into the system * Medium: Organizational structures can be formed by application of the system, but not integrated in the system * Low: Organizational structures cannot be formed by using this system High is given to a system that directly integrates organization structures directly into the system and builds around them. Medium is intended to be used for systems which have strong identities, but must conduct the implementation outside of the identity system. A low should be given to a system which doesn\u2019t allow organizational structures to form either through the system or outside of the system. Centralization: Centralization is a key feature of identity systems, more particularly digital identity systems which have utilized new structure formats. Centralization looks to address the structure of the identity system on a high level regarding the creation, and verification of identities. The rankings of centralization as a criterion is: * High: Decentralized allowing anyone apart of the system * Medium: Hierarchical with trusted parties used by the system * Low: Centralized trusted parties used by the system High has been given to a decentralized structure to allow for largest amount of control by the user of the identity. Medium has been given to identity systems that use hierarchy connecting trusted parties to establish trust in the identities of the system. Low has been given to systems that rely upon a central party to establish trust of the identities. Accessibility With identity, it is important that the identity has some accessibility guarantees. If the system is not accessible then the identities it impacts the usage of the system. Accessibility places an emphasis on how the system is used as well as the ability for the user to establish their control of their identity. Independence: Independence of an identity system is important to best adapt to the users of the actual identities. A system that is more adaptable to the users will provide better availability of the identities allowing for the integration of the identities in a better manor. The rankings of independence are: * High: Open source system changeable by system agreement * Medium: business controlled and developed, but open source and changeable by system agreement * Low: Business controlled and developed, proprietary system, licensed for use The high ranking places an emphasis on an open source system that allows for the system to be changed by the users as well as focusing on system agreement. System agreement is important to be able to keep the usage of the system. With medium, there is a focus on business controlled systems. By having the system controlled by a business as opposed to controlled by the users, it creates a vector for the system to be hijacked or steered in a direction against the will of the users. Low was given to an identity system that focuses on the proprietary aspects of a business which limits the adoption of the system. Devices: The focus of this subcategory is to establish the easy of the system to be integrated quickly. A system that can be deployed and requires less barriers of entry will allow the system greater access to the identity system. As such it is important to consider what is necessary to use the system which is what this criterion is for. The rankings for devices is as follows: * High: System provides all components necessary * Medium: System relies upon common devices to implement * Low: System requires users to purchase a new device to use the system An identity system that provides all necessary components or requires no components other than what is inherent to a person should receive a high in this criterion. A medium should be given to a system that relies upon common devices. This is a medium because it cannot guarantee that the system is adoptable by all people which could pose limitations for accessibility. A low should be given to a system which requires a new device to use the system. This creates a significant barrier on accessibility to the system. Deployability: If an identity system is too difficult to deploy or requires more work than current systems, it likely will not see a mass adoption. The accessibility of an identity system relies heavily on its ability to be deployed easily and efficiently by any user who wants to build on top of the identity system. The rankings for the criterion of deployability are: * High: Easy to adopt onto all current systems without changes being made * Medium: Requires some changes to current infrastructure to become widely adopted * Low: Requires \u201cscrap and replacing\u201d to adopt A high should be given to systems that can be easily used and adopted by any user who wishes to utilize the identity system. This grants the highest level of accessibility to the system. It is important to consider whether changes must be made to current infrastructure in order to adopt the system. If some changes must be made the system should be given a medium ranking. Last, a low should be given if the system requires a \u201cscrap and replace\u201d approach to implement the system. This will likely be costly pushing users away from the system and limiting its accessibility. Account recovery: Account recovery is important to the accessibility of a system because it is what allows for persistence of an identity if a user makes a mistake. As such the rankings for account recovery is as follows: * High: Account recovery is easy and secure * Medium: Account recovery is difficult and secure * Low: Account recovery is impossible or unsecure With account recovery, it is important to consider the tradeoff of convenience and security. A system that receives a high from this criterion should meet this tradeoff in an optimal fashion. A system that is not an optimal tradeoff between security and usability, and is still secure should receive a medium ranking. Last, a system that either does not allow for account recovery or is unsecure in the ability to recover an account should be considered low. A system with a low criterion can have a significant impact on accessibility to users. Portability: Portability looks at the accessibility of a system to be used in both physical applications and digital applications. As such the ranking for portability is as follows: * High: Able to be integrated with both physical and digital applications without additional requirements (nothing needed) * Medium: Able to be integrated with both physical and digital applications, but has additional requirements (devices, cards carried) * Low: Able to integrate with only digital or physical applications, but not both The purpose in ranking in this manor is to grant the greatest level of accessibility and as a system reduces how an identity system can be used in terms of physical and digital applications it reduces the accessibility of the identity system. Identity removal: Identity removal is important for the purposes of privacy of an identity system. Privacy provides some important properties, so a quality identity system must have a method to reconcile the removal of an identity. The levels of identity removal are: * High: A user may delete their identity from the system at anytime * Medium: A user may delete their identity with approval of a 3rd party * Low: A user is unable to delete an identity from the system The high ranking should be given to an identity system that allows for the easy removal of an identity from the system allowing for the user to have full control of the identity. Medium is a slight deviation from high in that to remove an identity from the system a user must gain 3rd party approval. This still allows for the user to delete an identity, however the 3rd party provides vectors for inefficiency and can potentially be abused. Last, low should be given to a system which does not allow the removal of an identity. This poses a threat to privacy. Security Security is an important part of an identity system as it maintains the trust in the system. A system that is secure is a system that can be relied upon. The criteria of security are integrity, confidentiality, theft prevention, data revocation, and anonymity. In this category, there is a large focus on the system\u2019s security as well as the control of the user data. Integrity: A system that provides integrity guarantees is important because it shows that the data that is being transacted is reliable data. This is an important factor in being able to trust the system, and be certain of its security as well. The rankings for integrity are: * High: Data history is immutable and all changes are recorded * Medium: Data is maintained by a trusted source with proper security controls * Low: Data has no integrity guarantees Through the integrity of this system a user will have full control of the data, but it can be verified that the data is kept in the state that it was intended. High ranking is intended to hold this by representing a system with a high level of data integrity because the history can be seen of when the data was created and changed. A medium ranking is given when a trusted source maintains the data securely. By relying upon a 3rd party it becomes possible that the data could be manipulated reducing the integrity of the data. Last, a low score is given to a system which disregards the integrity of the data of the identities. Confidentiality: Confidentiality is also an important point of security to consider. This focuses on user control of data. The rankings of confidentiality are: * High: Guaranteed protection through mathematical proof * Medium: Computationally unlikely to be broken * Low: Computationally likely to be broken A high ranking should be given to a system that provides mathematical proof that the system and its data is secure. This provides the strongest guarantees for the security of the system. A medium ranking should be given to identity systems that are computationally secure, but are not considered perfectly secure. Last, a low ranking should be given to an identity system which provides either no guarantees of confidentiality or utilizes deprecated processes that are easily broken. Theft prevention: Identity theft is important to address with an identity system. An identity system that is not secure to identity theft reduces the trust in the system as well as causes issues of concern for many other categories of this framework. The ranks for identity theft prevention are: * High: not possible * Medium: Difficult with little prevalence * Low: Easy and highly prevalent A system which can guarantee an identity cannot be miss represented or stolen will receive a ranking of high. A ranking of medium should be given to a system which allows for little prevalence of identity theft and is difficult to conduct. To meet the tolerance level should be very small (> 1% of the system) and anything with a greater amount of prevalence should be given a low ranking. Data revocation: Data revocation is important to the accessibility of an identity system because it places emphasis on the system\u2019s ability to remain under control of the user. If the data of a user can be stolen, or accessed in a way that the user did not intend it could impact the security of the identity severely. The rankings for data revocation are: * High: Data access can be shared and unshared by the identity holder * Medium: Data access is shareable, but not revocable * Low: All data is public, therefore irrevocable A system which grants more data control should be ranked higher in this criterion. As such a high should be given to a system that allows a user to take away all access to data shared at any point in the past and doesn\u2019t grant access to future data. A medium rank focuses on systems that allow some control of data, but do not allow for the system to remove data shared previously. A low should be given to a system that makes all data public as no control is given to the user. Anonymity: Anonymity is an important aspect to consider when it comes to privacy. A system that does not allow for privacy hurts the security of the system because it prevents users from remaining in control of their data as well as what they choose to share. The rankings for this criterion are: * High: Allows for Anonymous users to join the system * Medium: Anonymity is allowed, but the anonymous identity can be linked to the person * Low: Anonymity is not allowed or is easily broken The focus on high and medium is to address anonymity in terms of its difficulty to be broken. If a user\u2019s anonymity cannot be broken at all it should be given a high ranking. However, if it can be broken in some capacity, but is difficult or requires a large amount of resources, a medium ranking should be given. A low is to indicate that the identity system places little to no emphasis on anonymity. Conclusion: In conclusion, identity has grown throughout our society and the need for the different types of identity systems need to be assessed. This is important to place a focus on the ability to analyze and detect potential risks, threats, and problems of an identity system so that they can be improved upon. It is also important to consider that with so many teams working to build better identity systems, so it is important to compare these various systems. Sources: 1 Madsen, Paul, ed. \"Liberty ID-WSF People Service \u2013 Federated Social Identity.\" LIBERTY ALLIANCE PROJECT WHITE PAPER (n.d.): n. pag. 05 Dec. 2005. Web. 08 Apr. 2017. http://www.projectliberty.org/liberty/content/download/387/2720/file/Liberty_Federated_Social_Identity.pdf . 2 Shirky, Clay. \"Delegated vs. Federated ID.\" Nothing to See Here. Pennsylvania State University, 15 Feb. 2010. Web. 08 Apr. 2017. http://sites.psu.edu/ntsh/2010/02/15/delegated-vs-federated-id/ . 3 Jerzak, Connor T. \"A Brief History of National ID Cards.\" FXB Center for Health & Human Rights | Harvard University. FXB Center for Health and Human Rights, 12 Nov. 2015. Web. 08 Apr. 2017. 4 Yu, Haifeng, Michael Kaminsky, Phillip B. Gibbons, and Abraham Flaxman. \"SybilGuard: Defending Against Sybil Attacks via Social Networks.\" SIGCOMM (2006): n. pag. Web. 15 Apr. 2017. http://www.math.cmu.edu/~adf/research/SybilGuard.pdf . 5 Allen, Christopher. \" The Path to Self-Sovereign Identity\u201d. Life With Alacrity. N.p., 25 Apr. 2016. Web. 15 Apr. 2017. http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html . 6 \"Anti-Sybil Protocol Using Virtual Pseudonym Parties.\" Proofofindividuality.online. N.p., n.d. Web. 18 Apr. 2017. http://proofofindividuality.online/ . 7 Lundkvist, Christian, Rouven Heck, Joel Torstensson, Zac Mitton, and Michael Sena. \"UPORT: A PLATFORM FOR SELF-SOVEREIGN IDENTITY.\" UPORT: A PLATFORM FOR SELF-SOVEREIGN IDENTITY (2017): n. pag. Uport.me. 21 Feb. 2017. Web. 18 Apr. 2017. https://whitepaper.uport.me/uPort_whitepaper_DRAFT20170221.pdf . 8 Mittal, Prateek, Matthew Wright, and Nikita Borisov. \"Analyzing an Adaptive Reputation Metric for Anonymity Systems.\" HotSoS (2014): n. pag. Hatswitch.org. Web. 07 May 17. http://hatswitch.org/~nikita/papers/anon-reputation-hotsos14.pdf . 9 Shrier, David, Weige Wu, and Alex Pentland. \"MIT: Blockchain and Infrastructure (Identity & Data Security, Part 3).\" Transportation Infrastructure Security Utilizing Intelligent Transportation Systems (n.d.): 14-30. Getsmarter.com. MIT, 17 May 2016. Web. 07 May 2017. https://www.getsmarter.com/career-advice/wp-content/uploads/2016/12/mit_blockchain_and_infrastructure_report.pdf . 10 TSA. \"Transportation Security Timeline.\" Transportation Security Administration. N.p., n.d. Web. 07 May 2017. 11 ID2020. N.p., n.d. Web. 07 May 2017. http://id2020.org/home .","title":"Framework for Comparison of Identity Systems"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#author-kyle-den-hartog","text":"","title":"Author: Kyle Den Hartog"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#title-framework-for-the-comparison-of-identity-systems","text":"","title":"Title: Framework for the Comparison of Identity Systems"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#date-2017-05-12","text":"","title":"Date: 2017-05-12"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#abstract","text":"This paper intends to address the different types of identity systems and develop a method of comparing the identity systems. It is the goal of this paper to first identify the types of identity systems that have been created in the past as well as newly emerging identity systems. These identity systems can be used in many ways, but they often have many similarities allowing for the comparison of these identity systems. Outlined in this paper is a framework to make these comparisons by relying upon 4 categories that are apart of identity systems. They are verifiability, system architecture, accessibility, and security.","title":"Abstract:"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#introduction","text":"Throughout history, identity has come to mean many different things. An identity can be representative of a trait or many traits with which a person identifies with. It can also take on a third-party view in which other people choose traits to describe and identify another person other than themselves. In general, we will describe an identity as a collection of traits or properties used to describe a human or a group of humans to uniquely categorize a subset of a population. It should also be noted that an identity can also be used to describe things, however this paper will focus on human identities to build a framework for analyzing human identity systems. As new technologies have developed the methods of used to build systems of verifiable identity have changed. One of the earliest systems that predates National ID card systems, was an implementation put in place by Napoleon to streamline the capabilities of a centralized government. 1 As centralized governments progressed in their needs to identify their citizens they improved national ID systems eventually moving to a national ID card system. The ottoman empire was one of earliest adopters of a national ID card system launching it in 1844. 1 However, it wasn\u2019t until post World War II era that national ID card systems became a commonality between many countries. 1 Through the modernization of the computer, internet, and new sensors, we have seen many new systems put in place. With the usage of these new technologies, a different realm of identity exists built upon digital platforms. With these digital identities, humans can now maintain various pseudonymous identities. Pseudonymous identities such as email accounts, social media accounts, and even the emergence of fully anonymous identities have surged in recent years. One of the key uses today for identity systems, is to authenticate users through a variety of means. The three main ones are \u201cWhat you know\u201d (Passwords and Pins), \u201cWhat you have\u201d (Bluetooth key, NFC key, cell phones), and \u201cwhat you are\u201d (biometrics like fingerprints). Within the space of digital identities, we have seen organizational structures change. The main changes that have occurred is the shift away from centralized identity system architectures. Identity systems have shifted towards federated identity systems, delegated identity systems, and even some self-sovereign identity systems. 5 The key difference between these identity systems is who creates the identity and how the identity is validated. Traditionally with central identity systems, governments have been the ones to create and validate the identity, creating a low necessity for external verification. When federated identity systems emerged, digital identities begun to place more emphasis on cryptography as a means of validation relying upon protocols agreed upon by many parties. The primary purpose federated identity systems emerged was to reduce the number of identities a user may need when using the internet. With these new Federated identity systems, users could maintain fewer accounts while gaining access to more services on the internet. Similarly, delegated identity systems have emerged in recent years which operate very similarly to how federated identities are validated through cryptographic logic. The key difference between federated identities and delegated identities is that identities in a delegated system are created by a single party who then validates the identity for other service access. 2 This in turn pushes back towards centralization architecture of identity systems, which is much different from self-sovereign identities. With self-sovereign identity systems, a paradigm shift has occurred in how the identity system creates, validates, and verifies an identity. In a self-sovereign identity system, users now no longer need another party to create an identity, but rather create the identity themselves. 7 Another significant change is in the way a user\u2019s identity is accepted. In all the other types of identity systems, identities we\u2019re indirectly validated because they were created by a trusted 3rd party. However, in self-sovereign identity systems, identities are validated and trusted each time the identity is used. This happens because each time the identity is used it is verified by a new party among the system, which over time validates and gives more credibility to the identity. As such, self-sovereign identity systems create a system where trust is built up over time rather than being instantly granted. One of the more concerning issues that has arisen with digital identities is the issue of Sybil attacks in identity systems. 4 Sybil attacks occur because it is difficult to verify that a user has a single digital identity without a method of physical verification. This however is not always possible, which is where new projects have emerged to resolve this issue using video chats. 6","title":"Introduction:"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#benefits","text":"In this paper, there are 4 main categories that we use to compare identity systems. They are verifiability, system architecture, accessibility, and security. Using these categories, the focus is to establish a framework that can be used to compare identity systems in a standardized way. These categories were selected as way to cover a broad basis of properties that emerged from many different identity systems. Throughout this paper, a scale is used to identify the ability of a system to meet the requirements. The scale will be as follows, high will be for an identity system that meets the criteria entirely. It is intended to be representative of an ideal identity system. Medium will be used to represent an identity system that meets some of these needs of an identity system or meets all the criteria, but creates friction in doing so. Lastly, we will use a low to indicate that the system does not meet the criteria in any capacity. This will allow for a broader range of identity systems to be analyzed.","title":"Benefits:"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#verifiability","text":"Verifiability serves in important role in an identity system, particularly digital identity systems. Verification is one of the integral features of an identity system and must be quick, efficient, and decisive in its ability to verify a legitimate identity versus a falsified identity. The two subcategories of verifiability is Trusted and Verification method. Trusted source: The purpose in having a trusted source in an identity system is important to being able to guarantee the validity of the system. If falsified identities can be created, it degrades the integrity of all other identities in the system. As such the criteria for trusted source is as follows. * High: A user can independently determine its ability to trust another user * Medium: A user requires a trust link to verify another user * Low: A user cannot trust another user in the system The rationality of setting high as any node being able to independently verify another user is that it allows for the greatest level of ease as well as meeting the guarantee of verification. Medium is more representative of a centralized system which requires barriers of entry to verify another user. Last, a low is intended to be given to systems that have no trust and as such the identity system can be falsified. Verification method: The verification method criteria are intended to look specifically at how the verification of the data is conducted. This is important to be able to identify the level of ease a false identity would have in being able to pass as a legitimate identity. The ranks for verification methods are: * High: Mathematical proof * Medium: provides reasonable certainty of verification * Low: Verification doesn\u2019t provide certainty The rationality of setting high as a mathematical proof is that the system provides a perfect guarantee of the verification of the identity. With a perfect guarantee of the identity we can be certain the user is legitimate if the source of the identity didn\u2019t cheat the system. For medium, only reasonable certainty is needed to verify an identity of a system. Reasonable is subjective, so to better define reasonable, it shall be considered a system in which cheating the verification method requires an excessive amount of time to cheat once, or requires an excessive amount of resources to cheat the system once, or it is deemed highly economically inefficient to cheat the system once. The purpose in stating once is of importance because if it requires an excess once to cheat the system many times, the criteria of reasonable is not met. With Low, no verification is provided which makes the identity system weak in terms of its purpose.","title":"Verifiability"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#system-architecture","text":"System architecture addresses the ways in which the system is architected. This is an important factor in any system and can be the difference between a successful and a failed identity system. Organizational structures: Organizational structure is an important to be considered in an identity system because it allows for subsets of a population larger than a single user to emerge. As such an identity system benefits immensely from the formation of organizational structures, otherwise known as groups, within the system. The ranking for organization structures is: * High: Organizational Structures are integrated into the system * Medium: Organizational structures can be formed by application of the system, but not integrated in the system * Low: Organizational structures cannot be formed by using this system High is given to a system that directly integrates organization structures directly into the system and builds around them. Medium is intended to be used for systems which have strong identities, but must conduct the implementation outside of the identity system. A low should be given to a system which doesn\u2019t allow organizational structures to form either through the system or outside of the system. Centralization: Centralization is a key feature of identity systems, more particularly digital identity systems which have utilized new structure formats. Centralization looks to address the structure of the identity system on a high level regarding the creation, and verification of identities. The rankings of centralization as a criterion is: * High: Decentralized allowing anyone apart of the system * Medium: Hierarchical with trusted parties used by the system * Low: Centralized trusted parties used by the system High has been given to a decentralized structure to allow for largest amount of control by the user of the identity. Medium has been given to identity systems that use hierarchy connecting trusted parties to establish trust in the identities of the system. Low has been given to systems that rely upon a central party to establish trust of the identities.","title":"System Architecture:"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#accessibility","text":"With identity, it is important that the identity has some accessibility guarantees. If the system is not accessible then the identities it impacts the usage of the system. Accessibility places an emphasis on how the system is used as well as the ability for the user to establish their control of their identity. Independence: Independence of an identity system is important to best adapt to the users of the actual identities. A system that is more adaptable to the users will provide better availability of the identities allowing for the integration of the identities in a better manor. The rankings of independence are: * High: Open source system changeable by system agreement * Medium: business controlled and developed, but open source and changeable by system agreement * Low: Business controlled and developed, proprietary system, licensed for use The high ranking places an emphasis on an open source system that allows for the system to be changed by the users as well as focusing on system agreement. System agreement is important to be able to keep the usage of the system. With medium, there is a focus on business controlled systems. By having the system controlled by a business as opposed to controlled by the users, it creates a vector for the system to be hijacked or steered in a direction against the will of the users. Low was given to an identity system that focuses on the proprietary aspects of a business which limits the adoption of the system. Devices: The focus of this subcategory is to establish the easy of the system to be integrated quickly. A system that can be deployed and requires less barriers of entry will allow the system greater access to the identity system. As such it is important to consider what is necessary to use the system which is what this criterion is for. The rankings for devices is as follows: * High: System provides all components necessary * Medium: System relies upon common devices to implement * Low: System requires users to purchase a new device to use the system An identity system that provides all necessary components or requires no components other than what is inherent to a person should receive a high in this criterion. A medium should be given to a system that relies upon common devices. This is a medium because it cannot guarantee that the system is adoptable by all people which could pose limitations for accessibility. A low should be given to a system which requires a new device to use the system. This creates a significant barrier on accessibility to the system. Deployability: If an identity system is too difficult to deploy or requires more work than current systems, it likely will not see a mass adoption. The accessibility of an identity system relies heavily on its ability to be deployed easily and efficiently by any user who wants to build on top of the identity system. The rankings for the criterion of deployability are: * High: Easy to adopt onto all current systems without changes being made * Medium: Requires some changes to current infrastructure to become widely adopted * Low: Requires \u201cscrap and replacing\u201d to adopt A high should be given to systems that can be easily used and adopted by any user who wishes to utilize the identity system. This grants the highest level of accessibility to the system. It is important to consider whether changes must be made to current infrastructure in order to adopt the system. If some changes must be made the system should be given a medium ranking. Last, a low should be given if the system requires a \u201cscrap and replace\u201d approach to implement the system. This will likely be costly pushing users away from the system and limiting its accessibility. Account recovery: Account recovery is important to the accessibility of a system because it is what allows for persistence of an identity if a user makes a mistake. As such the rankings for account recovery is as follows: * High: Account recovery is easy and secure * Medium: Account recovery is difficult and secure * Low: Account recovery is impossible or unsecure With account recovery, it is important to consider the tradeoff of convenience and security. A system that receives a high from this criterion should meet this tradeoff in an optimal fashion. A system that is not an optimal tradeoff between security and usability, and is still secure should receive a medium ranking. Last, a system that either does not allow for account recovery or is unsecure in the ability to recover an account should be considered low. A system with a low criterion can have a significant impact on accessibility to users. Portability: Portability looks at the accessibility of a system to be used in both physical applications and digital applications. As such the ranking for portability is as follows: * High: Able to be integrated with both physical and digital applications without additional requirements (nothing needed) * Medium: Able to be integrated with both physical and digital applications, but has additional requirements (devices, cards carried) * Low: Able to integrate with only digital or physical applications, but not both The purpose in ranking in this manor is to grant the greatest level of accessibility and as a system reduces how an identity system can be used in terms of physical and digital applications it reduces the accessibility of the identity system. Identity removal: Identity removal is important for the purposes of privacy of an identity system. Privacy provides some important properties, so a quality identity system must have a method to reconcile the removal of an identity. The levels of identity removal are: * High: A user may delete their identity from the system at anytime * Medium: A user may delete their identity with approval of a 3rd party * Low: A user is unable to delete an identity from the system The high ranking should be given to an identity system that allows for the easy removal of an identity from the system allowing for the user to have full control of the identity. Medium is a slight deviation from high in that to remove an identity from the system a user must gain 3rd party approval. This still allows for the user to delete an identity, however the 3rd party provides vectors for inefficiency and can potentially be abused. Last, low should be given to a system which does not allow the removal of an identity. This poses a threat to privacy.","title":"Accessibility"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#security","text":"Security is an important part of an identity system as it maintains the trust in the system. A system that is secure is a system that can be relied upon. The criteria of security are integrity, confidentiality, theft prevention, data revocation, and anonymity. In this category, there is a large focus on the system\u2019s security as well as the control of the user data. Integrity: A system that provides integrity guarantees is important because it shows that the data that is being transacted is reliable data. This is an important factor in being able to trust the system, and be certain of its security as well. The rankings for integrity are: * High: Data history is immutable and all changes are recorded * Medium: Data is maintained by a trusted source with proper security controls * Low: Data has no integrity guarantees Through the integrity of this system a user will have full control of the data, but it can be verified that the data is kept in the state that it was intended. High ranking is intended to hold this by representing a system with a high level of data integrity because the history can be seen of when the data was created and changed. A medium ranking is given when a trusted source maintains the data securely. By relying upon a 3rd party it becomes possible that the data could be manipulated reducing the integrity of the data. Last, a low score is given to a system which disregards the integrity of the data of the identities. Confidentiality: Confidentiality is also an important point of security to consider. This focuses on user control of data. The rankings of confidentiality are: * High: Guaranteed protection through mathematical proof * Medium: Computationally unlikely to be broken * Low: Computationally likely to be broken A high ranking should be given to a system that provides mathematical proof that the system and its data is secure. This provides the strongest guarantees for the security of the system. A medium ranking should be given to identity systems that are computationally secure, but are not considered perfectly secure. Last, a low ranking should be given to an identity system which provides either no guarantees of confidentiality or utilizes deprecated processes that are easily broken. Theft prevention: Identity theft is important to address with an identity system. An identity system that is not secure to identity theft reduces the trust in the system as well as causes issues of concern for many other categories of this framework. The ranks for identity theft prevention are: * High: not possible * Medium: Difficult with little prevalence * Low: Easy and highly prevalent A system which can guarantee an identity cannot be miss represented or stolen will receive a ranking of high. A ranking of medium should be given to a system which allows for little prevalence of identity theft and is difficult to conduct. To meet the tolerance level should be very small (> 1% of the system) and anything with a greater amount of prevalence should be given a low ranking. Data revocation: Data revocation is important to the accessibility of an identity system because it places emphasis on the system\u2019s ability to remain under control of the user. If the data of a user can be stolen, or accessed in a way that the user did not intend it could impact the security of the identity severely. The rankings for data revocation are: * High: Data access can be shared and unshared by the identity holder * Medium: Data access is shareable, but not revocable * Low: All data is public, therefore irrevocable A system which grants more data control should be ranked higher in this criterion. As such a high should be given to a system that allows a user to take away all access to data shared at any point in the past and doesn\u2019t grant access to future data. A medium rank focuses on systems that allow some control of data, but do not allow for the system to remove data shared previously. A low should be given to a system that makes all data public as no control is given to the user. Anonymity: Anonymity is an important aspect to consider when it comes to privacy. A system that does not allow for privacy hurts the security of the system because it prevents users from remaining in control of their data as well as what they choose to share. The rankings for this criterion are: * High: Allows for Anonymous users to join the system * Medium: Anonymity is allowed, but the anonymous identity can be linked to the person * Low: Anonymity is not allowed or is easily broken The focus on high and medium is to address anonymity in terms of its difficulty to be broken. If a user\u2019s anonymity cannot be broken at all it should be given a high ranking. However, if it can be broken in some capacity, but is difficult or requires a large amount of resources, a medium ranking should be given. A low is to indicate that the identity system places little to no emphasis on anonymity.","title":"Security"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#conclusion","text":"In conclusion, identity has grown throughout our society and the need for the different types of identity systems need to be assessed. This is important to place a focus on the ability to analyze and detect potential risks, threats, and problems of an identity system so that they can be improved upon. It is also important to consider that with so many teams working to build better identity systems, so it is important to compare these various systems.","title":"Conclusion:"},{"location":"rwot5/topics-and-advance-readings/Framework-for-Comparison-of-Identity-Systems/#sources","text":"1 Madsen, Paul, ed. \"Liberty ID-WSF People Service \u2013 Federated Social Identity.\" LIBERTY ALLIANCE PROJECT WHITE PAPER (n.d.): n. pag. 05 Dec. 2005. Web. 08 Apr. 2017. http://www.projectliberty.org/liberty/content/download/387/2720/file/Liberty_Federated_Social_Identity.pdf . 2 Shirky, Clay. \"Delegated vs. Federated ID.\" Nothing to See Here. Pennsylvania State University, 15 Feb. 2010. Web. 08 Apr. 2017. http://sites.psu.edu/ntsh/2010/02/15/delegated-vs-federated-id/ . 3 Jerzak, Connor T. \"A Brief History of National ID Cards.\" FXB Center for Health & Human Rights | Harvard University. FXB Center for Health and Human Rights, 12 Nov. 2015. Web. 08 Apr. 2017. 4 Yu, Haifeng, Michael Kaminsky, Phillip B. Gibbons, and Abraham Flaxman. \"SybilGuard: Defending Against Sybil Attacks via Social Networks.\" SIGCOMM (2006): n. pag. Web. 15 Apr. 2017. http://www.math.cmu.edu/~adf/research/SybilGuard.pdf . 5 Allen, Christopher. \" The Path to Self-Sovereign Identity\u201d. Life With Alacrity. N.p., 25 Apr. 2016. Web. 15 Apr. 2017. http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html . 6 \"Anti-Sybil Protocol Using Virtual Pseudonym Parties.\" Proofofindividuality.online. N.p., n.d. Web. 18 Apr. 2017. http://proofofindividuality.online/ . 7 Lundkvist, Christian, Rouven Heck, Joel Torstensson, Zac Mitton, and Michael Sena. \"UPORT: A PLATFORM FOR SELF-SOVEREIGN IDENTITY.\" UPORT: A PLATFORM FOR SELF-SOVEREIGN IDENTITY (2017): n. pag. Uport.me. 21 Feb. 2017. Web. 18 Apr. 2017. https://whitepaper.uport.me/uPort_whitepaper_DRAFT20170221.pdf . 8 Mittal, Prateek, Matthew Wright, and Nikita Borisov. \"Analyzing an Adaptive Reputation Metric for Anonymity Systems.\" HotSoS (2014): n. pag. Hatswitch.org. Web. 07 May 17. http://hatswitch.org/~nikita/papers/anon-reputation-hotsos14.pdf . 9 Shrier, David, Weige Wu, and Alex Pentland. \"MIT: Blockchain and Infrastructure (Identity & Data Security, Part 3).\" Transportation Infrastructure Security Utilizing Intelligent Transportation Systems (n.d.): 14-30. Getsmarter.com. MIT, 17 May 2016. Web. 07 May 2017. https://www.getsmarter.com/career-advice/wp-content/uploads/2016/12/mit_blockchain_and_infrastructure_report.pdf . 10 TSA. \"Transportation Security Timeline.\" Transportation Security Administration. N.p., n.d. Web. 07 May 2017. 11 ID2020. N.p., n.d. Web. 07 May 2017. http://id2020.org/home .","title":"Sources:"},{"location":"rwot5/topics-and-advance-readings/Identifying-challenges-in-the-digital-economy/","text":"Identifying stakeholders' challenges in the digital economy By Irene Hernandez Existing identity management systems do not work in the digital economy. Internet\u2019s architecture mimicked the physical world, as issuers or central administrators were responsible for creating and managing large databases of validated user information. As a result, internet-based companies today require customers to a) fill out comprehensive online profiles with sensitive information and b) upload copies of physical documents to then spend millions of dollars in manual validation processes. For some companies, operating a large database of user data is an asset, but for many others is an expensive burden: 1. On-boarding user-facing processes are inefficient and cumbersome, resulting in poor customer experience. 2. New companies face high entry barriers, as they need to build trust on consumers for them to concede on providing sensitive attributes over the web. 3. Companies become an increasingly attractive target for security threats and, as such, have to invest in expensive security infrastructure. 4. Companies are unable to ensure that records are up to date. Thus, companies bear an obscure risk, as they cannot accurately assess the risk of doing business with their customers. 5. Data must be curated to allow for vertical integration with suppliers and partners, leading to errors and inefficiencies. 6. Despite all possible efforts, incorrect user information, poor authentication or fraud cannot be fully eliminated This is the case for many financial institutions like banks or investment management companies, insurance providers, governmental organizations, healthcare providers, P2P service platforms and eCommerce sites. The problem is acute for end users too, who have to manually create dozens if not hundreds of user accounts. This model has consequences: 1. Users are forced to give up on sensitive information repeated times, ultimately losing control of who is storing it. 2. Security risks are unknown, as there is no information about the number of copies made out of each attribute or what security measures are in place. 3. There are no standard procedures nor tools to audit the list of companies that store or have access to an individual\u2019s private data. 4. Data protection policies and regulations have borders. Hence, deleting customer records is not guaranteed nor is simple. 5. Updating attributes is inconvenient and leads to operational problems for both the company and individuals \u2013e.g. two-factor authentication problems due to dated phone number. 6. Individuals are forced to resign the right to monetize their own information. Since information of one single individual is not worthy, companies retain the power to offer aggregated data. We should radically change the way we structure our identification and authentication systems in the digital world. The self-sovereign identity concept coined by Christopher Allen solves most if not all of the end users\u2019 problems. Instead of multiple proprietary databases of users \u2013one per each organization-, we can think of a far more efficient process if we pursue a distributed architecture where individuals would hold a unique digital ID with a unique identifier and associated attributes. However, structural changes like this require massive adoption. Therefore, we need to address the challenges of both market sides and aim at bootstrapping a network not only of users but also of service providers. I claim that trusted, distributed, and global identification services are needed by organizations and users alike. They have the potential to unlock instant access to services worldwide, while keeping information secure, private and under customer\u2019s control. In return, companies can rely on trusted identification sources, overcome the advantage of traditional firms that enjoy broad confidence as guardians of information, lower entry barriers, and easily comply with regulatory requirements. Notwithstanding the benefits, many companies will have to overcome significant switching costs to capture the value created. So, how do we seamlessly transition to a decentralized model? I believe that the answer lies in a fully integrated solution that incorporates the full stack of needs: Data structure standards Authentication Authorization Interoperability Service delivery Applications (e.g. data monetization, customer experience, credit scoring) The idea is to provide organizations with an identity-as-a-service solution that leverages the concept of self-sovereign identity. The properties of this wholesome identity management solution should meet the 10 principles listed in Christopher Allen & Shannon Appelcline\u2019s paper A Primer on Self-Sovereign Identity , so as to ensure users rights: Existence. Users must have an independent existence. Control. Users must control their identities. Access. Users must have access to their own data. Transparency. Systems and algorithms must be transparent. Persistence. Identities must be long-lived. Portability. Information and services about identity must be transportable. Interoperability. Identities should be as widely usable as possible. Consent. Users must agree to the use of their identity. Minimalization. Disclosure of claims must be minimized. Protection. The rights of users must be protected. I suggest the addition of three design principles to incorporate organization\u2019s needs: Simplicity. Companies must find it easy to integrate the service within their existing processes and legacy systems. Flexibility. Different users may have different sets of attributes. Individuals may decide what attributes they want to append and companies what attributes they need to request. Moreover, some attribute classes may only make sense in specific regions or different regulations may apply. Allowing for local libraries of attributes can help expand access. Validation. Attributes should be verifiable by third parties. They may be user-provided and not validated, user-provided and externally validated or externally provided and user accepted. In summary, for a new identity management architecture to succeed in the rooted business model underlying the digital economy, we must first understand the challenges faced by all players in the market and incorporate these in our design principles. Next steps shall include validating priorities for organizations, identifying the properties of a full stack, and testing a vertical solution.","title":"Identifying challenges in the digital economy"},{"location":"rwot5/topics-and-advance-readings/Identifying-challenges-in-the-digital-economy/#identifying-stakeholders-challenges-in-the-digital-economy","text":"By Irene Hernandez Existing identity management systems do not work in the digital economy. Internet\u2019s architecture mimicked the physical world, as issuers or central administrators were responsible for creating and managing large databases of validated user information. As a result, internet-based companies today require customers to a) fill out comprehensive online profiles with sensitive information and b) upload copies of physical documents to then spend millions of dollars in manual validation processes. For some companies, operating a large database of user data is an asset, but for many others is an expensive burden: 1. On-boarding user-facing processes are inefficient and cumbersome, resulting in poor customer experience. 2. New companies face high entry barriers, as they need to build trust on consumers for them to concede on providing sensitive attributes over the web. 3. Companies become an increasingly attractive target for security threats and, as such, have to invest in expensive security infrastructure. 4. Companies are unable to ensure that records are up to date. Thus, companies bear an obscure risk, as they cannot accurately assess the risk of doing business with their customers. 5. Data must be curated to allow for vertical integration with suppliers and partners, leading to errors and inefficiencies. 6. Despite all possible efforts, incorrect user information, poor authentication or fraud cannot be fully eliminated This is the case for many financial institutions like banks or investment management companies, insurance providers, governmental organizations, healthcare providers, P2P service platforms and eCommerce sites. The problem is acute for end users too, who have to manually create dozens if not hundreds of user accounts. This model has consequences: 1. Users are forced to give up on sensitive information repeated times, ultimately losing control of who is storing it. 2. Security risks are unknown, as there is no information about the number of copies made out of each attribute or what security measures are in place. 3. There are no standard procedures nor tools to audit the list of companies that store or have access to an individual\u2019s private data. 4. Data protection policies and regulations have borders. Hence, deleting customer records is not guaranteed nor is simple. 5. Updating attributes is inconvenient and leads to operational problems for both the company and individuals \u2013e.g. two-factor authentication problems due to dated phone number. 6. Individuals are forced to resign the right to monetize their own information. Since information of one single individual is not worthy, companies retain the power to offer aggregated data. We should radically change the way we structure our identification and authentication systems in the digital world. The self-sovereign identity concept coined by Christopher Allen solves most if not all of the end users\u2019 problems. Instead of multiple proprietary databases of users \u2013one per each organization-, we can think of a far more efficient process if we pursue a distributed architecture where individuals would hold a unique digital ID with a unique identifier and associated attributes. However, structural changes like this require massive adoption. Therefore, we need to address the challenges of both market sides and aim at bootstrapping a network not only of users but also of service providers. I claim that trusted, distributed, and global identification services are needed by organizations and users alike. They have the potential to unlock instant access to services worldwide, while keeping information secure, private and under customer\u2019s control. In return, companies can rely on trusted identification sources, overcome the advantage of traditional firms that enjoy broad confidence as guardians of information, lower entry barriers, and easily comply with regulatory requirements. Notwithstanding the benefits, many companies will have to overcome significant switching costs to capture the value created. So, how do we seamlessly transition to a decentralized model? I believe that the answer lies in a fully integrated solution that incorporates the full stack of needs: Data structure standards Authentication Authorization Interoperability Service delivery Applications (e.g. data monetization, customer experience, credit scoring) The idea is to provide organizations with an identity-as-a-service solution that leverages the concept of self-sovereign identity. The properties of this wholesome identity management solution should meet the 10 principles listed in Christopher Allen & Shannon Appelcline\u2019s paper A Primer on Self-Sovereign Identity , so as to ensure users rights: Existence. Users must have an independent existence. Control. Users must control their identities. Access. Users must have access to their own data. Transparency. Systems and algorithms must be transparent. Persistence. Identities must be long-lived. Portability. Information and services about identity must be transportable. Interoperability. Identities should be as widely usable as possible. Consent. Users must agree to the use of their identity. Minimalization. Disclosure of claims must be minimized. Protection. The rights of users must be protected. I suggest the addition of three design principles to incorporate organization\u2019s needs: Simplicity. Companies must find it easy to integrate the service within their existing processes and legacy systems. Flexibility. Different users may have different sets of attributes. Individuals may decide what attributes they want to append and companies what attributes they need to request. Moreover, some attribute classes may only make sense in specific regions or different regulations may apply. Allowing for local libraries of attributes can help expand access. Validation. Attributes should be verifiable by third parties. They may be user-provided and not validated, user-provided and externally validated or externally provided and user accepted. In summary, for a new identity management architecture to succeed in the rooted business model underlying the digital economy, we must first understand the challenges faced by all players in the market and incorporate these in our design principles. Next steps shall include validating priorities for organizations, identifying the properties of a full stack, and testing a vertical solution.","title":"Identifying stakeholders' challenges in the digital economy"},{"location":"rwot5/topics-and-advance-readings/RWOT-User-Story/","text":"#RebootingWebOfTrust User Story & Tech Concept By Christopher Allen \u2014 Copyright 2017 Licensed CC-BY Contributors: Shannon Appelcline Who is Alice? Alice is a first-generation citizen of a western country (United States, Britain, Germany), born of two immigrants from a country of significant repression (Iran, Cuba, North Korea) who are now permanent residents. Her parents were settled in a small city near the agricultural heart of the county (Midwestern United States, Midlands of Britain, Saxony of Germany) where they have become successful small business owners. As a child Alice was told stories of the abuses her parents suffered in their country of origin, a place they cannot return to. She knows some cousins and other family members in other countries who have told similar stories, but mostly through the internet or through traveling to the various countries that her extended family has settled in. Both Alice and her parents have experienced some unkind words and minor discrimination in their newly adopted country, but largely the benefits of being in a modern western country overwhelm any disadvantages. The parents have found others to practice their religion with; Alice respects this, but in general is more secular. Alice has recently graduated with a computer science degree, and has pleased her parents by settling nearby, working for a small regional bank. The bank is quite conservative and has a \u201cno social media\u201d policy at work. This rule is regularly broken, but there have been some object lessons, especially for those who stand out as non-conformists. However, the winds of populism are blowing. Alice has personally heard more slurs lately and has seen more covered the media. Many local political parties of rising significance have publicly targeted her ethnicity and her parents' religion. Added to the stories of repression in her parent\u2019s birth country and even greater stories of conflict reported by distant family members, Alice has some legitimate fear. Alice , like most millennials, doesn\u2019t wish to leave her head in the sand; she wishes to take a more active role in making a better world. However, she knows if she stands out from the crowd, then her activism may affect not only her, but also her parents and even her extended family abroad. Increasingly Alice has been more careful: self-censoring social media, using end-to-end encryption software to talk to her family abroad, etc. Bob may have an opportunity for Alice Bob , a mainstream citizen of his western country, is also worried about the rising tide of populism in his own country and of extremism in other countries. Raised by a middle-class family he has travelled widely as a young adult and was supported by his parents when he joined a non-profit organization as an employee. Bob is not a programmer, but he is an active user of social media and other internet technology and he has some ideas about how his non-profit could use mobile applications to serve their advocacy. He shares his ideas out in the world in a public forum. Alice reads this request, and has the skills to help Bob to make a a mobile applications. However, she has a dilemma: she wishes to introduce herself and present credentials that she is qualified for the work, but wants to remain pseudo-anonymous. She has heard of other professional women who have been public about work and have been heavily criticized or even doxxed (such as in gamergate), and she doesn\u2019t want to risk her job or her extended family if she is revealed to be helping Bob . She also doesn\u2019t have a reputation in the developer community as a mobile programmer. Finally, she has limited time to be able to help and so has to be focused and careful about what she can offer. Alice establishes a pseudo-anonymous identity So first Alice establishes a DID (Decentralized Identifier) and a self-signed DDO (DID Data Object). Now she needs proof of her mobile compentency. Alice has a professional colleague and friend Donna with a public persona that is not anonymous, who indirectly knows Bob through yet another colleague. That is, Bob and Donna share a trust network but are connected by multiple degrees of separation. Donna issues a Verifiable Claim that she \"knows\" Alice and she is willing to attest to her competence in mobile development. She then gives a signed copy of the claim back to Alice . Alice countersigns this claim and adds it to her DDO. (This is something unique to the very self-sovereign BTCR method, which may not apply to other methods.) Alice then sends a response to Bob's request for programming assistance, along with the claim issued by Donna . How can Bob learn to trust Alice? Now we dive into some mechanics of how the the did:btcr method of creating pseudo-anonymous identies operates. (Different DID methods will function differently.) Bob receives an offer based his request from Alice (possibly itself a self-signed claim), along with the claim issued by Donna . The first thing his software does is do an INTEGRITY CHECK of Donna's claim itself. Is it properly formed? Has it expired? Is it properly signed by the issuer? Is it properly countersigned by the subject? If it fails any of these INTEGRITY CHECKS, Bob will not even know about it, and the whole message and its claims will be rejected. The next thing that Bob's software might do is INSPECT INTO the DID numbers found in Donna claim. This will typically be automatic, but if Bob is hyper-concerned about internet traffic correlation (e.g., if he were a vulnerable citizen advocating against their nation-state) it may require a human to decide if they wish to proceed further. But Bob is an EU citizen and feels sufficiently protected, so his software is set to INSPECT INTO claims he receives automatically. The first DID is Donna's . His software INSPECTS INTO the Bitcoin Blockchain for the appropriate transaction, and then looks at the first TXOUT of that transaction to see if it has been spent. In this case, it has, so this transaction cannot be VALIDATED. However, the claim has not totally failed quite yet, so the software now goes forward to that new transaction (the \"tip\" of the DID chain). The software INSPECTS INTO this transaction's first TXOUT and it is not spent and there is a properly formatted op_return pointing to a DDO, which reside's on Donna's github account. The software now INSPECTS INTO and finds the DDO. Now the the software does an INTEGRITY CHECK on the DDO, and if it is successful, then it VALIDATES the DDO by comparing it to the Control Key that was found in the signature that was used to send the transaction to the Bitcoin Blockchain, which was in turn revealed by the INSPECTION CHECK. If they match, both the DID and the DDO are now VALIDATED. However, the claim itself was not signed by the Control Key so it is not VALIDATED yet. So the software INSPECTS INTO the DDO, and finds another key (either by looking through all the appropriately listed keys, or possibly because of a hint added in the claim). If the signature on the claim matches, now the claim issuer is VALIDATED. However, the claim makes a statement to yet other DID, so it not yet VERIFIED, only VALID. The software must now do the same set of operations on Alice's DID to INSPECT INTO her DID and determine if it too can be VALIDATED. Finally, if both the issuer's DID and subject's DID are VALID, (which includes the previous INTEGRITY CHECK of Donna's claim and Alice's countersignature of Donna's signature on the claim), then the claim is VERIFIED. (Thus it is called a \"Verifiable Claim\".) However, this verified claim is not yet CONFIRMED. In order to be CONFIRMED, Bob's Web-of-Trust CONFIRMATION criteria needs to be met. In this case, Donna is a third-degree connection, making Alice a fourth-degree connection. Over half of the world is a fourth-degree connection! In this case, the software kicks out the verified claim for Bob to make a decision on: the claim and DIDs are INSPECTED, VALIDATED and VERIFIED, but not CONFIRMED. He decides to look further into what Donna is willing to share in her DID. In this case, Donna is vaguely known to him (\"a familiar stranger\") and her github repository is active and has a long history of mobile development. He looks now at what Alice shares in her DID; it is almost nothing, and contains no personal info. However, her response to his request for a proposal is interesting, and he hasn't found anyone yet, so he decides to CONFIRM and accept this claim to give her a trial. If Alice fails her trial task, Bob will change his criteria to never waste any time on her again, or even possibly to never even bother to consider CONFIRMING any more of Donna's claims. This would be a locally-negative trust that is non-transitive to others in the self-sovereign scenarios required by the BTC method. However, Alice doesn't fail her trial, and later Bob issues her a new verifiable claim saying that he also liked Alice's work, and maybe even issues a claim that countersigns Donna's original claim, showing appreciation for Donna's good recommendation. Alice and the Syrian Hacker Army As our user story continues, Alice believes she may become targeted the Syrian Hacker Army as the author of an Android app that helps Syrian refugees communicate with the families back home and send them care packages through a random peer-to-peer network of people sharing their car trunk capacity: an Uber of small gifts, an AirBNB of trunk space. Alice has been careful with her cryptographic keys, but she has prepared herself. The special set of public keys that are associated with this app are listed in her master DDO, and the app itself has a DID and DDO associated with it that she owns and controls. Alice has a password protected QR-code of the Owner Key for the current DID and DDO, but the Control Keys to update the DDO to a new transaction is with a friend in England locked on a hard drive. Unfortunately, while traveling through Turkey, Alice's laptop is stolen. She is suspicious that someone might be targeting her. The first thing she could do is just revoke and abandon the DID by simply spending the tip address. However, she doesn't have the Control Keys to enable that DID Control Key rotation transaction\u2014they are in England. She does however hold the current Owner Key, so she can use that to update the DDO object that might be a possibly compromised key. She puts a \"hold\" notice in the DDO, and adds a timestamp to warn people to no longer rely on values in the current DID transaction until resolved, and wait for the new transaction on the tip. If the hold expires without an update, or if the tip address is simply spent, all customers of her mobile app will be notified the next time they run the app that they may have a version that has be compromised, and they will be asked delete it before downloading it again from a trusted source. Instead however, when she returns home to England she spends the tip, revoking the old Owner Key, and enrolling the old Control Key as the new Owner Key. A new address is posted which will be the next Control Key and the new \"tip\". She now updates the DDO itself, signing it with the Owner Key just enrolled. She has no evidence of any key compromise yet, but she has in her DDO claims issued using her revoked old Owner Key; these claims continue to be valid if they were dated before the time the key was stolen (though any claimants should be notified to request for updated claims if possible). As all of her claims are timestamps, there is no way for someone to fake a claim even if they break her old key, as the signature may be valid, but the timestamp is not. In this case the customers of her app will continue to be able to use the old app (as the claims before the date of change can still be confirmed), but when the customers request an update they will be given the updated keys in the new app. It is also possible that Alice could have set up her Control Key to be 2 of 4 multisig, with keys held by her trusted colleagues Bob , Carol and Donna . In this case, if two of them observed bad behavior in claims being issued by the old DID/DDO while she was traveling, or if she was able to call at least two of them, they could immediately rotate the Control Keys, and give her the new Control Key when she gets home. She maybe even already possesses the new Control Key, at it could have been generated by an xpub key her friends share. (However, this technique does require two transactions rather than one for every Control Key rotation.) Finally, at some point in the future Alice passes this mobile project on to another team to develop, and she gracefully \"retires\" the DID/DDO pair with a gentle revocation and no new possible Control Key. However, all of her non-expiring claims are still valid: she didn't issue many, but a few are important and they need to remain. Our use case will continue showing how Alice is able to demonstrate and share a positive reputation for her quality work. How Carol is able to use Alice\u2019s software to protect herself from harm in a third-world country. How Ted is able to review Bob\u2019s ideas and Alice\u2019s work, and offer some funding to help both. How Alice is able to increase her reputation in her anonymously developed software to the point where she is offered a job at Ted\u2019s company that pays more than her job at a conservative bank. How Alice is able to selectively revoke parts of her anonymity to establish her personal reputation her peers and selective colleagues, without at any time putting Carol and her other advocacy clients at risk. (HISTORY: The initial user story was originally written up as an issue in the W3Cs Verifiable Claims Use Cases repo, with a goal of using it to create a formal use case for new pseudo-anonymous web of trust. Alice's story continues, along with some web of trust concepts and some proposed technical details , in issue developed as part of the DID:BTCR Hackathon . This version is currently being further drafted as a Topic and Advance Reading for the Fifth #RebootingWebOfTrust in Boston on October 3-5, 2017 ( register here ). Any comments on this version please use post in these issues . )","title":"RebootingWebOfTrust User Story & Tech Concept"},{"location":"rwot5/topics-and-advance-readings/RWOT-User-Story/#rebootingweboftrust-user-story-tech-concept","text":"","title":"#RebootingWebOfTrust User Story &amp; Tech Concept"},{"location":"rwot5/topics-and-advance-readings/RWOT-User-Story/#by-christopher-allen-copyright-2017-licensed-cc-by","text":"Contributors: Shannon Appelcline","title":"By Christopher Allen \u2014 Copyright 2017 Licensed CC-BY"},{"location":"rwot5/topics-and-advance-readings/RWOT-User-Story/#who-is-alice","text":"Alice is a first-generation citizen of a western country (United States, Britain, Germany), born of two immigrants from a country of significant repression (Iran, Cuba, North Korea) who are now permanent residents. Her parents were settled in a small city near the agricultural heart of the county (Midwestern United States, Midlands of Britain, Saxony of Germany) where they have become successful small business owners. As a child Alice was told stories of the abuses her parents suffered in their country of origin, a place they cannot return to. She knows some cousins and other family members in other countries who have told similar stories, but mostly through the internet or through traveling to the various countries that her extended family has settled in. Both Alice and her parents have experienced some unkind words and minor discrimination in their newly adopted country, but largely the benefits of being in a modern western country overwhelm any disadvantages. The parents have found others to practice their religion with; Alice respects this, but in general is more secular. Alice has recently graduated with a computer science degree, and has pleased her parents by settling nearby, working for a small regional bank. The bank is quite conservative and has a \u201cno social media\u201d policy at work. This rule is regularly broken, but there have been some object lessons, especially for those who stand out as non-conformists. However, the winds of populism are blowing. Alice has personally heard more slurs lately and has seen more covered the media. Many local political parties of rising significance have publicly targeted her ethnicity and her parents' religion. Added to the stories of repression in her parent\u2019s birth country and even greater stories of conflict reported by distant family members, Alice has some legitimate fear. Alice , like most millennials, doesn\u2019t wish to leave her head in the sand; she wishes to take a more active role in making a better world. However, she knows if she stands out from the crowd, then her activism may affect not only her, but also her parents and even her extended family abroad. Increasingly Alice has been more careful: self-censoring social media, using end-to-end encryption software to talk to her family abroad, etc.","title":"Who is Alice?"},{"location":"rwot5/topics-and-advance-readings/RWOT-User-Story/#bob-may-have-an-opportunity-for-alice","text":"Bob , a mainstream citizen of his western country, is also worried about the rising tide of populism in his own country and of extremism in other countries. Raised by a middle-class family he has travelled widely as a young adult and was supported by his parents when he joined a non-profit organization as an employee. Bob is not a programmer, but he is an active user of social media and other internet technology and he has some ideas about how his non-profit could use mobile applications to serve their advocacy. He shares his ideas out in the world in a public forum. Alice reads this request, and has the skills to help Bob to make a a mobile applications. However, she has a dilemma: she wishes to introduce herself and present credentials that she is qualified for the work, but wants to remain pseudo-anonymous. She has heard of other professional women who have been public about work and have been heavily criticized or even doxxed (such as in gamergate), and she doesn\u2019t want to risk her job or her extended family if she is revealed to be helping Bob . She also doesn\u2019t have a reputation in the developer community as a mobile programmer. Finally, she has limited time to be able to help and so has to be focused and careful about what she can offer.","title":"Bob may have an opportunity for Alice"},{"location":"rwot5/topics-and-advance-readings/RWOT-User-Story/#alice-establishes-a-pseudo-anonymous-identity","text":"So first Alice establishes a DID (Decentralized Identifier) and a self-signed DDO (DID Data Object). Now she needs proof of her mobile compentency. Alice has a professional colleague and friend Donna with a public persona that is not anonymous, who indirectly knows Bob through yet another colleague. That is, Bob and Donna share a trust network but are connected by multiple degrees of separation. Donna issues a Verifiable Claim that she \"knows\" Alice and she is willing to attest to her competence in mobile development. She then gives a signed copy of the claim back to Alice . Alice countersigns this claim and adds it to her DDO. (This is something unique to the very self-sovereign BTCR method, which may not apply to other methods.) Alice then sends a response to Bob's request for programming assistance, along with the claim issued by Donna .","title":"Alice establishes a pseudo-anonymous identity"},{"location":"rwot5/topics-and-advance-readings/RWOT-User-Story/#how-can-bob-learn-to-trust-alice","text":"Now we dive into some mechanics of how the the did:btcr method of creating pseudo-anonymous identies operates. (Different DID methods will function differently.) Bob receives an offer based his request from Alice (possibly itself a self-signed claim), along with the claim issued by Donna . The first thing his software does is do an INTEGRITY CHECK of Donna's claim itself. Is it properly formed? Has it expired? Is it properly signed by the issuer? Is it properly countersigned by the subject? If it fails any of these INTEGRITY CHECKS, Bob will not even know about it, and the whole message and its claims will be rejected. The next thing that Bob's software might do is INSPECT INTO the DID numbers found in Donna claim. This will typically be automatic, but if Bob is hyper-concerned about internet traffic correlation (e.g., if he were a vulnerable citizen advocating against their nation-state) it may require a human to decide if they wish to proceed further. But Bob is an EU citizen and feels sufficiently protected, so his software is set to INSPECT INTO claims he receives automatically. The first DID is Donna's . His software INSPECTS INTO the Bitcoin Blockchain for the appropriate transaction, and then looks at the first TXOUT of that transaction to see if it has been spent. In this case, it has, so this transaction cannot be VALIDATED. However, the claim has not totally failed quite yet, so the software now goes forward to that new transaction (the \"tip\" of the DID chain). The software INSPECTS INTO this transaction's first TXOUT and it is not spent and there is a properly formatted op_return pointing to a DDO, which reside's on Donna's github account. The software now INSPECTS INTO and finds the DDO. Now the the software does an INTEGRITY CHECK on the DDO, and if it is successful, then it VALIDATES the DDO by comparing it to the Control Key that was found in the signature that was used to send the transaction to the Bitcoin Blockchain, which was in turn revealed by the INSPECTION CHECK. If they match, both the DID and the DDO are now VALIDATED. However, the claim itself was not signed by the Control Key so it is not VALIDATED yet. So the software INSPECTS INTO the DDO, and finds another key (either by looking through all the appropriately listed keys, or possibly because of a hint added in the claim). If the signature on the claim matches, now the claim issuer is VALIDATED. However, the claim makes a statement to yet other DID, so it not yet VERIFIED, only VALID. The software must now do the same set of operations on Alice's DID to INSPECT INTO her DID and determine if it too can be VALIDATED. Finally, if both the issuer's DID and subject's DID are VALID, (which includes the previous INTEGRITY CHECK of Donna's claim and Alice's countersignature of Donna's signature on the claim), then the claim is VERIFIED. (Thus it is called a \"Verifiable Claim\".) However, this verified claim is not yet CONFIRMED. In order to be CONFIRMED, Bob's Web-of-Trust CONFIRMATION criteria needs to be met. In this case, Donna is a third-degree connection, making Alice a fourth-degree connection. Over half of the world is a fourth-degree connection! In this case, the software kicks out the verified claim for Bob to make a decision on: the claim and DIDs are INSPECTED, VALIDATED and VERIFIED, but not CONFIRMED. He decides to look further into what Donna is willing to share in her DID. In this case, Donna is vaguely known to him (\"a familiar stranger\") and her github repository is active and has a long history of mobile development. He looks now at what Alice shares in her DID; it is almost nothing, and contains no personal info. However, her response to his request for a proposal is interesting, and he hasn't found anyone yet, so he decides to CONFIRM and accept this claim to give her a trial. If Alice fails her trial task, Bob will change his criteria to never waste any time on her again, or even possibly to never even bother to consider CONFIRMING any more of Donna's claims. This would be a locally-negative trust that is non-transitive to others in the self-sovereign scenarios required by the BTC method. However, Alice doesn't fail her trial, and later Bob issues her a new verifiable claim saying that he also liked Alice's work, and maybe even issues a claim that countersigns Donna's original claim, showing appreciation for Donna's good recommendation.","title":"How can Bob learn to trust Alice?"},{"location":"rwot5/topics-and-advance-readings/RWOT-User-Story/#alice-and-the-syrian-hacker-army","text":"As our user story continues, Alice believes she may become targeted the Syrian Hacker Army as the author of an Android app that helps Syrian refugees communicate with the families back home and send them care packages through a random peer-to-peer network of people sharing their car trunk capacity: an Uber of small gifts, an AirBNB of trunk space. Alice has been careful with her cryptographic keys, but she has prepared herself. The special set of public keys that are associated with this app are listed in her master DDO, and the app itself has a DID and DDO associated with it that she owns and controls. Alice has a password protected QR-code of the Owner Key for the current DID and DDO, but the Control Keys to update the DDO to a new transaction is with a friend in England locked on a hard drive. Unfortunately, while traveling through Turkey, Alice's laptop is stolen. She is suspicious that someone might be targeting her. The first thing she could do is just revoke and abandon the DID by simply spending the tip address. However, she doesn't have the Control Keys to enable that DID Control Key rotation transaction\u2014they are in England. She does however hold the current Owner Key, so she can use that to update the DDO object that might be a possibly compromised key. She puts a \"hold\" notice in the DDO, and adds a timestamp to warn people to no longer rely on values in the current DID transaction until resolved, and wait for the new transaction on the tip. If the hold expires without an update, or if the tip address is simply spent, all customers of her mobile app will be notified the next time they run the app that they may have a version that has be compromised, and they will be asked delete it before downloading it again from a trusted source. Instead however, when she returns home to England she spends the tip, revoking the old Owner Key, and enrolling the old Control Key as the new Owner Key. A new address is posted which will be the next Control Key and the new \"tip\". She now updates the DDO itself, signing it with the Owner Key just enrolled. She has no evidence of any key compromise yet, but she has in her DDO claims issued using her revoked old Owner Key; these claims continue to be valid if they were dated before the time the key was stolen (though any claimants should be notified to request for updated claims if possible). As all of her claims are timestamps, there is no way for someone to fake a claim even if they break her old key, as the signature may be valid, but the timestamp is not. In this case the customers of her app will continue to be able to use the old app (as the claims before the date of change can still be confirmed), but when the customers request an update they will be given the updated keys in the new app. It is also possible that Alice could have set up her Control Key to be 2 of 4 multisig, with keys held by her trusted colleagues Bob , Carol and Donna . In this case, if two of them observed bad behavior in claims being issued by the old DID/DDO while she was traveling, or if she was able to call at least two of them, they could immediately rotate the Control Keys, and give her the new Control Key when she gets home. She maybe even already possesses the new Control Key, at it could have been generated by an xpub key her friends share. (However, this technique does require two transactions rather than one for every Control Key rotation.) Finally, at some point in the future Alice passes this mobile project on to another team to develop, and she gracefully \"retires\" the DID/DDO pair with a gentle revocation and no new possible Control Key. However, all of her non-expiring claims are still valid: she didn't issue many, but a few are important and they need to remain. Our use case will continue showing how Alice is able to demonstrate and share a positive reputation for her quality work. How Carol is able to use Alice\u2019s software to protect herself from harm in a third-world country. How Ted is able to review Bob\u2019s ideas and Alice\u2019s work, and offer some funding to help both. How Alice is able to increase her reputation in her anonymously developed software to the point where she is offered a job at Ted\u2019s company that pays more than her job at a conservative bank. How Alice is able to selectively revoke parts of her anonymity to establish her personal reputation her peers and selective colleagues, without at any time putting Carol and her other advocacy clients at risk. (HISTORY: The initial user story was originally written up as an issue in the W3Cs Verifiable Claims Use Cases repo, with a goal of using it to create a formal use case for new pseudo-anonymous web of trust. Alice's story continues, along with some web of trust concepts and some proposed technical details , in issue developed as part of the DID:BTCR Hackathon . This version is currently being further drafted as a Topic and Advance Reading for the Fifth #RebootingWebOfTrust in Boston on October 3-5, 2017 ( register here ). Any comments on this version please use post in these issues . )","title":"Alice and the Syrian Hacker Army"},{"location":"rwot5/topics-and-advance-readings/Visa-Really/","text":"Just like Visa? Yeah? Really? One of the most interesting things circulating amongst those who fancy creating ecosystems for identity is an idea that we should have \"frameworks\" \"trust frameworks\" just like what have now in credit card networks. Sounds great - just replicate what they have. It isn't that simple. For one thing it is a global worldwide network that has evolved for 40 years. For another it was created in a very unique process lead by a man named Dee Hock. He wrote a whole book about the process Birth of the Chaordic Age. Anyone seeking to replicate this success should read the book. I know about this history in part because the first people that hired me to work on identity efforts were inspired by this actual history and sought to build a Chaordic Organization to make it real. Visa History People working in digital identity frequently talk about the Visa network and its network of contracts that support the flow of transactions across the globe. Much of the focus is on what is perceived to be at the core of this network, the Trust Framework. This framework is a network of contracts including banks that issue credit, merchants that accept credit, the contracts amongst the issuing banks, and the contracts between the merchant bank and the merchant. This whole set of documents is over of 900 pages long. I have heard people thinking about the possibility of massive interoperable trust frameworks in digital identity saying that we should start by basing the outline of a framework for digital identity on the table of contents of this massive legal document. What kept circulating as these conversations about how they thought this network functioned were questions about how Visa formed and the unique approach its founder had to getting coherence across such a large community of stakeholders. In its founding, Dee Hock was able to get the coopetition (cooperation and competition) between the banks to happen based on the network of relationships and connections he formed between them, developing a shared purpose and articulating a core sent of principles that allowed them to work together effectively. Then from there all the artifacts like the trust framework arose. Hock had a clear understanding that the system as operated. When he began to form, the network was in chaos; it was breaking down. They needed some order within the system to help it function. It was a complex situation in the middle between chaos and order. He called this the domain of complexity Chaordic. Chaord: (kay'ord) 1: any autocatalytic, self-regulating, adaptive, nonlinear, complex organism, organization, or system, whether physical, biological or social, the behavior of which harmoniously exhibits characteristics of both order and chaos. 2: an entity whose behavior exhibits patterns and probabilities not governed or explained by the behavior of its parts. 3: the fundamental organizing principle of nature and evolution. Chaordic: (kay'ordic) 1: anything simultaneously orderly and chaotic. 2: patterned in a way dominated neither by order nor chaos. 3: existing in the phase between order and chaos. [Chaord.png] (sorry I'm not a master of the git hub and getting a picture in will take me a bit but I will figure it out) I am sharing this story to illustrate the context for the trust framework referenced within the Identity Management community. Learning how Visa was successful opens the way for us to wonder if it might be possible to try this type of coherence building. I'm not making this recommendation; however, it is not out of the realm of possibility as a path the community of stakeholders could take as a way to work together. Dee Hock's retelling told the story of how Visa came to be as posted on the Chaordic Commons Website in 2002. I thought the committee an exercise in futility and privately said as much to the BofA representatives, suggesting, instead, that the committee consider the sole question of how to create an orderly method of addressing all problems. They agreed, but concerned that the proposal might be suspect if advanced by them, insisted I put it before the meeting. The audience readily assented, in the way of all disorganized groups faced with a proposal creating the illusion of progress but requiring no money or effort. The meeting disbanded, the committee met, and I was elbowed into the chair, with no intent but to do a bit of civic duty. Within six months, a complex of regional and national committees had been formed, which had but one redeeming quality--it allowed organized information about problems to emerge. These problems were much worse than anyone had imagined, far beyond possibility of correction by the existing organization. Losses were not in the tens of millions, but in the hundreds of millions and accelerating. First: Money had become nothing more than guaranteed, alphanumeric data recorded on valueless paper and metal. It would become data in the form of organized electrons and photons moving around the world at the speed of light, at minuscule cost, by infinitely diverse paths throughout the entire electromagnetic spectrum. Second: \"Credit card\" was a misnomer, a false concept. It must be reconceived as a device for the exchange of value in the form of arranged electronic signals. The demand for that exchange would be huge--and global. Third: Whatever organization could best globally guarantee and exchange data in the form of arranged, electronic signals would have a potential market\u2013\u2013every exchange of value in the world\u2013\u2013whose size beggared the imagination. It became clear that no hierarchical corporation could do it, no nation\u2013state could do it. In fact, no existing form of organization could do it. The resources of banks worldwide were calculated. The total dwarfed the resources of most nations. Jointly they could do it, but how? It was beyond the power of reason to design an organization to deal with such complexity, and beyond the reach of imagination to perceive all the conditions it would encounter. Yet, evolution routinely tossed off much more complex chaords with seeming ease. It gradually became apparent that such an organization would have to be based on biological concepts and methods. It would have to evolve--in effect, to invent and organize itself. I asked three others to join me to address a single question based upon a single assumption. If there were no constraints whatever, if anything imaginable was possible, what would be the nature (not the structure) of an ideal organization to create the world's premier device for the exchange of value? We isolated ourselves in a small, remote hotel. There followed a week of intense, night\u2013and\u2013day discussion. Slowly, a dozen or so simple principles emerged. Let me give you some examples. It must be equitably owned by all participants. No member should have intrinsic preferential position. All advantage must result from individual ability and initiative. Power and function must be distributive to the maximum degree. No function should be performed by any part of the whole that could reasonably be done by any more peripheral part, and no power vested in any part that might reasonably be exercised by any lesser part. Governance must be distributive. No individual, institution, and no combination of either or both should be able to dominate deliberations or control decisions. It must be infinitely malleable yet extremely durable. It should be capable of constant, self\u2013generated, modification of form or function without sacrificing its essential nature or embodied principle. It must embrace diversity and change. It must attract people and institutions comfortable with such conditions and provide an environment in which they could flourish. It took six months to perfect and gain acceptance of the principles. There followed an intense, year\u2013long effort involving a great many people and disciplines. The principles were gradually enlarged into a concept, the concept into a theoretical structure, and the structure fitted into the interstices of law, custom, and culture. In June 1970 the VISA chaord came into being. Simple, clear purpose and principles give rise to complex and intelligent behavior. Complex rules and regulations give rise to simple and stupid behavior. -Dee Hock What would it be like if out of this effort the stakeholders came to alignment around a common purpose, core principles and came up with clear practices to operate with. Could this simple set things give rise to a coherent interoperable system of systems for identity? I think there is a possibility this could happen if the connections between stakeholders are based on healthy strong relationships. I will leave you with this from their website about the process. The chaordic design process has six dimensions, beginning with purpose and ending with practice. Each of the six dimensions can be thought of as a lens through which participants examine the circumstances giving rise to the need for a new organization or to reconceive an existing one. Developing a self-organizing, self-governing organization worthy of the trust of all participants usually requires intensive effort. To maximize their chances of success, most groups have taken a year or more on the process. During that time, a representative group of individuals (sometimes called a drafting team) from all parts of the engaged organization or community meet regularly and work through the chaordic design process. To learn more about the process and the details of the six steps one can go to the site of the chaordic commons that is now just an archive http://www.chaordic.org .","title":"Visa Really"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/","text":"This paper was written originally for the 2017 Rebooting Web of Trust summit. Contributors * Christopher Webber, Independent Introduction ActivityPub is a protocol being developed at the W3C for the purpose of building federated social systems. Users can use implementations of ActivityPub like Mastodon and MediaGoblin as libre alternatives to large siloed social networking systems such as Facebook, Twitter, YouTube, and Instagram. 1 In general ActivityPub follows the client-server paradigm that has been popular on the World Wide Web, while restoring some level of decentralization. Current implementations of ActivityPub go as far as to bring a level of decentralization akin to email, 2 but there are many opportunities to go further. By attaching public keys to the profiles of actors (users) on the network and using Linked Data Signatures , we can add a web of trust to the federated social web and use it to enhance user privacy and assert the integrity of messages sent over the network. By using a decentralized identifier system such as Decentralized Identifiers (DIDs) we can move fully from a decentralized to a distributed system, 3 by escaping the core centralization mechanisms of DNS and SSL certificate authorities. At this point, users could even optionally transition from a client-server model system to a fully peer to peer system. ActivityPub overview *This section is borrowed from the ActivityPub standard's* *Overview section; if you are already familiar with ActivityPub* *then you may skip this section.* ActivityPub provides two layers: A server to server federation protocol: so decentralized websites can share information A client to server protocol: so users can communicate with ActivityPub using servers, from a phone or desktop or web application or whatever ActivityPub implementations can implement just one of these things or both of them. However, once you've implemented one, it isn't too many steps to implement the other, and there are a lot of benefits to both (making your website part of the decentralized social web, and being able to use clients and client libraries that work across a wide variety of social websites). In ActivityPub, every actor (users are represented as \"actors\" here) has: An inbox : How they get messages from the world An outbox : How they send messages to others These are endpoints, or really, just URLs which are listed in the ActivityPub actor's ActivityStreams description. (More on ActivityStreams later.) Here's an example of the record of our friend Alyssa P. Hacker: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Person\", \"id\": \"https://social.example/alyssa/\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"https://social.example/alyssa/inbox/\", \"outbox\": \"https://social.example/alyssa/outbox/\", \"followers\": \"https://social.example/alyssa/followers/\", \"following\": \"https://social.example/alyssa/following/\", \"liked\": \"https://social.example/alyssa/liked/\"} ActivityPub uses ActivityStreams for its vocabulary . This is pretty great because ActivityStreams includes all the common terms you need to represent all the activities and content flowing around a social network. It's likely that ActivityStreams already includes all the vocabulary you need, but even if it doesn't, ActivityStreams can be extended via JSON-LD . If you know what JSON-LD is, you can take advantage of the cool linked data approaches provided by JSON-LD. If you don't, don't worry, JSON-LD documents and ActivityStreams can be understood as plain old simple JSON. (If you're going to add extensions, that's the point at which JSON-LD really helps you out.) So, okay. Alyssa wants to talk to her friends, and her friends want to talk to her! Luckily these \"inbox\" and \"outbox\" things can help us out. They both behave differently for GET and POST. So the full workflow is: You can POST to someone's inbox to send them a message (server-to-server / federation only\u2026 this is federation!) You can GET from your inbox to read your latest messages (client-to-server; this is like reading your social network stream) You can POST to your outbox to send messages to the world (client-to-server) You can GET from someone's outbox to see what messages they've posted (or at least the ones you're authorized to see). (client-to-server and/or server-to-server) Of course, if that last one (GET'ing from someone's outbox) was the only way to see what people have sent, this wouldn't be a very efficient federation protocol! Indeed, federation happens usually by servers posting messages sent by actors to actors on other servers' inboxes. Let's see an example! Let's say Alyssa wants to catch up with her friend, Ben Bitdiddle. She lent him a book recently and she wants to make sure he returns it to her. Here's the message she composes, as an ActivityStreams object: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Note\", \"to\": [\"https://chatty.example/ben/\"], \"attributedTo\": \"https://social.example/alyssa/\", \"content\": \"Say, did you finish reading that book I lent you?\"} This is a note addressed to Ben. She POSTs it to her outbox. Since this is a non-activity object, the server recognizes that this is an object being newly created, and does the courtesy of wrapping it in a Create activity. (Activities sent around in ActivityPub generally follow the pattern of some activity by some actor being taken on some object. In this case the activity is a Create of a Note object, posted by a Person.) {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://social.example/alyssa/posts/a29a6843-9feb-4c74-...\", \"to\": [\"https://chatty.example/ben/\"], \"actor\": \"https://social.example/alyssa/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/49e2d03d-b53a-4c4c-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://chatty.example/ben/\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} Alyssa's server looks up Ben's ActivityStreams actor object, finds his inbox endpoint, and POST's her object to his inbox. Technically these are two separate steps\u2026 one is client to server communication, and one is server to server communication (federation). But, since we're using them both in this example, we can abstractly think of this as being a streamlined submission from outbox to inbox. A while later, Alyssa checks what new messages she's gotten. Her phone polls her inbox via GET, and amongst a bunch of cat videos posted by friends and photos of her nephew posted by her sister, she sees 4 the following: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://chatty.example/ben/p/51086\", \"to\": [\"https://social.example/alyssa/\"], \"actor\": \"https://chatty.example/ben/\", \"object\": {\"type\": \"Note\", \"id\": \"https://chatty.example/ben/p/51085\", \"attributedTo\": \"https://chatty.example/ben/\", \"to\": [\"https://social.example/alyssa/\"], \"inReplyTo\": \"https://social.example/alyssa/posts/49e2d03d-b53a-...\", \"content\": \"Argh, yeah, sorry, I'll get it back to you tomorrow. I was reviewing the section on register machines, since it's been a while since I wrote one.\"}} Alyssa is relieved, and likes Ben's post: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Like\", \"id\": \"https://social.example/alyssa/posts/5312e10e-5110-42e5-...\", \"to\": [\"https://chatty.example/ben/\"], \"actor\": \"https://social.example/alyssa/\", \"object\": \"https://chatty.example/ben/p/51086\"} She POSTs this message to her outbox. (Since it's an activity, her server knows it doesn't need to wrap it in a Create object.) Feeling happy about things, she decides to post a public message to her followers. Soon the following message is blasted to all the members of her followers collection, and since it has the special Public group addressed, is generally readable by anyone. {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://social.example/alyssa/posts/9282e9cc-14d0-42b3-...\", \"to\": [\"https://social.example/alyssa/followers/\", \"https://www.w3.org/ns/activitystreams#Public\"], \"actor\": \"https://social.example/alyssa/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/d18c55d4-8a63-4181-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://social.example/alyssa/followers/\", \"https://www.w3.org/ns/activitystreams#Public\"], \"content\": \"Lending books to friends is nice. Getting them back is even nicer! :)\"}} Bringing public key cryptography to the federated social web We can dramatically improve the state of the federated social web by having each actor on the system hold a public and private keypair, and by having actors have their public key attached directly to their actor object: {\"@context\": [\"https://www.w3.org/ns/activitystreams\", \"https://w3id.org/security/v1\"], \"id\": \"https://schemers.example/u/alyssa\", \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"publicKey\": [{ \"id\": \"https://schemers.example/u/alyssa#main-key\", \"owner\": \"https://schemers.example/u/alyssa\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} This provides significant improvements to the system which we explore below. Signing objects Sharing messages is common in social networks. But how can you verify that someone really said what they claimed? The user Mallet is trying to cause havoc in their social network. They pretend to \"share\" 5 the following post they pretend to have seen from Alyssa to the pasta-enthusiasts group, which Ben is a member of. {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Announce\", \"id\": \"https://havoc.example/~mallet/p/90815\", \"to\": [\"https://pastalovers.example/groups/pasta-enthusiasts/\"], \"actor\": \"https://havoc.example/~mallet/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/63cc87ec-416e-437d-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://havoc.example~mallet/\"], \"content\": \"Tortellini is a poor and disgusting imitation of ravioli. Any chef serving tortellini should hang up their aprons in disgrace and never cook again.\", \"signature\": { \"type\": \"RsaSignature2017\", \"creator\": \"https://social.example/alyssa/\", \"created\": \"2017-09-23T20:21:34Z\", \"nonce\": \"e3689a56da9b4bc\", \"signatureValue\": \"mJfe5OCb7J3WwI...8t5/m=\"}}, \"signature\": { \"type\": \"RsaSignature2017\", \"creator\": \"https://social.example/alyssa/\", \"created\": \"2017-09-23T21:32:21Z\", \"nonce\": \"22e8e7683f56c08bb873\", \"signatureValue\": \"wTjLtnZVYF79pq9Ts...OU1jYPSjvcE2jNc=\"}} In general ActivityPub follows the client-server paradigm that has been popular on the World Wide Web, while restoring some level of decentralization. Current implementations of ActivityPub go as far as to bring a level of decentralization akin to email, Ben's server, or even the server hosting pastalovers.example , can check the signature against the publicKey listed on Alyssa's actor object. This check fails, and so Mallet's attempt at slander of Alyssa amongst the pasta enthusiast community fails. While the above example looks at protecting against a malicious interaction, forwarding and sharing content is desirable for positive reasons. One common problem in federated social networks that support private interactions is that a conversation can become fragmented; if Ben is posting to private collection 5 she has curated of both his friends and coworkers, if members of coworkers can't see who is in the private family collection, even if they address to include the family in the conversation they can't traverse the collection of family actors to deliver to all relevant participants. (This \"ghost replies\" problem happens frequently on federated networks even when messages are being sent to the actor's own followers , where breaks tend to happen across server boundaries.) ActivityPub includes a solution to this via a forwarding mechanism , but the solution does not really work without signatures, as the forwards are happening indirectly rather than from the \"same origin/domain\", so servers are unable to check/trust that the content is as claimed. Even if the receiving server tries to look up the object the receiving actor's credentials, access control may not have been enabled for the actor who was forwarded to, since the commenter had no way of knowing who was in the private collection to enable access for. This is a frequently requested feature in federated social networks, so we should ensure that the necessary public key infrastructure is provided. 6 An easier to use web of trust? The PGP-style \"web of trust\" has been around for some time now, but the term \"web of trust\" is somewhat mired by the historically most popular method by which the trust network has populated. Key signing parties, while effective, have never taken off beyond a very small set of the population. Such parties are rewarding but difficult for most of the population to attend and organize, and even more difficult still is learning the (generally) command line tooling necessary to participate in the system. While some work has been done in this area (for example with Monkeysign and Gibberbot ), it would be even better if building your trust network was incidental to participating in the network. 7 To a certain extent, this could come \"for free, with caveats\" in ActivityPub deployments that exist today, where subscriptions and object lookup are done over HTTPS. Merely by sending a follow request (or some other action connecting users on the social graph) a certain amount of trust between users can be expressed. Keys can be looked up and recorded at actor profile urls, and users can even observe and share information about whom else they know on the social network. There's a major caveat using HTTPS for these lookups requires trust in SSL certificate authorities. Better than nothing, but not great, and not the distributed systems we want. Furthermore, a malicious actor can still trick users; a user may believe they are subscribing to https://social.example/alyssa/ , but perhaps Mallet tricked them into subscribing to https://social.example/alyssaa/ instead. 8 Happily there are other ways to encourage stronger trust networks. Carl Ellison's paper Establishing Identity Without Certification Authorities describes several classes of relationships amongst users, and many Off The Record clients (such as available in Pidgin, etc) provide interfaces for verifying challenges between users. Users on a federated social network could be provided an opportunity to perform a textual challenge, perform a brief video call where they verify a shared code (as done in Jitsi), or scan a QR code (as in Monkeysign and Gibberbot) to establish stronger trust that an actor on the network is the entity they claim to be. The level of trust gained could be signed, recorded, and itself propagated as appropriate throughout the network. This kind of mechanism would work nicely even in a system like DIDs, where a human-readable identifier does not exist. End to end encryption A malicious server administrator may still snoop on all communication of participants on a system. Even a non-malicious administrator may be coerced into snooping on their users, or may have their entire system compromised without their knowledge. SSL Certificate Authorities may also be compromised into giving out fake certificates, allowing man in the middle attacks that neither the user nor server administrator may be aware of. End to end encryption can solve this (with some tradeoffs); in this case, rather than having the server manage the public and private keys of a user, a user may provide a public key on their actor object to which only their own computer(s) hold the corresponding private key. Other actors on the network may then send an object encrypted to the actor's inbox. For example, an actor may receive the following object 9 in their inbox: {\"@context\": [\"https://securityns.example/\", \"https://www.w3.org/ns/activitystreams\"], \"type\": \"EncryptedEnvelope\", \"encryptedMessage\": \"-----BEGIN PGP MESSAGE-----\\r\\n...\", \"mediaType\": \"application/ld+json; profile=\\\"https://www.w3.org/ns/activitystreams\\\"\"} The server would put this object in the user's inbox, but if only the user's own computers hold the key, even the server would be unable to read the contents held within the envelope. Upon retrieving the object from the server via the client to server protocol, the user's client can decrypt the message. In this case, the message went directly to Alyssa's inbox. Upon decrypting the component in encryptedMessage, another object is found: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Note\", \"id\": \"https://chatty.example/ben/p/86187\", \"to\": [\"https://social.example/alyssa/\"], \"attributedTo\": \"https://chatty.example/ben/\", \"content\": \"Up for some root beer floats at my friend's house? Here's the address: ...\"} Note that while this improves privacy, it does come with several tradeoffs: ActivityPub contains an entire suite of server side side-effects for federating common activities on a social network. For example, a Like object will increment a counter of how often a post has been liked, and even add that liked object to both the user's collection of liked objects as well as a collection of all users who have liked this object. Since servers are unable to observe data being sent across the network, these kinds of side effects will break. The server will also be unable to provide additional features such as being able to have server-based indexing of messages for easy search. 10 In a \"more peer to peer\" system (as discussed in the Distributed identity section) this becomes less of an issue because the distinction between client server blurs. Nevertheless, for existing client to server implementations, this is a strong issue to consider. User maintenance of keys in end-to-end encryption systems is known to be a difficult user experience problem. Key recovery is even harder. DIDs explore a method for key recovery, but this will not help users read old messages encrypted with keys they no longer have and which the original senders cannot send (or do not know how to). Distributed identity ActivityPub implementations at the present moment rely on HTTPS as their transport, which in turn relies on two centralized systems: DNS and SSL certificate authorities. Is there any way to bring self-sovereignty to the federated social web? Thankfully there is; ActivityPub was written intentionally to be layerable on any protocol that can support HTTP GET and POST verbs. The Decentralized Identifiers specification looks to be a good fit for ActivityPub. The simplest version of this can be seen simply by replacing the actor ids with DIDs. To transform an example from the overview from: {\"type\": \"Note\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://chatty.example/ben/\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} to: {\"type\": \"Note\", \"attributedTo\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"to\": [\"did:example:nJx2fgreaSfCujA0kMsiEW8Oz\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} Gosh! That was simple-ish. All we did was replace the human-readable identifiers representing the users with DIDs. If we look up Alyssa's DID based id, we can retrieve her actor object as a DDO , but this time there is extra information: { \"@context\": [\"https://example.org/did/v1\", \"https://www.w3.org/ns/activitystreams\"], \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", // ActivityPub actor information \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"https://9GaksjPhy0mWToTV.onion/alyssa/inbox/\", \"outbox\": \"https://9GaksjPhy0mWToTV.onion/alyssa/outbox/\", \"followers\": \"https://9GaksjPhy0mWToTV.onion/alyssa/followers/\", \"following\": \"https://9GaksjPhy0mWToTV.onion/alyssa/following/\", \"liked\": \"https://9GaksjPhy0mWToTV.onion/alyssa/liked/\", // DDO information \"owner\": [{ \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#key-1\", \"type\": [\"CryptographicKey\", \"EdDsaPublicKey\"], \"curve\": \"ed25519\", \"expires\": \"2017-02-08T16:02:20Z\", \"publicKeyBase64\": \"lji9qTtkCydxtez/bt1zdLxVMMbz4SzWvlqgOBmURoM=\" }, { \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#key-2\", \"type\": [\"CryptographicKey\", \"RsaPublicKey\"], \"expires\": \"2017-03-22T00:00:00Z\", \"publicKeyPem\": \"----BEGIN PUBLIC KEY-----\\r\\n..\" }], \"control\": [{ \"type\": \"OrControl\", \"signer\": [ \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"did:example:8uQhQMGzWxR8vw5P3UWH1j\" ] }], \"created\": \"2002-10-10T17:00:00Z\", \"updated\": \"2016-10-17T02:41:00Z\", \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:8uQhQMGzWxR8vw5P3UWH1j#key/1\", \"signatureValue\": \"IOmA4R7TfhkYTYW8...CBMq2/gi25s=\" } } Hoo! That's a lot of additions. Except here we see an example of Alyssa's profile that is entirely free of traditional centralized DNS authorities. We were able to look up Alyssa's object via her DID, but we still have access to all her endpoints, which in this case are pointing to Tor Hidden Services . No central DNS required! Maybe in the future there is even a protocol \u2013 let's call it httpeer \u2013 which supports all the standard HTTP verbage, but over some other peer to peer network. The DID spec supports service endpoints , and Alyssa could take advantage of these to use her DID as base of the inbox , outbox , etc URIs. Here's a cut down and modified version of the previous example: { \"@context\": [\"https://example.org/did/v1\", \"https://www.w3.org/ns/activitystreams\"], \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", // ActivityPub actor information \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/inbox/\", \"outbox\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/outbox/\", \"followers\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/followers/\", \"following\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/following/\", \"liked\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/liked/\", // DDO information goes here \"service\": { \"httpeer\": \"dI0tuXjISZEadSH6QV9EhBEdccL4ouePdF8P57BJ\"}} Now that's an identity system! Append only systems and content addressed storage Finally, it's worth mentioning the idea of moving ActivityPub to an entirely append-only, content-addressed system for object storage, \"modification\", and retrieval. Much success has been seen in recent years with these systems, and doing so would allow for many of the side effects in the federation system to be dropped entirely. We leave this as a topic for a future paper. Conclusions ActivityPub goes a long way towards providing a standardized way to move the social web from isolated, centralized silos towards the decentralized nature that the World Wide Web is meant to encompass. Still, there is much to be done and improved. Yet there are risks in trying to engineer the right system all at once, and great is well known to be the enemy of good. Thankfully we do not need to throw out what we have to make the improvements which are discussed in this paper. ActivityPub already exists and works, and we can incrementally improve the systems we have and blur the line between the federated social web that works and more peer to peer systems which are desirable. By adding public key infrastructure and distributed identifiers to ActivityPub we can move from a decentralized system to a distributed one and truly build a network that is both self-sovereign and social. Acknowledgments Thanks to Evan Prodromou and Owen Shepherd for working on initial revisions of the ActivityPub standard. Thanks also to Jessica Tallon for being co-editor on ActivityPub and everyone who helped make ActivityPub happen , which is no small list. (Some of those people caught typos in the original version of the overview, as included in ActivityPub.) Thanks to Manu Sporny and Stephen Webber for thoughts and feedback about how to make ActivityPub into a more robust distributed system. Thanks to Dave Longley for pointing to the vocabulary drift / ambiguity of the terms \"decentralized\" and \"distributed\", which lead to the addition of Paul Baran's diagrams and some terminology clarification. 3 Thanks to Morgan Lemmer-Webber for the patience and for careful proofreading of this document. Thanks to Spec-Ops and Digital Bazaar for supporting my work/time on Verifiable Claims/Credentials. Though separate from my work on ActivityPub, obviously I've been thinking a lot about how to combine them this whole time. Thanks also to mray for the gorgeous illustrations in the overview section. This document and its images (with the exception of Paul Baran's drawings), like ActivityPub itself, are licensed under W3C's permissive document license . Footnotes: 1 Of course, there is nothing stopping current-silos of social networking from adopting ActivityPub, would they be willing to un-silo their users. 2 Observant readers may note that email is no longer as decentralized of a system as it once was. Consider this a lesson that a protocol alone cannot build a distributed network; the community must build and maintain a healthy number of nodes and avoid the temptation to let a few large providers control the space of a federated network. 3 It is worth spending some time to discuss what is meant by \"centralized\" versus \"distributed\" versus \"decentralized\". ![img](./activitypub-decentralized-distributed-diagrams/centralized_decentralized_distributed.png \"Centralized, Decentralized, and Distributed drawings, from 'On Distributed Communications, part 1' by Paul Baran, 1964\") In the figure above we see images from Paul Baran's 1964 paper on the subject, and from these shapes we can see the kinds of shapes we mean: social silos resemble the the spoke-like centralized model, client-server federated social networks resemble the tree-like decentralized model, and a peer to peer network resembles the mesh-like distributed model. Since the writing of that paper there has been significant vocabulary drift (perhaps because English is such a decentralized/distributed language) and [clarifying the meaning of these terms](https://medium.com/@VitalikButerin/the-meaning-of-decentralization-a0c92b76a274) can be difficult. (In [one popular post on the Ethereum Stack Exchange](https://ethereum.stackexchange.com/a/7813), a diagram that looks almost exactly like Baran's diagram appears, but with the Decentralized and Distributed labels reversed!) The goal of this paper is really to seek out the systems which promote the greatest amount of reliability, security, and user autonomy, and some of the methods discussed, such as public key cryptography, promote both. Nonetheless, when the terms \"decentralized\" and \"distributed\" are used and meaning is to be sought out, look to Baran, for sometimes pictures are more descriptive than words. 4 Alyssa probably would not likely see the JSON-LD objects directly as described here, but the author believes that some narrative context still assists in the explaination of a UI-agnostic protocol. 5 `Announce` is essentially `Share` in ActivityStreams. The author of this document is not responsible for that terminology decision. 6 In ActivityPub, `Collection` objects may be used to contain sets of objects. Users of the system can curate sets of actors in collections that are publicly or privately readable which may be used for the addressing of distributed objects (similar to Google+'s circles or Diaspora's aspects). Indeed, even the actor's `followers` is a `Collection` like this! 7 Several decisions need to be made when storing signatures on objects which themselves reference other signed objects that may mutate, and this is [currently a topic of open discussion](https://github.com/w3c-dvcg/ld-signatures/issues/7). This may motivate more work on append only systems and content addressed storage. Existing implementations which operate in a mutation-prone environment must decide between letting signatures referencing mutated objects fail, including such objects recursively on every parent object, or employ some sort of content addressing of objects stored by the revision seen. The latter two options may pose some challenge to highly relational systems which were not originally designed with signatures in mind. 8 [GNU Ring](https://ring.cx/) is an interesting example of a peer to peer social network system where a user's identity is actually their fingerprint. While not the first system to have this concept, it's very pleasant to see in action (and the interface is itself aesthetically pleasing); to build up your buddy list is quite literally to build your web of trust. 9 There are an incredible number of [unicode hacks](http://www.unicode.org/Public/security/latest/confusables.txt) which can trick even the most careful of technical users as well. 10 `https://securityns.example/` is an imaginary json-ld context which is used only as a placeholder for the terms of `EncryptedEnvelope` and `encryptedMessage`. Perhaps in the future terms along these lines (maybe with better names) would appear in one of the other contexts/namespaces that appear in this document. 11 This is not unlike how PGP-wrapped email works. Receiving PGP-encrypted email means that a webmail interface would be unable to search through your messages. However, that does not mean searching is impossible; some programs like [mu](http://www.djcbsoftware.nl/code/mu) / [mu4e](https://www.djcbsoftware.nl/code/mu/mu4e.html) can index encrypted email locally and provide such a search interface, on a user's local machine.","title":"Activitypub decentralized distributed"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#introduction","text":"ActivityPub is a protocol being developed at the W3C for the purpose of building federated social systems. Users can use implementations of ActivityPub like Mastodon and MediaGoblin as libre alternatives to large siloed social networking systems such as Facebook, Twitter, YouTube, and Instagram. 1 In general ActivityPub follows the client-server paradigm that has been popular on the World Wide Web, while restoring some level of decentralization. Current implementations of ActivityPub go as far as to bring a level of decentralization akin to email, 2 but there are many opportunities to go further. By attaching public keys to the profiles of actors (users) on the network and using Linked Data Signatures , we can add a web of trust to the federated social web and use it to enhance user privacy and assert the integrity of messages sent over the network. By using a decentralized identifier system such as Decentralized Identifiers (DIDs) we can move fully from a decentralized to a distributed system, 3 by escaping the core centralization mechanisms of DNS and SSL certificate authorities. At this point, users could even optionally transition from a client-server model system to a fully peer to peer system.","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#activitypub-overview","text":"*This section is borrowed from the ActivityPub standard's* *Overview section; if you are already familiar with ActivityPub* *then you may skip this section.* ActivityPub provides two layers: A server to server federation protocol: so decentralized websites can share information A client to server protocol: so users can communicate with ActivityPub using servers, from a phone or desktop or web application or whatever ActivityPub implementations can implement just one of these things or both of them. However, once you've implemented one, it isn't too many steps to implement the other, and there are a lot of benefits to both (making your website part of the decentralized social web, and being able to use clients and client libraries that work across a wide variety of social websites). In ActivityPub, every actor (users are represented as \"actors\" here) has: An inbox : How they get messages from the world An outbox : How they send messages to others These are endpoints, or really, just URLs which are listed in the ActivityPub actor's ActivityStreams description. (More on ActivityStreams later.) Here's an example of the record of our friend Alyssa P. Hacker: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Person\", \"id\": \"https://social.example/alyssa/\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"https://social.example/alyssa/inbox/\", \"outbox\": \"https://social.example/alyssa/outbox/\", \"followers\": \"https://social.example/alyssa/followers/\", \"following\": \"https://social.example/alyssa/following/\", \"liked\": \"https://social.example/alyssa/liked/\"} ActivityPub uses ActivityStreams for its vocabulary . This is pretty great because ActivityStreams includes all the common terms you need to represent all the activities and content flowing around a social network. It's likely that ActivityStreams already includes all the vocabulary you need, but even if it doesn't, ActivityStreams can be extended via JSON-LD . If you know what JSON-LD is, you can take advantage of the cool linked data approaches provided by JSON-LD. If you don't, don't worry, JSON-LD documents and ActivityStreams can be understood as plain old simple JSON. (If you're going to add extensions, that's the point at which JSON-LD really helps you out.) So, okay. Alyssa wants to talk to her friends, and her friends want to talk to her! Luckily these \"inbox\" and \"outbox\" things can help us out. They both behave differently for GET and POST. So the full workflow is: You can POST to someone's inbox to send them a message (server-to-server / federation only\u2026 this is federation!) You can GET from your inbox to read your latest messages (client-to-server; this is like reading your social network stream) You can POST to your outbox to send messages to the world (client-to-server) You can GET from someone's outbox to see what messages they've posted (or at least the ones you're authorized to see). (client-to-server and/or server-to-server) Of course, if that last one (GET'ing from someone's outbox) was the only way to see what people have sent, this wouldn't be a very efficient federation protocol! Indeed, federation happens usually by servers posting messages sent by actors to actors on other servers' inboxes. Let's see an example! Let's say Alyssa wants to catch up with her friend, Ben Bitdiddle. She lent him a book recently and she wants to make sure he returns it to her. Here's the message she composes, as an ActivityStreams object: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Note\", \"to\": [\"https://chatty.example/ben/\"], \"attributedTo\": \"https://social.example/alyssa/\", \"content\": \"Say, did you finish reading that book I lent you?\"} This is a note addressed to Ben. She POSTs it to her outbox. Since this is a non-activity object, the server recognizes that this is an object being newly created, and does the courtesy of wrapping it in a Create activity. (Activities sent around in ActivityPub generally follow the pattern of some activity by some actor being taken on some object. In this case the activity is a Create of a Note object, posted by a Person.) {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://social.example/alyssa/posts/a29a6843-9feb-4c74-...\", \"to\": [\"https://chatty.example/ben/\"], \"actor\": \"https://social.example/alyssa/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/49e2d03d-b53a-4c4c-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://chatty.example/ben/\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} Alyssa's server looks up Ben's ActivityStreams actor object, finds his inbox endpoint, and POST's her object to his inbox. Technically these are two separate steps\u2026 one is client to server communication, and one is server to server communication (federation). But, since we're using them both in this example, we can abstractly think of this as being a streamlined submission from outbox to inbox. A while later, Alyssa checks what new messages she's gotten. Her phone polls her inbox via GET, and amongst a bunch of cat videos posted by friends and photos of her nephew posted by her sister, she sees 4 the following: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://chatty.example/ben/p/51086\", \"to\": [\"https://social.example/alyssa/\"], \"actor\": \"https://chatty.example/ben/\", \"object\": {\"type\": \"Note\", \"id\": \"https://chatty.example/ben/p/51085\", \"attributedTo\": \"https://chatty.example/ben/\", \"to\": [\"https://social.example/alyssa/\"], \"inReplyTo\": \"https://social.example/alyssa/posts/49e2d03d-b53a-...\", \"content\": \"Argh, yeah, sorry, I'll get it back to you tomorrow. I was reviewing the section on register machines, since it's been a while since I wrote one.\"}} Alyssa is relieved, and likes Ben's post: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Like\", \"id\": \"https://social.example/alyssa/posts/5312e10e-5110-42e5-...\", \"to\": [\"https://chatty.example/ben/\"], \"actor\": \"https://social.example/alyssa/\", \"object\": \"https://chatty.example/ben/p/51086\"} She POSTs this message to her outbox. (Since it's an activity, her server knows it doesn't need to wrap it in a Create object.) Feeling happy about things, she decides to post a public message to her followers. Soon the following message is blasted to all the members of her followers collection, and since it has the special Public group addressed, is generally readable by anyone. {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Create\", \"id\": \"https://social.example/alyssa/posts/9282e9cc-14d0-42b3-...\", \"to\": [\"https://social.example/alyssa/followers/\", \"https://www.w3.org/ns/activitystreams#Public\"], \"actor\": \"https://social.example/alyssa/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/d18c55d4-8a63-4181-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://social.example/alyssa/followers/\", \"https://www.w3.org/ns/activitystreams#Public\"], \"content\": \"Lending books to friends is nice. Getting them back is even nicer! :)\"}}","title":"ActivityPub overview"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#bringing-public-key-cryptography-to-the-federated-social-web","text":"We can dramatically improve the state of the federated social web by having each actor on the system hold a public and private keypair, and by having actors have their public key attached directly to their actor object: {\"@context\": [\"https://www.w3.org/ns/activitystreams\", \"https://w3id.org/security/v1\"], \"id\": \"https://schemers.example/u/alyssa\", \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"publicKey\": [{ \"id\": \"https://schemers.example/u/alyssa#main-key\", \"owner\": \"https://schemers.example/u/alyssa\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n...\"}]} This provides significant improvements to the system which we explore below.","title":"Bringing public key cryptography to the federated social web"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#signing-objects","text":"Sharing messages is common in social networks. But how can you verify that someone really said what they claimed? The user Mallet is trying to cause havoc in their social network. They pretend to \"share\" 5 the following post they pretend to have seen from Alyssa to the pasta-enthusiasts group, which Ben is a member of. {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Announce\", \"id\": \"https://havoc.example/~mallet/p/90815\", \"to\": [\"https://pastalovers.example/groups/pasta-enthusiasts/\"], \"actor\": \"https://havoc.example/~mallet/\", \"object\": {\"type\": \"Note\", \"id\": \"https://social.example/alyssa/posts/63cc87ec-416e-437d-...\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://havoc.example~mallet/\"], \"content\": \"Tortellini is a poor and disgusting imitation of ravioli. Any chef serving tortellini should hang up their aprons in disgrace and never cook again.\", \"signature\": { \"type\": \"RsaSignature2017\", \"creator\": \"https://social.example/alyssa/\", \"created\": \"2017-09-23T20:21:34Z\", \"nonce\": \"e3689a56da9b4bc\", \"signatureValue\": \"mJfe5OCb7J3WwI...8t5/m=\"}}, \"signature\": { \"type\": \"RsaSignature2017\", \"creator\": \"https://social.example/alyssa/\", \"created\": \"2017-09-23T21:32:21Z\", \"nonce\": \"22e8e7683f56c08bb873\", \"signatureValue\": \"wTjLtnZVYF79pq9Ts...OU1jYPSjvcE2jNc=\"}} In general ActivityPub follows the client-server paradigm that has been popular on the World Wide Web, while restoring some level of decentralization. Current implementations of ActivityPub go as far as to bring a level of decentralization akin to email, Ben's server, or even the server hosting pastalovers.example , can check the signature against the publicKey listed on Alyssa's actor object. This check fails, and so Mallet's attempt at slander of Alyssa amongst the pasta enthusiast community fails. While the above example looks at protecting against a malicious interaction, forwarding and sharing content is desirable for positive reasons. One common problem in federated social networks that support private interactions is that a conversation can become fragmented; if Ben is posting to private collection 5 she has curated of both his friends and coworkers, if members of coworkers can't see who is in the private family collection, even if they address to include the family in the conversation they can't traverse the collection of family actors to deliver to all relevant participants. (This \"ghost replies\" problem happens frequently on federated networks even when messages are being sent to the actor's own followers , where breaks tend to happen across server boundaries.) ActivityPub includes a solution to this via a forwarding mechanism , but the solution does not really work without signatures, as the forwards are happening indirectly rather than from the \"same origin/domain\", so servers are unable to check/trust that the content is as claimed. Even if the receiving server tries to look up the object the receiving actor's credentials, access control may not have been enabled for the actor who was forwarded to, since the commenter had no way of knowing who was in the private collection to enable access for. This is a frequently requested feature in federated social networks, so we should ensure that the necessary public key infrastructure is provided. 6","title":"Signing objects"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#an-easier-to-use-web-of-trust","text":"The PGP-style \"web of trust\" has been around for some time now, but the term \"web of trust\" is somewhat mired by the historically most popular method by which the trust network has populated. Key signing parties, while effective, have never taken off beyond a very small set of the population. Such parties are rewarding but difficult for most of the population to attend and organize, and even more difficult still is learning the (generally) command line tooling necessary to participate in the system. While some work has been done in this area (for example with Monkeysign and Gibberbot ), it would be even better if building your trust network was incidental to participating in the network. 7 To a certain extent, this could come \"for free, with caveats\" in ActivityPub deployments that exist today, where subscriptions and object lookup are done over HTTPS. Merely by sending a follow request (or some other action connecting users on the social graph) a certain amount of trust between users can be expressed. Keys can be looked up and recorded at actor profile urls, and users can even observe and share information about whom else they know on the social network. There's a major caveat using HTTPS for these lookups requires trust in SSL certificate authorities. Better than nothing, but not great, and not the distributed systems we want. Furthermore, a malicious actor can still trick users; a user may believe they are subscribing to https://social.example/alyssa/ , but perhaps Mallet tricked them into subscribing to https://social.example/alyssaa/ instead. 8 Happily there are other ways to encourage stronger trust networks. Carl Ellison's paper Establishing Identity Without Certification Authorities describes several classes of relationships amongst users, and many Off The Record clients (such as available in Pidgin, etc) provide interfaces for verifying challenges between users. Users on a federated social network could be provided an opportunity to perform a textual challenge, perform a brief video call where they verify a shared code (as done in Jitsi), or scan a QR code (as in Monkeysign and Gibberbot) to establish stronger trust that an actor on the network is the entity they claim to be. The level of trust gained could be signed, recorded, and itself propagated as appropriate throughout the network. This kind of mechanism would work nicely even in a system like DIDs, where a human-readable identifier does not exist.","title":"An easier to use web of trust?"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#end-to-end-encryption","text":"A malicious server administrator may still snoop on all communication of participants on a system. Even a non-malicious administrator may be coerced into snooping on their users, or may have their entire system compromised without their knowledge. SSL Certificate Authorities may also be compromised into giving out fake certificates, allowing man in the middle attacks that neither the user nor server administrator may be aware of. End to end encryption can solve this (with some tradeoffs); in this case, rather than having the server manage the public and private keys of a user, a user may provide a public key on their actor object to which only their own computer(s) hold the corresponding private key. Other actors on the network may then send an object encrypted to the actor's inbox. For example, an actor may receive the following object 9 in their inbox: {\"@context\": [\"https://securityns.example/\", \"https://www.w3.org/ns/activitystreams\"], \"type\": \"EncryptedEnvelope\", \"encryptedMessage\": \"-----BEGIN PGP MESSAGE-----\\r\\n...\", \"mediaType\": \"application/ld+json; profile=\\\"https://www.w3.org/ns/activitystreams\\\"\"} The server would put this object in the user's inbox, but if only the user's own computers hold the key, even the server would be unable to read the contents held within the envelope. Upon retrieving the object from the server via the client to server protocol, the user's client can decrypt the message. In this case, the message went directly to Alyssa's inbox. Upon decrypting the component in encryptedMessage, another object is found: {\"@context\": \"https://www.w3.org/ns/activitystreams\", \"type\": \"Note\", \"id\": \"https://chatty.example/ben/p/86187\", \"to\": [\"https://social.example/alyssa/\"], \"attributedTo\": \"https://chatty.example/ben/\", \"content\": \"Up for some root beer floats at my friend's house? Here's the address: ...\"} Note that while this improves privacy, it does come with several tradeoffs: ActivityPub contains an entire suite of server side side-effects for federating common activities on a social network. For example, a Like object will increment a counter of how often a post has been liked, and even add that liked object to both the user's collection of liked objects as well as a collection of all users who have liked this object. Since servers are unable to observe data being sent across the network, these kinds of side effects will break. The server will also be unable to provide additional features such as being able to have server-based indexing of messages for easy search. 10 In a \"more peer to peer\" system (as discussed in the Distributed identity section) this becomes less of an issue because the distinction between client server blurs. Nevertheless, for existing client to server implementations, this is a strong issue to consider. User maintenance of keys in end-to-end encryption systems is known to be a difficult user experience problem. Key recovery is even harder. DIDs explore a method for key recovery, but this will not help users read old messages encrypted with keys they no longer have and which the original senders cannot send (or do not know how to).","title":"End to end encryption"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#distributed-identity","text":"ActivityPub implementations at the present moment rely on HTTPS as their transport, which in turn relies on two centralized systems: DNS and SSL certificate authorities. Is there any way to bring self-sovereignty to the federated social web? Thankfully there is; ActivityPub was written intentionally to be layerable on any protocol that can support HTTP GET and POST verbs. The Decentralized Identifiers specification looks to be a good fit for ActivityPub. The simplest version of this can be seen simply by replacing the actor ids with DIDs. To transform an example from the overview from: {\"type\": \"Note\", \"attributedTo\": \"https://social.example/alyssa/\", \"to\": [\"https://chatty.example/ben/\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} to: {\"type\": \"Note\", \"attributedTo\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"to\": [\"did:example:nJx2fgreaSfCujA0kMsiEW8Oz\"], \"content\": \"Say, did you finish reading that book I lent you?\"}} Gosh! That was simple-ish. All we did was replace the human-readable identifiers representing the users with DIDs. If we look up Alyssa's DID based id, we can retrieve her actor object as a DDO , but this time there is extra information: { \"@context\": [\"https://example.org/did/v1\", \"https://www.w3.org/ns/activitystreams\"], \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", // ActivityPub actor information \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"https://9GaksjPhy0mWToTV.onion/alyssa/inbox/\", \"outbox\": \"https://9GaksjPhy0mWToTV.onion/alyssa/outbox/\", \"followers\": \"https://9GaksjPhy0mWToTV.onion/alyssa/followers/\", \"following\": \"https://9GaksjPhy0mWToTV.onion/alyssa/following/\", \"liked\": \"https://9GaksjPhy0mWToTV.onion/alyssa/liked/\", // DDO information \"owner\": [{ \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#key-1\", \"type\": [\"CryptographicKey\", \"EdDsaPublicKey\"], \"curve\": \"ed25519\", \"expires\": \"2017-02-08T16:02:20Z\", \"publicKeyBase64\": \"lji9qTtkCydxtez/bt1zdLxVMMbz4SzWvlqgOBmURoM=\" }, { \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt#key-2\", \"type\": [\"CryptographicKey\", \"RsaPublicKey\"], \"expires\": \"2017-03-22T00:00:00Z\", \"publicKeyPem\": \"----BEGIN PUBLIC KEY-----\\r\\n..\" }], \"control\": [{ \"type\": \"OrControl\", \"signer\": [ \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", \"did:example:8uQhQMGzWxR8vw5P3UWH1j\" ] }], \"created\": \"2002-10-10T17:00:00Z\", \"updated\": \"2016-10-17T02:41:00Z\", \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:8uQhQMGzWxR8vw5P3UWH1j#key/1\", \"signatureValue\": \"IOmA4R7TfhkYTYW8...CBMq2/gi25s=\" } } Hoo! That's a lot of additions. Except here we see an example of Alyssa's profile that is entirely free of traditional centralized DNS authorities. We were able to look up Alyssa's object via her DID, but we still have access to all her endpoints, which in this case are pointing to Tor Hidden Services . No central DNS required! Maybe in the future there is even a protocol \u2013 let's call it httpeer \u2013 which supports all the standard HTTP verbage, but over some other peer to peer network. The DID spec supports service endpoints , and Alyssa could take advantage of these to use her DID as base of the inbox , outbox , etc URIs. Here's a cut down and modified version of the previous example: { \"@context\": [\"https://example.org/did/v1\", \"https://www.w3.org/ns/activitystreams\"], \"id\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt\", // ActivityPub actor information \"type\": \"Person\", \"name\": \"Alyssa P. Hacker\", \"preferredUsername\": \"alyssa\", \"summary\": \"Lisp enthusiast hailing from MIT\", \"inbox\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/inbox/\", \"outbox\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/outbox/\", \"followers\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/followers/\", \"following\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/following/\", \"liked\": \"did:example:d20Hg0teN72oFeo0iNYrblwqt/liked/\", // DDO information goes here \"service\": { \"httpeer\": \"dI0tuXjISZEadSH6QV9EhBEdccL4ouePdF8P57BJ\"}} Now that's an identity system!","title":"Distributed identity"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#append-only-systems-and-content-addressed-storage","text":"Finally, it's worth mentioning the idea of moving ActivityPub to an entirely append-only, content-addressed system for object storage, \"modification\", and retrieval. Much success has been seen in recent years with these systems, and doing so would allow for many of the side effects in the federation system to be dropped entirely. We leave this as a topic for a future paper.","title":"Append only systems and content addressed storage"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#conclusions","text":"ActivityPub goes a long way towards providing a standardized way to move the social web from isolated, centralized silos towards the decentralized nature that the World Wide Web is meant to encompass. Still, there is much to be done and improved. Yet there are risks in trying to engineer the right system all at once, and great is well known to be the enemy of good. Thankfully we do not need to throw out what we have to make the improvements which are discussed in this paper. ActivityPub already exists and works, and we can incrementally improve the systems we have and blur the line between the federated social web that works and more peer to peer systems which are desirable. By adding public key infrastructure and distributed identifiers to ActivityPub we can move from a decentralized system to a distributed one and truly build a network that is both self-sovereign and social.","title":"Conclusions"},{"location":"rwot5/topics-and-advance-readings/activitypub-decentralized-distributed/#acknowledgments","text":"Thanks to Evan Prodromou and Owen Shepherd for working on initial revisions of the ActivityPub standard. Thanks also to Jessica Tallon for being co-editor on ActivityPub and everyone who helped make ActivityPub happen , which is no small list. (Some of those people caught typos in the original version of the overview, as included in ActivityPub.) Thanks to Manu Sporny and Stephen Webber for thoughts and feedback about how to make ActivityPub into a more robust distributed system. Thanks to Dave Longley for pointing to the vocabulary drift / ambiguity of the terms \"decentralized\" and \"distributed\", which lead to the addition of Paul Baran's diagrams and some terminology clarification. 3 Thanks to Morgan Lemmer-Webber for the patience and for careful proofreading of this document. Thanks to Spec-Ops and Digital Bazaar for supporting my work/time on Verifiable Claims/Credentials. Though separate from my work on ActivityPub, obviously I've been thinking a lot about how to combine them this whole time. Thanks also to mray for the gorgeous illustrations in the overview section. This document and its images (with the exception of Paul Baran's drawings), like ActivityPub itself, are licensed under W3C's permissive document license .","title":"Acknowledgments"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/","text":"BTCR DIDs and DDOs IN PROGRESS By Kim Hamilton Duffy (Learning Machine), Ryan Grant, and Christopher Allen This assumes you are familiar with DID terminology at a basic level. Introduction BTCR is a DID method that is based on the Bitcoin blockchain. The BTCR DID scheme uses a TX Ref-encoded (described below) transaction on the Bitcoin blockchain. The DID Description is constructed from a combination of the transaction details and an optional \"continuation\" DID Description, the address of which is stored in the OP_RETURN field. This could be a link to an IPFS address of a DID Description with additional entities. The BTCR Hackathon readme has more context on BTCR DIDs beyond these basics. Note that at the time of the BTCR Hackathon, we were not yet using the new capabilities-based DID approach, so many details (e.g. control/owner keys) are out of date in the BTCR Hackathon repo. BTCR Transaction Structure Abbreviations: - Bi = bitcoin address i - Pi = public key i - Si = signing key i (or private key i) Creating the initial BTCR DID: - Create key set ( B0 / P0 / S0 ) - Create key set ( B1 / P1 / S1 ) - Create Bitcoin transaction as follows: - Output: Change address B1 - Optional output: OP_RETURN <link to DDO continuation> - Signing key is S0 , which reveals public key P0 in the transaction - Issue TX0 and wait for confirmation. Get TX Ref encoding of the transaction TXREF(TX0) At this point we have a DID of the format did:btcr:<TXREF(TX0)> The initial BTCR DID is shown in the left side of this diagram. Definitions and details BTCR DID Scheme The standard scheme for DIDs is: did:<method>:<specific-idstring> In this case, the method is \"btcr\". In the BTCR DID method, specific-idstring is a TX Ref of confirmed transactions on a Bitcoin chain. TX Refs TX Refs are described in BIP 136 \"Bech32 Encoded Transaction Position References\" (https://github.com/bitcoin/bips/pull/555). Among other advantages, they provide a concise way to refer to the confirmed transaction on a specific chain (testnet or mainnet) as a function of the block height and index. The important difference is that txid is just a hash of the transaction, which may not yet be confirmed, and does not encode the chain, whereas TX Ref must be confirmed (since it is based on the block height and index). Looking up a BTCR DID DID consumers need to be able to construct a DID Description from a DID. In BTCR that works as follows: Given a DID, we know transaction reference ( did:btcr:<TXREF(TX0)> ) Look up transaction. Is the transaction output spent? no: this is the latest version of the DID. From this we can construct the DID Description (described below) yes: keep following transaction chain until the latest with an unspent output is found Updating a DID Descripton An entity updates the BTCR DID Description by spending the current transaction output. The BTCR Transaction Structure diagram shows how that is done in this second transaction. Create new tx like above, but send to B2 Set the OP_RETURN to the new DID Description Sign tx with S1 (P1 is revealed) Example from the BTCR Playground This section demonstrates BTCR DIDs and DID Descriptions using the default example shown in the BTCR Playground . The playground supports looking up BTCR DID Description for both mainnet and testnet chains. In general, we work with the testnet chain in these examples as we experiment with and develop BTCR. The playground supports 3 means of entering a transaction: - TX Ref (note that we don't need to enter the chain; described below) - TXID and chain (testnet or mainnet) - TX block height, index, and chain (testnet or mainnet) To start, click \"Convert from TX Ref\" on the site with the default TX Ref. DID Description The DID Description resulting from DID txtest1-xkyt-fzgq-qq87-xnhn is listed below. Output { \"@context\": [ \"https://schema.org/\", \"https://w3id.org/security/v1\" ], \"authorization\": [ { \"capability\": \"UpdateDidDescription\", \"permittedProofType\": [ { \"proofType\": \"SatoshiBlockchainSignature2017\", \"authenticationCredential\": [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"hash-base58check\": \"mvq9zXGAr76uSoRG5ybEdECuXoPGY42ihh\" } ] } ] }, { \"capability\": \"IssueCredential\", \"permittedProofType\": [ [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn/keys/fundingKey\" } ] ], \"entity\": \"did:btcr:xkyt-fzgq-qq87-xnhn\" }, { \"capability\": \"IssueCredential\", \"entity\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"permittedProofType\": [ [ { \"id\": \"did:example:12345678/keys/1\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:example:12345678\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" } ] ] } ], \"authenticationCredential\": [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"curve\": \"secp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn/keys/fundingKey\" }, { \"id\": \"did:example:12345678/keys/1\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:example:12345678\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" } ], \"signature\": { \"type\": \"SatoshiBlockchainSignature2017\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"chain\": \"testnet\" } } Constructing a BTCR DID Description The DID Description for DID txtest1-xkyt-fzgq-qq87-xnhn was determined as follows. From the TX Ref, we look up details about the Bitcoin transaction. To see how to do this, refer to the transaction details for the TXID this resolves to, 67c0ee676221d9e0e08b98a55a8bf8add9cba854f13dda393e38ffa1b982b833 . This transaction has an OP_RETURN data output. For Blockcypher this is in the data_string field: https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld](https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld]). That means this DID Description will include additional authorizations and authenticationCredentials listed in the target of that URL. Parse the partial DID Description Important: the steps below assume that the partial DID Description is stored in an immutable store as opposed to github. If this were stored in github, the partial DID Description should be signed. This partial DID Description grants the following abilities: 1. Two entities may issue credentials. - One is an entity (currently) without an id, but described by its authentication method ( SatoshiBlockchainSignature2017 ) and its hex-encoded public key. Note this is the same as the key used to sign the transaction. - One is an entity defined in a separate DID Description, with a proofType of RsaSignature2017 2. Two entities may authenticate (as a TBD DID). These entities are similar to above. { \"@context\": \"https://w3id.org/btcr/v1\", \"authorization\": [ { // gives the entity with TBD DID the ability to issue credentials where the \"issuer\" field is TBD DID. TODO: This could be a default ability. \"capability\": \"IssueCredential\", \"permittedProofType\": [ { // Would we need a different type for off-chain signatures? \"proofType\": \"SatoshiBlockchainSignature2017\", \"authenticationCredential\": [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" } ] } ] }, { // enables an entity to issue credentials where the \"issuer\" field is TBD DID as long as this specific RSA key is used \"capability\": \"IssueCredential\", \"entity\": \"did:example:12345678\", \"permittedProofType\": [ { \"proofType\": \"RsaSignature2017\", \"authenticationCredential\": [ { \"id\": \"did:example:12345678/keys/1\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:example:12345678\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" } ] } ] } ], \"authenticationCredential\": [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"curve\": \"secp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" }, { \"id\": \"did:example:12345678/keys/1\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:example:12345678\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" } ] } About missing ids, entities, and owners Again we're assuming the partial DID Description is stored in an immutable store. Because the content is immutable, the transaction signature signs the content hash as well. If not using content-addressable store, then another LD signature scheme would be used. Because changing the content changes the address, and because the DID depends on the Bitcoin transaction reference, we have a bootstrapping problem where we cannot yet use the DID in the DID Description fragment shown above. We avoid this problem by omitting id , entity , and owner where the DID is not yet known. It's valid for a Linked Data object to omit an id; this is called a \"blank node\" and is the way you indicate you don't know the identifier yet, but you know its attributes. Merging the partial DID Description and transaction details With the partial DID Description and transaction details, we can form the complete DID Description. If an authorization is missing an entity in the partial DID Description, update it with the now known DID (did:btcr:xkyt-fzgq-qq87-xnhn) If an authenticationCredential is missing an owner , update it with the now known DID (same as above) If an authenticationCredential is missing an id there are 2 cases: If the credential's hex-encoded public key and proof type match that of the transaction signing key, populate the id with did:btcr:xkyt-fzgq-qq87-xnhn/keys/fundingKey . Note the /keys/fundingKey path is a convention Otherwise, populate the id with did:btcr:xkyt-fzgq-qq87-xnhn/keys/i , where i is the position of the authenticationCredential in the flattened partial DID Description. As of now, this is a very tentative convention. Populate an authorization with the ability to update the DID Description from the transaction output A BTCR DID is updated by spending the transaction output. We can inspect the transaction to determine the output Bitcoin address At this point, we only reveal the hashed, base58 encoded version of the output key (i.e. the Bitcoin address) Note that we do not yet know the entity value; that will not exist until the DID Description is updated We could create an id convention for this, e.g did:btcr:xyv2-xzyq-qqm5-tyke/keys/output_address Comments The method described above gives no default capabilities to an entity authenticating with the transaction signing key Capabilities are only granted if merged with a partial DID Description as described above We could consider granting default capabilities, but this does against the best practice of avoiding key reuse This means the BTCR DID is mostly useless without a partial DID Description, so we should consider this carefully Omitting id as opposed to the previous * microsyntax introduces the problem above in which we don't know the path suffix to assign to keys. We can solve this in at least 2 ways: Add a field to the BTCR DID method spec to address this. I.e. path and the assumption is that it will get merged with the DID to form the id value. Use an algorithmic approach like above, but I think that's a very bad idea The number of confirmations of a transaction underlying a BTCR DID should be considered (and automated in the tool constructing the DID Description) Censorship resistance With IPFS URI of raw hash value; this is not-censorship resistant First BTCR DID does not need an OP_RETURN. This increases censorship resistance. Subsequently must have OP_RETURN","title":"BTCR DIDs and DDOs"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#btcr-dids-and-ddos","text":"IN PROGRESS By Kim Hamilton Duffy (Learning Machine), Ryan Grant, and Christopher Allen This assumes you are familiar with DID terminology at a basic level.","title":"BTCR DIDs and DDOs"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#introduction","text":"BTCR is a DID method that is based on the Bitcoin blockchain. The BTCR DID scheme uses a TX Ref-encoded (described below) transaction on the Bitcoin blockchain. The DID Description is constructed from a combination of the transaction details and an optional \"continuation\" DID Description, the address of which is stored in the OP_RETURN field. This could be a link to an IPFS address of a DID Description with additional entities. The BTCR Hackathon readme has more context on BTCR DIDs beyond these basics. Note that at the time of the BTCR Hackathon, we were not yet using the new capabilities-based DID approach, so many details (e.g. control/owner keys) are out of date in the BTCR Hackathon repo.","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#btcr-transaction-structure","text":"Abbreviations: - Bi = bitcoin address i - Pi = public key i - Si = signing key i (or private key i) Creating the initial BTCR DID: - Create key set ( B0 / P0 / S0 ) - Create key set ( B1 / P1 / S1 ) - Create Bitcoin transaction as follows: - Output: Change address B1 - Optional output: OP_RETURN <link to DDO continuation> - Signing key is S0 , which reveals public key P0 in the transaction - Issue TX0 and wait for confirmation. Get TX Ref encoding of the transaction TXREF(TX0) At this point we have a DID of the format did:btcr:<TXREF(TX0)> The initial BTCR DID is shown in the left side of this diagram.","title":"BTCR Transaction Structure"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#definitions-and-details","text":"","title":"Definitions and details"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#btcr-did-scheme","text":"The standard scheme for DIDs is: did:<method>:<specific-idstring> In this case, the method is \"btcr\". In the BTCR DID method, specific-idstring is a TX Ref of confirmed transactions on a Bitcoin chain.","title":"BTCR DID Scheme"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#tx-refs","text":"TX Refs are described in BIP 136 \"Bech32 Encoded Transaction Position References\" (https://github.com/bitcoin/bips/pull/555). Among other advantages, they provide a concise way to refer to the confirmed transaction on a specific chain (testnet or mainnet) as a function of the block height and index. The important difference is that txid is just a hash of the transaction, which may not yet be confirmed, and does not encode the chain, whereas TX Ref must be confirmed (since it is based on the block height and index).","title":"TX Refs"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#looking-up-a-btcr-did","text":"DID consumers need to be able to construct a DID Description from a DID. In BTCR that works as follows: Given a DID, we know transaction reference ( did:btcr:<TXREF(TX0)> ) Look up transaction. Is the transaction output spent? no: this is the latest version of the DID. From this we can construct the DID Description (described below) yes: keep following transaction chain until the latest with an unspent output is found","title":"Looking up a BTCR DID"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#updating-a-did-descripton","text":"An entity updates the BTCR DID Description by spending the current transaction output. The BTCR Transaction Structure diagram shows how that is done in this second transaction. Create new tx like above, but send to B2 Set the OP_RETURN to the new DID Description Sign tx with S1 (P1 is revealed)","title":"Updating a DID Descripton"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#example-from-the-btcr-playground","text":"This section demonstrates BTCR DIDs and DID Descriptions using the default example shown in the BTCR Playground . The playground supports looking up BTCR DID Description for both mainnet and testnet chains. In general, we work with the testnet chain in these examples as we experiment with and develop BTCR. The playground supports 3 means of entering a transaction: - TX Ref (note that we don't need to enter the chain; described below) - TXID and chain (testnet or mainnet) - TX block height, index, and chain (testnet or mainnet) To start, click \"Convert from TX Ref\" on the site with the default TX Ref.","title":"Example from the BTCR Playground"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#did-description","text":"The DID Description resulting from DID txtest1-xkyt-fzgq-qq87-xnhn is listed below.","title":"DID Description"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#output","text":"{ \"@context\": [ \"https://schema.org/\", \"https://w3id.org/security/v1\" ], \"authorization\": [ { \"capability\": \"UpdateDidDescription\", \"permittedProofType\": [ { \"proofType\": \"SatoshiBlockchainSignature2017\", \"authenticationCredential\": [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"hash-base58check\": \"mvq9zXGAr76uSoRG5ybEdECuXoPGY42ihh\" } ] } ] }, { \"capability\": \"IssueCredential\", \"permittedProofType\": [ [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn/keys/fundingKey\" } ] ], \"entity\": \"did:btcr:xkyt-fzgq-qq87-xnhn\" }, { \"capability\": \"IssueCredential\", \"entity\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"permittedProofType\": [ [ { \"id\": \"did:example:12345678/keys/1\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:example:12345678\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" } ] ] } ], \"authenticationCredential\": [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"curve\": \"secp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\", \"owner\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn/keys/fundingKey\" }, { \"id\": \"did:example:12345678/keys/1\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:example:12345678\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" } ], \"signature\": { \"type\": \"SatoshiBlockchainSignature2017\", \"id\": \"did:btcr:xkyt-fzgq-qq87-xnhn\", \"chain\": \"testnet\" } }","title":"Output"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#constructing-a-btcr-did-description","text":"The DID Description for DID txtest1-xkyt-fzgq-qq87-xnhn was determined as follows. From the TX Ref, we look up details about the Bitcoin transaction. To see how to do this, refer to the transaction details for the TXID this resolves to, 67c0ee676221d9e0e08b98a55a8bf8add9cba854f13dda393e38ffa1b982b833 . This transaction has an OP_RETURN data output. For Blockcypher this is in the data_string field: https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld](https://raw.githubusercontent.com/kimdhamilton/did/master/ddo.jsonld]). That means this DID Description will include additional authorizations and authenticationCredentials listed in the target of that URL.","title":"Constructing a BTCR DID Description"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#parse-the-partial-did-description","text":"Important: the steps below assume that the partial DID Description is stored in an immutable store as opposed to github. If this were stored in github, the partial DID Description should be signed. This partial DID Description grants the following abilities: 1. Two entities may issue credentials. - One is an entity (currently) without an id, but described by its authentication method ( SatoshiBlockchainSignature2017 ) and its hex-encoded public key. Note this is the same as the key used to sign the transaction. - One is an entity defined in a separate DID Description, with a proofType of RsaSignature2017 2. Two entities may authenticate (as a TBD DID). These entities are similar to above. { \"@context\": \"https://w3id.org/btcr/v1\", \"authorization\": [ { // gives the entity with TBD DID the ability to issue credentials where the \"issuer\" field is TBD DID. TODO: This could be a default ability. \"capability\": \"IssueCredential\", \"permittedProofType\": [ { // Would we need a different type for off-chain signatures? \"proofType\": \"SatoshiBlockchainSignature2017\", \"authenticationCredential\": [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" } ] } ] }, { // enables an entity to issue credentials where the \"issuer\" field is TBD DID as long as this specific RSA key is used \"capability\": \"IssueCredential\", \"entity\": \"did:example:12345678\", \"permittedProofType\": [ { \"proofType\": \"RsaSignature2017\", \"authenticationCredential\": [ { \"id\": \"did:example:12345678/keys/1\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:example:12345678\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" } ] } ] } ], \"authenticationCredential\": [ { \"type\": [ \"EdDsaSAPublicKey\", \"CryptographicKey\" ], \"curve\": \"secp256k1\", \"publicKeyHex\": \"0280e0b456b9e97eecb8028215664c5b99ffa79628b60798edd9d562c6db1e4f85\" }, { \"id\": \"did:example:12345678/keys/1\", \"type\": \"RsaCryptographicKey\", \"owner\": \"did:example:12345678\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" } ] }","title":"Parse the partial DID Description"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#about-missing-ids-entities-and-owners","text":"Again we're assuming the partial DID Description is stored in an immutable store. Because the content is immutable, the transaction signature signs the content hash as well. If not using content-addressable store, then another LD signature scheme would be used. Because changing the content changes the address, and because the DID depends on the Bitcoin transaction reference, we have a bootstrapping problem where we cannot yet use the DID in the DID Description fragment shown above. We avoid this problem by omitting id , entity , and owner where the DID is not yet known. It's valid for a Linked Data object to omit an id; this is called a \"blank node\" and is the way you indicate you don't know the identifier yet, but you know its attributes.","title":"About missing ids, entities, and owners"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#merging-the-partial-did-description-and-transaction-details","text":"With the partial DID Description and transaction details, we can form the complete DID Description. If an authorization is missing an entity in the partial DID Description, update it with the now known DID (did:btcr:xkyt-fzgq-qq87-xnhn) If an authenticationCredential is missing an owner , update it with the now known DID (same as above) If an authenticationCredential is missing an id there are 2 cases: If the credential's hex-encoded public key and proof type match that of the transaction signing key, populate the id with did:btcr:xkyt-fzgq-qq87-xnhn/keys/fundingKey . Note the /keys/fundingKey path is a convention Otherwise, populate the id with did:btcr:xkyt-fzgq-qq87-xnhn/keys/i , where i is the position of the authenticationCredential in the flattened partial DID Description. As of now, this is a very tentative convention. Populate an authorization with the ability to update the DID Description from the transaction output A BTCR DID is updated by spending the transaction output. We can inspect the transaction to determine the output Bitcoin address At this point, we only reveal the hashed, base58 encoded version of the output key (i.e. the Bitcoin address) Note that we do not yet know the entity value; that will not exist until the DID Description is updated We could create an id convention for this, e.g did:btcr:xyv2-xzyq-qqm5-tyke/keys/output_address","title":"Merging the partial DID Description and transaction details"},{"location":"rwot5/topics-and-advance-readings/btcr-dids-ddos/#comments","text":"The method described above gives no default capabilities to an entity authenticating with the transaction signing key Capabilities are only granted if merged with a partial DID Description as described above We could consider granting default capabilities, but this does against the best practice of avoiding key reuse This means the BTCR DID is mostly useless without a partial DID Description, so we should consider this carefully Omitting id as opposed to the previous * microsyntax introduces the problem above in which we don't know the path suffix to assign to keys. We can solve this in at least 2 ways: Add a field to the BTCR DID method spec to address this. I.e. path and the assumption is that it will get merged with the DID to form the id value. Use an algorithmic approach like above, but I think that's a very bad idea The number of confirmations of a transaction underlying a BTCR DID should be considered (and automated in the tool constructing the DID Description) Censorship resistance With IPFS URI of raw hash value; this is not-censorship resistant First BTCR DID does not need an OP_RETURN. This increases censorship resistance. Subsequently must have OP_RETURN","title":"Comments"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/","text":"Implementation available at https://github.com/yahoo/bftkv BFTKV Reliable data storage is one of the fundamental problems. BFTKV uses b-masking quorum based read / write operations to ensure Byzantine fault-tolerance and GPG's Web of Trust mechanism to build trust relationships between entities. Trust relationships are used to build quorums. Moreover, BFTKV provides the following guarantees: Value corresponding to the key is up to date and not forged Entities trying to deceive users will be revoked immediately Entities can join and leave the system dynamically Communications between entities are encrypted using public keys Design BFTKV leverages integration of three concepts to provide a Byzantine fault tolerant distributed key-value storage: Byzantine Quorum Systems Web of Trust Quorum Certificate In this document, we will first describe PGP's Web of Trust mechanism and then build the other concepts on top of it. Background Web of Trust Web of Trust is a way of building trust between entities without a central authority, unlike Public Key Infrastructure (PKI). Trust is established by signing public keys (implies the signer trusts the owner of the signed public key). A Web of Trust is created by exchanging signed keys between entities. Trust relationships can be represented with a graph, such as this: The graph can be transcribed as: Alice trusts Bob and Erin Bob trusts Erin and Dave Carol trusts Bob and Erin Erin trusts Alice, Carol and Dave Web of Trust mechanism plays a huge role in BFTKV's quorum selection mechanism. Byzantine Quorum Systems In a network system, servers might be inaccessible or return wrong/not up to date data. Byzantine failure refers to both of these failures and the naming is based on The Byzantine General's Problem . BFTKV uses b-masking quorum s to tolerate Byzantine failures where b is the number of failure nodes. b-masking quorum s are due to Malkhi and Reiter ( Paper ). Quorum Certificate Castro and Liskov ( Paper ) introduced a Byzantine fault tolerant replication mechanism that expects f+1 responses from the servers to verify the data, where f is the number of faulty nodes. We use the Web of Trust mechanism to specify the nodes that their responses will be accepted by a quorum member. The following parts of this document deals with how previously discussed concepts are used in BFTKV. Implementation Quorum Selection Quorum selection is based on the trust graph built using the Web of Trust mechanism. BFTKV, usually, chooses the maximal cliques that are L hops away from the clients. For example, Client1 has two cliques: Clique 1 and Clique 2 where L=1 Client2 has two cliques: Clique 2 and Clique 3 where L=1 Write The Write procedure saves a value associated with a key in the system. A high-level pseudocode for the procedure: Choose a quorum. Get times for the key from quorum members. Pick a new time that is higher than the maximum time returned by the quorum members. Request and gather signatures from quorum members for new value for the key with the new timestamp. Choose another quorum that includes the first quorum. Write the key, value and signature set to the new quorum members. Read The read procedure reads a value associated with a key in the system. A high level pseudocode for the procedure: Choose a quorum. Collect values associated with the key Revoke signers who signed different values with the same timestamp. Return value having signatures more than the number of faulty nodes and has the maximum timestamp. Design Decisions and Security Analysis In this section, we will go over somewhat unclear points in the chapters we discussed read / write operations. Write The quorum Q chosen here should have the property |Q| >= 3b + 1 where b is the number of faulty nodes. This is required for Byzantine fault tolerance since f nodes may be inaccessible and f nodes may be returning a previous value for the key. The remaining honest f + 1 nodes will keep the system in a safe condition. As Q , BFTKV uses a maximal clique that a client is connected to in the trust graph. Number of timestamps should be greater than or equal to 2b + 1 since we will tolerate b inaccessible nodes. - The number of signatures m should be greater than b + (n - b) / 2 . Please see the security analysis for details. All nodes may be chosen which is BFTKV's current strategy. Before writing, each server verifies the signature, checks the number of valid signatures gathered from quorum members and accepts write if the number is greater than b + (n - b) / 2 . Moreover, every server makes sure that they haven't signed this key with the same timestamp before. write operation succeeds if the received acks from server is greater than 2f + 1 . Read BFTKV chooses a random quorum Q that has the property |Q| > b + (n - b) / 2 . Collect pairs up to 2f + 1 . The reason is the same with write operation second explanation. A server should not sign the same key, same timestamp and a different value. This is equivocation attack. It is very important that servers revoke these nodes at this phase for the system to survive. To return a value for the key, the value should have at least b + 1 signatures, which guarantees that the value is valid, and have the maximum timestamp. Security Analysis Equivocation Attack: An adversary can try to create two different views of a quorum by trying to store different values for a key in half of the quorum and another value in the other half (Half is the best option from an attacker's perspective if he wants to succeed). With the help of the b faulty nodes, the basic check for b + 1 signatures will succeed. However, if the quorum size is chosen carefully, this can be prevented. Consider the node states below ( n = the number of nodes in the quorum, b = faulty nodes in the quorum): Maximum number of signatures an attacker can get is b + (n - b) / 2 . To make sure that the majority has the correct value n - b > b + (n - b) / 2 should hold. Therefore n should be greater than 3b . Detecting Equivocation on Read Let Fp define the failure probability of an adversary (i.e., he won't be detected); H1 and H2 honest node sets, F the set of faulty nodes and N = H1 U H2 U F . Then the nodes chosen from the quorum Q should be either from H1 U F or H2 U F to prevent detection. This probability is Fp ~= 1 - ((|F| + |N|) / 2|N|)^|Q| In a reqular quorum system after if the number of faulty nodes exceed n/3 trust to data drops down to 0. BFTKV can keep the adversary's failure probability close to 1 for more than f failing nodes. Below two graphs represent this: Visualization of the System Write in Action Revoke on Read in Action","title":"Byzantine fault tolerant web of trust based key value storage"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#bftkv","text":"Reliable data storage is one of the fundamental problems. BFTKV uses b-masking quorum based read / write operations to ensure Byzantine fault-tolerance and GPG's Web of Trust mechanism to build trust relationships between entities. Trust relationships are used to build quorums. Moreover, BFTKV provides the following guarantees: Value corresponding to the key is up to date and not forged Entities trying to deceive users will be revoked immediately Entities can join and leave the system dynamically Communications between entities are encrypted using public keys","title":"BFTKV"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#design","text":"BFTKV leverages integration of three concepts to provide a Byzantine fault tolerant distributed key-value storage: Byzantine Quorum Systems Web of Trust Quorum Certificate In this document, we will first describe PGP's Web of Trust mechanism and then build the other concepts on top of it.","title":"Design"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#background","text":"","title":"Background"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#web-of-trust","text":"Web of Trust is a way of building trust between entities without a central authority, unlike Public Key Infrastructure (PKI). Trust is established by signing public keys (implies the signer trusts the owner of the signed public key). A Web of Trust is created by exchanging signed keys between entities. Trust relationships can be represented with a graph, such as this: The graph can be transcribed as: Alice trusts Bob and Erin Bob trusts Erin and Dave Carol trusts Bob and Erin Erin trusts Alice, Carol and Dave Web of Trust mechanism plays a huge role in BFTKV's quorum selection mechanism.","title":"Web of Trust"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#byzantine-quorum-systems","text":"In a network system, servers might be inaccessible or return wrong/not up to date data. Byzantine failure refers to both of these failures and the naming is based on The Byzantine General's Problem . BFTKV uses b-masking quorum s to tolerate Byzantine failures where b is the number of failure nodes. b-masking quorum s are due to Malkhi and Reiter ( Paper ).","title":"Byzantine Quorum Systems"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#quorum-certificate","text":"Castro and Liskov ( Paper ) introduced a Byzantine fault tolerant replication mechanism that expects f+1 responses from the servers to verify the data, where f is the number of faulty nodes. We use the Web of Trust mechanism to specify the nodes that their responses will be accepted by a quorum member. The following parts of this document deals with how previously discussed concepts are used in BFTKV.","title":"Quorum Certificate"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#implementation","text":"","title":"Implementation"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#quorum-selection","text":"Quorum selection is based on the trust graph built using the Web of Trust mechanism. BFTKV, usually, chooses the maximal cliques that are L hops away from the clients. For example, Client1 has two cliques: Clique 1 and Clique 2 where L=1 Client2 has two cliques: Clique 2 and Clique 3 where L=1","title":"Quorum Selection"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#write","text":"The Write procedure saves a value associated with a key in the system. A high-level pseudocode for the procedure: Choose a quorum. Get times for the key from quorum members. Pick a new time that is higher than the maximum time returned by the quorum members. Request and gather signatures from quorum members for new value for the key with the new timestamp. Choose another quorum that includes the first quorum. Write the key, value and signature set to the new quorum members.","title":"Write"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#read","text":"The read procedure reads a value associated with a key in the system. A high level pseudocode for the procedure: Choose a quorum. Collect values associated with the key Revoke signers who signed different values with the same timestamp. Return value having signatures more than the number of faulty nodes and has the maximum timestamp.","title":"Read"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#design-decisions-and-security-analysis","text":"In this section, we will go over somewhat unclear points in the chapters we discussed read / write operations.","title":"Design Decisions and Security Analysis"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#write_1","text":"The quorum Q chosen here should have the property |Q| >= 3b + 1 where b is the number of faulty nodes. This is required for Byzantine fault tolerance since f nodes may be inaccessible and f nodes may be returning a previous value for the key. The remaining honest f + 1 nodes will keep the system in a safe condition. As Q , BFTKV uses a maximal clique that a client is connected to in the trust graph. Number of timestamps should be greater than or equal to 2b + 1 since we will tolerate b inaccessible nodes. - The number of signatures m should be greater than b + (n - b) / 2 . Please see the security analysis for details. All nodes may be chosen which is BFTKV's current strategy. Before writing, each server verifies the signature, checks the number of valid signatures gathered from quorum members and accepts write if the number is greater than b + (n - b) / 2 . Moreover, every server makes sure that they haven't signed this key with the same timestamp before. write operation succeeds if the received acks from server is greater than 2f + 1 .","title":"Write"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#read_1","text":"BFTKV chooses a random quorum Q that has the property |Q| > b + (n - b) / 2 . Collect pairs up to 2f + 1 . The reason is the same with write operation second explanation. A server should not sign the same key, same timestamp and a different value. This is equivocation attack. It is very important that servers revoke these nodes at this phase for the system to survive. To return a value for the key, the value should have at least b + 1 signatures, which guarantees that the value is valid, and have the maximum timestamp.","title":"Read"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#security-analysis","text":"Equivocation Attack: An adversary can try to create two different views of a quorum by trying to store different values for a key in half of the quorum and another value in the other half (Half is the best option from an attacker's perspective if he wants to succeed). With the help of the b faulty nodes, the basic check for b + 1 signatures will succeed. However, if the quorum size is chosen carefully, this can be prevented. Consider the node states below ( n = the number of nodes in the quorum, b = faulty nodes in the quorum): Maximum number of signatures an attacker can get is b + (n - b) / 2 . To make sure that the majority has the correct value n - b > b + (n - b) / 2 should hold. Therefore n should be greater than 3b .","title":"Security Analysis"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#detecting-equivocation-on-read","text":"Let Fp define the failure probability of an adversary (i.e., he won't be detected); H1 and H2 honest node sets, F the set of faulty nodes and N = H1 U H2 U F . Then the nodes chosen from the quorum Q should be either from H1 U F or H2 U F to prevent detection. This probability is Fp ~= 1 - ((|F| + |N|) / 2|N|)^|Q| In a reqular quorum system after if the number of faulty nodes exceed n/3 trust to data drops down to 0. BFTKV can keep the adversary's failure probability close to 1 for more than f failing nodes. Below two graphs represent this:","title":"Detecting Equivocation on Read"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#visualization-of-the-system","text":"","title":"Visualization of the System"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#write-in-action","text":"","title":"Write in Action"},{"location":"rwot5/topics-and-advance-readings/byzantine-fault-tolerant-web-of-trust-based-key-value-storage/#revoke-on-read-in-action","text":"","title":"Revoke on Read in Action"},{"location":"rwot5/topics-and-advance-readings/credential-handler-api/","text":"Decentralized Identifier Tooling by Dave Longley and Manu Sporny, Digital Bazaar Introduction The Verifiable Claims Ecosystem envisions a variety of ways that credentials will be exchanged between Issuers, Holders, and Verifiers. Protocols that use Verifiable Claims to exchange data could be designed such that they are compatible with existing SAML and OpenID Connect-based systems. We also expect Verifiable Claims to be carried over protocols such as HTTP, Bluetooth, and RFID. Over the years, Digital Bazaar has proposed various mechanisms that could be used to carry Verifiable Claims over browser-based protocols. This paper focuses on a recent advance in this area. Credential Handler API Digital Bazaar has created a browser-based polyfill called the Credential Handler API that is designed to send and receive Verifiable Claims. A polyfill is a browser fallback, made in JavaScript, that allows functionality that a developer expects to work in modern browsers to work in older browsers, e.g., support for new Javascript (ES6) features. When incubating a technology intended for browsers, it is often useful to polyfill the technology and deploy it into limited usage to ensure that it works in the field. This data can later be used to convince browser manufacturers that there is a measured need for the technology to be built into the browser natively. What follows is a video demonstrating the polyfill for Verifiable Claims that Digital Bazaar has created: Credential Handler API Video Credential Handler Source Code The Credential Handler source code is available on Github: Credential Handler Github Repository Credential Handler Demo Websites Github Repository The source code can be used on the demonstration websites here: Credential Handler Demo Websites Collaboration We are seeking to collaborate with individuals at this Rebooting Web of Trust event to review the user interaction flow for the Credential Handler API. We are also looking for developers who think that this polyfill would be useful to their organization when deploying Verifiable Claims to their customers. Finally, we're looking for feedback from developers on the code examples to determine if they would like to see any changes or modifications to the polyfill.","title":"Decentralized Identifier Tooling"},{"location":"rwot5/topics-and-advance-readings/credential-handler-api/#decentralized-identifier-tooling","text":"by Dave Longley and Manu Sporny, Digital Bazaar","title":"Decentralized Identifier Tooling"},{"location":"rwot5/topics-and-advance-readings/credential-handler-api/#introduction","text":"The Verifiable Claims Ecosystem envisions a variety of ways that credentials will be exchanged between Issuers, Holders, and Verifiers. Protocols that use Verifiable Claims to exchange data could be designed such that they are compatible with existing SAML and OpenID Connect-based systems. We also expect Verifiable Claims to be carried over protocols such as HTTP, Bluetooth, and RFID. Over the years, Digital Bazaar has proposed various mechanisms that could be used to carry Verifiable Claims over browser-based protocols. This paper focuses on a recent advance in this area.","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/credential-handler-api/#credential-handler-api","text":"Digital Bazaar has created a browser-based polyfill called the Credential Handler API that is designed to send and receive Verifiable Claims. A polyfill is a browser fallback, made in JavaScript, that allows functionality that a developer expects to work in modern browsers to work in older browsers, e.g., support for new Javascript (ES6) features. When incubating a technology intended for browsers, it is often useful to polyfill the technology and deploy it into limited usage to ensure that it works in the field. This data can later be used to convince browser manufacturers that there is a measured need for the technology to be built into the browser natively. What follows is a video demonstrating the polyfill for Verifiable Claims that Digital Bazaar has created: Credential Handler API Video","title":"Credential Handler API"},{"location":"rwot5/topics-and-advance-readings/credential-handler-api/#credential-handler-source-code","text":"The Credential Handler source code is available on Github: Credential Handler Github Repository Credential Handler Demo Websites Github Repository The source code can be used on the demonstration websites here: Credential Handler Demo Websites","title":"Credential Handler Source Code"},{"location":"rwot5/topics-and-advance-readings/credential-handler-api/#collaboration","text":"We are seeking to collaborate with individuals at this Rebooting Web of Trust event to review the user interaction flow for the Credential Handler API. We are also looking for developers who think that this polyfill would be useful to their organization when deploying Verifiable Claims to their customers. Finally, we're looking for feedback from developers on the code examples to determine if they would like to see any changes or modifications to the polyfill.","title":"Collaboration"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/","text":"DID for the 3D Web By Alberto Elias Introduction Virtual Personas are a P2P Web and blockchain based DID identity system. It\u2019s an identity system thought out to fulfill all needs on the decentralized 3D Web, that requires a single global identity for all sites to make the experience seamless. The user experience to have a different avatar and profile for each site would be subpar. The DID is a content address that points to a DDO that is hosted as an RDF file in a decentralized file system such as IPFS , DAT or Swarm . The DDO can hold certificates that prove the information stated within it, but also the ownership of blockchain addresses. Both IPFS and Dat, when creating a new decentralized directory, they create a private key, and only the owner of said key can update the content. To properly tie the Web identity with the blockchain identity, I\u2019m looking into using the same private key that holds the DDO for a blockchain public address. Currently, they might use different signature systems, but there might be a way to derive one from the other. Looking forward, it\u2019d be interesting to look deeper into this idea of having just one key pair to control every aspect of an identity, and the public key acting as both the DID and the public address on the blockchain. Blockchains can bring in value as they allow identities to easily handle payments in different cryptocurrencies, hold virtual property or interact in new token based protocols. They are not efficient enough to have an identity system that\u2019s solely blockchain based, and they\u2019re not Web compliant, but this identity system can work off-chain, and do all necessary transactions on-chain easily. Also, many of their shortcomings are being worked on, like what the Ethereum project is doing with the transition to Proof of Stake or Plasma . Currently, DIDs would be public keys which are very hard to remember. DNS is a centralized system, but both IPFS and Dat are working to make their decentralized protocols work better with DNS. This could be a temporary option while decentralized naming systems like Blockstack and ENS are further developed. DDO Format It should be in an RDF compatible format like JSON-LD or Turtle . The base format is suggested in the DID spec . On top of that, I\u2019d like to suggest some additions. I believe the SOLID project has many interesting ideas about designing a Web compliant identity system. It has some equivalences with the DID spec like a WebID being a DID, and a WebID profile being a DDO. The DDO would be a multi file object, each file holding different pieces of information and with different access control policies. An identity could decide to share all her information just to herself, most of it just with friends and maybe just her job title to unknown identities. Verifiable Claims would also work just fine with this proposal, which will allow entities to prove facts about their identities by providing a proof from a 3rd party (like a government issued ID), without having to reveal information that wasn\u2019t solicited. For the 3D Web, some other properties are needed in the DDO. A clear example would be a GLTF formatted avatar, so the identity can have a single avatar for all sites. At some point, we\u2019ll have virtual objects that can be possesed by identities. There are several blockchain projects like Mediachain to handle property which can be held and transferred by an identity. Web APIs There are many existing specs, and others that are being worked on that can make this possible. For starters, DID s and WebID s have specs, and the Verifiable Claims Working Group are doing great work on that front. Authentication is another important element. We have the Credential Management API , though it only supports passwords and federated logins. It is compatible with creating custom credentials, so a new key-pair system could be implemented. A site could check that it\u2019s the owner of a DID by sending a message encrypted with the DID\u2019s public key, the owner decrypts it with their private key, and sends it back encrypted with the sites public key. From there, we would need to start working on a browser API for identity management. This would provide features such as soliciting permissions to read specific information fields and handling social aspects like adding friends, following another identity or adding new information. The team from Beaker Browser , a Dat based browser, are building a social API based on Dat archives. Framework This paper by Kyle Den Hartog sets a framework for identity systems to compare them with each other. Below I explain how Virtual Personas apply to this framework. There are four categories with different subcategories ranked as High, Medium and Low*. Verifiability Trusted source: High . It is provable that the DID is associated to a DDO, and that the DDO is owned by a public key. That DDO can only be modified by the entity that owns the corresponding private key. There can be Verifiable Claims in the DDO that can prove information about the identity, but these come from third party trusted sources such as government agencies. Verification method: High . A DDO has an owner associated with it, and only the entity that has the private key corresponding to the public key in the DDO can be verified as said owner. System Architecture Organizational structures: High. DIDs can have multiple owners. Centralization: High. Completely decentralized system since DDOs are hosted on the network, and anyone can host them. DIDs are just the address to that content wherever it may be in the network. Accessibility Independence: High. Everything will be completely open source and is based on open standards. Elements that aren\u2019t standards yet will be proposed as such. Devices: Medium. Entity needs a device (phone, laptop, hardware key..) to hold the private key. Deployability: Medium. Virtual Personas are based on current standards and new ones that can be implemented as tools, like libraries, while they\u2019re developed. Portability: Medium. You do need to carry a device with the private key to use the identity in both physical and digital environments. Identity removal: Low. An entity owner could stop hosting a DDO, but there could be others hosting it, specially if it\u2019s a public person. Blockchain addresses can\u2019t be deleted, nor the transactions linked to it, but it\u2019s usage with its assigned identity can be blocked. Also, entities control which pieces of information they decide to share, and at any time they could decide to share less. Security Integrity: High. DDOs can only be modified by those holding the private key to that content. Also, protocols like DAT keep a history of changes. Confidentiality: High. Only entities with the allowed private keys can read/write information related to an identity that explicitly gives them the necessary permissions. Theft prevention: Medium. Devices holding a private key can be compromised, though systems can be in place to revoke a private key and swap it for a new one. Data revocation: Medium. At any time, an entity could reemove any data they want except for transactions made with other entities as they could still hold a public log of transactions. Also, some protocols keep a history of changes, so they wouldn't be deleted from there. With proper access control policies, access to the information can be revoked. Non-anonymous transactions made on a public blockchain also can\u2019t be hidden. Anonymity: High. DDOs don\u2019t need to hold critical pieces of information about an entity. They can be empty even. Also, they can contain private information. I believe this to be a good compromise on the different categories, with identity removal being the main issue. It\u2019d be great to work more on finding a way around current issues.","title":"DID for the 3D Web"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/#did-for-the-3d-web","text":"By Alberto Elias","title":"DID for the 3D Web"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/#introduction","text":"Virtual Personas are a P2P Web and blockchain based DID identity system. It\u2019s an identity system thought out to fulfill all needs on the decentralized 3D Web, that requires a single global identity for all sites to make the experience seamless. The user experience to have a different avatar and profile for each site would be subpar. The DID is a content address that points to a DDO that is hosted as an RDF file in a decentralized file system such as IPFS , DAT or Swarm . The DDO can hold certificates that prove the information stated within it, but also the ownership of blockchain addresses. Both IPFS and Dat, when creating a new decentralized directory, they create a private key, and only the owner of said key can update the content. To properly tie the Web identity with the blockchain identity, I\u2019m looking into using the same private key that holds the DDO for a blockchain public address. Currently, they might use different signature systems, but there might be a way to derive one from the other. Looking forward, it\u2019d be interesting to look deeper into this idea of having just one key pair to control every aspect of an identity, and the public key acting as both the DID and the public address on the blockchain. Blockchains can bring in value as they allow identities to easily handle payments in different cryptocurrencies, hold virtual property or interact in new token based protocols. They are not efficient enough to have an identity system that\u2019s solely blockchain based, and they\u2019re not Web compliant, but this identity system can work off-chain, and do all necessary transactions on-chain easily. Also, many of their shortcomings are being worked on, like what the Ethereum project is doing with the transition to Proof of Stake or Plasma . Currently, DIDs would be public keys which are very hard to remember. DNS is a centralized system, but both IPFS and Dat are working to make their decentralized protocols work better with DNS. This could be a temporary option while decentralized naming systems like Blockstack and ENS are further developed.","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/#ddo-format","text":"It should be in an RDF compatible format like JSON-LD or Turtle . The base format is suggested in the DID spec . On top of that, I\u2019d like to suggest some additions. I believe the SOLID project has many interesting ideas about designing a Web compliant identity system. It has some equivalences with the DID spec like a WebID being a DID, and a WebID profile being a DDO. The DDO would be a multi file object, each file holding different pieces of information and with different access control policies. An identity could decide to share all her information just to herself, most of it just with friends and maybe just her job title to unknown identities. Verifiable Claims would also work just fine with this proposal, which will allow entities to prove facts about their identities by providing a proof from a 3rd party (like a government issued ID), without having to reveal information that wasn\u2019t solicited. For the 3D Web, some other properties are needed in the DDO. A clear example would be a GLTF formatted avatar, so the identity can have a single avatar for all sites. At some point, we\u2019ll have virtual objects that can be possesed by identities. There are several blockchain projects like Mediachain to handle property which can be held and transferred by an identity.","title":"DDO Format"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/#web-apis","text":"There are many existing specs, and others that are being worked on that can make this possible. For starters, DID s and WebID s have specs, and the Verifiable Claims Working Group are doing great work on that front. Authentication is another important element. We have the Credential Management API , though it only supports passwords and federated logins. It is compatible with creating custom credentials, so a new key-pair system could be implemented. A site could check that it\u2019s the owner of a DID by sending a message encrypted with the DID\u2019s public key, the owner decrypts it with their private key, and sends it back encrypted with the sites public key. From there, we would need to start working on a browser API for identity management. This would provide features such as soliciting permissions to read specific information fields and handling social aspects like adding friends, following another identity or adding new information. The team from Beaker Browser , a Dat based browser, are building a social API based on Dat archives.","title":"Web APIs"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/#framework","text":"This paper by Kyle Den Hartog sets a framework for identity systems to compare them with each other. Below I explain how Virtual Personas apply to this framework. There are four categories with different subcategories ranked as High, Medium and Low*.","title":"Framework"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/#verifiability","text":"Trusted source: High . It is provable that the DID is associated to a DDO, and that the DDO is owned by a public key. That DDO can only be modified by the entity that owns the corresponding private key. There can be Verifiable Claims in the DDO that can prove information about the identity, but these come from third party trusted sources such as government agencies. Verification method: High . A DDO has an owner associated with it, and only the entity that has the private key corresponding to the public key in the DDO can be verified as said owner.","title":"Verifiability"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/#system-architecture","text":"Organizational structures: High. DIDs can have multiple owners. Centralization: High. Completely decentralized system since DDOs are hosted on the network, and anyone can host them. DIDs are just the address to that content wherever it may be in the network.","title":"System Architecture"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/#accessibility","text":"Independence: High. Everything will be completely open source and is based on open standards. Elements that aren\u2019t standards yet will be proposed as such. Devices: Medium. Entity needs a device (phone, laptop, hardware key..) to hold the private key. Deployability: Medium. Virtual Personas are based on current standards and new ones that can be implemented as tools, like libraries, while they\u2019re developed. Portability: Medium. You do need to carry a device with the private key to use the identity in both physical and digital environments. Identity removal: Low. An entity owner could stop hosting a DDO, but there could be others hosting it, specially if it\u2019s a public person. Blockchain addresses can\u2019t be deleted, nor the transactions linked to it, but it\u2019s usage with its assigned identity can be blocked. Also, entities control which pieces of information they decide to share, and at any time they could decide to share less.","title":"Accessibility"},{"location":"rwot5/topics-and-advance-readings/did-3d-web/#security","text":"Integrity: High. DDOs can only be modified by those holding the private key to that content. Also, protocols like DAT keep a history of changes. Confidentiality: High. Only entities with the allowed private keys can read/write information related to an identity that explicitly gives them the necessary permissions. Theft prevention: Medium. Devices holding a private key can be compromised, though systems can be in place to revoke a private key and swap it for a new one. Data revocation: Medium. At any time, an entity could reemove any data they want except for transactions made with other entities as they could still hold a public log of transactions. Also, some protocols keep a history of changes, so they wouldn't be deleted from there. With proper access control policies, access to the information can be revoked. Non-anonymous transactions made on a public blockchain also can\u2019t be hidden. Anonymity: High. DDOs don\u2019t need to hold critical pieces of information about an entity. They can be empty even. Also, they can contain private information. I believe this to be a good compromise on the different categories, with identity removal being the main issue. It\u2019d be great to work more on finding a way around current issues.","title":"Security"},{"location":"rwot5/topics-and-advance-readings/did-gentle-intro/","text":"DID Gentle Introduction By Kim Hamilton Duffy, Learning Machine IN PROGRESS The BTCR hackathon in July was my first exposure to DIDs and DDOs. I wrote this very introductory document to help consolidate my understanding. Because my understanding of DIDs/DDOs is largely restricted to BTCR (and relatively new at that), details may be wrong. But I hope it will provide others with a more user-friendly intro to the DID spec . Motivation At a vert high level, DID is solving the same problems as PGP. But PGP does things in a way that's confusing from a usability perspective. The primary concept that's difficult in PGP is key recovery. (Technically, PGP can be set up with a cold storage key, but this is an option that many users overlook). The goal is to take the lessons from PGP and improve on usage. Decentralized Identifiers (DIDs) are a fundamental tool in accomplishing this. Improvements over PGP DID aims to provide simple key recovery options, including: - social recovery, i.e. a group of friends you've designated in advance. Examples: - uPort - Sovrin - hardware wallets: for secure self-recovery DID provides a flexible mechanism of proof of control; proof of control can be provided in multiple ways (in contrast to PGP). Additionally, use of a blockchain (which is not critical to DID, but is used in many DID methods) provides an advantage over central servers used in PGP. Important concepts DDOs DIDs are almost always accompanied by DID description objects (DDOs). DDOs contain additional detail about the DID, including: - How an individual can cryptographically prove ownership of the DID and DDO. - How an individual can cryptographically prove control of the DDO. To drill into this distinction, proof of control applies to the ability to update the DDO itself. Updating the DDO would be desired, for example, when rotating keys, adding other new keys (e.g. a PGP key), revoking leaked keys. Owner vs control and Guardians Note that an owner of a DID/DDO may want to give some other trusted party the ability to update (\"control\") the DDO. This is useful, for example, if I want to let someone else update the DDO on my behalf if I lose my private key(s). I can give the trusted party the ability to update the DDO, without them impersonating me, by listing their DID (or DID per trusted party) in the control block of my DDO. Control is also relevant in a \"Guardian\"-managed DDO, i.e. when a trustee is managing an identity on behalf of a user. Guardian-managed identities take the burden off an individual to manage their own private keys. So this is relevant for usability, but also when a person cannot manage private keys. For the latter, think newborn baby: perhaps we want to assign a decentralized identity, but it cannot reasonably be owned by the baby until it is 6 months? 1 year? I guess it depends on how smart the baby is. If I allow a guardian to manage my DID, the guardian may sign on my behalf, and another party would trust them when they sign on my behalf (per the DDO rules). However, the guardian is not in a position to impersonate me. Key-related FAQs A DDO may contain other keys that are useful. In an owner-managed DDO, I will always have at least one primary key, but I may also add other keys that I use in different contexts. If the DDO is signed, the creator must correspond to a control key","title":"DID Gentle Introduction"},{"location":"rwot5/topics-and-advance-readings/did-gentle-intro/#did-gentle-introduction","text":"By Kim Hamilton Duffy, Learning Machine IN PROGRESS The BTCR hackathon in July was my first exposure to DIDs and DDOs. I wrote this very introductory document to help consolidate my understanding. Because my understanding of DIDs/DDOs is largely restricted to BTCR (and relatively new at that), details may be wrong. But I hope it will provide others with a more user-friendly intro to the DID spec .","title":"DID Gentle Introduction"},{"location":"rwot5/topics-and-advance-readings/did-gentle-intro/#motivation","text":"At a vert high level, DID is solving the same problems as PGP. But PGP does things in a way that's confusing from a usability perspective. The primary concept that's difficult in PGP is key recovery. (Technically, PGP can be set up with a cold storage key, but this is an option that many users overlook). The goal is to take the lessons from PGP and improve on usage. Decentralized Identifiers (DIDs) are a fundamental tool in accomplishing this.","title":"Motivation"},{"location":"rwot5/topics-and-advance-readings/did-gentle-intro/#improvements-over-pgp","text":"DID aims to provide simple key recovery options, including: - social recovery, i.e. a group of friends you've designated in advance. Examples: - uPort - Sovrin - hardware wallets: for secure self-recovery DID provides a flexible mechanism of proof of control; proof of control can be provided in multiple ways (in contrast to PGP). Additionally, use of a blockchain (which is not critical to DID, but is used in many DID methods) provides an advantage over central servers used in PGP.","title":"Improvements over PGP"},{"location":"rwot5/topics-and-advance-readings/did-gentle-intro/#important-concepts","text":"","title":"Important concepts"},{"location":"rwot5/topics-and-advance-readings/did-gentle-intro/#ddos","text":"DIDs are almost always accompanied by DID description objects (DDOs). DDOs contain additional detail about the DID, including: - How an individual can cryptographically prove ownership of the DID and DDO. - How an individual can cryptographically prove control of the DDO. To drill into this distinction, proof of control applies to the ability to update the DDO itself. Updating the DDO would be desired, for example, when rotating keys, adding other new keys (e.g. a PGP key), revoking leaked keys.","title":"DDOs"},{"location":"rwot5/topics-and-advance-readings/did-gentle-intro/#owner-vs-control-and-guardians","text":"Note that an owner of a DID/DDO may want to give some other trusted party the ability to update (\"control\") the DDO. This is useful, for example, if I want to let someone else update the DDO on my behalf if I lose my private key(s). I can give the trusted party the ability to update the DDO, without them impersonating me, by listing their DID (or DID per trusted party) in the control block of my DDO. Control is also relevant in a \"Guardian\"-managed DDO, i.e. when a trustee is managing an identity on behalf of a user. Guardian-managed identities take the burden off an individual to manage their own private keys. So this is relevant for usability, but also when a person cannot manage private keys. For the latter, think newborn baby: perhaps we want to assign a decentralized identity, but it cannot reasonably be owned by the baby until it is 6 months? 1 year? I guess it depends on how smart the baby is. If I allow a guardian to manage my DID, the guardian may sign on my behalf, and another party would trust them when they sign on my behalf (per the DDO rules). However, the guardian is not in a position to impersonate me.","title":"Owner vs control and Guardians"},{"location":"rwot5/topics-and-advance-readings/did-gentle-intro/#key-related-faqs","text":"A DDO may contain other keys that are useful. In an owner-managed DDO, I will always have at least one primary key, but I may also add other keys that I use in different contexts. If the DDO is signed, the creator must correspond to a control key","title":"Key-related FAQs"},{"location":"rwot5/topics-and-advance-readings/did-primer/","text":"DID Primer This is a community document maintained by editors Drummond Reed and Manu Sporny and other contributors and implementers of the Decentralized Identifier 1.0 specification . Introduction At a superficial level, a decentralized identifier ( DID ) is simply a new type of globally unique identifier with special features designed for blockchains. But at a deeper level, DIDs are actually the tip of the iceberg -- or the tip of the spear -- of an entirely new layer of decentralized digital identity and public key infrastructure (PKI) for the Internet. This decentralized public key infrastructure (DPKI) could have as much impact on global cybersecurity and cyberprivacy as the development of the SSL/TLS protocol for encrypted Web traffic (now the largest PKI in the world). This primer is designed to give newcomers to DID architecture the background they need to understand not just the DID specification, but the overall architecture for decentralized identity represented by the family of DID-related specifications currently under development. It covers: Background on the origin of DIDs and the DID specification. How DIDs differ from other globally-unique identifiers. How the syntax of DIDs can be adapted to work with any modern blockchain. How DIDs resolve to DID documents containing public keys and service endpoints. The key role that DID methods play in the implementation of DID infrastructure. Privacy considerations for use of DIDs. How DID infrastructure lays the foundation for verifiable claims . Setting the Stage: The Origin of DIDs In the history of the Internet, every identifier that is both globally unique and globally resolvable -- meaning you can look it up and obtain metadata about the resource it identifies -- has required some type of centralized administration. For example, both IP (Internet Protocol) addresses and DNS (Domain Name System) names -- the foundations for the Internet and the Web -- require centralized registries and registrars. Although these centralized systems are very efficient, this architecture has long been recognized as both a single point of control (and thus potential censorship) and a single point of failure. So, in the last few years, several groups began independently investigating decentralized alternatives. In chronological order: The W3C Web Payments Working Group and W3C Verifiable Claims Task Force , led by Manu Sporny and David Longley of Digital Bazaar, recognized that truly portable digital credentials for individuals would require a new type of identifier that was not dependent on a third-party for registration or resolution. The XDI.org Registry Working Group, led by OASIS XDI Technical Committee co-chairs Drummond Reed and Markus Sabadello and Internet Identity Workshop (IIW) co-founder Phil Windley, began looking for a decentralized solution for identifying participants in a global peer-to-peer XDI semantic data interchange network. The Rebooting the Web of Trust (RWOT) community, led by Christopher Allen, began exploring how blockchain technology could be used to enable the decentralized digital identity and trust network originally envisioned by Phil Zimmermann for PGP . The U.S. Department of Homeland Security (DHS) Science & Technology Directorate (S&T), led by Identity and Data Privacy Program Manager Anil John, began researching how blockchain technology could be used for privacy-respecting decentralized identity management . In the spring of 2016, all four groups converged on the concept of DIDs, a term originally coined by the W3C Verifiable Claims Task Force. Thanks in part to R&D funding provided by DHS S&T, work on the first Decentralized Identifier 1.0 specification began in earnest at RWOT #2 in May 2016. The draft spec underwent review at RWOT #3 and IIW #23 in October 2016, and was published as Implementer\u2019s Draft 01 on 21 November 2016. After the W3C Verifiable Claims Working Group was approved in March 2017, in July 2017 the DID specification was contributed to the W3C Credentials Community Group. Work on several related specifications (see below) is continuing at RWOT and IIW events held every six months, as well as other industry events and conferences. See Appendix A for a list of resources and ways to become involved in the DID family of specifications. How DIDs Differ from Other Globally Unique Identifiers The need for globally unique identifiers that do not require a centralized registration authority is not new. UUIDs (Universally Unique Identifiers, also called GUIDs, Globally Unique Identifiers) were developed for this purpose in the 1980s and standardized first by the Open Software Foundation and then by IETF RFC 4122 . The need for persistent identifiers (identifiers that can be assigned once to an entity and never need to change) is also not new. This class of identifiers was standardized as URNs (Uniform Resource Names) first by IETF RFC 2141 and more recently by RFC 8141 . As a rule, however, UUIDs are not globally resolvable and URNs -- if resolvable -- require a centralized registration authority. In addition, neither UUIDs or URNs inherently address a third characteristic -- the ability to cryptographically verify ownership of the identifier . For blockchain identity -- and more specifically self-sovereign identity , which can be defined as a lifetime portable digital identity that does not depend on any centralized authority and can never be taken away -- we need a new class of identifier that fulfills all three requirements. The Format of a DID In 2016 the developers of the DID specification agreed with a suggestion from Christopher Allen that DIDs could be adapted to work with multiple blockchains by following the same basic pattern as the URN specification: The key difference is that with DIDs the namespace component identifies a DID method , and a DID method specification specifies the format of the method-specific identifier. DID methods define how DIDs work with a specific blockchain. They are explained further below, but all DID method specs must define the format and generation of the method-specific identifier. Note that the method specific identifier string must be unique in the namespace of that DID method. For example the DID above uses the Sovrin DID method in which the method-specific identifier is generated by base-56-encoding the first half of an Ed25519 verification key. DID Documents DID infrastructure can be thought of as a global key-value database in which the database is all DID-compatible blockchains, distributed ledgers, or decentralized networks. In this virtual database, the key is a DID, and the value is a DID document . The purpose of the DID document is to describe the public keys and service endpoints necessary to bootstrap cryptographically-verifiable interactions with the identified entity. A DID document is a valid JSON-LD object that uses the DID context (the RDF vocabulary of property names) defined in the DID specification. This includes six core components: The DID itself , so the DID document is fully self-describing. A set of public keys or other proofs that can be used for authentication or interaction with the identified entity. A set of service endpoints that describe where and how to interact with the identified entity. A set of authorized capabilities for the identified entity -- or other delegated entities -- to make changes to the DID document. Timestamps for auditing. A optional JSON-LD signature if needed for verifying the integrity of the document. See the DID specification for several examples of DID documents. DID Methods A specific goal of DID architecture is to enable DIDs and DID documents to be adapted to any modern blockchain, distributed ledger, or other decentralized network capable of resolving a unique key into a unique value. It does not matter whether the blockchain is public, private, permissionless, or permissioned. What does matter is how a DID and DID document are created, resolved, and managed on a specific blockchain. Defining this is the role of a DID method specification . DID method specifications are to the generic DID specification as URN namespace specifications (UUID, ISBN, OID, LSID, etc.) are to the generic IETF URN specification ( RFC 8141 ). A DID method specification must define the following: The DID method name. The ABNF structure of the method-specific identifier. How the method-specific identifier is generated or derived. How the CRUD operations are performed on a DID and DID document: a. Creating a new DID. b. Reading (resolving) a DID document. c. Updating a DID document. d. Deleting (revoking) a DID. It is these CRUD operations that may vary the most across different DID methods. For example: Create. Some DID methods may generate a DID directly from a cryptographic key pair. Others may use the address of a transaction or a smart contract on the blockchain itself. Read. Some DID methods use blockchains that can store DID documents directly on the blockchain. Others may instruct DID resolvers to construct them dynamically based on attributes of a blockchain record. Still others may store a pointer on the blockchain to a DID document stored in one or more parts on other decentralized storage networks such as IPFS or STORJ . Update. The update operation is the most critical from a security standpoint because control of a DID document represents control of the public keys or proofs necessary to authenticate an entity (and therefore for an attacker to impersonate the entity). Since verification of DID document update permissions can only be enforced by the target blockchain, the DID method specification must define precisely how authentication and authorization are performed for any update operation. Delete. DID entries on a blockchain are by definition immutable, so they can never be \u201cdeleted\u201d in the conventional database sense. However they can be revoked in the cryptographic sense. A DID method specification must define how this termination is performed, e.g., by writing a null DID document. Related Specifications DIDs are the \u201catomic units\u201d of a new layer of decentralized identity infrastructure. This is a list of the other specifications in the DID family that are currently under development. DKMS (Decentralized Key Management System) DIDs are only possible with public/private key cryptography; the ability to generate, write, and update a DID and DID document to a blockchain without any intermediary requires control of the associated private key. This key management cannot itself rely on centralized authorities or it would defeat the whole purpose. In short, decentralized identity requires decentralized key management . How to manage the lifecycle of private keys and other private data associated with a DID in a way that is interoperable across different blockchains, apps, and vendors is the focus of the DKMS (Decentralized Key Management System) specification. This spec is being developed under a grant from the Science & Technology Directorate of the U.S. Department of Homeland Security. The DKMS specification will be based on the requirements and best practices set forth in NIST Special Publication 800-130 , \u201cA Framework for Designing Key Management Systems\u201d. DID TLS Today's TLS infrastructure uses X.509 certs based on traditional hierarchical PKI, where certificate authorities (CAs) follow standardized best practices in order to qualify as trust roots that will be recognized by browser vendors. DID TLS decentralizes this process by enabling the standard X.509 cert elements required to establish a TLS session to be generated dynamically from any DID and DID document that conforms to the DID spec. The DID TLS specification will enable encrypted, peer-to-peer connections to be negotiated in real time between any two DID-identified entities (people, organizations, things). This will radically expand the protections of the TLS protocol and could potentially turn them into the default for all nearly all forms of Internet communication. DID Names The DID specification is intentionally limited to machine-generated decentralized identifiers that are completely lacking in human memorability or usability. However there are many use cases where it is desirable to be able to discover a DID using a human-friendly semantic name. Such a naming service would look like a flat, cryptographically-verifiable version of DNS. The big difference, of course, is that a DID naming service needs to be fully decentralized, i.e., not have centralized registries and registrars. Registration of DID names would be made directly by identity owners to the blockchain itself using the same cryptographic verification as DID transactions. The goal of the DID Names specification is to standardize how an interoperable decentralized naming layer can operate directly on top of the DID layer. A DID name is mapped to a DID the same way a DID is mapped to a DID document. DID names will be an optional feature of a DID method, so the governance and economics of a DID namespace can be specified by the same community that defines the associated DID method. DID Auth A common goal of all blockchain identity systems is the cryptographic authentication of an identity owner. The various protocols all use some type of cryptographic challenge/response similar to the SQRL protocol originally proposed by Steve Gibson and the Web Authentication protocol currently being standardized by W3C. In these protocols, a one-time challenge is issued by the relying party, signed by the identity owner's private key, and then verified by the relying party using the identity owner\u2019s public key. Whereas SQRL and Web Authentication use pairwise public keys that cannot be externally verified, DIDs will enable verification of the public key against the blockchain identified by the DID method. The DID Auth specification will standardize this cryptographic challenge/response authentication protocol so it can be used with any DID that supports it. DID Auth endpoints would then become one of the standard DID identity services than can be discovered via a DID document. DIDs and Privacy by Design Privacy is an essential component of any identity management solution; it is especially critical for a global identity system that uses immutable public blockchains. Thankfully DID architecture can incorporate Privacy by Design at the very lowest levels of infrastructure and thus become a powerful, new, privacy-preserving technology if deployed using best practices such as: Pairwise-unique DIDs. While DIDs can be used as well-known public identifiers, they can also be used as private identifiers issued on a per-relationship basis. So rather than a person having a single DID, like a cell phone number or national ID number, she can have hundreds of pairwise-unique DIDs that cannot be correlated without her consent, yet can still be managed as easily as an address book. Off-chain private data. Storing any type of PII on a public blockchain, even encrypted or hashed, is dangerous for two reasons: 1) the encrypted or hashed data is a global correlation point when the data is shared with multiple parties, and 2) if the encryption is eventually broken (e.g., quantum computing ), the data will be forever accessible on an immutable public ledger. So the best practice is to store all private data off-chain and exchange it only over encrypted, private, peer-to-peer connections. Selective disclosure. The decentralized PKI (DPKI) that DIDs make possible opens the door to individuals gaining greater control over their personal data in two ways. First, it enables it to be shared using encrypted digital credentials (see below). Second, these credentials can use zero-knowledge proof cryptography for data minimization , e.g., you can disclose that you are over a certain age without disclosing your exact birthdate. DIDs and Verifiable Claims DIDs are only the base layer of decentralized identity infrastructure. The next higher layer -- where most of the value is unlocked -- is verifiable claims . This is the technical term for a digitally signed electronic credential that conforms to the interoperability standards being developed by the W3C Verifiable Claims Working Group . For a complete introduction, please see the Verifiable Claims Primer . The diagram below (from Manu Sporny) illustrates the three primary roles in the verifiable claims ecosystem: issuers sign claims and give them to holders (identity owners) who present them to verifiers who verify the signatures in order to grant access to resources. Note that in all three cases, the parties interact with the DID layer to register DIDs as persistent identifiers for issuers or holders, and to resolve those DIDs to obtain the public keys needed to verify the signature of an issuer or holder. Since any issuer may provide claims to any holder who may present them to any verifier, this results in set of rich, interlocking trust relationships that do not need to conform to any pre-established hierarchy -- a web of trust . Appendix A: DID Community Resources Besides the links throughout this primer, these additional resources are available to anyone interested in joining the DID community. W3C Verifiable Claims Working Group mailing list W3C Credentials Community Group DID specification issues list Rebooting the Web of Trust event (held every six months) Internet Identity Workshop event (held every six months)","title":"DID Primer"},{"location":"rwot5/topics-and-advance-readings/did-primer/#did-primer","text":"This is a community document maintained by editors Drummond Reed and Manu Sporny and other contributors and implementers of the Decentralized Identifier 1.0 specification .","title":"DID Primer"},{"location":"rwot5/topics-and-advance-readings/did-primer/#introduction","text":"At a superficial level, a decentralized identifier ( DID ) is simply a new type of globally unique identifier with special features designed for blockchains. But at a deeper level, DIDs are actually the tip of the iceberg -- or the tip of the spear -- of an entirely new layer of decentralized digital identity and public key infrastructure (PKI) for the Internet. This decentralized public key infrastructure (DPKI) could have as much impact on global cybersecurity and cyberprivacy as the development of the SSL/TLS protocol for encrypted Web traffic (now the largest PKI in the world). This primer is designed to give newcomers to DID architecture the background they need to understand not just the DID specification, but the overall architecture for decentralized identity represented by the family of DID-related specifications currently under development. It covers: Background on the origin of DIDs and the DID specification. How DIDs differ from other globally-unique identifiers. How the syntax of DIDs can be adapted to work with any modern blockchain. How DIDs resolve to DID documents containing public keys and service endpoints. The key role that DID methods play in the implementation of DID infrastructure. Privacy considerations for use of DIDs. How DID infrastructure lays the foundation for verifiable claims .","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/did-primer/#setting-the-stage-the-origin-of-dids","text":"In the history of the Internet, every identifier that is both globally unique and globally resolvable -- meaning you can look it up and obtain metadata about the resource it identifies -- has required some type of centralized administration. For example, both IP (Internet Protocol) addresses and DNS (Domain Name System) names -- the foundations for the Internet and the Web -- require centralized registries and registrars. Although these centralized systems are very efficient, this architecture has long been recognized as both a single point of control (and thus potential censorship) and a single point of failure. So, in the last few years, several groups began independently investigating decentralized alternatives. In chronological order: The W3C Web Payments Working Group and W3C Verifiable Claims Task Force , led by Manu Sporny and David Longley of Digital Bazaar, recognized that truly portable digital credentials for individuals would require a new type of identifier that was not dependent on a third-party for registration or resolution. The XDI.org Registry Working Group, led by OASIS XDI Technical Committee co-chairs Drummond Reed and Markus Sabadello and Internet Identity Workshop (IIW) co-founder Phil Windley, began looking for a decentralized solution for identifying participants in a global peer-to-peer XDI semantic data interchange network. The Rebooting the Web of Trust (RWOT) community, led by Christopher Allen, began exploring how blockchain technology could be used to enable the decentralized digital identity and trust network originally envisioned by Phil Zimmermann for PGP . The U.S. Department of Homeland Security (DHS) Science & Technology Directorate (S&T), led by Identity and Data Privacy Program Manager Anil John, began researching how blockchain technology could be used for privacy-respecting decentralized identity management . In the spring of 2016, all four groups converged on the concept of DIDs, a term originally coined by the W3C Verifiable Claims Task Force. Thanks in part to R&D funding provided by DHS S&T, work on the first Decentralized Identifier 1.0 specification began in earnest at RWOT #2 in May 2016. The draft spec underwent review at RWOT #3 and IIW #23 in October 2016, and was published as Implementer\u2019s Draft 01 on 21 November 2016. After the W3C Verifiable Claims Working Group was approved in March 2017, in July 2017 the DID specification was contributed to the W3C Credentials Community Group. Work on several related specifications (see below) is continuing at RWOT and IIW events held every six months, as well as other industry events and conferences. See Appendix A for a list of resources and ways to become involved in the DID family of specifications.","title":"Setting the Stage: The Origin of DIDs"},{"location":"rwot5/topics-and-advance-readings/did-primer/#how-dids-differ-from-other-globally-unique-identifiers","text":"The need for globally unique identifiers that do not require a centralized registration authority is not new. UUIDs (Universally Unique Identifiers, also called GUIDs, Globally Unique Identifiers) were developed for this purpose in the 1980s and standardized first by the Open Software Foundation and then by IETF RFC 4122 . The need for persistent identifiers (identifiers that can be assigned once to an entity and never need to change) is also not new. This class of identifiers was standardized as URNs (Uniform Resource Names) first by IETF RFC 2141 and more recently by RFC 8141 . As a rule, however, UUIDs are not globally resolvable and URNs -- if resolvable -- require a centralized registration authority. In addition, neither UUIDs or URNs inherently address a third characteristic -- the ability to cryptographically verify ownership of the identifier . For blockchain identity -- and more specifically self-sovereign identity , which can be defined as a lifetime portable digital identity that does not depend on any centralized authority and can never be taken away -- we need a new class of identifier that fulfills all three requirements.","title":"How DIDs Differ from Other Globally Unique Identifiers"},{"location":"rwot5/topics-and-advance-readings/did-primer/#the-format-of-a-did","text":"In 2016 the developers of the DID specification agreed with a suggestion from Christopher Allen that DIDs could be adapted to work with multiple blockchains by following the same basic pattern as the URN specification: The key difference is that with DIDs the namespace component identifies a DID method , and a DID method specification specifies the format of the method-specific identifier. DID methods define how DIDs work with a specific blockchain. They are explained further below, but all DID method specs must define the format and generation of the method-specific identifier. Note that the method specific identifier string must be unique in the namespace of that DID method. For example the DID above uses the Sovrin DID method in which the method-specific identifier is generated by base-56-encoding the first half of an Ed25519 verification key.","title":"The Format of a DID"},{"location":"rwot5/topics-and-advance-readings/did-primer/#did-documents","text":"DID infrastructure can be thought of as a global key-value database in which the database is all DID-compatible blockchains, distributed ledgers, or decentralized networks. In this virtual database, the key is a DID, and the value is a DID document . The purpose of the DID document is to describe the public keys and service endpoints necessary to bootstrap cryptographically-verifiable interactions with the identified entity. A DID document is a valid JSON-LD object that uses the DID context (the RDF vocabulary of property names) defined in the DID specification. This includes six core components: The DID itself , so the DID document is fully self-describing. A set of public keys or other proofs that can be used for authentication or interaction with the identified entity. A set of service endpoints that describe where and how to interact with the identified entity. A set of authorized capabilities for the identified entity -- or other delegated entities -- to make changes to the DID document. Timestamps for auditing. A optional JSON-LD signature if needed for verifying the integrity of the document. See the DID specification for several examples of DID documents.","title":"DID Documents"},{"location":"rwot5/topics-and-advance-readings/did-primer/#did-methods","text":"A specific goal of DID architecture is to enable DIDs and DID documents to be adapted to any modern blockchain, distributed ledger, or other decentralized network capable of resolving a unique key into a unique value. It does not matter whether the blockchain is public, private, permissionless, or permissioned. What does matter is how a DID and DID document are created, resolved, and managed on a specific blockchain. Defining this is the role of a DID method specification . DID method specifications are to the generic DID specification as URN namespace specifications (UUID, ISBN, OID, LSID, etc.) are to the generic IETF URN specification ( RFC 8141 ). A DID method specification must define the following: The DID method name. The ABNF structure of the method-specific identifier. How the method-specific identifier is generated or derived. How the CRUD operations are performed on a DID and DID document: a. Creating a new DID. b. Reading (resolving) a DID document. c. Updating a DID document. d. Deleting (revoking) a DID. It is these CRUD operations that may vary the most across different DID methods. For example: Create. Some DID methods may generate a DID directly from a cryptographic key pair. Others may use the address of a transaction or a smart contract on the blockchain itself. Read. Some DID methods use blockchains that can store DID documents directly on the blockchain. Others may instruct DID resolvers to construct them dynamically based on attributes of a blockchain record. Still others may store a pointer on the blockchain to a DID document stored in one or more parts on other decentralized storage networks such as IPFS or STORJ . Update. The update operation is the most critical from a security standpoint because control of a DID document represents control of the public keys or proofs necessary to authenticate an entity (and therefore for an attacker to impersonate the entity). Since verification of DID document update permissions can only be enforced by the target blockchain, the DID method specification must define precisely how authentication and authorization are performed for any update operation. Delete. DID entries on a blockchain are by definition immutable, so they can never be \u201cdeleted\u201d in the conventional database sense. However they can be revoked in the cryptographic sense. A DID method specification must define how this termination is performed, e.g., by writing a null DID document.","title":"DID Methods"},{"location":"rwot5/topics-and-advance-readings/did-primer/#related-specifications","text":"DIDs are the \u201catomic units\u201d of a new layer of decentralized identity infrastructure. This is a list of the other specifications in the DID family that are currently under development.","title":"Related Specifications"},{"location":"rwot5/topics-and-advance-readings/did-primer/#dkms-decentralized-key-management-system","text":"DIDs are only possible with public/private key cryptography; the ability to generate, write, and update a DID and DID document to a blockchain without any intermediary requires control of the associated private key. This key management cannot itself rely on centralized authorities or it would defeat the whole purpose. In short, decentralized identity requires decentralized key management . How to manage the lifecycle of private keys and other private data associated with a DID in a way that is interoperable across different blockchains, apps, and vendors is the focus of the DKMS (Decentralized Key Management System) specification. This spec is being developed under a grant from the Science & Technology Directorate of the U.S. Department of Homeland Security. The DKMS specification will be based on the requirements and best practices set forth in NIST Special Publication 800-130 , \u201cA Framework for Designing Key Management Systems\u201d.","title":"DKMS (Decentralized Key Management System)"},{"location":"rwot5/topics-and-advance-readings/did-primer/#did-tls","text":"Today's TLS infrastructure uses X.509 certs based on traditional hierarchical PKI, where certificate authorities (CAs) follow standardized best practices in order to qualify as trust roots that will be recognized by browser vendors. DID TLS decentralizes this process by enabling the standard X.509 cert elements required to establish a TLS session to be generated dynamically from any DID and DID document that conforms to the DID spec. The DID TLS specification will enable encrypted, peer-to-peer connections to be negotiated in real time between any two DID-identified entities (people, organizations, things). This will radically expand the protections of the TLS protocol and could potentially turn them into the default for all nearly all forms of Internet communication.","title":"DID TLS"},{"location":"rwot5/topics-and-advance-readings/did-primer/#did-names","text":"The DID specification is intentionally limited to machine-generated decentralized identifiers that are completely lacking in human memorability or usability. However there are many use cases where it is desirable to be able to discover a DID using a human-friendly semantic name. Such a naming service would look like a flat, cryptographically-verifiable version of DNS. The big difference, of course, is that a DID naming service needs to be fully decentralized, i.e., not have centralized registries and registrars. Registration of DID names would be made directly by identity owners to the blockchain itself using the same cryptographic verification as DID transactions. The goal of the DID Names specification is to standardize how an interoperable decentralized naming layer can operate directly on top of the DID layer. A DID name is mapped to a DID the same way a DID is mapped to a DID document. DID names will be an optional feature of a DID method, so the governance and economics of a DID namespace can be specified by the same community that defines the associated DID method.","title":"DID Names"},{"location":"rwot5/topics-and-advance-readings/did-primer/#did-auth","text":"A common goal of all blockchain identity systems is the cryptographic authentication of an identity owner. The various protocols all use some type of cryptographic challenge/response similar to the SQRL protocol originally proposed by Steve Gibson and the Web Authentication protocol currently being standardized by W3C. In these protocols, a one-time challenge is issued by the relying party, signed by the identity owner's private key, and then verified by the relying party using the identity owner\u2019s public key. Whereas SQRL and Web Authentication use pairwise public keys that cannot be externally verified, DIDs will enable verification of the public key against the blockchain identified by the DID method. The DID Auth specification will standardize this cryptographic challenge/response authentication protocol so it can be used with any DID that supports it. DID Auth endpoints would then become one of the standard DID identity services than can be discovered via a DID document.","title":"DID Auth"},{"location":"rwot5/topics-and-advance-readings/did-primer/#dids-and-privacy-by-design","text":"Privacy is an essential component of any identity management solution; it is especially critical for a global identity system that uses immutable public blockchains. Thankfully DID architecture can incorporate Privacy by Design at the very lowest levels of infrastructure and thus become a powerful, new, privacy-preserving technology if deployed using best practices such as: Pairwise-unique DIDs. While DIDs can be used as well-known public identifiers, they can also be used as private identifiers issued on a per-relationship basis. So rather than a person having a single DID, like a cell phone number or national ID number, she can have hundreds of pairwise-unique DIDs that cannot be correlated without her consent, yet can still be managed as easily as an address book. Off-chain private data. Storing any type of PII on a public blockchain, even encrypted or hashed, is dangerous for two reasons: 1) the encrypted or hashed data is a global correlation point when the data is shared with multiple parties, and 2) if the encryption is eventually broken (e.g., quantum computing ), the data will be forever accessible on an immutable public ledger. So the best practice is to store all private data off-chain and exchange it only over encrypted, private, peer-to-peer connections. Selective disclosure. The decentralized PKI (DPKI) that DIDs make possible opens the door to individuals gaining greater control over their personal data in two ways. First, it enables it to be shared using encrypted digital credentials (see below). Second, these credentials can use zero-knowledge proof cryptography for data minimization , e.g., you can disclose that you are over a certain age without disclosing your exact birthdate.","title":"DIDs and Privacy by Design"},{"location":"rwot5/topics-and-advance-readings/did-primer/#dids-and-verifiable-claims","text":"DIDs are only the base layer of decentralized identity infrastructure. The next higher layer -- where most of the value is unlocked -- is verifiable claims . This is the technical term for a digitally signed electronic credential that conforms to the interoperability standards being developed by the W3C Verifiable Claims Working Group . For a complete introduction, please see the Verifiable Claims Primer . The diagram below (from Manu Sporny) illustrates the three primary roles in the verifiable claims ecosystem: issuers sign claims and give them to holders (identity owners) who present them to verifiers who verify the signatures in order to grant access to resources. Note that in all three cases, the parties interact with the DID layer to register DIDs as persistent identifiers for issuers or holders, and to resolve those DIDs to obtain the public keys needed to verify the signature of an issuer or holder. Since any issuer may provide claims to any holder who may present them to any verifier, this results in set of rich, interlocking trust relationships that do not need to conform to any pre-established hierarchy -- a web of trust .","title":"DIDs and Verifiable Claims"},{"location":"rwot5/topics-and-advance-readings/did-primer/#appendix-a-did-community-resources","text":"Besides the links throughout this primer, these additional resources are available to anyone interested in joining the DID community. W3C Verifiable Claims Working Group mailing list W3C Credentials Community Group DID specification issues list Rebooting the Web of Trust event (held every six months) Internet Identity Workshop event (held every six months)","title":"Appendix A: DID Community Resources"},{"location":"rwot5/topics-and-advance-readings/did-tooling/","text":"Decentralized Identifier Tooling by Manu Sporny and Matt Collier, Digital Bazaar Introduction In order for Decentralized Identifiers (DIDs) to proliferate, tooling is necessary. We should build this tooling as a community to reduce duplicated effort. The tooling that we create should first be targeted at developers so that they can easily integrate DIDs into their projects. Once we have built an acceptable number of low-level libraries and tools, we can move on to more customer-facing technologies. Since there will be several ledgers that support DIDs, it is imperative that we create a common set of tools and libraries for managing DIDs. It is expected that these tools will use a driver or plugin-based architecture. This approach will enable the general tool to be universal and have a well-known feature set with additions to the tool provided by developers for each ledger the tool supports. did-client Digital Bazaar offers the first sort of this tool, which is a command-line utility for creating, retrieving, and updating DID Documents across multiple ledgers. A demonstration video of the tool, as well as an explanation of how it works, can be viewed here: did-client Demo Video did-client Source Code The did-client source code is available on Github: did-client Github Repository The current client supports the creation of Testnet DIDs on the Veres One ledger as well as the retrieval of those DIDs from the Testnet. You can try the tool out by doing the following commands on a system that has node.js and a C++ compiler installed: npm install did-client cd node_modules/did-client ./did create To download the source and install the client: git clone https://github.com/digitalbazaar/did-client.git cd did-client npm install ./did create Once you have created a DID, you can retrieve it from the ledger doing this command: ./did get <DID> There are plans to support the following other commands and features: Adding, rotating, and removing authentication credentials Adding and removing authorization capability descriptions Adding and removing service descriptions Checking the validity of a DID (deep blockchain check) Collaboration We are seeking individuals at this Rebooting Web of Trust event to coordinate on how this tool should be built, what features it should have, and to recruit implementers to write plugins for their favorite DID-supporting ledgers.","title":"Decentralized Identifier Tooling"},{"location":"rwot5/topics-and-advance-readings/did-tooling/#decentralized-identifier-tooling","text":"by Manu Sporny and Matt Collier, Digital Bazaar","title":"Decentralized Identifier Tooling"},{"location":"rwot5/topics-and-advance-readings/did-tooling/#introduction","text":"In order for Decentralized Identifiers (DIDs) to proliferate, tooling is necessary. We should build this tooling as a community to reduce duplicated effort. The tooling that we create should first be targeted at developers so that they can easily integrate DIDs into their projects. Once we have built an acceptable number of low-level libraries and tools, we can move on to more customer-facing technologies. Since there will be several ledgers that support DIDs, it is imperative that we create a common set of tools and libraries for managing DIDs. It is expected that these tools will use a driver or plugin-based architecture. This approach will enable the general tool to be universal and have a well-known feature set with additions to the tool provided by developers for each ledger the tool supports.","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/did-tooling/#did-client","text":"Digital Bazaar offers the first sort of this tool, which is a command-line utility for creating, retrieving, and updating DID Documents across multiple ledgers. A demonstration video of the tool, as well as an explanation of how it works, can be viewed here: did-client Demo Video","title":"did-client"},{"location":"rwot5/topics-and-advance-readings/did-tooling/#did-client-source-code","text":"The did-client source code is available on Github: did-client Github Repository The current client supports the creation of Testnet DIDs on the Veres One ledger as well as the retrieval of those DIDs from the Testnet. You can try the tool out by doing the following commands on a system that has node.js and a C++ compiler installed: npm install did-client cd node_modules/did-client ./did create To download the source and install the client: git clone https://github.com/digitalbazaar/did-client.git cd did-client npm install ./did create Once you have created a DID, you can retrieve it from the ledger doing this command: ./did get <DID> There are plans to support the following other commands and features: Adding, rotating, and removing authentication credentials Adding and removing authorization capability descriptions Adding and removing service descriptions Checking the validity of a DID (deep blockchain check)","title":"did-client Source Code"},{"location":"rwot5/topics-and-advance-readings/did-tooling/#collaboration","text":"We are seeking individuals at this Rebooting Web of Trust event to coordinate on how this tool should be built, what features it should have, and to recruit implementers to write plugins for their favorite DID-supporting ledgers.","title":"Collaboration"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/","text":"Recommendations for Decentralized Key Management Systems by Michael Lodder Evernym, Inc. Abstract A decentralized key management system (DKMS) aims to solve how consumers can manage their own keys and certificates without relying on a third-party provider having access or controls over the keys.. This method helps to ensure that no third-party can compromise the integrity and security of the system as a whole. Entities can use the system to safely authenticate each other and validate keys and certificates. Centralized key management systems (CKMS) manage key and certificate creation, signing, and validity. Specific problems arise when these authorities become unavailable, or the data they control becomes corrupted or known. Central authorities often become choice targets for attackers. This document proposes to meet these requirements with a decentralized blockchain ledger for providing an oracle of trust and leave control over all keys with end users. The use of a blockchain permits globally readable identifiers and public data to be shared in a secure manner that is not vulnerable to the man-in-the-middle attack or system wide compromise and permits consumers to be self-sovereign. This leaves consumers with the task of key management and protection. This document covers various ideas for how users may create, recover, backup, and revoke keys and provides recommended suggestions. 1 Introduction In a typical centralized key management system, a third-party such as a certificate authority, kerberos, or enterprise deployment will maintain control over users keys and certificates. Systems will create keys on user's behalf, protect them from unauthorized use, rotate them as specified by best industry standards, and revoke them in the event of compromise. This simplifies the experience for users by abstracting away key management altogether. Security administrators like this setup as it leaves the difficulty with them. System administrators will typically harden the systems to prevent unauthorized access. Negatively this setup simplifies the target to one location for attackers. Also users may not have any degree of control over their keys and certificates. Instead they must notify their administrators if they suspect compromise or lose their secrets. Personal identifiable information (PII) is usually stored alongside these keys which makes the central authority even more valuable to attackers. Key management can be a difficult task as it requires knowing what types to use, which crypto systems are more secure, how often they should be changed, how to distribute them to others in a secure manner, how to indicate which of them are no longer trusted, how to recover if they are lost or stolen, and how to protect them. Privacy is also a major concern as any information that can be correlated to them can potentially be used against them. DKMS should preserve privacy, allow endpoints to manage their own secrets, and provide a method for establishing trust for distributing public information in a secure manner. Endpoints can be implemented in the form of decentralized identifiers (DIDs) and any keys used by that DID in DID description objects (DDOs) which provide anonymity for identities and their keys. A DID is a globally unique identifier that is generated cryptographically and self-registered with the identity owner\u2019s choice of a DID-compatible distributed ledger so it requires no central registration authority. Each DID points a DDO, a JSON-LD object containing the associated public key(s) and a pointer to off-ledger agent(s) supporting peer-to-peer interactions with the identity owner. From this baseline, trust between DID-identified peers can be built up in two ways: Challenge/response messages for real-time verification of public keys. Exchange of verifiable claims\u2014claims about identity attributes that include digital signatures or other cryptographic proofs from other DID-identified trusted peers whose public keys or proofs can also be verified against the ledger. This decentralized web of trust model leverages the security, immutability, and resiliency properties of distributed ledgers to provide highly scalable key distribution, verification, and recovery, finally making PKI accessible to everyone. However, because it does not depend on any centralized authorities, it also shifts some measure of responsibility for key management directly to each participating identity owner. This demands the decentralized equivalent of the centralized cryptographic key management systems (CKMS) that are the current best practice in most enterprises. 2 Keys The DKMS architecture proposed here addresses the following points: what keys might be needed and their purpose, where they should be stored and protected, how to recover from loss or compromise, rotate them as needed, and revoke when the key is no longer needed or becomes publicly known. Keys may distributed to other entities but must remain in control by their owners. 2.1 Key Types Every key should have a specific purpose. The ledger must prove that any changes made to data are only made by those that have been authorized. All unauthorized requests must be ignored or flagged by the system. If anyone else can change the data, the security of the system is compromised. This paper recommends owners use a master private key to authenticate their actions. The master private key thus represents the owner\u2019s identity. To limit master private key exposure, subkeys should be used for all other purposes like securing storage, communication, and delegation. Delegation could be for devices, people, or software used by the owner. If a device is detected to be unauthorized, then actions and messages received from that device can safely be ignored or flagged by systems and the owner can be notified. 2.2 Key Derivation and Safeguarding As keys are created and used they should be stored based on frequency of use and sensitivity. Keys used more often should carry less risk if lost, require little effort to access, fast to retrieve, and should be used for more specific purposes. An example of a such a key is one used for communicating with a specific endpoint multiple times per day. If this key becomes known, only the channel is compromised until the existing key is replaced. Owners can use a file encrypted by a hardware key, an encrypted wallet like KeePass, an encrypted database like SQLCipher, or operating system keystores. Keys used less often are more sensitive and carry more risk if lost, should require more effort to access, possibly longer retrieve times, and be used for more general purposes. These are called rare keys. A rare key is like the master private key used for authenticating owner actions. If discovered by anyone other than the owner, the consequences are very severe like loss of control over their identity. Rare keys should be stored somewhere that requires secure interactions to access and potentially slower retrieval times. Owners should be able to choose where to store their keys. Physically uncloneable functions (PUF) are a key protection where keys are instead derived from unique physical properties of a system and exist only when powered up. That is, rather than securely storing the private key, the same key can be regenerated over and over again (for the lifetime of the device) on demand. Using an SRAM-based PUF, these are guaranteed to be unique since they utilize the inherent randomness in silicon bit patterns. More common key protections use secure hardware like an HSM, TPM, secure element, smart cards, or a TEE. Another method is to split the key into pieces that are distributed to trusted custodians. After its intended use a rare key should immediately be forgotten. 2.3 Key Recovery There multiple methods for creating and managing keys. Depending on the method used affects how to recover keys. Key derivation functions (KDF), pseudo random number generators (PRNG), and Bitcoin\u2019s BIP0032 are all examples of key creation using a seed value with a derivation function. Standard tools typically use a random seed that is by a PRNG to create a key. If all keys are created from a single seed then only the seed may need to be safeguarded as every other key can be recreated if necessary. This method simplifies recovery as only one value would be necessary to begin. The master private key should be used for this purpose. If KDFs or PRNGs are used, a passphrase, biometric input, or social data from multiple users combined with random salt should be used as the input to create the seed. Alternately a QR code or words from a list such as the PGP word list can be used. In either case, the input must not be stored anywhere connected to the Internet. Most users will opt for something online because of the ease of use. It is recommended to split the keys into pieces and distribute them to trusted custodians chosen by the owner. When recovery is needed, custodians can be contacted and the key is recovered once enough pieces have been received. Shamir Secret Sharing could be used to generate the pieces and recombine them or secure multiparty computation. 2.4 Key Revocation A crucial requirement for all key systems is that keys can be revoked. There must be a revocation solution for verifiers to check which keys should no longer be trusted and allowing owners to list their revoked keys. It is not good enough to simply forget a key because it cannot be known if the key has been compromised. To prevent these from exploding in size and slowing over time, a cryptographic accumulator may be used for verifying non-inclusion. Accumulators allow for quick checks of non-inclusion and updates. Accumulator updates are only needed when keys are revoked and not when they are created. Control over who can update the lists must be enforced so no one but the owner can revoke keys. Existing solutions do not always allow owners to control their keys in this way. 2.5 Key Rotation Keys should be changed as often as possible to limit tampering. As much as possible keys should have an expiration for to the following reasons: Technology advances. When better methods become available such as stronger encryption keys, existing keys become weak. Expiration helps to enforce moving to better technology and methods. Compromise mitigation. Keys are changed often to prevent attackers from using them even if they are able to learn them. As keys are invalidated, attackers lose the ability to use them, which is really good if attackers acquired the key without the knowledge of the owner. Expiration helps to enforce key rotation. Need changes. Key owners may only use certain secrets while performing a specific task. The task may end after a certain date and all secrets tied to that task should also. Expiration helps to enforce this limited period. 3 System Architecture A decentralized key management system may be implemented using a blockchain ledger as a method for sharing public data in a secure manner but any decentralized secure system could work. The role of third parties participating on blockchain is limited to ensuring the security and integrity of the system. Owners are responsible for data they write to the ledger. The ledger serves as solely an oracle of trust i.e. users should be able to trust the data that is written there but the ledger has no authority by itself. Alice can write her public key to the ledger along with an endpoint where she can be reached. Bob can do the same with his public key and can contact Alice and authenticate her using her public key on the ledger. To prevent an intruder or eavesdropper from overwriting either of their public keys, the ledger protocol should limit updates to Alice\u2019s to only Alice and the same for Bob using their respective master private keys . Users may use any method of authenticating their actions but key signatures are relatively easy to do and validate. If Alice wants to rotate her keys, she can write the new one to the ledger. Then Bob just needs to reference the ledger for the latest version. Users can delegate authority to third parties to perform actions on their behalf like a butler answering the door for their employer or a lawyer or real estate agent or software agent. Owners may create a certificate or key for delegatable entities or delegates may have their keys signed by the owner. This can be verified using the ledger. Owners can remove these authorizations by updating a revocation list on the ledger or replacing their verification keys on the ledger. Users may limit interactions with others solely to approved devices such as smartphones or tablets while government employees may only allow hardened laptops or desktop machines. 4 Conclusion A DKMS architecture and DPKI provides the following major benefits: No single point of failure. With DKMS, there is no central CA or other registration authority whose failure can jeopardize large swaths of users. Interoperability. DKMS will enable any two apps and identity owners to perform key exchange and create encrypted P2P connections without reliance on proprietary software or service providers. Resilience. DKMS incorporates all the advantages of distributed ledger technology for decentralized access to cryptographically verifiable data, and then builds on top of it a distributed web of trust where any peer can exchange keys, form connections, and issue/accept verifiable claims from any other peer. Key recovery. Rather than app-specific or domain-specific key recovery solutions, DKMS can build robust key recovery directly into the infrastructure, including agent-automated encrypted backup, DKMS key escrow services, and social recovery of keys, for example by backing up or sharding across trusted DKMS connections.","title":"Recommendations for Decentralized Key Management Systems"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#recommendations-for-decentralized-key-management-systems","text":"","title":"Recommendations for Decentralized Key Management Systems"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#by-michael-lodder","text":"","title":"by Michael Lodder"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#evernym-inc","text":"","title":"Evernym, Inc."},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#abstract","text":"A decentralized key management system (DKMS) aims to solve how consumers can manage their own keys and certificates without relying on a third-party provider having access or controls over the keys.. This method helps to ensure that no third-party can compromise the integrity and security of the system as a whole. Entities can use the system to safely authenticate each other and validate keys and certificates. Centralized key management systems (CKMS) manage key and certificate creation, signing, and validity. Specific problems arise when these authorities become unavailable, or the data they control becomes corrupted or known. Central authorities often become choice targets for attackers. This document proposes to meet these requirements with a decentralized blockchain ledger for providing an oracle of trust and leave control over all keys with end users. The use of a blockchain permits globally readable identifiers and public data to be shared in a secure manner that is not vulnerable to the man-in-the-middle attack or system wide compromise and permits consumers to be self-sovereign. This leaves consumers with the task of key management and protection. This document covers various ideas for how users may create, recover, backup, and revoke keys and provides recommended suggestions.","title":"Abstract"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#1-introduction","text":"In a typical centralized key management system, a third-party such as a certificate authority, kerberos, or enterprise deployment will maintain control over users keys and certificates. Systems will create keys on user's behalf, protect them from unauthorized use, rotate them as specified by best industry standards, and revoke them in the event of compromise. This simplifies the experience for users by abstracting away key management altogether. Security administrators like this setup as it leaves the difficulty with them. System administrators will typically harden the systems to prevent unauthorized access. Negatively this setup simplifies the target to one location for attackers. Also users may not have any degree of control over their keys and certificates. Instead they must notify their administrators if they suspect compromise or lose their secrets. Personal identifiable information (PII) is usually stored alongside these keys which makes the central authority even more valuable to attackers. Key management can be a difficult task as it requires knowing what types to use, which crypto systems are more secure, how often they should be changed, how to distribute them to others in a secure manner, how to indicate which of them are no longer trusted, how to recover if they are lost or stolen, and how to protect them. Privacy is also a major concern as any information that can be correlated to them can potentially be used against them. DKMS should preserve privacy, allow endpoints to manage their own secrets, and provide a method for establishing trust for distributing public information in a secure manner. Endpoints can be implemented in the form of decentralized identifiers (DIDs) and any keys used by that DID in DID description objects (DDOs) which provide anonymity for identities and their keys. A DID is a globally unique identifier that is generated cryptographically and self-registered with the identity owner\u2019s choice of a DID-compatible distributed ledger so it requires no central registration authority. Each DID points a DDO, a JSON-LD object containing the associated public key(s) and a pointer to off-ledger agent(s) supporting peer-to-peer interactions with the identity owner. From this baseline, trust between DID-identified peers can be built up in two ways: Challenge/response messages for real-time verification of public keys. Exchange of verifiable claims\u2014claims about identity attributes that include digital signatures or other cryptographic proofs from other DID-identified trusted peers whose public keys or proofs can also be verified against the ledger. This decentralized web of trust model leverages the security, immutability, and resiliency properties of distributed ledgers to provide highly scalable key distribution, verification, and recovery, finally making PKI accessible to everyone. However, because it does not depend on any centralized authorities, it also shifts some measure of responsibility for key management directly to each participating identity owner. This demands the decentralized equivalent of the centralized cryptographic key management systems (CKMS) that are the current best practice in most enterprises.","title":"1  Introduction"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#2-keys","text":"The DKMS architecture proposed here addresses the following points: what keys might be needed and their purpose, where they should be stored and protected, how to recover from loss or compromise, rotate them as needed, and revoke when the key is no longer needed or becomes publicly known. Keys may distributed to other entities but must remain in control by their owners.","title":"2  Keys"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#21-key-types","text":"Every key should have a specific purpose. The ledger must prove that any changes made to data are only made by those that have been authorized. All unauthorized requests must be ignored or flagged by the system. If anyone else can change the data, the security of the system is compromised. This paper recommends owners use a master private key to authenticate their actions. The master private key thus represents the owner\u2019s identity. To limit master private key exposure, subkeys should be used for all other purposes like securing storage, communication, and delegation. Delegation could be for devices, people, or software used by the owner. If a device is detected to be unauthorized, then actions and messages received from that device can safely be ignored or flagged by systems and the owner can be notified.","title":"2.1  Key Types"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#22-key-derivation-and-safeguarding","text":"As keys are created and used they should be stored based on frequency of use and sensitivity. Keys used more often should carry less risk if lost, require little effort to access, fast to retrieve, and should be used for more specific purposes. An example of a such a key is one used for communicating with a specific endpoint multiple times per day. If this key becomes known, only the channel is compromised until the existing key is replaced. Owners can use a file encrypted by a hardware key, an encrypted wallet like KeePass, an encrypted database like SQLCipher, or operating system keystores. Keys used less often are more sensitive and carry more risk if lost, should require more effort to access, possibly longer retrieve times, and be used for more general purposes. These are called rare keys. A rare key is like the master private key used for authenticating owner actions. If discovered by anyone other than the owner, the consequences are very severe like loss of control over their identity. Rare keys should be stored somewhere that requires secure interactions to access and potentially slower retrieval times. Owners should be able to choose where to store their keys. Physically uncloneable functions (PUF) are a key protection where keys are instead derived from unique physical properties of a system and exist only when powered up. That is, rather than securely storing the private key, the same key can be regenerated over and over again (for the lifetime of the device) on demand. Using an SRAM-based PUF, these are guaranteed to be unique since they utilize the inherent randomness in silicon bit patterns. More common key protections use secure hardware like an HSM, TPM, secure element, smart cards, or a TEE. Another method is to split the key into pieces that are distributed to trusted custodians. After its intended use a rare key should immediately be forgotten.","title":"2.2  Key Derivation and Safeguarding"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#23-key-recovery","text":"There multiple methods for creating and managing keys. Depending on the method used affects how to recover keys. Key derivation functions (KDF), pseudo random number generators (PRNG), and Bitcoin\u2019s BIP0032 are all examples of key creation using a seed value with a derivation function. Standard tools typically use a random seed that is by a PRNG to create a key. If all keys are created from a single seed then only the seed may need to be safeguarded as every other key can be recreated if necessary. This method simplifies recovery as only one value would be necessary to begin. The master private key should be used for this purpose. If KDFs or PRNGs are used, a passphrase, biometric input, or social data from multiple users combined with random salt should be used as the input to create the seed. Alternately a QR code or words from a list such as the PGP word list can be used. In either case, the input must not be stored anywhere connected to the Internet. Most users will opt for something online because of the ease of use. It is recommended to split the keys into pieces and distribute them to trusted custodians chosen by the owner. When recovery is needed, custodians can be contacted and the key is recovered once enough pieces have been received. Shamir Secret Sharing could be used to generate the pieces and recombine them or secure multiparty computation.","title":"2.3  Key Recovery"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#24-key-revocation","text":"A crucial requirement for all key systems is that keys can be revoked. There must be a revocation solution for verifiers to check which keys should no longer be trusted and allowing owners to list their revoked keys. It is not good enough to simply forget a key because it cannot be known if the key has been compromised. To prevent these from exploding in size and slowing over time, a cryptographic accumulator may be used for verifying non-inclusion. Accumulators allow for quick checks of non-inclusion and updates. Accumulator updates are only needed when keys are revoked and not when they are created. Control over who can update the lists must be enforced so no one but the owner can revoke keys. Existing solutions do not always allow owners to control their keys in this way.","title":"2.4  Key Revocation"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#25-key-rotation","text":"Keys should be changed as often as possible to limit tampering. As much as possible keys should have an expiration for to the following reasons: Technology advances. When better methods become available such as stronger encryption keys, existing keys become weak. Expiration helps to enforce moving to better technology and methods. Compromise mitigation. Keys are changed often to prevent attackers from using them even if they are able to learn them. As keys are invalidated, attackers lose the ability to use them, which is really good if attackers acquired the key without the knowledge of the owner. Expiration helps to enforce key rotation. Need changes. Key owners may only use certain secrets while performing a specific task. The task may end after a certain date and all secrets tied to that task should also. Expiration helps to enforce this limited period.","title":"2.5  Key Rotation"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#3-system-architecture","text":"A decentralized key management system may be implemented using a blockchain ledger as a method for sharing public data in a secure manner but any decentralized secure system could work. The role of third parties participating on blockchain is limited to ensuring the security and integrity of the system. Owners are responsible for data they write to the ledger. The ledger serves as solely an oracle of trust i.e. users should be able to trust the data that is written there but the ledger has no authority by itself. Alice can write her public key to the ledger along with an endpoint where she can be reached. Bob can do the same with his public key and can contact Alice and authenticate her using her public key on the ledger. To prevent an intruder or eavesdropper from overwriting either of their public keys, the ledger protocol should limit updates to Alice\u2019s to only Alice and the same for Bob using their respective master private keys . Users may use any method of authenticating their actions but key signatures are relatively easy to do and validate. If Alice wants to rotate her keys, she can write the new one to the ledger. Then Bob just needs to reference the ledger for the latest version. Users can delegate authority to third parties to perform actions on their behalf like a butler answering the door for their employer or a lawyer or real estate agent or software agent. Owners may create a certificate or key for delegatable entities or delegates may have their keys signed by the owner. This can be verified using the ledger. Owners can remove these authorizations by updating a revocation list on the ledger or replacing their verification keys on the ledger. Users may limit interactions with others solely to approved devices such as smartphones or tablets while government employees may only allow hardened laptops or desktop machines.","title":"3  System Architecture"},{"location":"rwot5/topics-and-advance-readings/dkms-recommendations/#4-conclusion","text":"A DKMS architecture and DPKI provides the following major benefits: No single point of failure. With DKMS, there is no central CA or other registration authority whose failure can jeopardize large swaths of users. Interoperability. DKMS will enable any two apps and identity owners to perform key exchange and create encrypted P2P connections without reliance on proprietary software or service providers. Resilience. DKMS incorporates all the advantages of distributed ledger technology for decentralized access to cryptographically verifiable data, and then builds on top of it a distributed web of trust where any peer can exchange keys, form connections, and issue/accept verifiable claims from any other peer. Key recovery. Rather than app-specific or domain-specific key recovery solutions, DKMS can build robust key recovery directly into the infrastructure, including agent-automated encrypted backup, DKMS key escrow services, and social recovery of keys, for example by backing up or sharding across trusted DKMS connections.","title":"4  Conclusion"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/","text":"First XDI Link Contract between \"btcr\" DID and \"sov\" DID Markus Sabadello, Danube Tech (https://danubetech.com), Vienna, September 19th 2017 We describe an XDI link contract established between two XDI peers, one of which is identified by a btcr DID, and one by a sov DID. We believe this is the first working example of cross-DID-method data sharing and messaging. Note: Due to the fact that most of the technologies described in this paper are work-in-progress, the specific formats and data structures are provisional and expected to change. DIDs DIDs (Decentralized Identifiers, see [1] ) are persistent, resolvable, and cryptographically verifiable URIs. They represent one of the major breakthroughs of the RWoT community as the foundational component of \"self-sovereign\" identity networks. DIDs are in some ways similar to earlier types of identifier that XDI has historically used (\"I-Numbers\", \"Cloud Numbers\"). DIDs support different \"methods\", i.e. ways how they can be registered, resolved, updated, and revoked on a specific distributed ledger or network. This means that although all DIDs are interoperable and provide common functionality, they differ in their underlying properties which can make them more or less suitable for certain use cases. For example, if a use case requires the creation of many cheap DIDs for pairwise relationships, the sov DID method (registered in Sovrin) is ideal. On the other hand, if a DID backed by the strongest existing network is desired, the btcr DID method (registered in Bitcoin) makes sense. Universal Resolver Work is currently underway at the Decentralized Identity Foundation (DIF, see [2] ) to design and implement a \"Universal Resolver\", which provides a client, a web service, and multiple drivers to be able to resolve DIDs (and other identifiers such as human-meaningful names) in a uniform way. Currently, a Java implementation (see [3] ) of the Universal Resolver exists, which contains experimental drivers for the btcr DID method and the sov DID method. The driver for the btcr method builds on txref-conversion-java (see [4] ), which was developed after the RWoT BTCR Virtual Hackathon in July 2017 (see [5] ). The driver for the sov method builds on indy-sdk (see [6] ) and its Java wrapper. In order to build XDI link contracts, data sharing, and messaging on top of DIDs, we use the Universal Resolver for discovering a DID's XDI service endpoint, as well as associated cryptographic keys. The \"btcr\" DID We registered the DID did:btcr:xkrn-xzcr-qqlv-j6sl in the Bitcoin testnet3 . The Universal Resolver produces the following DDO: curl -i -X GET https://uniresolver.danubetech.com/1.0/identifiers/did:btcr:xkrn-xzcr-qqlv-j6sl HTTP/1.1 200 Server: nginx/1.10.3 Date: Tue, 19 Sep 2017 08:16:18 GMT Content-Type: application/ld+json;charset=UTF-8 Transfer-Encoding: chunked Connection: keep-alive { \"id\" : \"did:btcr:xkrn-xzcr-qqlv-j6sl\", \"control\" : [ ], \"service\" : { \"agent\" : \"https://azure.microsoft.com/dif/hub/did:btcr:xkrn-xzcr-qqlv-j6sl\", \"xdi\" : \"https://xdi03-at.danubeclouds.com/cl/=!:did:btcr:xkrn-xzcr-qqlv-j6sl\" }, \"owner\" : { \"id\" : \"did:btcr:xkrn-xzcr-qqlv-j6sl\", \"type\" : [ \"CryptographicKey\", \"EdDsaSAPublicKey\" ], \"curve\" : \"secp256k1\", \"publicKeyHex\" : \"024a63c4362772b0fafc51ac02470dae3f8da8a05d90bae9e1ef3f5243180120dd\" }, \"@context\" : \"https://example.org/did/v1\" } The XDI service endpoint for this DID is https://xdi03-at.danubeclouds.com/cl/=!:did:btcr:xkrn-xzcr-qqlv-j6sl . Note: The BTCR TX Conversion Playground (see [7] ) can also be used to retrieve/produce the DDO associated with a btcr DID. The \"sov\" DID We registered the DID did:sov:WRfXPg8dantKVubE3HX8pw in the Sovrin Provisional Network . The Universal Resolver produces the following DDO: curl -i -X GET https://uniresolver.danubetech.com/1.0/identifiers/did:sov:WRfXPg8dantKVubE3HX8pw HTTP/1.1 200 Server: nginx/1.10.3 Date: Tue, 19 Sep 2017 08:21:03 GMT Content-Type: application/ld+json;charset=UTF-8 Transfer-Encoding: chunked Connection: keep-alive { \"id\" : \"did:sov:WRfXPg8dantKVubE3HX8pw\", \"control\" : [ ], \"service\" : { \"xdi\" : \"https://xdi03-at.danubeclouds.com/cl/=!:did:sov:WRfXPg8dantKVubE3HX8pw\" }, \"owner\" : { \"id\" : \"did:sov:WRfXPg8dantKVubE3HX8pw\", \"type\" : [ \"CryptographicKey\", \"EdDsaSAPublicKey\" ], \"curve\" : \"ed25519\", \"publicKeyBase64\" : \"H3C2AVvLMv6gmMNam3uVAjZpfkcJCwDwnZn6z3wXmqPV\" }, \"@context\" : \"https://example.org/did/v1\" } The XDI service endpoint for this DID is https://xdi03-at.danubeclouds.com/cl/=!:did:sov:WRfXPg8dantKVubE3HX8pw . Note: The sovrin-client (see [8] ) can also be used to retrieve data associated with a sov DID: sovrin@live> send GET_NYM dest=WRfXPg8dantKVubE3HX8pw Getting nym WRfXPg8dantKVubE3HX8pw Current verkey for NYM WRfXPg8dantKVubE3HX8pw is ~P7F3BNs5VmQ6eVpwkNKJ5D sovrin@live> send GET_ATTR dest=WRfXPg8dantKVubE3HX8pw raw=endpoint Getting attr WRfXPg8dantKVubE3HX8pw Found attribute {\"endpoint\": {\"xdi\": \"https://xdi03-at.danubeclouds.com/cl/=!:did:sov:WRfXPg8dantKVubE3HX8pw\"}} The XDI Link Contract An XDI link contract is a data sharing agreement that is human- and machine-understandable and enforceable (see [9] ). It is itself expressed in XDI and part of an XDI graph associated with an XDI peer. The link contract contains information who is authorized to perform certain operations if a certain policy is met. The XDI graph associated with the btcr DID contains the following XDI link contract: (=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)$contract$do/$get/=!:did:btcr:xkrn-xzcr-qqlv-j6sl<#email> (=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)($contract$if$and/$true){$from}/$is/=!:did:sov:WRfXPg8dantKVubE3HX8pw (=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)($contract$if$and/$true){$msg}<$sig><$valid>/&/true Here the btcr DID is called the \"authorizing peer\", and the sov DID is called the \"requesting peer\". The link contract authorizes the sov DID to request the e-mail address in the XDI graph of the btcr DID (note the $get operation). The XDI graph associated with the sov DID contains the following XDI link contract: (=!:did:sov:WRfXPg8dantKVubE3HX8pw/=!:did:btcr:xkrn-xzcr-qqlv-j6sl)$contract$do/$connect/ (=!:did:sov:WRfXPg8dantKVubE3HX8pw/=!:did:btcr:xkrn-xzcr-qqlv-j6sl)($contract$defer$if$and/$true){$from}/$is/=!:did:btcr:xkrn-xzcr-qqlv-j6sl (=!:did:sov:WRfXPg8dantKVubE3HX8pw/=!:did:btcr:xkrn-xzcr-qqlv-j6sl)($contract$defer$if$and/$true){$msg}<$sig><$valid>/&/true Here the sov DID is called the \"authorizing peer\", and the btcr DID is called the \"requesting peer\". The link contract authorizes the btcr DID to request additional link contracts from the sov DID (note the $connect operation). Note: Even though two link contracts are shown in this example, they are in fact independent, i.e. it is perfectly valid just to have one or the other. The XDI Request and Response Based on the first link contract shown above, the sov DID can send a signed XDI message to request the e-mail address in the XDI graph of the btcr DID: =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0/$from/(=!:did:sov:WRfXPg8dantKVubE3HX8pw) =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0/$to/(=!:did:btcr:xkrn-xzcr-qqlv-j6sl) =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0/$contract/(=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)$contract =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0$do/$get/=!:did:btcr:xkrn-xzcr-qqlv-j6sl<#email> =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0<$sig>/&/\"f7c99hAN3hI1E7ttf9+ulwG+x0AmXT4J6C8DV/vs3UPkVk99cvDkXqSe0+dMXG005D6R1GiGuZBEFHNrffDkAg==\" Note how the XDI message references the link contract address (=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)$contract . The btcr DID's XDI peer will validate the signature on the XDI message by obtaining the DID/DDO keys of the sov DID's XDI peer via the Universal Resolver. It will then execute the XDI message and respond: =!:did:btcr:xkrn-xzcr-qqlv-j6sl<#email>/&/\"markus@danubetech.com\" Architectural Options Where are the keys that control a DID/DDO? Stored in a web browser (extension)? In a local wallet on a mobile device? In a cloud service? Many terms are currently being considered to describe various architectural components, e.g. \"personal data store\", \"personal cloud\", \"hub\", \"cloud agent\", \"edge agent\", \"cloud wallet\", \"edge wallet\", etc. One possible architecture involves identity owners holding the DID/DDO keys on local mobile devices, and communicating with cloud-based \"agent\" services they control: SOVRIN BITCOIN ______ ______ ______ ______ ______ ______ | __||__ __||__ | | __||__ __||__ | |___|__||__||__||__|___| |___|__||__||__||__|___| |______||______| <___ |______||______| \\___ | \\___ DDO | XDI SERVICE | \\___ LOOKUP | XDI SERVICE V \\___ V ________________________ \\ ________________________ | | LINK CONTRACT | | | \"sov\" XDI cloud agent | < < < < < < < < < < | \"btcr\" XDI cloud agent | | =!:did:sov:WRfXPg8d... | | =!:did:btcr:xkrn-xz... | |________________________| ___> |________________________| ___/ | ___/ | CONTROL | ___/ | CONTROL | ___/ SIGNED | ___ ___/ XDI MSG ___ ~o/ / \\ ___/ _o / \\ /| | O | |\\ | o | / \\ \\___/ edge device / > \\___/ edge device Another possible architecture involves the cloud-based \"agent\" services to hold the DID/DDO keys, to act on behalf of identity owners: SOVRIN BITCOIN ______ ______ ______ ______ ______ ______ | __||__ __||__ | | __||__ __||__ | |___|__||__||__||__|___| |___|__||__||__||__|___| |______||______| <___ |______||______| \\___ | \\___ DDO | XDI SERVICE | \\___ LOOKUP | XDI SERVICE V \\___ V ________________________ \\ ________________________ | | LINK CONTRACT | | | \"sov\" XDI cloud agent | < < < < < < < < < < | \"btcr\" XDI cloud agent | | =!:did:sov:WRfXPg8d... | | =!:did:btcr:xkrn-xz... | |________________________| ------------------> |________________________| SIGNED | XDI MSG | CONTROL | | CONTROL | | ___ ___ ~o/ / \\ _o / \\ /| | O | |\\ | o | / \\ \\___/ edge device / > \\___/ edge device These are just two simplified options. Many more architectural compositions will be available in a decentralized identity system. Additional Notes XDI examples in this document use the \"XDI DISPLAY\" format. Conversion to other formats such as \"JXD\" is possible (see [10] ). Certain language in this paper is imprecise. For example, instead of saying \" a DID can send a message\", it would be more accurate to say \" the entity that the DID identifies can send a message\" (for a discussion, see [11] ). There is ongoing discussion on how the cryptographic keys used to control the DDO relate to \"off-chain\" or \"peer-to-peer\" functionality, such as verifiable claims or XDI messaging. In this paper, the \"owner\" key described by a DDO is used for signing and validating peer-to-peer XDI messages, i.e. the keys to control the DDO are the same as the keys used for XDI messaging. More diverse scenarios are possible, e.g. a DDO can publish multiple keys associated with a DID. This means that for example, even though btcr DIDs use an \"EdDsaSAPublicKey\" on the \"secp256k1\" curve, and sov DIDs use an \"EdDsaSAPublicKey\" key on the \"ed25519\" curve, they could both publish RSA keys in their DDOs for use by verifiable claims or XDI messaging. Digital signatures are only one kind of proof that can be used for authentication and authorization decisions (for a discussion, see [12] ). In an XDI link contract's policy, more complex proofs than just plain digital signatures can be supported. In this paper we demonstrate an XDI link contract between XDI peers that use different key types. This allows for diversity of both DID methods and key types. In some scenarios however it may be preferential to use the same key types for all participants, e.g. when protocols such as DID-TLS (see [13] ) or CurveCP are used for peer-to-peer communication. Related Work Previous XDI-related contributions to RWoT include: XDI, Blockstore, and BIP32: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust/blob/master/topics-and-advance-readings/cool-hack-xdi-blockstore-bip32.md XDI Link Contracts: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust/blob/master/topics-and-advance-readings/xdi-link-contracts.md XDI Graphs in IPFS: https://github.com/WebOfTrustInfo/ID2020DesignWorkshop/blob/master/topics-and-advance-readings/XDI-Graphs-in-IPFS.md JXD Examples: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2016/blob/master/topics-and-advance-readings/JXD-Examples.md XDI Verifiable Claims and Link Contracts: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2017/blob/master/topics-and-advance-readings/xdi-verifiable-claims-link-contracts.md Thank you to the wonderful RWoT community for its great and idealistic work! References [1] https://docs.google.com/document/d/1Z-9jX4PEWtyRFD5fEyyzEnWK_0ir0no1JJLuRu8O9Gs/ [2] http://identity.foundation/ [3] https://github.com/decentralized-identity/universal-resolver/tree/master/implementations/java [4] https://github.com/WebOfTrustInfo/txref-conversion-java [5] https://github.com/WebOfTrustInfo/btcr-hackathon [6] https://github.com/hyperledger/indy-sdk [7] https://weboftrustinfo.github.io/btcr-tx-playground.github.io/ [8] https://github.com/evernym/sovrin-client [9] https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust/blob/master/topics-and-advance-readings/xdi-link-contracts.md [10] https://server.xdi2.org/XDIConverter [11] https://github.com/w3c-ccg/did-spec/pull/22 [12] https://github.com/w3c-ccg/did-spec/issues/15 [13] https://docs.google.com/document/d/1-aPY1eeHdR_TnF7_WpEs58RZ_jNdDeptVrNEu3groFc/","title":"First xdi link contract between btcr did and sov did"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#first-xdi-link-contract-between-btcr-did-and-sov-did","text":"Markus Sabadello, Danube Tech (https://danubetech.com), Vienna, September 19th 2017 We describe an XDI link contract established between two XDI peers, one of which is identified by a btcr DID, and one by a sov DID. We believe this is the first working example of cross-DID-method data sharing and messaging. Note: Due to the fact that most of the technologies described in this paper are work-in-progress, the specific formats and data structures are provisional and expected to change.","title":"First XDI Link Contract between \"btcr\" DID and \"sov\" DID"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#dids","text":"DIDs (Decentralized Identifiers, see [1] ) are persistent, resolvable, and cryptographically verifiable URIs. They represent one of the major breakthroughs of the RWoT community as the foundational component of \"self-sovereign\" identity networks. DIDs are in some ways similar to earlier types of identifier that XDI has historically used (\"I-Numbers\", \"Cloud Numbers\"). DIDs support different \"methods\", i.e. ways how they can be registered, resolved, updated, and revoked on a specific distributed ledger or network. This means that although all DIDs are interoperable and provide common functionality, they differ in their underlying properties which can make them more or less suitable for certain use cases. For example, if a use case requires the creation of many cheap DIDs for pairwise relationships, the sov DID method (registered in Sovrin) is ideal. On the other hand, if a DID backed by the strongest existing network is desired, the btcr DID method (registered in Bitcoin) makes sense.","title":"DIDs"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#universal-resolver","text":"Work is currently underway at the Decentralized Identity Foundation (DIF, see [2] ) to design and implement a \"Universal Resolver\", which provides a client, a web service, and multiple drivers to be able to resolve DIDs (and other identifiers such as human-meaningful names) in a uniform way. Currently, a Java implementation (see [3] ) of the Universal Resolver exists, which contains experimental drivers for the btcr DID method and the sov DID method. The driver for the btcr method builds on txref-conversion-java (see [4] ), which was developed after the RWoT BTCR Virtual Hackathon in July 2017 (see [5] ). The driver for the sov method builds on indy-sdk (see [6] ) and its Java wrapper. In order to build XDI link contracts, data sharing, and messaging on top of DIDs, we use the Universal Resolver for discovering a DID's XDI service endpoint, as well as associated cryptographic keys.","title":"Universal Resolver"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#the-btcr-did","text":"We registered the DID did:btcr:xkrn-xzcr-qqlv-j6sl in the Bitcoin testnet3 . The Universal Resolver produces the following DDO: curl -i -X GET https://uniresolver.danubetech.com/1.0/identifiers/did:btcr:xkrn-xzcr-qqlv-j6sl HTTP/1.1 200 Server: nginx/1.10.3 Date: Tue, 19 Sep 2017 08:16:18 GMT Content-Type: application/ld+json;charset=UTF-8 Transfer-Encoding: chunked Connection: keep-alive { \"id\" : \"did:btcr:xkrn-xzcr-qqlv-j6sl\", \"control\" : [ ], \"service\" : { \"agent\" : \"https://azure.microsoft.com/dif/hub/did:btcr:xkrn-xzcr-qqlv-j6sl\", \"xdi\" : \"https://xdi03-at.danubeclouds.com/cl/=!:did:btcr:xkrn-xzcr-qqlv-j6sl\" }, \"owner\" : { \"id\" : \"did:btcr:xkrn-xzcr-qqlv-j6sl\", \"type\" : [ \"CryptographicKey\", \"EdDsaSAPublicKey\" ], \"curve\" : \"secp256k1\", \"publicKeyHex\" : \"024a63c4362772b0fafc51ac02470dae3f8da8a05d90bae9e1ef3f5243180120dd\" }, \"@context\" : \"https://example.org/did/v1\" } The XDI service endpoint for this DID is https://xdi03-at.danubeclouds.com/cl/=!:did:btcr:xkrn-xzcr-qqlv-j6sl . Note: The BTCR TX Conversion Playground (see [7] ) can also be used to retrieve/produce the DDO associated with a btcr DID.","title":"The \"btcr\" DID"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#the-sov-did","text":"We registered the DID did:sov:WRfXPg8dantKVubE3HX8pw in the Sovrin Provisional Network . The Universal Resolver produces the following DDO: curl -i -X GET https://uniresolver.danubetech.com/1.0/identifiers/did:sov:WRfXPg8dantKVubE3HX8pw HTTP/1.1 200 Server: nginx/1.10.3 Date: Tue, 19 Sep 2017 08:21:03 GMT Content-Type: application/ld+json;charset=UTF-8 Transfer-Encoding: chunked Connection: keep-alive { \"id\" : \"did:sov:WRfXPg8dantKVubE3HX8pw\", \"control\" : [ ], \"service\" : { \"xdi\" : \"https://xdi03-at.danubeclouds.com/cl/=!:did:sov:WRfXPg8dantKVubE3HX8pw\" }, \"owner\" : { \"id\" : \"did:sov:WRfXPg8dantKVubE3HX8pw\", \"type\" : [ \"CryptographicKey\", \"EdDsaSAPublicKey\" ], \"curve\" : \"ed25519\", \"publicKeyBase64\" : \"H3C2AVvLMv6gmMNam3uVAjZpfkcJCwDwnZn6z3wXmqPV\" }, \"@context\" : \"https://example.org/did/v1\" } The XDI service endpoint for this DID is https://xdi03-at.danubeclouds.com/cl/=!:did:sov:WRfXPg8dantKVubE3HX8pw . Note: The sovrin-client (see [8] ) can also be used to retrieve data associated with a sov DID: sovrin@live> send GET_NYM dest=WRfXPg8dantKVubE3HX8pw Getting nym WRfXPg8dantKVubE3HX8pw Current verkey for NYM WRfXPg8dantKVubE3HX8pw is ~P7F3BNs5VmQ6eVpwkNKJ5D sovrin@live> send GET_ATTR dest=WRfXPg8dantKVubE3HX8pw raw=endpoint Getting attr WRfXPg8dantKVubE3HX8pw Found attribute {\"endpoint\": {\"xdi\": \"https://xdi03-at.danubeclouds.com/cl/=!:did:sov:WRfXPg8dantKVubE3HX8pw\"}}","title":"The \"sov\" DID"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#the-xdi-link-contract","text":"An XDI link contract is a data sharing agreement that is human- and machine-understandable and enforceable (see [9] ). It is itself expressed in XDI and part of an XDI graph associated with an XDI peer. The link contract contains information who is authorized to perform certain operations if a certain policy is met. The XDI graph associated with the btcr DID contains the following XDI link contract: (=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)$contract$do/$get/=!:did:btcr:xkrn-xzcr-qqlv-j6sl<#email> (=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)($contract$if$and/$true){$from}/$is/=!:did:sov:WRfXPg8dantKVubE3HX8pw (=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)($contract$if$and/$true){$msg}<$sig><$valid>/&/true Here the btcr DID is called the \"authorizing peer\", and the sov DID is called the \"requesting peer\". The link contract authorizes the sov DID to request the e-mail address in the XDI graph of the btcr DID (note the $get operation). The XDI graph associated with the sov DID contains the following XDI link contract: (=!:did:sov:WRfXPg8dantKVubE3HX8pw/=!:did:btcr:xkrn-xzcr-qqlv-j6sl)$contract$do/$connect/ (=!:did:sov:WRfXPg8dantKVubE3HX8pw/=!:did:btcr:xkrn-xzcr-qqlv-j6sl)($contract$defer$if$and/$true){$from}/$is/=!:did:btcr:xkrn-xzcr-qqlv-j6sl (=!:did:sov:WRfXPg8dantKVubE3HX8pw/=!:did:btcr:xkrn-xzcr-qqlv-j6sl)($contract$defer$if$and/$true){$msg}<$sig><$valid>/&/true Here the sov DID is called the \"authorizing peer\", and the btcr DID is called the \"requesting peer\". The link contract authorizes the btcr DID to request additional link contracts from the sov DID (note the $connect operation). Note: Even though two link contracts are shown in this example, they are in fact independent, i.e. it is perfectly valid just to have one or the other.","title":"The XDI Link Contract"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#the-xdi-request-and-response","text":"Based on the first link contract shown above, the sov DID can send a signed XDI message to request the e-mail address in the XDI graph of the btcr DID: =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0/$from/(=!:did:sov:WRfXPg8dantKVubE3HX8pw) =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0/$to/(=!:did:btcr:xkrn-xzcr-qqlv-j6sl) =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0/$contract/(=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)$contract =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0$do/$get/=!:did:btcr:xkrn-xzcr-qqlv-j6sl<#email> =!:did:sov:WRfXPg8dantKVubE3HX8pw[$msg]@~0<$sig>/&/\"f7c99hAN3hI1E7ttf9+ulwG+x0AmXT4J6C8DV/vs3UPkVk99cvDkXqSe0+dMXG005D6R1GiGuZBEFHNrffDkAg==\" Note how the XDI message references the link contract address (=!:did:btcr:xkrn-xzcr-qqlv-j6sl/=!:did:sov:WRfXPg8dantKVubE3HX8pw)$contract . The btcr DID's XDI peer will validate the signature on the XDI message by obtaining the DID/DDO keys of the sov DID's XDI peer via the Universal Resolver. It will then execute the XDI message and respond: =!:did:btcr:xkrn-xzcr-qqlv-j6sl<#email>/&/\"markus@danubetech.com\"","title":"The XDI Request and Response"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#architectural-options","text":"Where are the keys that control a DID/DDO? Stored in a web browser (extension)? In a local wallet on a mobile device? In a cloud service? Many terms are currently being considered to describe various architectural components, e.g. \"personal data store\", \"personal cloud\", \"hub\", \"cloud agent\", \"edge agent\", \"cloud wallet\", \"edge wallet\", etc. One possible architecture involves identity owners holding the DID/DDO keys on local mobile devices, and communicating with cloud-based \"agent\" services they control: SOVRIN BITCOIN ______ ______ ______ ______ ______ ______ | __||__ __||__ | | __||__ __||__ | |___|__||__||__||__|___| |___|__||__||__||__|___| |______||______| <___ |______||______| \\___ | \\___ DDO | XDI SERVICE | \\___ LOOKUP | XDI SERVICE V \\___ V ________________________ \\ ________________________ | | LINK CONTRACT | | | \"sov\" XDI cloud agent | < < < < < < < < < < | \"btcr\" XDI cloud agent | | =!:did:sov:WRfXPg8d... | | =!:did:btcr:xkrn-xz... | |________________________| ___> |________________________| ___/ | ___/ | CONTROL | ___/ | CONTROL | ___/ SIGNED | ___ ___/ XDI MSG ___ ~o/ / \\ ___/ _o / \\ /| | O | |\\ | o | / \\ \\___/ edge device / > \\___/ edge device Another possible architecture involves the cloud-based \"agent\" services to hold the DID/DDO keys, to act on behalf of identity owners: SOVRIN BITCOIN ______ ______ ______ ______ ______ ______ | __||__ __||__ | | __||__ __||__ | |___|__||__||__||__|___| |___|__||__||__||__|___| |______||______| <___ |______||______| \\___ | \\___ DDO | XDI SERVICE | \\___ LOOKUP | XDI SERVICE V \\___ V ________________________ \\ ________________________ | | LINK CONTRACT | | | \"sov\" XDI cloud agent | < < < < < < < < < < | \"btcr\" XDI cloud agent | | =!:did:sov:WRfXPg8d... | | =!:did:btcr:xkrn-xz... | |________________________| ------------------> |________________________| SIGNED | XDI MSG | CONTROL | | CONTROL | | ___ ___ ~o/ / \\ _o / \\ /| | O | |\\ | o | / \\ \\___/ edge device / > \\___/ edge device These are just two simplified options. Many more architectural compositions will be available in a decentralized identity system.","title":"Architectural Options"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#additional-notes","text":"XDI examples in this document use the \"XDI DISPLAY\" format. Conversion to other formats such as \"JXD\" is possible (see [10] ). Certain language in this paper is imprecise. For example, instead of saying \" a DID can send a message\", it would be more accurate to say \" the entity that the DID identifies can send a message\" (for a discussion, see [11] ). There is ongoing discussion on how the cryptographic keys used to control the DDO relate to \"off-chain\" or \"peer-to-peer\" functionality, such as verifiable claims or XDI messaging. In this paper, the \"owner\" key described by a DDO is used for signing and validating peer-to-peer XDI messages, i.e. the keys to control the DDO are the same as the keys used for XDI messaging. More diverse scenarios are possible, e.g. a DDO can publish multiple keys associated with a DID. This means that for example, even though btcr DIDs use an \"EdDsaSAPublicKey\" on the \"secp256k1\" curve, and sov DIDs use an \"EdDsaSAPublicKey\" key on the \"ed25519\" curve, they could both publish RSA keys in their DDOs for use by verifiable claims or XDI messaging. Digital signatures are only one kind of proof that can be used for authentication and authorization decisions (for a discussion, see [12] ). In an XDI link contract's policy, more complex proofs than just plain digital signatures can be supported. In this paper we demonstrate an XDI link contract between XDI peers that use different key types. This allows for diversity of both DID methods and key types. In some scenarios however it may be preferential to use the same key types for all participants, e.g. when protocols such as DID-TLS (see [13] ) or CurveCP are used for peer-to-peer communication.","title":"Additional Notes"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#related-work","text":"Previous XDI-related contributions to RWoT include: XDI, Blockstore, and BIP32: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust/blob/master/topics-and-advance-readings/cool-hack-xdi-blockstore-bip32.md XDI Link Contracts: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust/blob/master/topics-and-advance-readings/xdi-link-contracts.md XDI Graphs in IPFS: https://github.com/WebOfTrustInfo/ID2020DesignWorkshop/blob/master/topics-and-advance-readings/XDI-Graphs-in-IPFS.md JXD Examples: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2016/blob/master/topics-and-advance-readings/JXD-Examples.md XDI Verifiable Claims and Link Contracts: https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2017/blob/master/topics-and-advance-readings/xdi-verifiable-claims-link-contracts.md Thank you to the wonderful RWoT community for its great and idealistic work!","title":"Related Work"},{"location":"rwot5/topics-and-advance-readings/first-xdi-link-contract-between-btcr-did-and-sov-did/#references","text":"[1] https://docs.google.com/document/d/1Z-9jX4PEWtyRFD5fEyyzEnWK_0ir0no1JJLuRu8O9Gs/ [2] http://identity.foundation/ [3] https://github.com/decentralized-identity/universal-resolver/tree/master/implementations/java [4] https://github.com/WebOfTrustInfo/txref-conversion-java [5] https://github.com/WebOfTrustInfo/btcr-hackathon [6] https://github.com/hyperledger/indy-sdk [7] https://weboftrustinfo.github.io/btcr-tx-playground.github.io/ [8] https://github.com/evernym/sovrin-client [9] https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust/blob/master/topics-and-advance-readings/xdi-link-contracts.md [10] https://server.xdi2.org/XDIConverter [11] https://github.com/w3c-ccg/did-spec/pull/22 [12] https://github.com/w3c-ccg/did-spec/issues/15 [13] https://docs.google.com/document/d/1-aPY1eeHdR_TnF7_WpEs58RZ_jNdDeptVrNEu3groFc/","title":"References"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/","text":"A Primer on Functional Identity By Joe Andrieu, Legendary Requirements joe@legreq.com There are many ways to approach identity. For the Rebooting Web of Trust, we prefer a functional approach to focus conversations on how identity works and how we use it. The varied facets of identity are so rich that we each bring our own hot buttons and agendas to any discussion. Some people engage from a philosophical perspective, others cultural . Some dive into political issues and others get meta-physical and spiritual . These different perspectives are valid views of identity\u2019s impact on our lives. More than valid. Vital . They help answer the question of \u201cWhy?\u201d Why identity matters, why we should care. Unfortunately, they also inflame passions . We sometimes talk past each other to make points that have minimal relevance for others, leaving people frustrated and unheard. As engineers and system designers , we\u2019re concerned with how things work . We want to fix what\u2019s broken and build new things. To do that, we want to discuss how things function. With identity, this functional perspective sidesteps the inflammatory rabbit holes, without dismissing them. Functional Identity lets us investigate the HOW without prejudice to WHY , viewing identity systems based on how they work and then, in turn, how they affect individuals and society. A Functional Definition Identity is how we keep track of people and things and, in turn, how they keep track of us. That\u2019s it . We learn people\u2019s names, we observe them and hear gossip and consume media. We then apply our sense of who they are to our dealings with them. Others do the same in return. For many, this simple definition is provocative . It triggers thoughts of Big Brother and the surveillance state. It brings up ideas about embedded chips and tattooed serial numbers. It conjures fears of government or corporations constantly tracking what we do. Which is ok, because, in fact, those are the most feared abuses of identity. It\u2019s important to realize when we talk about identity that we are always talking about how we keep track of people. There are also a number of wonderful uses of identity. The joy of a child asking for \u201cMomma\u201d or a lover calling out your name. The pride in your name on a diploma. The simple benefit of seeing another\u2019s name tag at a workshop and better remembering that fascinating conversation. Identity enables so much good stuff because it helps us keep track of people and things. The functional approach reaches beyond digital systems to understand how identity works throughout society. Our identity is bigger than our digital selves. Our identities existed before and continue to exist independent of any digital representation. Digital identities are simply tools which help organizations and individuals manage real-world identity. In the simplest terms, digital identities help keep track of people digitally. Unfortunately, digital systems can unwittingly compromise real-world identity. Sometimes this occurs because digital identity systems neglect to consider external effects. Other times, it happens with systems that didn\u2019t even realize they were dealing with identity-related personal information. A functional perspective allows engineers to see beyond static attributes and traditional notions of \u201cPersonally Identifiable Information\u201d to better understand how engineering choices can impact identity, even outside their systems. With a better understanding of how identity functions, we will be able to build systems that enhance privacy and human dignity, while improving identity assurance and security. Identity Systems An identity system is a collection of tools and techniques used to keep track of people and things. As individuals, we do this naturally , in our minds. We name things, then use names and distinguishing features to remember what we learn. We treat people differently based on their identity: treating our friends and family differently from strangers and known threats. Organizations create processes, software, and services to achieve similar ends. These identity systems are best understood in terms of how they function, which is the same as how identity has worked since the dawn of civilization . Terminology In the diagrams below, the blue boxes are nouns and the red ovals are verbs \u2013 the building blocks for describing identity systems. We start with the simplest identity system, using three nouns and a verb: Subjects are entities\u2014people or things\u2014under consideration. Identifiers are labels which refer to entities. They are used to keep track of what we know about those entities. Attributes are what we know about people and things. They describe the state, appearance, or other qualities of an entity. Correlate means to associate attributes with particular entities, to associate what we know about someone with either an identifier in the system or a subject in question. Identity systems correlate subjects with attributes in two ways. First, attributes are associated with identifiers referring to specific subjects , thus building a body of knowledge. Then, when we recognize a subject, we associate them with one or more identifiers, and in doing so, associate them with everything we know about those identifiers. These terms apply equally to things other than people, such as organizations, groups, or places. We correlate new attributes with identifiers and vice-versa as we learn about subjects. When we recognize a person or thing we can apply everything we have learned about them. In digital systems, this set of related attributes is sometimes referred to as a digital identity or profile. Input and Effect We learn or acquire identity information over time, then apply what we\u2019ve learned to various interactions, usually elsewhere. Acquire means to gather identity information for use by the system. Apply means to use identity information to affect change outside the identity system, typically to moderate an interaction of the subject with a related system. Identity information might be acquired by observation or by importing from elsewhere. We may learn about someone by watching them, or we may learn through references, rumors, and reputation. Identity systems acquire new information throughout their operational life, just as we continue to learn about people throughout our lives. Once acquired, identity information must be applied in a specific situation to have impact. If we know something about someone and that information never influences our behavior and is never shared, it doesn\u2019t affect the world. The way that identity information is applied tells us how an identity system affects our lives. For example, a website might apply the email associated with my account to allow me to reset my password or it may send me unwanted advertisements. The U.S. Transportation Security Administration (TSA) applies the information on its no-fly list to prevent those identified as potential threats from flying. Making New Ideas We gain new insights by considering both existing identity information and previously unrelated observations. Identity is more than just what we know about people and apply to our interactions. It\u2019s also how we make judgments based on what we know, gaining insights into character, capabilities, and proclivities. Raw data are data which may or may not contain information relatable to any particular person or thing. Derived attributes are conclusions reached by reasoning over identity information. They are what we learn when we consider what we know about people and things. Reason means to evaluate existing identity information to generate new derived attributes. Derived attributes are created by reasoning using raw data and known attributes. By applying reasoning to existing observations and related knowledge, we can gain insights that neither the subject nor the original author anticipated. Raw data such as search history, web browsing, and the time & location information captured by our phones, may contain identity-related information, even when that was neither the purpose nor the intention at the time of capture. We also reason using known attributes to derive new ones. For example, we calculate a person\u2019s age based on the birthdate on their driver\u2019s license to determine if they are old enough to drink legally. Credit companies evaluate recent income, past transactions, and projections of future income to set interest rates and make loan approvals. We remember how people treat us and alter our behavior in future interactions. If someone repeatedly breaks their word, we may stop depending on them. Securing Identity Information We go to great lengths to keep identity information secure. Secure means to restrict the creation and flow of identity information to the right people at the right time. Sometimes we keep secrets to prevent information from reaching certain people. We do this with tools like encryption, access control, and minimal disclosure. Legal agreements between people, businesses, institutions, and governments specify appropriate use of certain information while laws, regulations, and the courts allow governments and institutions to oversee , monitor, and intervene in the capture and use of identity information. How identity systems secure certain information, and not others, defines how they preserve and respect privacy . The right to keep private information private is often referred to as the right of privacy. Many people feel their privacy is threatened because so much information is shared over the Internet, in our workplaces, and through our devices. Information we share in different contexts (business, family, community, etc.) can leak unexpectedly and undesirably into other contexts. It is difficult as individuals to understand of all the ways we are publicly or privately tracked. Information is shared on social media, tracked in Internet searches, monitored when using navigation software, and captured as we use our phones. The sheer magnitude and complexity of the information tracked and used means the average person is essentially incapable of making informed decisions to consent to appropriate use. Some people give up , divulging personal information without regard to consequences. Others opt-out , participating as little as possible in our digitally connected world. Identity matters because the myriad ways we are tracked can impact our lives. It pays to understand the options we have as engineers, and as individuals, for protecting ourselves, our families, and our businesses. Bridging the Gap The nouns and verbs above are grounded in the world of technology and may be unfamiliar for the average individual. More conversational synonyms are presented in the tables below. Use the most appropriate terms for your audience. People, Places and Things This is the point of identity: those people, places, and things we recognize. Technologists Laypeople Common Meaning Subject Person Someone under consideration. The focus of inquiry. Identity Information These are the abstract nouns of identity, the informational assets created and used by identity systems. Technologists Laypeople Common Meaning Identifiers Names Refers to entities. Used to keep track of people and things. Attributes Statements What we know about people and things. They describe the state, appearance, or other qualities of an entity. Raw data Observations Data which may or may not contain correlatable information. Derived attributes Beliefs Conclusions reached by reasoning over identity information. These are what we learn when we consider what we know about people and things. Identity Actions These are the verbs of identity. These are the actions taken by identity systems working with identity information. Technologists Laypeople Common Meaning Acquire Collect Intake or generate identity information for use by the system. Correlate Relate Associate attributes or observations with particular entities. We associate what we know about someone with either an identifier in the system or with a subject in question. Reason Reason Evaluate existing identity information to generate new beliefs, expressed in attributes, captured in statements. Apply Apply Use identity information in a system, typically to moderate interactions with known entities. Secure Protect Restrict the creation and flow of identity information to the right people at the right time. For technologists We assign identifiers to subjects . We collect raw data and correlate attributes to the subjects we track. We reason over raw data and attributes, to derive new attributes . We then apply this information to current and future interactions with subjects. We secure identity information to keep secrets and preserve privacy. For laypeople We give names to people . We collect observations and record statements relating those observations to people we know. We reason over these observations, statements, and beliefs to generate new beliefs. We then apply what we know and believe when dealing with those we recognize. We protect identity information to keep secrets and preserve privacy. This is the vocabulary of Functional Identity, a way to discuss identity in terms of functionality: how it works and what it does for us. This is the language of identity for the Rebooting Web of Trust. Why? Engineers, entrepreneurs, and financiers have asked \u201cWhy are we spending so much time with a definition of identity? Why not just build something and fix it if it is broken?\u201d The vital, simple reason is human dignity . When we build interconnected systems without a core understanding of identity, we risk inadvertently compromising human dignity. We risk accidentally building systems that deny self-expression, place individuals in harm\u2019s way, and unintentionally oppress those most in need of self-determination. There are times when the needs of security outweigh the need for human dignity. Fine. It\u2019s the job of our political systems\u2014local, national, and international\u2014to minimize abuse and to establish boundaries and practices that respect basic human rights. But when engineers unwittingly compromise the ability of individuals to self-express their identity, when we expose personal information in unexpected ways, when our systems deny basic services because of a flawed understanding of identity, these are avoidable tragedies . What might seem a minor technicality in one conversation could lead to the loss of privacy, liberty, or even life for an individual whose identity is unintentionally compromised . That\u2019s why it pays to understand identity, so the systems we build intentionally enable human dignity instead of accidentally destroy it. Summary Functional Identity focuses on how identity works . At the Rebooting Web of Trust, we ground our work in the functional notion of identity and avoid the psychological, cultural, political, and philosophical. These notions are important, but they can also distract us from understanding the technical choices involved in building and using identity in today\u2019s networked world. This functional notion of identity began with a conversation at the Internet Identity Workshop in May of 2016, followed by conversations at ID2020 and the second Rebooting Web of Trust workshop that summer, resulting in the paper Identity Crisis . It continued in subsequent meetings in all three venues, and in two articles published by the People Centered Internet Speaking of Identity and How Identity Can Enable A People-Centered Internet . This primer represents a current take on that conversation, geared to help Rebooting Web of Trust participants communicate more clearly and collaborate more effectively. We encourage your feedback and look forward to continuing the conversation.","title":"A Primer on Functional Identity"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#a-primer-on-functional-identity","text":"By Joe Andrieu, Legendary Requirements joe@legreq.com There are many ways to approach identity. For the Rebooting Web of Trust, we prefer a functional approach to focus conversations on how identity works and how we use it. The varied facets of identity are so rich that we each bring our own hot buttons and agendas to any discussion. Some people engage from a philosophical perspective, others cultural . Some dive into political issues and others get meta-physical and spiritual . These different perspectives are valid views of identity\u2019s impact on our lives. More than valid. Vital . They help answer the question of \u201cWhy?\u201d Why identity matters, why we should care. Unfortunately, they also inflame passions . We sometimes talk past each other to make points that have minimal relevance for others, leaving people frustrated and unheard. As engineers and system designers , we\u2019re concerned with how things work . We want to fix what\u2019s broken and build new things. To do that, we want to discuss how things function. With identity, this functional perspective sidesteps the inflammatory rabbit holes, without dismissing them. Functional Identity lets us investigate the HOW without prejudice to WHY , viewing identity systems based on how they work and then, in turn, how they affect individuals and society.","title":"A Primer on Functional Identity"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#a-functional-definition","text":"Identity is how we keep track of people and things and, in turn, how they keep track of us. That\u2019s it . We learn people\u2019s names, we observe them and hear gossip and consume media. We then apply our sense of who they are to our dealings with them. Others do the same in return. For many, this simple definition is provocative . It triggers thoughts of Big Brother and the surveillance state. It brings up ideas about embedded chips and tattooed serial numbers. It conjures fears of government or corporations constantly tracking what we do. Which is ok, because, in fact, those are the most feared abuses of identity. It\u2019s important to realize when we talk about identity that we are always talking about how we keep track of people. There are also a number of wonderful uses of identity. The joy of a child asking for \u201cMomma\u201d or a lover calling out your name. The pride in your name on a diploma. The simple benefit of seeing another\u2019s name tag at a workshop and better remembering that fascinating conversation. Identity enables so much good stuff because it helps us keep track of people and things. The functional approach reaches beyond digital systems to understand how identity works throughout society. Our identity is bigger than our digital selves. Our identities existed before and continue to exist independent of any digital representation. Digital identities are simply tools which help organizations and individuals manage real-world identity. In the simplest terms, digital identities help keep track of people digitally. Unfortunately, digital systems can unwittingly compromise real-world identity. Sometimes this occurs because digital identity systems neglect to consider external effects. Other times, it happens with systems that didn\u2019t even realize they were dealing with identity-related personal information. A functional perspective allows engineers to see beyond static attributes and traditional notions of \u201cPersonally Identifiable Information\u201d to better understand how engineering choices can impact identity, even outside their systems. With a better understanding of how identity functions, we will be able to build systems that enhance privacy and human dignity, while improving identity assurance and security.","title":"A Functional Definition"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#identity-systems","text":"An identity system is a collection of tools and techniques used to keep track of people and things. As individuals, we do this naturally , in our minds. We name things, then use names and distinguishing features to remember what we learn. We treat people differently based on their identity: treating our friends and family differently from strangers and known threats. Organizations create processes, software, and services to achieve similar ends. These identity systems are best understood in terms of how they function, which is the same as how identity has worked since the dawn of civilization .","title":"Identity Systems"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#terminology","text":"In the diagrams below, the blue boxes are nouns and the red ovals are verbs \u2013 the building blocks for describing identity systems. We start with the simplest identity system, using three nouns and a verb: Subjects are entities\u2014people or things\u2014under consideration. Identifiers are labels which refer to entities. They are used to keep track of what we know about those entities. Attributes are what we know about people and things. They describe the state, appearance, or other qualities of an entity. Correlate means to associate attributes with particular entities, to associate what we know about someone with either an identifier in the system or a subject in question. Identity systems correlate subjects with attributes in two ways. First, attributes are associated with identifiers referring to specific subjects , thus building a body of knowledge. Then, when we recognize a subject, we associate them with one or more identifiers, and in doing so, associate them with everything we know about those identifiers. These terms apply equally to things other than people, such as organizations, groups, or places. We correlate new attributes with identifiers and vice-versa as we learn about subjects. When we recognize a person or thing we can apply everything we have learned about them. In digital systems, this set of related attributes is sometimes referred to as a digital identity or profile.","title":"Terminology"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#input-and-effect","text":"We learn or acquire identity information over time, then apply what we\u2019ve learned to various interactions, usually elsewhere. Acquire means to gather identity information for use by the system. Apply means to use identity information to affect change outside the identity system, typically to moderate an interaction of the subject with a related system. Identity information might be acquired by observation or by importing from elsewhere. We may learn about someone by watching them, or we may learn through references, rumors, and reputation. Identity systems acquire new information throughout their operational life, just as we continue to learn about people throughout our lives. Once acquired, identity information must be applied in a specific situation to have impact. If we know something about someone and that information never influences our behavior and is never shared, it doesn\u2019t affect the world. The way that identity information is applied tells us how an identity system affects our lives. For example, a website might apply the email associated with my account to allow me to reset my password or it may send me unwanted advertisements. The U.S. Transportation Security Administration (TSA) applies the information on its no-fly list to prevent those identified as potential threats from flying.","title":"Input and Effect"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#making-new-ideas","text":"We gain new insights by considering both existing identity information and previously unrelated observations. Identity is more than just what we know about people and apply to our interactions. It\u2019s also how we make judgments based on what we know, gaining insights into character, capabilities, and proclivities. Raw data are data which may or may not contain information relatable to any particular person or thing. Derived attributes are conclusions reached by reasoning over identity information. They are what we learn when we consider what we know about people and things. Reason means to evaluate existing identity information to generate new derived attributes. Derived attributes are created by reasoning using raw data and known attributes. By applying reasoning to existing observations and related knowledge, we can gain insights that neither the subject nor the original author anticipated. Raw data such as search history, web browsing, and the time & location information captured by our phones, may contain identity-related information, even when that was neither the purpose nor the intention at the time of capture. We also reason using known attributes to derive new ones. For example, we calculate a person\u2019s age based on the birthdate on their driver\u2019s license to determine if they are old enough to drink legally. Credit companies evaluate recent income, past transactions, and projections of future income to set interest rates and make loan approvals. We remember how people treat us and alter our behavior in future interactions. If someone repeatedly breaks their word, we may stop depending on them.","title":"Making New Ideas"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#securing-identity-information","text":"We go to great lengths to keep identity information secure. Secure means to restrict the creation and flow of identity information to the right people at the right time. Sometimes we keep secrets to prevent information from reaching certain people. We do this with tools like encryption, access control, and minimal disclosure. Legal agreements between people, businesses, institutions, and governments specify appropriate use of certain information while laws, regulations, and the courts allow governments and institutions to oversee , monitor, and intervene in the capture and use of identity information. How identity systems secure certain information, and not others, defines how they preserve and respect privacy . The right to keep private information private is often referred to as the right of privacy. Many people feel their privacy is threatened because so much information is shared over the Internet, in our workplaces, and through our devices. Information we share in different contexts (business, family, community, etc.) can leak unexpectedly and undesirably into other contexts. It is difficult as individuals to understand of all the ways we are publicly or privately tracked. Information is shared on social media, tracked in Internet searches, monitored when using navigation software, and captured as we use our phones. The sheer magnitude and complexity of the information tracked and used means the average person is essentially incapable of making informed decisions to consent to appropriate use. Some people give up , divulging personal information without regard to consequences. Others opt-out , participating as little as possible in our digitally connected world. Identity matters because the myriad ways we are tracked can impact our lives. It pays to understand the options we have as engineers, and as individuals, for protecting ourselves, our families, and our businesses.","title":"Securing Identity Information"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#bridging-the-gap","text":"The nouns and verbs above are grounded in the world of technology and may be unfamiliar for the average individual. More conversational synonyms are presented in the tables below. Use the most appropriate terms for your audience.","title":"Bridging the Gap"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#people-places-and-things","text":"This is the point of identity: those people, places, and things we recognize. Technologists Laypeople Common Meaning Subject Person Someone under consideration. The focus of inquiry.","title":"People, Places and Things"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#identity-information","text":"These are the abstract nouns of identity, the informational assets created and used by identity systems. Technologists Laypeople Common Meaning Identifiers Names Refers to entities. Used to keep track of people and things. Attributes Statements What we know about people and things. They describe the state, appearance, or other qualities of an entity. Raw data Observations Data which may or may not contain correlatable information. Derived attributes Beliefs Conclusions reached by reasoning over identity information. These are what we learn when we consider what we know about people and things.","title":"Identity Information"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#identity-actions","text":"These are the verbs of identity. These are the actions taken by identity systems working with identity information. Technologists Laypeople Common Meaning Acquire Collect Intake or generate identity information for use by the system. Correlate Relate Associate attributes or observations with particular entities. We associate what we know about someone with either an identifier in the system or with a subject in question. Reason Reason Evaluate existing identity information to generate new beliefs, expressed in attributes, captured in statements. Apply Apply Use identity information in a system, typically to moderate interactions with known entities. Secure Protect Restrict the creation and flow of identity information to the right people at the right time.","title":"Identity Actions"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#for-technologists","text":"We assign identifiers to subjects . We collect raw data and correlate attributes to the subjects we track. We reason over raw data and attributes, to derive new attributes . We then apply this information to current and future interactions with subjects. We secure identity information to keep secrets and preserve privacy.","title":"For technologists"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#for-laypeople","text":"We give names to people . We collect observations and record statements relating those observations to people we know. We reason over these observations, statements, and beliefs to generate new beliefs. We then apply what we know and believe when dealing with those we recognize. We protect identity information to keep secrets and preserve privacy. This is the vocabulary of Functional Identity, a way to discuss identity in terms of functionality: how it works and what it does for us. This is the language of identity for the Rebooting Web of Trust.","title":"For laypeople"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#why","text":"Engineers, entrepreneurs, and financiers have asked \u201cWhy are we spending so much time with a definition of identity? Why not just build something and fix it if it is broken?\u201d The vital, simple reason is human dignity . When we build interconnected systems without a core understanding of identity, we risk inadvertently compromising human dignity. We risk accidentally building systems that deny self-expression, place individuals in harm\u2019s way, and unintentionally oppress those most in need of self-determination. There are times when the needs of security outweigh the need for human dignity. Fine. It\u2019s the job of our political systems\u2014local, national, and international\u2014to minimize abuse and to establish boundaries and practices that respect basic human rights. But when engineers unwittingly compromise the ability of individuals to self-express their identity, when we expose personal information in unexpected ways, when our systems deny basic services because of a flawed understanding of identity, these are avoidable tragedies . What might seem a minor technicality in one conversation could lead to the loss of privacy, liberty, or even life for an individual whose identity is unintentionally compromised . That\u2019s why it pays to understand identity, so the systems we build intentionally enable human dignity instead of accidentally destroy it.","title":"Why?"},{"location":"rwot5/topics-and-advance-readings/functional-identity-primer/#summary","text":"Functional Identity focuses on how identity works . At the Rebooting Web of Trust, we ground our work in the functional notion of identity and avoid the psychological, cultural, political, and philosophical. These notions are important, but they can also distract us from understanding the technical choices involved in building and using identity in today\u2019s networked world. This functional notion of identity began with a conversation at the Internet Identity Workshop in May of 2016, followed by conversations at ID2020 and the second Rebooting Web of Trust workshop that summer, resulting in the paper Identity Crisis . It continued in subsequent meetings in all three venues, and in two articles published by the People Centered Internet Speaking of Identity and How Identity Can Enable A People-Centered Internet . This primer represents a current take on that conversation, geared to help Rebooting Web of Trust participants communicate more clearly and collaborate more effectively. We encourage your feedback and look forward to continuing the conversation.","title":"Summary"},{"location":"rwot5/topics-and-advance-readings/hypercerts-blockcerts_revocation/","text":"Hypercerts: Blockcerts Revocation Improvements By Jo\u00e3o Santos, Instituto Superior T\u00e9cnico Contributors: Kim Hamilton Duffy, Learning Machine This is a proposal for an architecture of Blockcerts' certificate revocation using Ethereum smart-contracts and it follows the \"Hypercerts Thesis Proposal\" . Intro The Blockcerts iniciative allows for Open Badges compliant certificates to be issued in the Bitcoin blockchain. This has a lot of blockchain inherited bennefits. However, the current Blockcerts implementation does not offer good mechanisms to handle certificate revocation and certificate permanence. Hypercerts consists on a set of aditions to Blockcerts that aims at improving the latter's revocation functionality (allow certificates to be revoked by any pre-agreed on combination of parties, contemplate temporary revocation) and also add certificate permanence (ensure that from the point of its issuance, a certificate's link is always the same). This body of work extends the Hypercerts Thesis Proposal revocation mechanism section. The main goals of this mechanism are to allow revocation by an arbitrary combination of parties and to allow for batch issuing and revocation in an economical fashion. It is also desirable to have an architecture that can be easily implemented in other blockchains. The proposed architecture for multi-party revocation consists on using revocation proofs . For each certificate to be considered revoked it needs to have a set of revocation proofs issued to it. The number of revocation proofs each certificate requires to be considered revoked can vary from certificate to certificate, even if they are part of the same batch, and is defined upon batch issuance. An ethereum smart contract controls the issuance of revocation proofs and contains a an IPFS link to all the files relevant to the system\u2019s operation. Revocation Mechanism After the creation of an unsigned batch of certificates, three items are generated, as can be seen in the figure below. A file, accessible via IPFS, which can be embedded in the certificate, that entails the revocation policy for each certificate in that batch, this means that even in the same batch, different certificates can have different revocation policies, which is a very flexible feature. A file, accessible via IPFS, that maintains a list of the revocation proofs (explained in the next section) for each certificate in a batch. A smart-contract that will be responsible for managing the list of revocation proofs. Its job is to update the IPFS link when new revocation proofs are added to a certificate. After the creation of the smart-contract and revocation policy file, URIs to these two are embedded in the unsigned certificate. At this point, the certificate is signed and issued in the Bitcoin blockchain. Revocation Status, Rules and Proofs Revocation Status The revocation status of a certificate is not kept in any place. Instead it is calculated every time a client has interest in it. The way to verify the revocation state of a certificate is to analyse what are called revocation proofs which are documents that are digitally signed by parties authorized to revoke a certificate. Revocation Rules File The schema of the revocation rules file is still unclear, but will contain the following elements: The period in which revocation can occurr. This can be an open or closed interval. The identities of the parties who can issue revocation proofs. A simple solution for this would be to put the parties public keys as identifiers, but this raises a future proof issue, as it would require the revoking parties to maintain those sets of keys for entire lifetime of the certificate. A better approach would be to use distributed identifiers. The conditions upon which proofs operate under. This defines is all the listed proofs are required (an intersection, and ) or only a subset. Revocation Proofs File The schema of this file is still unclear, it can be a string signed by a revocation key, being that each party has its own revocation key. So, for instance, if we have a certificate that is revocable by a combination of Issuer and Receiver, the way to verify that such certificate had been revoked would be to search for revocation proofs of the Issuer and the Receiver. Structure of the smart-contract There is one smart-contract per batch and each is responsible for maintaining the state of revocation of a batch of certificates and/or of specific certificates. A naive approach to this problem could be to have a list with a number of elements equal to the number of certificates and per each element of the list, a revocation status and revocation proofs would be kept. The problem with this approach is that it does not scale in two situations: Large number of certificates: As the number of certificates grows the list would grow larger and larger. Large number of revocation parties: The Multiple Parties Revocation method allows for an arbitrary number of parties to be required to revoke a certificate. Given that for each party a revocation proof would be generated, items of the revocation list would become too big. The architecture we propose consists on having the smart contract controlling the issuance of revocation proofs and keeping the revocation proofs accessible via IPFS, being that the smart contract only maintains the link to the most recent list of certificate revocation proofs. Verification of Revocation Status In the Blockcerts ecosystem, there are several verification steps to validate a certificate, verifying its integrity, verifying the validity of the key it was signed and issued with and, finally, verifying that said certificate has not been revoked. Given that this document is only concerned with revocation, we are going to focus on the latter (the reader can refer to Blockcerts\u2019 documentation to learn about the other verification steps). To understand how revocation verification is carried on, let\u2019s assume a scenario where two certificates, Certificate A and Certificate B , require both the Issuer and the Receiver. The verifier starts by retrieving the Revocation Rules file and the ID of the Ethereum smart contract responsible for keeping track of the revocation proofs for that batch, both of which can be found directly on the certificate. From there the verifier checks the smart-contract for the link to the latest revocation proofs file and then it checks the file for the required proofs. At this point, one of two things can occur, either the revocation proofs are enough to consider the certificate revoked (figure on top) and the verifier considers the certificate not to be valid, or there are not enough proofs for the certificate to be considered revoked (figure on the bottom) and the verifier considers the certificate valid.","title":"Hypercerts: Blockcerts Revocation Improvements"},{"location":"rwot5/topics-and-advance-readings/hypercerts-blockcerts_revocation/#hypercerts-blockcerts-revocation-improvements","text":"By Jo\u00e3o Santos, Instituto Superior T\u00e9cnico Contributors: Kim Hamilton Duffy, Learning Machine This is a proposal for an architecture of Blockcerts' certificate revocation using Ethereum smart-contracts and it follows the \"Hypercerts Thesis Proposal\" .","title":"Hypercerts: Blockcerts Revocation Improvements"},{"location":"rwot5/topics-and-advance-readings/hypercerts-blockcerts_revocation/#intro","text":"The Blockcerts iniciative allows for Open Badges compliant certificates to be issued in the Bitcoin blockchain. This has a lot of blockchain inherited bennefits. However, the current Blockcerts implementation does not offer good mechanisms to handle certificate revocation and certificate permanence. Hypercerts consists on a set of aditions to Blockcerts that aims at improving the latter's revocation functionality (allow certificates to be revoked by any pre-agreed on combination of parties, contemplate temporary revocation) and also add certificate permanence (ensure that from the point of its issuance, a certificate's link is always the same). This body of work extends the Hypercerts Thesis Proposal revocation mechanism section. The main goals of this mechanism are to allow revocation by an arbitrary combination of parties and to allow for batch issuing and revocation in an economical fashion. It is also desirable to have an architecture that can be easily implemented in other blockchains. The proposed architecture for multi-party revocation consists on using revocation proofs . For each certificate to be considered revoked it needs to have a set of revocation proofs issued to it. The number of revocation proofs each certificate requires to be considered revoked can vary from certificate to certificate, even if they are part of the same batch, and is defined upon batch issuance. An ethereum smart contract controls the issuance of revocation proofs and contains a an IPFS link to all the files relevant to the system\u2019s operation.","title":"Intro"},{"location":"rwot5/topics-and-advance-readings/hypercerts-blockcerts_revocation/#revocation-mechanism","text":"After the creation of an unsigned batch of certificates, three items are generated, as can be seen in the figure below. A file, accessible via IPFS, which can be embedded in the certificate, that entails the revocation policy for each certificate in that batch, this means that even in the same batch, different certificates can have different revocation policies, which is a very flexible feature. A file, accessible via IPFS, that maintains a list of the revocation proofs (explained in the next section) for each certificate in a batch. A smart-contract that will be responsible for managing the list of revocation proofs. Its job is to update the IPFS link when new revocation proofs are added to a certificate. After the creation of the smart-contract and revocation policy file, URIs to these two are embedded in the unsigned certificate. At this point, the certificate is signed and issued in the Bitcoin blockchain.","title":"Revocation Mechanism"},{"location":"rwot5/topics-and-advance-readings/hypercerts-blockcerts_revocation/#revocation-status-rules-and-proofs","text":"Revocation Status The revocation status of a certificate is not kept in any place. Instead it is calculated every time a client has interest in it. The way to verify the revocation state of a certificate is to analyse what are called revocation proofs which are documents that are digitally signed by parties authorized to revoke a certificate. Revocation Rules File The schema of the revocation rules file is still unclear, but will contain the following elements: The period in which revocation can occurr. This can be an open or closed interval. The identities of the parties who can issue revocation proofs. A simple solution for this would be to put the parties public keys as identifiers, but this raises a future proof issue, as it would require the revoking parties to maintain those sets of keys for entire lifetime of the certificate. A better approach would be to use distributed identifiers. The conditions upon which proofs operate under. This defines is all the listed proofs are required (an intersection, and ) or only a subset. Revocation Proofs File The schema of this file is still unclear, it can be a string signed by a revocation key, being that each party has its own revocation key. So, for instance, if we have a certificate that is revocable by a combination of Issuer and Receiver, the way to verify that such certificate had been revoked would be to search for revocation proofs of the Issuer and the Receiver.","title":"Revocation Status, Rules and Proofs"},{"location":"rwot5/topics-and-advance-readings/hypercerts-blockcerts_revocation/#structure-of-the-smart-contract","text":"There is one smart-contract per batch and each is responsible for maintaining the state of revocation of a batch of certificates and/or of specific certificates. A naive approach to this problem could be to have a list with a number of elements equal to the number of certificates and per each element of the list, a revocation status and revocation proofs would be kept. The problem with this approach is that it does not scale in two situations: Large number of certificates: As the number of certificates grows the list would grow larger and larger. Large number of revocation parties: The Multiple Parties Revocation method allows for an arbitrary number of parties to be required to revoke a certificate. Given that for each party a revocation proof would be generated, items of the revocation list would become too big. The architecture we propose consists on having the smart contract controlling the issuance of revocation proofs and keeping the revocation proofs accessible via IPFS, being that the smart contract only maintains the link to the most recent list of certificate revocation proofs.","title":"Structure of the smart-contract"},{"location":"rwot5/topics-and-advance-readings/hypercerts-blockcerts_revocation/#verification-of-revocation-status","text":"In the Blockcerts ecosystem, there are several verification steps to validate a certificate, verifying its integrity, verifying the validity of the key it was signed and issued with and, finally, verifying that said certificate has not been revoked. Given that this document is only concerned with revocation, we are going to focus on the latter (the reader can refer to Blockcerts\u2019 documentation to learn about the other verification steps). To understand how revocation verification is carried on, let\u2019s assume a scenario where two certificates, Certificate A and Certificate B , require both the Issuer and the Receiver. The verifier starts by retrieving the Revocation Rules file and the ID of the Ethereum smart contract responsible for keeping track of the revocation proofs for that batch, both of which can be found directly on the certificate. From there the verifier checks the smart-contract for the link to the latest revocation proofs file and then it checks the file for the required proofs. At this point, one of two things can occur, either the revocation proofs are enough to consider the certificate revoked (figure on top) and the verifier considers the certificate not to be valid, or there are not enough proofs for the certificate to be considered revoked (figure on the bottom) and the verifier considers the certificate valid.","title":"Verification of Revocation Status"},{"location":"rwot5/topics-and-advance-readings/open-badges-as-verifiable-claims/","text":"Open Badges as Verifiable Claims By Kim Duffy and Nate Otto IM PROGRESS Open Badges are awarded to describe skills and accomplishments. The learner-centric approach allows individuals to collect and curate their badges through their lifelong learning. Open Badges is used by thousands of organizations, and has developed a rich ecosystem in the educational credentialing/awards space. The Open Badges contributors above (listed as authors) have also been involved in the Verifiable Claims community. Open Badges already incorporates Verifiable Claims as the Endorsement mode. We would like to explore ways to combine Open Badges and Verifiable Claims to increase interoperability, and to leverage the benefits of both systems. Draft of an Open Badge Verifiable Claim TODO: compare to current OB/Blockcerts example { \"id\": \"https://some.university.edu/credentials/9732\", \"type\": [ \"Credential\", \"OpenBadgeCredential\" ], \"issuer\": \"did:example:issuer_did\", \"issued\": \"2017-06-29T14:58:57.461422+00:00\", \"claim\": [ { \"id\": \"did:example:recipient_did\", \"earnedAssertion\": { \"type\": \"Assertion\", \"badge\": { \"type\": \"BadgeClass\", \"id\": \"https://some.university.edu/badges/6415\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"Lorem ipsum dolor sit amet, mei docendi concludaturque ad, cu nec partem graece. Est aperiam consetetur cu, expetenda moderatius neglegentur ei nam, suas dolor laudem eam an.\", \"criteria\": { \"narrative\": \"Nibh iriure ei nam, modo ridens neglegentur mel eu. At his cibo mucius.\" } } } } ], \"revocation\": { \"id\": \"http://example.gov/revocations/738\", \"type\": \"SimpleRevocationList2017\" }, \"signature\": {} } Discussion, tradeoffs TODO: describe tradeoffs above. Some choices I made above (to reduce redundancy) may not be acceptable to the Open Badges community. We'll have to find the right balance. Signature schemes, Blockcerts TODO","title":"Open Badges as Verifiable Claims"},{"location":"rwot5/topics-and-advance-readings/open-badges-as-verifiable-claims/#open-badges-as-verifiable-claims","text":"By Kim Duffy and Nate Otto IM PROGRESS Open Badges are awarded to describe skills and accomplishments. The learner-centric approach allows individuals to collect and curate their badges through their lifelong learning. Open Badges is used by thousands of organizations, and has developed a rich ecosystem in the educational credentialing/awards space. The Open Badges contributors above (listed as authors) have also been involved in the Verifiable Claims community. Open Badges already incorporates Verifiable Claims as the Endorsement mode. We would like to explore ways to combine Open Badges and Verifiable Claims to increase interoperability, and to leverage the benefits of both systems.","title":"Open Badges as Verifiable Claims"},{"location":"rwot5/topics-and-advance-readings/open-badges-as-verifiable-claims/#draft-of-an-open-badge-verifiable-claim","text":"TODO: compare to current OB/Blockcerts example { \"id\": \"https://some.university.edu/credentials/9732\", \"type\": [ \"Credential\", \"OpenBadgeCredential\" ], \"issuer\": \"did:example:issuer_did\", \"issued\": \"2017-06-29T14:58:57.461422+00:00\", \"claim\": [ { \"id\": \"did:example:recipient_did\", \"earnedAssertion\": { \"type\": \"Assertion\", \"badge\": { \"type\": \"BadgeClass\", \"id\": \"https://some.university.edu/badges/6415\", \"name\": \"Certificate of Accomplishment\", \"image\": \"data:image/png;base64,...\", \"description\": \"Lorem ipsum dolor sit amet, mei docendi concludaturque ad, cu nec partem graece. Est aperiam consetetur cu, expetenda moderatius neglegentur ei nam, suas dolor laudem eam an.\", \"criteria\": { \"narrative\": \"Nibh iriure ei nam, modo ridens neglegentur mel eu. At his cibo mucius.\" } } } } ], \"revocation\": { \"id\": \"http://example.gov/revocations/738\", \"type\": \"SimpleRevocationList2017\" }, \"signature\": {} }","title":"Draft of an Open Badge Verifiable Claim"},{"location":"rwot5/topics-and-advance-readings/open-badges-as-verifiable-claims/#discussion-tradeoffs","text":"TODO: describe tradeoffs above. Some choices I made above (to reduce redundancy) may not be acceptable to the Open Badges community. We'll have to find the right balance.","title":"Discussion, tradeoffs"},{"location":"rwot5/topics-and-advance-readings/open-badges-as-verifiable-claims/#signature-schemes-blockcerts","text":"TODO","title":"Signature schemes, Blockcerts"},{"location":"rwot5/topics-and-advance-readings/owned-vs-unowned-claims-and-ssi/","text":"Owned vs. Unowned Claims and Self-Sovereign Identity Natalie Smolenski Cultural Anthropologist and Business Development Learning Machine When Christopher Allen presented ten principles of self-sovereign identity for community discussion, he listed \u201cControl\u201d as the second principle. He described it as follows: Control. Users must control their identities. Subject to well-understood and secure algorithms that ensure the continued validity of an identity and its claims, the user is the ultimate authority on their identity. They should always be able to refer to it, update it, or even hide it. They must be able to choose celebrity or privacy as they prefer. This doesn\u2019t mean that a user controls all of the claims on their identity: other users may make claims about a user, but they should not be central to the identity itself. The distinction here between owned and unowned claims is important, and how they are both managed will be key to functional implementations of self-sovereign identity. The term \u201cself-sovereign\u201d has been catching on as of late, and has been used to describe instances of digital identity (government-controlled digital passports, the internet of things) which are clearly not self-sovereign. This confusion stems in part from difficulties individuals have understanding the difference between claim ownership vs. claim sharing and verification. Yet this distinction is one on which the future of individual privacy and autonomy depends. Claims made about individuals by other parties already proliferate and will continue to proliferate with blockchain technologies as well. Yet as the pseudonymous transactional structure of chains becomes increasingly tied to digital identities, the danger is that the blockchain could become the infrastructure for a regime of control unprecedented in human history. There is no reason, in principle, why individuals could not become objects in a giant supply chain, tracked like commodities as they move and transact around the world. Participating in this tracking could become the precondition for access to all forms of tokenized resources\u2014including government currencies, utilities, financial services, the ability to make purchases within certain jurisdictions, and basic rights and privileges. In short, what began as the best attempt to date to preclude the momentous expansion of surveillance capitalism could end up becoming its apotheosis if we do not address early on the individual ability to manage not only claims about themselves that they own but claims they do not own. Obviously the two cases entail different sets of considerations, and this is why I suggest beginning discussion about them now. Sources: Christopher Allen, \u201cThe Path to Self-Sovereign Identity.\u201d Life with Alacrity. April 26, 2016. http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html.","title":"Owned vs. Unowned Claims and Self-Sovereign Identity"},{"location":"rwot5/topics-and-advance-readings/owned-vs-unowned-claims-and-ssi/#owned-vs-unowned-claims-and-self-sovereign-identity","text":"Natalie Smolenski Cultural Anthropologist and Business Development Learning Machine When Christopher Allen presented ten principles of self-sovereign identity for community discussion, he listed \u201cControl\u201d as the second principle. He described it as follows: Control. Users must control their identities. Subject to well-understood and secure algorithms that ensure the continued validity of an identity and its claims, the user is the ultimate authority on their identity. They should always be able to refer to it, update it, or even hide it. They must be able to choose celebrity or privacy as they prefer. This doesn\u2019t mean that a user controls all of the claims on their identity: other users may make claims about a user, but they should not be central to the identity itself. The distinction here between owned and unowned claims is important, and how they are both managed will be key to functional implementations of self-sovereign identity. The term \u201cself-sovereign\u201d has been catching on as of late, and has been used to describe instances of digital identity (government-controlled digital passports, the internet of things) which are clearly not self-sovereign. This confusion stems in part from difficulties individuals have understanding the difference between claim ownership vs. claim sharing and verification. Yet this distinction is one on which the future of individual privacy and autonomy depends. Claims made about individuals by other parties already proliferate and will continue to proliferate with blockchain technologies as well. Yet as the pseudonymous transactional structure of chains becomes increasingly tied to digital identities, the danger is that the blockchain could become the infrastructure for a regime of control unprecedented in human history. There is no reason, in principle, why individuals could not become objects in a giant supply chain, tracked like commodities as they move and transact around the world. Participating in this tracking could become the precondition for access to all forms of tokenized resources\u2014including government currencies, utilities, financial services, the ability to make purchases within certain jurisdictions, and basic rights and privileges. In short, what began as the best attempt to date to preclude the momentous expansion of surveillance capitalism could end up becoming its apotheosis if we do not address early on the individual ability to manage not only claims about themselves that they own but claims they do not own. Obviously the two cases entail different sets of considerations, and this is why I suggest beginning discussion about them now. Sources: Christopher Allen, \u201cThe Path to Self-Sovereign Identity.\u201d Life with Alacrity. April 26, 2016. http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html.","title":"Owned vs. Unowned Claims and Self-Sovereign Identity"},{"location":"rwot5/topics-and-advance-readings/paper/","text":"A Verifiable Claims Primer by Manu Sporny, Digital Bazaar Introduction It is currently difficult to transmit credentials such as driver's licenses, proofs of age, education qualifications, and healthcare data, via the Internet in a way that is verifiable yet protects individual privacy. These credentials are composed of statements called verifiable claims . Starting in 2013, the W3C Credentials Community Group started to work in earnest on solutions in this space followed shortly thereafter by the Rebooting Web of Trust Community and W3C Verifiable Claims Working Group . These groups, composed of 150+ individuals and organizations, are currently focused on the creation, storage, transmission, and verification of digital credentials via the Internet. This document is a primer for those that want to learn about the Verifiable Claims initiative, the use cases and ecosystem, a basic overview of the technology, and how to get involved. Use Cases Verifiable Claims are useful when a person needs to prove that they are: above a certain age, capable of driving a particular motor vehicle, require a particular medication, trained and certified as an electrician, professionally licensed to practice medicine, and cleared to travel internationally. The use cases above are merely a high-level introduction to the problem space. Readers that would like to explore the use cases in more detail are urged to read the Verifiable Claims Working Groups' Use Cases document. Ecosystem The Verifiable Claims ecosystem is composed of four primary roles: The Issuer , who issues verifiable credentials about a specific Subject . The Holder stores credentials on behalf of a Subject . Holders are typically also the Subject of a credential. The Verifier requests a profile of the Subject . A profile contains a specific set of credentials. The verifier verifies that the credentials provided in the profile are fit-for-purpose. The Identifier Registry is a mechanism that is used to issue identifiers for Subjects . A visual depiction of the ecosystem above is shown below: Claims, Credentials, and Profiles The ecosystem roles exchange data that enables the realization of the previously mentioned use cases. The data that is exchanged differs based on the roles participating, but is fundamentally composed of Claims, Credentials, and Profiles. A claim is statement about a subject, expressed as a subject-property-value relationship: The data model for claims described above is powerful and can be used to express a large variety of statements. For example, whether or not someone is over the age of 21 may be expressed as follows: These claims may be merged together to express a graph of information about a particular subject. The example below extends the data model above by adding claims that state that Pat knows Sam and that Sam is a student. When an Issuer sends data to a Holder, it bundles a set of claims into a data structure called a credential and digitally signs the data structure: When a Verifier asks for data from a Holder, the Holder typically bundles a set of credentials into a data structure called a profile and digitally signs the data structure: The depictions above are a high-level introduction to the data model and gloss over specifics. Readers that would like to explore the data model in more depth are urged to read the Verifiable Claims Working Groups' Data Model Specification . Participating If you would like to participate in shaping this work, there are multiple ways to participate: If you want weekly updates and are NOT a W3C Member, or want to participate in the more experimental work, you should join the Credentials Community Group . The W3C Credentials Community Group holds weekly calls that are open to the public . If you want weekly updates and are a W3C Member, you should join the Verifiable Claims Working Group . The W3C Verifiable Claims Working Group holds weekly calls that are open to W3C Members . We hold bi-yearly face-to-face meetings in the spring and fall at Rebooting Web of Trust and once a year in the fall at the W3C Technical Plenary . The groups are very inclusive and welcome input and participation people from all disciplines and levels of expertise.","title":"A Verifiable Claims Primer"},{"location":"rwot5/topics-and-advance-readings/paper/#a-verifiable-claims-primer","text":"by Manu Sporny, Digital Bazaar","title":"A Verifiable Claims Primer"},{"location":"rwot5/topics-and-advance-readings/paper/#introduction","text":"It is currently difficult to transmit credentials such as driver's licenses, proofs of age, education qualifications, and healthcare data, via the Internet in a way that is verifiable yet protects individual privacy. These credentials are composed of statements called verifiable claims . Starting in 2013, the W3C Credentials Community Group started to work in earnest on solutions in this space followed shortly thereafter by the Rebooting Web of Trust Community and W3C Verifiable Claims Working Group . These groups, composed of 150+ individuals and organizations, are currently focused on the creation, storage, transmission, and verification of digital credentials via the Internet. This document is a primer for those that want to learn about the Verifiable Claims initiative, the use cases and ecosystem, a basic overview of the technology, and how to get involved.","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/paper/#use-cases","text":"Verifiable Claims are useful when a person needs to prove that they are: above a certain age, capable of driving a particular motor vehicle, require a particular medication, trained and certified as an electrician, professionally licensed to practice medicine, and cleared to travel internationally. The use cases above are merely a high-level introduction to the problem space. Readers that would like to explore the use cases in more detail are urged to read the Verifiable Claims Working Groups' Use Cases document.","title":"Use Cases"},{"location":"rwot5/topics-and-advance-readings/paper/#ecosystem","text":"The Verifiable Claims ecosystem is composed of four primary roles: The Issuer , who issues verifiable credentials about a specific Subject . The Holder stores credentials on behalf of a Subject . Holders are typically also the Subject of a credential. The Verifier requests a profile of the Subject . A profile contains a specific set of credentials. The verifier verifies that the credentials provided in the profile are fit-for-purpose. The Identifier Registry is a mechanism that is used to issue identifiers for Subjects . A visual depiction of the ecosystem above is shown below:","title":"Ecosystem"},{"location":"rwot5/topics-and-advance-readings/paper/#claims-credentials-and-profiles","text":"The ecosystem roles exchange data that enables the realization of the previously mentioned use cases. The data that is exchanged differs based on the roles participating, but is fundamentally composed of Claims, Credentials, and Profiles. A claim is statement about a subject, expressed as a subject-property-value relationship: The data model for claims described above is powerful and can be used to express a large variety of statements. For example, whether or not someone is over the age of 21 may be expressed as follows: These claims may be merged together to express a graph of information about a particular subject. The example below extends the data model above by adding claims that state that Pat knows Sam and that Sam is a student. When an Issuer sends data to a Holder, it bundles a set of claims into a data structure called a credential and digitally signs the data structure: When a Verifier asks for data from a Holder, the Holder typically bundles a set of credentials into a data structure called a profile and digitally signs the data structure: The depictions above are a high-level introduction to the data model and gloss over specifics. Readers that would like to explore the data model in more depth are urged to read the Verifiable Claims Working Groups' Data Model Specification .","title":"Claims, Credentials, and Profiles"},{"location":"rwot5/topics-and-advance-readings/paper/#participating","text":"If you would like to participate in shaping this work, there are multiple ways to participate: If you want weekly updates and are NOT a W3C Member, or want to participate in the more experimental work, you should join the Credentials Community Group . The W3C Credentials Community Group holds weekly calls that are open to the public . If you want weekly updates and are a W3C Member, you should join the Verifiable Claims Working Group . The W3C Verifiable Claims Working Group holds weekly calls that are open to W3C Members . We hold bi-yearly face-to-face meetings in the spring and fall at Rebooting Web of Trust and once a year in the fall at the W3C Technical Plenary . The groups are very inclusive and welcome input and participation people from all disciplines and levels of expertise.","title":"Participating"},{"location":"rwot5/topics-and-advance-readings/self-sovereign-identity-primer/","text":"A Primer on Self-Sovereign Identity By Christopher Allen & Shannon Appelcline Joe Andrieu's Primer on Functional Identity offers a descriptive definition of identity. Quite simply, Andrieu says, \"Identity is how we keep track of people and things and, in turn, how they keep track of us.\" It's an astute description that cuts through the confusion of what an identity database should or shouldn't encompass by instead suggesting that models need only include what's required for persistent identification. However, the content design of an identity system is only half the battle. There are also questions of jurisdiction, management, and oversight. In other words, once you have an identity system, who runs it, and who are they beholden to? That's where self-sovereign identity enters the picture, as an orthogonal way to look at the question of identity systems. A Self-Sovereign Definition Self-sovereign identity is centered on a person, free from dependence on any corporation, organization, or nation-state. This core definition of self-sovereign identity is a simple one, affirming that identity belongs to an individual person and cannot be taken from them. However, that flies in the face of how identity systems are administered in the modern world. The constraints that currently exist don't match the natural definition of identity, which says that we are ourselves. They don't match the functional definition of identity, which says we are who we're known to be. They instead make us identity serfs, bound to the custodians of our crucial identity records. With social security cards, passports, and driver licenses, the US and state governments control our fundamental rights of existence. Other governments act similarly. In recent years, there has been some hope of emancipation. UN Sustainable Development Goal 16.9 requires that \"By 2030, [we] provide legal identity for all, including birth registration\". It's not the first time the UN has spoken out for human identity rights. The question is: can we leverage that support into the creation of truly self-sovereign identities? The danger is that we instead repeat past mistakes; that's unfortunately what we've seen so far on the next frontier of identity, the internet. There, internet corporations like Facebook constrain our ability to project our identity online; they can terminate our ability to interact with our identity circles at a moment's notice and may even be able to deauthenticate our access to linked remote sites. Like nation-states, these internet corporations are dangerously centralized, creating a single point of failure for digital identities. And so the future echoes the past, with corporations taking over the centralized roles of nation-states, and the losers are still the people themselves. Even with an understanding of the problems of state-sovereign identity, even with the support of the UN, we have not yet had the ability to change. It demonstrates why we need to radically rethink our concepts of identity, to redefine them, so that we don't end up right back where we started. And we need to do so quickly, because the internet's ideas of identity are coalescing now . Ten Principles The Path to Self-Sovereign Identity was first detailed in April, 2016. Since, the concept has received widespread adoption. However, it's at an early stage of development. Though we know the fundamentals of what a self-sovereign identity aspires to be, we're still discussing the specifics of how to get there. The original article on self-sovereign identity included a list of ten self-sovereign principles. They were intended as a starting point for discussion. Some may indeed by critical, but others may be extraneous complications that should be pruned away. In brief, these ten potential principles are: 1. **Existence.** Users must have an independent existence. 2. **Control.** Users must control their identities. 3. **Access.** Users must have access to their own data. 4. **Transparency.** Systems and algorithms must be transparent. 5. **Persistence.** Identities must be long-lived. 6. **Portability.** Information and services about identity must be transportable. 7. **Interoperability.** Identities should be as widely usable as possible. 8. **Consent.** Users must agree to the use of their identity. 9. **Minimalization.** Disclosure of claims must be minimized. 10. **Protection.** The rights of users must be protected. Further discussion of these principles is available in the original article . More recently, Natalie Smolenski wrote about the specific issues of \"control\" in Owned vs. Unowned Claims and Self-Sovereign Identity . Self-Sovereign Identity Systems These definitions and principles describe the philosophy of self-sovereign identity, and they also lay out the technical requirements needed to build such systems. The first three principles are the most important bases of a technical implementation: when we say that users must have existence, and that they must have access and control over their identity, we imagine an identity system where the user is king. He is his identity's root and its executor. Put that together with the sixth principle, on portability, and you have an identity system that creates an identity as a discrete unit, with specific access, and with the ability to package it up and bring it to a different identity service. Thus, identity is no longer locked to an organization, a corporation, or a nation-state; instead it resides with the user, who can temporarily home it where he sees fit. Any self-sovereign identity needs to begin with a trusted root, and that's the focus of the first major implementation of self-sovereign identity: the decentralized identity, or DID, which is now a W3C draft . The DID Primer contains extensive information on DIDs and other related specifications; in short, DIDs provide a methodology for proving control over a key, which can be used to prove identity. Blockchains are the basis of many DID methodologies. Self-sovereign identity also require that these free-floating entities be able to make claims about each other; it's how people \"keep track of each other\", the basis of functional identity. This is the heart of another major project related to self-sovereign identity, the Verifiable Claims initiative, which includes a W3C working group . Manu Sporny has written about it in A Primer on Verifiable Claims . Doubtless, there will be other technical solutions for self-sovereign identity in the future, as the philosophy behind these identities is still evolving.","title":"A Primer on Self-Sovereign Identity"},{"location":"rwot5/topics-and-advance-readings/self-sovereign-identity-primer/#a-primer-on-self-sovereign-identity","text":"","title":"A Primer on Self-Sovereign Identity"},{"location":"rwot5/topics-and-advance-readings/self-sovereign-identity-primer/#by-christopher-allen-shannon-appelcline","text":"Joe Andrieu's Primer on Functional Identity offers a descriptive definition of identity. Quite simply, Andrieu says, \"Identity is how we keep track of people and things and, in turn, how they keep track of us.\" It's an astute description that cuts through the confusion of what an identity database should or shouldn't encompass by instead suggesting that models need only include what's required for persistent identification. However, the content design of an identity system is only half the battle. There are also questions of jurisdiction, management, and oversight. In other words, once you have an identity system, who runs it, and who are they beholden to? That's where self-sovereign identity enters the picture, as an orthogonal way to look at the question of identity systems.","title":"By Christopher Allen &amp; Shannon Appelcline"},{"location":"rwot5/topics-and-advance-readings/self-sovereign-identity-primer/#a-self-sovereign-definition","text":"Self-sovereign identity is centered on a person, free from dependence on any corporation, organization, or nation-state. This core definition of self-sovereign identity is a simple one, affirming that identity belongs to an individual person and cannot be taken from them. However, that flies in the face of how identity systems are administered in the modern world. The constraints that currently exist don't match the natural definition of identity, which says that we are ourselves. They don't match the functional definition of identity, which says we are who we're known to be. They instead make us identity serfs, bound to the custodians of our crucial identity records. With social security cards, passports, and driver licenses, the US and state governments control our fundamental rights of existence. Other governments act similarly. In recent years, there has been some hope of emancipation. UN Sustainable Development Goal 16.9 requires that \"By 2030, [we] provide legal identity for all, including birth registration\". It's not the first time the UN has spoken out for human identity rights. The question is: can we leverage that support into the creation of truly self-sovereign identities? The danger is that we instead repeat past mistakes; that's unfortunately what we've seen so far on the next frontier of identity, the internet. There, internet corporations like Facebook constrain our ability to project our identity online; they can terminate our ability to interact with our identity circles at a moment's notice and may even be able to deauthenticate our access to linked remote sites. Like nation-states, these internet corporations are dangerously centralized, creating a single point of failure for digital identities. And so the future echoes the past, with corporations taking over the centralized roles of nation-states, and the losers are still the people themselves. Even with an understanding of the problems of state-sovereign identity, even with the support of the UN, we have not yet had the ability to change. It demonstrates why we need to radically rethink our concepts of identity, to redefine them, so that we don't end up right back where we started. And we need to do so quickly, because the internet's ideas of identity are coalescing now .","title":"A Self-Sovereign Definition"},{"location":"rwot5/topics-and-advance-readings/self-sovereign-identity-primer/#ten-principles","text":"The Path to Self-Sovereign Identity was first detailed in April, 2016. Since, the concept has received widespread adoption. However, it's at an early stage of development. Though we know the fundamentals of what a self-sovereign identity aspires to be, we're still discussing the specifics of how to get there. The original article on self-sovereign identity included a list of ten self-sovereign principles. They were intended as a starting point for discussion. Some may indeed by critical, but others may be extraneous complications that should be pruned away. In brief, these ten potential principles are: 1. **Existence.** Users must have an independent existence. 2. **Control.** Users must control their identities. 3. **Access.** Users must have access to their own data. 4. **Transparency.** Systems and algorithms must be transparent. 5. **Persistence.** Identities must be long-lived. 6. **Portability.** Information and services about identity must be transportable. 7. **Interoperability.** Identities should be as widely usable as possible. 8. **Consent.** Users must agree to the use of their identity. 9. **Minimalization.** Disclosure of claims must be minimized. 10. **Protection.** The rights of users must be protected. Further discussion of these principles is available in the original article . More recently, Natalie Smolenski wrote about the specific issues of \"control\" in Owned vs. Unowned Claims and Self-Sovereign Identity .","title":"Ten Principles"},{"location":"rwot5/topics-and-advance-readings/self-sovereign-identity-primer/#self-sovereign-identity-systems","text":"These definitions and principles describe the philosophy of self-sovereign identity, and they also lay out the technical requirements needed to build such systems. The first three principles are the most important bases of a technical implementation: when we say that users must have existence, and that they must have access and control over their identity, we imagine an identity system where the user is king. He is his identity's root and its executor. Put that together with the sixth principle, on portability, and you have an identity system that creates an identity as a discrete unit, with specific access, and with the ability to package it up and bring it to a different identity service. Thus, identity is no longer locked to an organization, a corporation, or a nation-state; instead it resides with the user, who can temporarily home it where he sees fit. Any self-sovereign identity needs to begin with a trusted root, and that's the focus of the first major implementation of self-sovereign identity: the decentralized identity, or DID, which is now a W3C draft . The DID Primer contains extensive information on DIDs and other related specifications; in short, DIDs provide a methodology for proving control over a key, which can be used to prove identity. Blockchains are the basis of many DID methodologies. Self-sovereign identity also require that these free-floating entities be able to make claims about each other; it's how people \"keep track of each other\", the basis of functional identity. This is the heart of another major project related to self-sovereign identity, the Verifiable Claims initiative, which includes a W3C working group . Manu Sporny has written about it in A Primer on Verifiable Claims . Doubtless, there will be other technical solutions for self-sovereign identity in the future, as the philosophy behind these identities is still evolving.","title":"Self-Sovereign Identity Systems"},{"location":"rwot5/topics-and-advance-readings/trust-objects-proposal/","text":"Trust Objects: Enabling advanced reputation services on the Web of Trust Presented by Moses Ma and Dr. Rutu Mulkar-Mehta, FutureLab Submitted to the 5th Rebooting the Web of Trust Technical Workshop as a discussion paper Boston, October 3-5, 2017 Keywords: reputation, trust, verified claims, collaboration, innovation, framework, blockchain, decentralized, self-sovereign PROPOSAL We propose to facilitate the collaborative drafting of a technical paper that describes the principles and key design considerations for an online framework to manage fully functional reputation services within a web of trust. The approach uses both transactional and non-transactional reputation data. Non-transactional reputation data includes both trust primitives such as verified claims as well as indeterminate trust assertions. We will also describe the potential use of incentive tokens for incremental optimization of the eco-system, in a manner that is especially suited for decentralized, self-sovereign eco-systems. Finally, we propose to discuss the complexities of online disagreements and how to resolve and adjudicate them in a pareto-optimal manner. We base much of our work on key design considerations for decentralized reputation, developed by C. Allen and by A.C. de Crespigny et al (see references), at the Spring 2017 RWOT design conference in Paris. Reputation is social concept that is an essential component of social and business networks, because it serves as an optimizing influence on such systems. However, while there have been numerous analyses of how reputation may be computed and managed, there has to date been no systematic approach for implementing reputation systems, nor strategies for self-optimizing reputation management, proposed for decentralized networks. Our goal is to develop an inter-disciplinary, game-theoretic model for computational trust and reputation based on psychology and micro-economics. The proposed approach utilizes both transactional and non-transactional trust data. Transactional data includes a record of failed vs successful transactions, such as the history of successful vs unsuccessful transactions at eBay. Non-transactional data includes trust primitives such as verified claims, as well as indeterminate trust assertions. This paper also shows that it is possible use incentive tokens to drive incremental optimization of the eco-system, in a manner that is especially suited for decentralized, self-sovereign eco-systems. Finally, we propose to discuss the complexities of online disagreements and propose key design considerations for systems that could more effectively resolve and adjudicate disputes, in a pareto-optimal manner. We believe there are several important principles that apply to non-transactional reputation systems, which could be used to help govern their design and operation within enterprises and organizations. These are: Reputation is complex. Trust and reputation are transitive. Reputation is a convolution of trust primitives. Reputation is a narrative, a dynamic social process, not a static credit score. Reputation is a currency. Reputation is all about people. Using these principles, we offer a model for computational reputation that is functional and useful, adaptive and self-optimizing. In our model, reputation is defined to be a convolution of transactional and non-transactional data, with associated weighting based on the trustability of the rater. We will also discuss the requirements for continuous reputation tracking, adaptive weighting with artificial intelligence based adaptive learning, neural networks for behavioral pattern detector, and automatic normalization of voting weights. Finally, the most important goal for this working draft is to map the emergent model to the proposed DID and Verified Claims standards. Our proposal is simply to add a field to the basic verified claim system, in the form of a \u201cprotocol cookie\u201d. Cookies were designed to be a reliable mechanism for websites to remember stateful information or to record the user's browsing activity. Therefore, this field would enable the claim to remember stateful information \u2013 such as a URI for the claim offerer\u2019s reputation rating, or to record the history of entities that have accessed the claim, or to manage visibility settings for the claim, so that disclosure of the claim could be selectively permissioned. The most valuable function of the field would be to provide a URI to an ontology or classification system for the claim to provide metadata about the size or scope of the claim. The paper will include a design philosophy for the implementation of reputation engines that are \u201cself-optimizing\u201d using \u201coptimization tokens\u201d that promote more effective and truthful rating and reporting by people. These tokens can also be earned by artificial intelligence systems that act in a manner similar to miners, but providing optimization services. For example, neural network based fraud detection systems could be developed to look for \u201cratings extortionists\u201d, who threaten negative reviews if not provided a discount. Next Steps We would like to collaborate with the participants of the Rebooting Web of Trust Workshop to refine the concepts and use cases. Our goal is to produce both an improved, more easily extensible standard, as well as a demo that demonstrates a compelling use case directly after the workshop. References http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2017/blob/master/topics-and-advance-readings/ProjectVouch_Peer-attestation-and-reputation-based-identity.md","title":"Trust Objects: Enabling advanced reputation services on the Web of Trust"},{"location":"rwot5/topics-and-advance-readings/trust-objects-proposal/#trust-objects-enabling-advanced-reputation-services-on-the-web-of-trust","text":"","title":"Trust Objects: Enabling advanced reputation services on the Web of Trust"},{"location":"rwot5/topics-and-advance-readings/trust-objects-proposal/#presented-by-moses-ma-and-dr-rutu-mulkar-mehta-futurelab","text":"","title":"Presented by Moses Ma and Dr. Rutu Mulkar-Mehta, FutureLab"},{"location":"rwot5/topics-and-advance-readings/trust-objects-proposal/#submitted-to-the-5th-rebooting-the-web-of-trust-technical-workshop-as-a-discussion-paper","text":"","title":"Submitted to the 5th Rebooting the Web of Trust Technical Workshop as a discussion paper"},{"location":"rwot5/topics-and-advance-readings/trust-objects-proposal/#boston-october-3-5-2017","text":"Keywords: reputation, trust, verified claims, collaboration, innovation, framework, blockchain, decentralized, self-sovereign","title":"Boston, October 3-5, 2017"},{"location":"rwot5/topics-and-advance-readings/trust-objects-proposal/#proposal","text":"We propose to facilitate the collaborative drafting of a technical paper that describes the principles and key design considerations for an online framework to manage fully functional reputation services within a web of trust. The approach uses both transactional and non-transactional reputation data. Non-transactional reputation data includes both trust primitives such as verified claims as well as indeterminate trust assertions. We will also describe the potential use of incentive tokens for incremental optimization of the eco-system, in a manner that is especially suited for decentralized, self-sovereign eco-systems. Finally, we propose to discuss the complexities of online disagreements and how to resolve and adjudicate them in a pareto-optimal manner. We base much of our work on key design considerations for decentralized reputation, developed by C. Allen and by A.C. de Crespigny et al (see references), at the Spring 2017 RWOT design conference in Paris. Reputation is social concept that is an essential component of social and business networks, because it serves as an optimizing influence on such systems. However, while there have been numerous analyses of how reputation may be computed and managed, there has to date been no systematic approach for implementing reputation systems, nor strategies for self-optimizing reputation management, proposed for decentralized networks. Our goal is to develop an inter-disciplinary, game-theoretic model for computational trust and reputation based on psychology and micro-economics. The proposed approach utilizes both transactional and non-transactional trust data. Transactional data includes a record of failed vs successful transactions, such as the history of successful vs unsuccessful transactions at eBay. Non-transactional data includes trust primitives such as verified claims, as well as indeterminate trust assertions. This paper also shows that it is possible use incentive tokens to drive incremental optimization of the eco-system, in a manner that is especially suited for decentralized, self-sovereign eco-systems. Finally, we propose to discuss the complexities of online disagreements and propose key design considerations for systems that could more effectively resolve and adjudicate disputes, in a pareto-optimal manner. We believe there are several important principles that apply to non-transactional reputation systems, which could be used to help govern their design and operation within enterprises and organizations. These are: Reputation is complex. Trust and reputation are transitive. Reputation is a convolution of trust primitives. Reputation is a narrative, a dynamic social process, not a static credit score. Reputation is a currency. Reputation is all about people. Using these principles, we offer a model for computational reputation that is functional and useful, adaptive and self-optimizing. In our model, reputation is defined to be a convolution of transactional and non-transactional data, with associated weighting based on the trustability of the rater. We will also discuss the requirements for continuous reputation tracking, adaptive weighting with artificial intelligence based adaptive learning, neural networks for behavioral pattern detector, and automatic normalization of voting weights. Finally, the most important goal for this working draft is to map the emergent model to the proposed DID and Verified Claims standards. Our proposal is simply to add a field to the basic verified claim system, in the form of a \u201cprotocol cookie\u201d. Cookies were designed to be a reliable mechanism for websites to remember stateful information or to record the user's browsing activity. Therefore, this field would enable the claim to remember stateful information \u2013 such as a URI for the claim offerer\u2019s reputation rating, or to record the history of entities that have accessed the claim, or to manage visibility settings for the claim, so that disclosure of the claim could be selectively permissioned. The most valuable function of the field would be to provide a URI to an ontology or classification system for the claim to provide metadata about the size or scope of the claim. The paper will include a design philosophy for the implementation of reputation engines that are \u201cself-optimizing\u201d using \u201coptimization tokens\u201d that promote more effective and truthful rating and reporting by people. These tokens can also be earned by artificial intelligence systems that act in a manner similar to miners, but providing optimization services. For example, neural network based fraud detection systems could be developed to look for \u201cratings extortionists\u201d, who threaten negative reviews if not provided a discount.","title":"PROPOSAL"},{"location":"rwot5/topics-and-advance-readings/trust-objects-proposal/#next-steps","text":"We would like to collaborate with the participants of the Rebooting Web of Trust Workshop to refine the concepts and use cases. Our goal is to produce both an improved, more easily extensible standard, as well as a demo that demonstrates a compelling use case directly after the workshop.","title":"Next Steps"},{"location":"rwot5/topics-and-advance-readings/trust-objects-proposal/#references","text":"http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-spring2017/blob/master/topics-and-advance-readings/ProjectVouch_Peer-attestation-and-reputation-based-identity.md","title":"References"},{"location":"rwot5/topics-and-advance-readings/veres-one-did-method/","text":"Veres One DID Method Specification by Manu Sporny, Dave Longley, and Matt Collier, Digital Bazaar Introduction The Veres One Ledger is a permissionless public ledger designed specifically for the creation and management of decentralized identifiers (DIDs). Veres One DIDs are self-sovereign identifiers that may be used by people, organizations, and digital devices to establish an identifier that is under their control. Veres One DIDs are useful in ecosystems where one needs to issue, store, and use Verifiable Claims. The Veres One DID Method specification has been implemented and a testnet is available here: Veres One Testnet There is a video demo of a DID being registered and retrieved from the Veres One Testnet here: Veres One Testnet DID Registration Demo The current draft of the Veres One DID Method specification is available here: Veres One DID Method Specification An Example Veres One DID Document What follows is a complete DID Document registration event that is sent to the ledger including line by line documentation regarding the DID Document registration event. { // The Web Ledger context is a part of the Web Ledger specification // and provides the core framing for the DID Registration event. \"@context\": \"https://w3id.org/webledger/v1\", \"type\": \"WebLedgerEvent\", // The type of ledger operation being performed is a \"Create\" // which creates objects (the DID Document) in the state machine \"operation\": \"Create\", // input can be an array of DID Documents in case batch processing // is desired \"input\": [{ // The Veres One context includes the https://w3id.org/did/v1 // context and adds Veres One specific vocabulary terms \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"authorizationCapability\": [{ // This authorization capability notes that the DID is capable // of updating the entire DID Document \"permission\": \"UpdateDidDocument\", // This is a truly self-sovereign DID in that only the entity // associated with the DID update the DID Document \"entity\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", // The permitted proof types states that a capability to // update the DID Document will be granted if the submitting // entity provides both an Equihash proof of work (that is // tuned to the Veres One ledger) and a signature on the // event. The proof of work provides some protection against // a DDoS of the ledger while the signature ensures that // the proper entity is requesting the change. \"permittedProofType\": [{ \"proofType\": \"LinkedDataSignature2015\" }, { \"proofType\": \"EquihashProof2017\", \"equihashParameterAlgorithm\": \"VeresOne2017\" }] }], // There is only one RSA signature-based authentication credential // that is registered at the time of DID creation \"authenticationCredential\": [{ \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmbDqPu6IKHiiIQ4d0AQ6\\r\\n PBduDhUUVqyQirvxqsdcNdKgZ2L8whBml/nTyuB4cd+hHrsfMDiHiT5kX2pbZ7Yy\\r\\n 2ctWkGw8e0J94CbwVh2H15gBQBUCjLiGvVIHO2pni693qmre+3Ya2NJ8gGwPLJ7h\\r\\n TLca2b2dX0y16qu0MT0osUGGEoPsdg6ibD2pxnADS3GNPObHT12GrAuxjYFMHecF\\r\\n A4hLZ8U+MIcVmHZuokqqbcyJyjOV+kmhFNeTKFP5P5U8HA3Y42/rE1UJp/wyy7Lc\\r\\n ZAvq0t75ddXKyvYh5dkzxxeeELNKNWVxJ2yvgAr0SatLEPzxJoeYdCyU5N5E22Fj\\r\\n jQIDAQAB\\r\\n-----END PUBLIC KEY-----\\r\\n\" }] }], // The ledger event itself is secured using an Equihash proof and is // also digitally signed with the RSA private key listed above. This // multi-signature is what is used to grant the capability to create the // DID Document. \"signature\": [{ \"type\": \"EquihashProof2017\", \"equihashParameterN\": 64, \"equihashParameterK\": 3, \"nonce\": \"AQAAAA==\", \"proofValue\": \"AAAaPwABxrIAAFOKAAGo4QAAVW0AAN7cAACXcgABjEI=\" }, { \"type\": \"LinkedDataSignature2015\", \"created\": \"2017-09-30T02:54:31Z\", \"creator\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"signatureValue\": \"SNMbsPqLnB+hJFhXzS6hcpZnm9cGvSZZg7o26UYnyGYTvKder/S+Xk hNhXisS5385Ljlf5CXTQT5j6qYZtP8ut1Benaae8TMH17txP0CfzHbUMJFnHA1+Nru+e/Pw yPwuQ+VZYlXOB7g/tKVVZsxAYTKCAOJvJMIE+nlHjpB+RsKs9z4ZzVtddntqqAcvbZxV/o7 azBFDizeJu/gHVVMncCJ00SRoOzCOZUABRJV/k68bNSAfpELkrdWx8/xvMIF8r+LWhwdKCS iOw4DjSwIK40yD5rOvQn/GlC+unyB8zFe60jCToz/UOJNZBiIYwo+Pwwx28Wqd4Jkb3IeDr /L2Q==\" }] } Collaboration We would like to collaborate with the other DID Method implementers at Rebooting the Web of Trust to align both the core DID specification as well as other DID Method specifications. We are also seeking to partner with organizations that would be interested in integrating Veres One DIDs into their platform to field test the capabilities of the specifications and technologies. We are also interested in developing common technologies that can not only access and update the Veres One DID ledger, but other ledgers as well.","title":"Veres One DID Method Specification"},{"location":"rwot5/topics-and-advance-readings/veres-one-did-method/#veres-one-did-method-specification","text":"by Manu Sporny, Dave Longley, and Matt Collier, Digital Bazaar","title":"Veres One DID Method Specification"},{"location":"rwot5/topics-and-advance-readings/veres-one-did-method/#introduction","text":"The Veres One Ledger is a permissionless public ledger designed specifically for the creation and management of decentralized identifiers (DIDs). Veres One DIDs are self-sovereign identifiers that may be used by people, organizations, and digital devices to establish an identifier that is under their control. Veres One DIDs are useful in ecosystems where one needs to issue, store, and use Verifiable Claims. The Veres One DID Method specification has been implemented and a testnet is available here: Veres One Testnet There is a video demo of a DID being registered and retrieved from the Veres One Testnet here: Veres One Testnet DID Registration Demo The current draft of the Veres One DID Method specification is available here: Veres One DID Method Specification","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/veres-one-did-method/#an-example-veres-one-did-document","text":"What follows is a complete DID Document registration event that is sent to the ledger including line by line documentation regarding the DID Document registration event. { // The Web Ledger context is a part of the Web Ledger specification // and provides the core framing for the DID Registration event. \"@context\": \"https://w3id.org/webledger/v1\", \"type\": \"WebLedgerEvent\", // The type of ledger operation being performed is a \"Create\" // which creates objects (the DID Document) in the state machine \"operation\": \"Create\", // input can be an array of DID Documents in case batch processing // is desired \"input\": [{ // The Veres One context includes the https://w3id.org/did/v1 // context and adds Veres One specific vocabulary terms \"@context\": \"https://w3id.org/veres-one/v1\", \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"authorizationCapability\": [{ // This authorization capability notes that the DID is capable // of updating the entire DID Document \"permission\": \"UpdateDidDocument\", // This is a truly self-sovereign DID in that only the entity // associated with the DID update the DID Document \"entity\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", // The permitted proof types states that a capability to // update the DID Document will be granted if the submitting // entity provides both an Equihash proof of work (that is // tuned to the Veres One ledger) and a signature on the // event. The proof of work provides some protection against // a DDoS of the ledger while the signature ensures that // the proper entity is requesting the change. \"permittedProofType\": [{ \"proofType\": \"LinkedDataSignature2015\" }, { \"proofType\": \"EquihashProof2017\", \"equihashParameterAlgorithm\": \"VeresOne2017\" }] }], // There is only one RSA signature-based authentication credential // that is registered at the time of DID creation \"authenticationCredential\": [{ \"id\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"type\": \"CryptographicKey\", \"owner\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY-----\\r\\n MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAmbDqPu6IKHiiIQ4d0AQ6\\r\\n PBduDhUUVqyQirvxqsdcNdKgZ2L8whBml/nTyuB4cd+hHrsfMDiHiT5kX2pbZ7Yy\\r\\n 2ctWkGw8e0J94CbwVh2H15gBQBUCjLiGvVIHO2pni693qmre+3Ya2NJ8gGwPLJ7h\\r\\n TLca2b2dX0y16qu0MT0osUGGEoPsdg6ibD2pxnADS3GNPObHT12GrAuxjYFMHecF\\r\\n A4hLZ8U+MIcVmHZuokqqbcyJyjOV+kmhFNeTKFP5P5U8HA3Y42/rE1UJp/wyy7Lc\\r\\n ZAvq0t75ddXKyvYh5dkzxxeeELNKNWVxJ2yvgAr0SatLEPzxJoeYdCyU5N5E22Fj\\r\\n jQIDAQAB\\r\\n-----END PUBLIC KEY-----\\r\\n\" }] }], // The ledger event itself is secured using an Equihash proof and is // also digitally signed with the RSA private key listed above. This // multi-signature is what is used to grant the capability to create the // DID Document. \"signature\": [{ \"type\": \"EquihashProof2017\", \"equihashParameterN\": 64, \"equihashParameterK\": 3, \"nonce\": \"AQAAAA==\", \"proofValue\": \"AAAaPwABxrIAAFOKAAGo4QAAVW0AAN7cAACXcgABjEI=\" }, { \"type\": \"LinkedDataSignature2015\", \"created\": \"2017-09-30T02:54:31Z\", \"creator\": \"did:v1:770f2d84-5e62-4caa-af95-111a3205bc84/keys/1\", \"signatureValue\": \"SNMbsPqLnB+hJFhXzS6hcpZnm9cGvSZZg7o26UYnyGYTvKder/S+Xk hNhXisS5385Ljlf5CXTQT5j6qYZtP8ut1Benaae8TMH17txP0CfzHbUMJFnHA1+Nru+e/Pw yPwuQ+VZYlXOB7g/tKVVZsxAYTKCAOJvJMIE+nlHjpB+RsKs9z4ZzVtddntqqAcvbZxV/o7 azBFDizeJu/gHVVMncCJ00SRoOzCOZUABRJV/k68bNSAfpELkrdWx8/xvMIF8r+LWhwdKCS iOw4DjSwIK40yD5rOvQn/GlC+unyB8zFe60jCToz/UOJNZBiIYwo+Pwwx28Wqd4Jkb3IeDr /L2Q==\" }] }","title":"An Example Veres One DID Document"},{"location":"rwot5/topics-and-advance-readings/veres-one-did-method/#collaboration","text":"We would like to collaborate with the other DID Method implementers at Rebooting the Web of Trust to align both the core DID specification as well as other DID Method specifications. We are also seeking to partner with organizations that would be interested in integrating Veres One DIDs into their platform to field test the capabilities of the specifications and technologies. We are also interested in developing common technologies that can not only access and update the Veres One DID ledger, but other ledgers as well.","title":"Collaboration"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/","text":"Verifiable Claims of Impact Public Working Draft dated 21 September 2017 Cedric Franz (Trustlab), Dr Shaun Conway (ixo Foundation) Impact Claims are a new use-case for Verifiable Claims and the DID specification. We propose this could become a standard that will transform how information gets collected, evaluated, valued and exchanged to optimise Sustainable Development impacts. The context for this is the UN\u2019s 17 Global Goals to end poverty, protect the planet, and ensure prosperity for all, by 2030. To create a world that counts, the UN Secretary-General has called for a data revolution for sustainable development. The prototype Impact Claims specification is a core part of the ixo Protocol that provides a decentralised mechanism to generate verified impact data as a fungible asset that can be traded for funding and other forms of value-exchange. This protocol increases the accountability and transparency of impact funding and service provision, whilst also generating a dynamic data commons for sustainable development. See the ixo White Paper for more information. The ixo protocol follows the archetypical process for transforming raw data into valuable, classified data assets, through an evaluation process, as depicted in the figure. It is interesting to think that this generic pattern applies to virtually any type of information. For instance, when music files are evaluated or curated by listeners applying star-ratings. The use-case we describe here could be extended to any evaluation process where the identities of service agents, evaluation agents, beneficiaries and purchasers is of some consequence. We are interested to explore the generalisability of this model, which could be thought of as Verifiable Claims about facts that are extrinsic to the identities of the subject/s or participants in an event or process. This contrasts with the established use of Verifiable Claims about facts that are intrinsic to the identity of the subject/s of the claim. In the case of Impact Claims, data inputs are semantically structured using a data model that we have derived from the Verifiable Claims standard. Claims are evaluated through a decentralised impact exchange mechanism, using a system of Ethereum Blockchain smart contracts to coordinate and incentivise participants to process and verify the claims data in this way. The evaluation can be carried out by identified human evaluation agents (augmented with software tools), or entirely conducted by identified software oracles. As claims pass the evaluation requirements, this produces valuable Verified Impact Claims Data and the tokenized representation of the data, which gets recorded as transactions in a distributed public ledger. We use the popular term High-Definition to describe information outputs that are produced through this process. This is higher-resolution because all data points resolve to unique identifiers (using the DID spec) and it is higher-fidelity because the data gets reliably structured (using semantically defined linked-data schemas), as well as cryptographically authenticated and secured. A prototype data model for Impact Claims is described below. We are interested to review this as a canonical equivalent to or derivative of the Verifiable Claims data model. We are also keen to address a number of outstanding specification and technical implementation questions (outlined at the end of this paper) that have arisen through our initial phase of work. Impact Claims Data Model The Impact Claims data model is derived from the Verifiable Claim standard that is currently under consideration by the W3C Verifiable Claims Working Group . This incorporates standards such as Linked Data Signatures 1.0 Draft . Impact Claims are distinctive in that they record structured information about the delivery of impact services and/or goods by identified service agents , to identified beneficiaries . It is useful to understand the basic terminology and structure of an Impact Claim. How Impact Claims compare with Verifiable Claims: Terminlology impact claims Statements made by an entity about an event that involves an identified subject. The claim is effectively tamper-proof as this is cryptographically secured. The identity of the author is cryptographically authenticated by a digital signature. entity An identifiable person, organization, concept, or device with a provable existence. holder An entity that is in control of one or more verifiable impact claims. Typically a holder is also the issuer of their verifiable impact claims. issuer An entity that creates verifiable impact claims, associates these with a particular event, and transmits the claims to a holder. For Impact Claims, this is typically a service agent service agent An entity that delivers a services and/or goods that result in sustainable development impacts. The entity could be a person or a machine. Examples: nurse administring a vaccinantion; fingerprint scanner recording attendance evaluation agent An entity that undertakes an evaluation processes to verify Impact Claims. This could be a manual task performed by a person and/or automated and augmented by a software agent. Impact claim processing requirements This section outlines the roles and relationships between identified participants that are required to process Impact Claims. A service agent creates Impact Claims A holder receives and stores Impact Claims from the service agent The holder mediates the transmission of Impact claims between the service agents and evaluation agents Impact claims are associated with impact indicators Service agents should be able to easily control and own their own identifiers Holders must be able to freely choose and change the agents they employ to help them manage and share their impact claims. Data Model Impact Claims contain the following information in a structured linked-data model: claims data A set of data elements captured by an entity. Impact claims are effectively tamper-proof. The authorship of impact claims can be cryptographically verified. Claims must include the following properties: - templateID : Hash Value of the impact claim template - contractID : Hash Value of the contract governing these claims - indicator : A standard measurement metric defined in an indicator schema - claimDate : The date and time the impact claim is made - impactDate : The date and time the impact was delivered - location : The geo-location where the impact claim was delivered - serviceAgentID : The DID of the service agent - serviceID : The DID of an entity with which the service agent is associated - reason : A text field descibing why this impact was delivered. This is captured from the impact claim template - result : The result of the outcome of the service delivery captured by the service agent - beneficiaries (optional) : Is a list of DIDs for beneficiaries of this impact service - productsUsed (optional) : An array of the product identifiers for commodities used in the delivery of the impact metadata A set of metadata that is included with the claim that captures information regarding how, what, where and when the claim event occurred signature The claim is signed by the claim issuer using a cryptographic signature to ensure the authenticity of the authorship of the impact claim JSON-LD Syntax This section defines how the data model described in Data Model Section is realized in JSON-LD. Although syntactic mappings are only provided for these three different languages, applications and services may also use any other data representation language (XML, for example) that can support the data model. Example: Verifiable Impact Claim { \"@context\": [ \"http://schema.cnsnt.io/\", \"http://schema.org/\", \u201chttp://ixo.foundation/schema\u201d, \"https://w3id.org/identity\", \"https://w3id.org/security/v1\", \"https://iris.thegiin.org/indicators\", ], \"type\": \"ImpactClaim\", \u201ctemplateID\u201d : \"0x4600a18666a9f08f2f99a79ce8734e5b6f353a91\", \"contractID\" : \"0xb4b59c3acfeb9afd9398c88b2f6f003cbf29b553\" \"indicator\": { \"type\": \"Indicator\", \"brand\": \"IRIS\", \"code\": \"PI9468\" }, \"claimDate\": \"2016-02-08T16:02:20Z\", \"impactDate\": \"2016-02-08T16:02:20Z\", \"location\": { \"latitude\": \"12.01156874\", \"longitude\": \"-175.57177874\" }, \"serviceAgentID\": \"did:sov:21tDAKCERh95uGgKbJNHYp\", \"serviceCenterID\": { \"branch\": \"Cape Town\", \"geo\": { \"location\": { \"latitude\": \"12.01156874\", \"longitude\": \"-175.57177874\" }, } }, \"productsUsed\": [], \u201cclaimData\u201d: \"ixo:ImpactClaimData\" \"reason\": \"Water provided\", \"result\": { \"type\": \"Rating\", \"ratingValue\": \"79\" }, \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:8uQhQMGzWxR8vw5P3UWH1j#key/1\", \"signatureValue\": \"IOmA4R7TfhkYTYW8...CBMq2/gi25s=\" } } Schema { \"@context\": { \"type\": \"@type\", \"cn\": \"http://schema.cnsnt.io/\", \"so\": \"http://schema.org/\", \"ixo\": \u201chttp://ixo.foundation/schema\u201d, \"id\": \"https://w3id.org/identity\", \"sec\": \"https://w3id.org/security/v1\", \"ind\": \"https://iris.thegiin.org/indicators\", \u201ctemplateID\u201d : \"ixo:TemplateRef\" \u201ccontractID\u201d : \"ixo:ContractRef\" \"indicator\": ixo:ImpactIndicator\" \"claimDate\": \"so:Date\", \"impactDate\": \"so:Date\", \"location\": \"so:GeoCoordinates\" \"serviceAgentID\": \"ixo:ServiceAgent\", \"secondaryServiceAgents\": \"ixo:ServiceAgents\" \"serviceCenterID\": \"so:Place\", \"beneficiaries\": \"so:People\", \"productsUsed\": \"so:ItemList\" \u201cclaimData\u201d: \"ixo:ImpactClaimData\" \"reason\": \"ixo:ImpactReason\", \"result\": \"ixo:ImpactResult\", \"signatureChain\": \"sec:SignatureChain\" } Next steps We would like to invite collaborators to help further develop this specification and to consider if this use-case could feed into the W3C Verifiable Claims standardisation process. Questions Is a Verifiable Impact Claim a subtype of a Verifiable Claim or should the Verifiable Claim be extended to include claims about events as well as subjects? How to represent nested claims (for instance, when a device generates a claim within a service-delivery process) How to bundle or connect a series of Impact Claims? When new DID records are created for the same entity (such as a beneficiary), how to link together the claims so that these can be associated with the entity and avoid duplications? Could Impact Claims be introduced to the W3C standards Verifiable Claims process? References Verifiable Claims Working Group Linked Data Signatures 1.0 Draft A World That Counts: Mobilising The Data Revolution for Sustainable Development","title":"Verifiable Claims of Impact #"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#verifiable-claims-of-impact","text":"","title":"Verifiable Claims of Impact"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#public-working-draft-dated-21-september-2017","text":"Cedric Franz (Trustlab), Dr Shaun Conway (ixo Foundation) Impact Claims are a new use-case for Verifiable Claims and the DID specification. We propose this could become a standard that will transform how information gets collected, evaluated, valued and exchanged to optimise Sustainable Development impacts. The context for this is the UN\u2019s 17 Global Goals to end poverty, protect the planet, and ensure prosperity for all, by 2030. To create a world that counts, the UN Secretary-General has called for a data revolution for sustainable development. The prototype Impact Claims specification is a core part of the ixo Protocol that provides a decentralised mechanism to generate verified impact data as a fungible asset that can be traded for funding and other forms of value-exchange. This protocol increases the accountability and transparency of impact funding and service provision, whilst also generating a dynamic data commons for sustainable development. See the ixo White Paper for more information. The ixo protocol follows the archetypical process for transforming raw data into valuable, classified data assets, through an evaluation process, as depicted in the figure. It is interesting to think that this generic pattern applies to virtually any type of information. For instance, when music files are evaluated or curated by listeners applying star-ratings. The use-case we describe here could be extended to any evaluation process where the identities of service agents, evaluation agents, beneficiaries and purchasers is of some consequence. We are interested to explore the generalisability of this model, which could be thought of as Verifiable Claims about facts that are extrinsic to the identities of the subject/s or participants in an event or process. This contrasts with the established use of Verifiable Claims about facts that are intrinsic to the identity of the subject/s of the claim. In the case of Impact Claims, data inputs are semantically structured using a data model that we have derived from the Verifiable Claims standard. Claims are evaluated through a decentralised impact exchange mechanism, using a system of Ethereum Blockchain smart contracts to coordinate and incentivise participants to process and verify the claims data in this way. The evaluation can be carried out by identified human evaluation agents (augmented with software tools), or entirely conducted by identified software oracles. As claims pass the evaluation requirements, this produces valuable Verified Impact Claims Data and the tokenized representation of the data, which gets recorded as transactions in a distributed public ledger. We use the popular term High-Definition to describe information outputs that are produced through this process. This is higher-resolution because all data points resolve to unique identifiers (using the DID spec) and it is higher-fidelity because the data gets reliably structured (using semantically defined linked-data schemas), as well as cryptographically authenticated and secured. A prototype data model for Impact Claims is described below. We are interested to review this as a canonical equivalent to or derivative of the Verifiable Claims data model. We are also keen to address a number of outstanding specification and technical implementation questions (outlined at the end of this paper) that have arisen through our initial phase of work.","title":"Public Working Draft dated 21 September 2017"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#impact-claims-data-model","text":"The Impact Claims data model is derived from the Verifiable Claim standard that is currently under consideration by the W3C Verifiable Claims Working Group . This incorporates standards such as Linked Data Signatures 1.0 Draft . Impact Claims are distinctive in that they record structured information about the delivery of impact services and/or goods by identified service agents , to identified beneficiaries . It is useful to understand the basic terminology and structure of an Impact Claim. How Impact Claims compare with Verifiable Claims:","title":"Impact Claims Data Model"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#terminlology","text":"impact claims Statements made by an entity about an event that involves an identified subject. The claim is effectively tamper-proof as this is cryptographically secured. The identity of the author is cryptographically authenticated by a digital signature. entity An identifiable person, organization, concept, or device with a provable existence. holder An entity that is in control of one or more verifiable impact claims. Typically a holder is also the issuer of their verifiable impact claims. issuer An entity that creates verifiable impact claims, associates these with a particular event, and transmits the claims to a holder. For Impact Claims, this is typically a service agent service agent An entity that delivers a services and/or goods that result in sustainable development impacts. The entity could be a person or a machine. Examples: nurse administring a vaccinantion; fingerprint scanner recording attendance evaluation agent An entity that undertakes an evaluation processes to verify Impact Claims. This could be a manual task performed by a person and/or automated and augmented by a software agent.","title":"Terminlology"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#impact-claim-processing-requirements","text":"This section outlines the roles and relationships between identified participants that are required to process Impact Claims. A service agent creates Impact Claims A holder receives and stores Impact Claims from the service agent The holder mediates the transmission of Impact claims between the service agents and evaluation agents Impact claims are associated with impact indicators Service agents should be able to easily control and own their own identifiers Holders must be able to freely choose and change the agents they employ to help them manage and share their impact claims.","title":"Impact claim processing requirements"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#data-model","text":"Impact Claims contain the following information in a structured linked-data model: claims data A set of data elements captured by an entity. Impact claims are effectively tamper-proof. The authorship of impact claims can be cryptographically verified. Claims must include the following properties: - templateID : Hash Value of the impact claim template - contractID : Hash Value of the contract governing these claims - indicator : A standard measurement metric defined in an indicator schema - claimDate : The date and time the impact claim is made - impactDate : The date and time the impact was delivered - location : The geo-location where the impact claim was delivered - serviceAgentID : The DID of the service agent - serviceID : The DID of an entity with which the service agent is associated - reason : A text field descibing why this impact was delivered. This is captured from the impact claim template - result : The result of the outcome of the service delivery captured by the service agent - beneficiaries (optional) : Is a list of DIDs for beneficiaries of this impact service - productsUsed (optional) : An array of the product identifiers for commodities used in the delivery of the impact metadata A set of metadata that is included with the claim that captures information regarding how, what, where and when the claim event occurred signature The claim is signed by the claim issuer using a cryptographic signature to ensure the authenticity of the authorship of the impact claim","title":"Data Model"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#json-ld-syntax","text":"This section defines how the data model described in Data Model Section is realized in JSON-LD. Although syntactic mappings are only provided for these three different languages, applications and services may also use any other data representation language (XML, for example) that can support the data model.","title":"JSON-LD Syntax"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#example-verifiable-impact-claim","text":"{ \"@context\": [ \"http://schema.cnsnt.io/\", \"http://schema.org/\", \u201chttp://ixo.foundation/schema\u201d, \"https://w3id.org/identity\", \"https://w3id.org/security/v1\", \"https://iris.thegiin.org/indicators\", ], \"type\": \"ImpactClaim\", \u201ctemplateID\u201d : \"0x4600a18666a9f08f2f99a79ce8734e5b6f353a91\", \"contractID\" : \"0xb4b59c3acfeb9afd9398c88b2f6f003cbf29b553\" \"indicator\": { \"type\": \"Indicator\", \"brand\": \"IRIS\", \"code\": \"PI9468\" }, \"claimDate\": \"2016-02-08T16:02:20Z\", \"impactDate\": \"2016-02-08T16:02:20Z\", \"location\": { \"latitude\": \"12.01156874\", \"longitude\": \"-175.57177874\" }, \"serviceAgentID\": \"did:sov:21tDAKCERh95uGgKbJNHYp\", \"serviceCenterID\": { \"branch\": \"Cape Town\", \"geo\": { \"location\": { \"latitude\": \"12.01156874\", \"longitude\": \"-175.57177874\" }, } }, \"productsUsed\": [], \u201cclaimData\u201d: \"ixo:ImpactClaimData\" \"reason\": \"Water provided\", \"result\": { \"type\": \"Rating\", \"ratingValue\": \"79\" }, \"signature\": { \"type\": \"RsaSignature2016\", \"created\": \"2016-02-08T16:02:20Z\", \"creator\": \"did:example:8uQhQMGzWxR8vw5P3UWH1j#key/1\", \"signatureValue\": \"IOmA4R7TfhkYTYW8...CBMq2/gi25s=\" } }","title":"Example: Verifiable Impact Claim"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#schema","text":"{ \"@context\": { \"type\": \"@type\", \"cn\": \"http://schema.cnsnt.io/\", \"so\": \"http://schema.org/\", \"ixo\": \u201chttp://ixo.foundation/schema\u201d, \"id\": \"https://w3id.org/identity\", \"sec\": \"https://w3id.org/security/v1\", \"ind\": \"https://iris.thegiin.org/indicators\", \u201ctemplateID\u201d : \"ixo:TemplateRef\" \u201ccontractID\u201d : \"ixo:ContractRef\" \"indicator\": ixo:ImpactIndicator\" \"claimDate\": \"so:Date\", \"impactDate\": \"so:Date\", \"location\": \"so:GeoCoordinates\" \"serviceAgentID\": \"ixo:ServiceAgent\", \"secondaryServiceAgents\": \"ixo:ServiceAgents\" \"serviceCenterID\": \"so:Place\", \"beneficiaries\": \"so:People\", \"productsUsed\": \"so:ItemList\" \u201cclaimData\u201d: \"ixo:ImpactClaimData\" \"reason\": \"ixo:ImpactReason\", \"result\": \"ixo:ImpactResult\", \"signatureChain\": \"sec:SignatureChain\" }","title":"Schema"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#next-steps","text":"We would like to invite collaborators to help further develop this specification and to consider if this use-case could feed into the W3C Verifiable Claims standardisation process.","title":"Next steps"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#questions","text":"Is a Verifiable Impact Claim a subtype of a Verifiable Claim or should the Verifiable Claim be extended to include claims about events as well as subjects? How to represent nested claims (for instance, when a device generates a claim within a service-delivery process) How to bundle or connect a series of Impact Claims? When new DID records are created for the same entity (such as a beneficiary), how to link together the claims so that these can be associated with the entity and avoid duplications? Could Impact Claims be introduced to the W3C standards Verifiable Claims process?","title":"Questions"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-of-impact/#references","text":"Verifiable Claims Working Group Linked Data Signatures 1.0 Draft A World That Counts: Mobilising The Data Revolution for Sustainable Development","title":"References"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-primer/","text":"A Verifiable Claims Primer by Manu Sporny, Digital Bazaar Introduction It is currently difficult to transmit credentials such as driver's licenses, proofs of age, education qualifications, and healthcare data, via the Internet in a way that is verifiable yet protects individual privacy. These credentials are composed of statements called verifiable claims . Starting in 2013, the W3C Credentials Community Group started to work in earnest on solutions in this space followed shortly thereafter by the Rebooting Web of Trust Community and W3C Verifiable Claims Working Group . These groups, composed of 150+ individuals and organizations, are currently focused on the creation, storage, transmission, and verification of digital credentials via the Internet. This document is a primer for those that want to learn about the Verifiable Claims initiative, the use cases and ecosystem, a basic overview of the technology, and how to get involved. Use Cases Verifiable Claims are useful when a person needs to prove that they are: above a certain age, capable of driving a particular motor vehicle, require a particular medication, trained and certified as an electrician, professionally licensed to practice medicine, and cleared to travel internationally. The use cases above are merely a high-level introduction to the problem space. Readers that would like to explore the use cases in more detail are urged to read the Verifiable Claims Working Groups' Use Cases document. Ecosystem The Verifiable Claims ecosystem is composed of four primary roles: The Issuer , who issues verifiable credentials about a specific Subject . The Holder stores credentials on behalf of a Subject . Holders are typically also the Subject of a credential. The Verifier requests a profile of the Subject . A profile contains a specific set of credentials. The verifier verifies that the credentials provided in the profile are fit-for-purpose. The Identifier Registry is a mechanism that is used to issue identifiers for Subjects . A visual depiction of the ecosystem above is shown below: Claims, Credentials, and Profiles The ecosystem roles exchange data that enables the realization of the previously mentioned use cases. The data that is exchanged differs based on the roles participating, but is fundamentally composed of Claims, Credentials, and Profiles. A claim is statement about a subject, expressed as a subject-property-value relationship: The data model for claims described above is powerful and can be used to express a large variety of statements. For example, whether or not someone is over the age of 21 may be expressed as follows: These claims may be merged together to express a graph of information about a particular subject. The example below extends the data model above by adding claims that state that Pat knows Sam and that Sam is a student. When an Issuer sends data to a Holder, it bundles a set of claims into a data structure called a credential and digitally signs the data structure: When a Verifier asks for data from a Holder, the Holder typically bundles a set of credentials into a data structure called a profile and digitally signs the data structure: The depictions above are a high-level introduction to the data model and gloss over specifics. Readers that would like to explore the data model in more depth are urged to read the Verifiable Claims Working Groups' Data Model Specification . Participating If you would like to participate in shaping this work, there are multiple ways to participate: If you want weekly updates and are NOT a W3C Member, or want to participate in the more experimental work, you should join the Credentials Community Group . The W3C Credentials Community Group holds weekly calls that are open to the public . If you want weekly updates and are a W3C Member, you should join the Verifiable Claims Working Group . The W3C Verifiable Claims Working Group holds weekly calls that are open to W3C Members . We hold bi-yearly face-to-face meetings in the spring and fall at Rebooting Web of Trust and once a year in the fall at the W3C Technical Plenary . The groups are very inclusive and welcome input and participation people from all disciplines and levels of expertise.","title":"A Verifiable Claims Primer"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-primer/#a-verifiable-claims-primer","text":"by Manu Sporny, Digital Bazaar","title":"A Verifiable Claims Primer"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-primer/#introduction","text":"It is currently difficult to transmit credentials such as driver's licenses, proofs of age, education qualifications, and healthcare data, via the Internet in a way that is verifiable yet protects individual privacy. These credentials are composed of statements called verifiable claims . Starting in 2013, the W3C Credentials Community Group started to work in earnest on solutions in this space followed shortly thereafter by the Rebooting Web of Trust Community and W3C Verifiable Claims Working Group . These groups, composed of 150+ individuals and organizations, are currently focused on the creation, storage, transmission, and verification of digital credentials via the Internet. This document is a primer for those that want to learn about the Verifiable Claims initiative, the use cases and ecosystem, a basic overview of the technology, and how to get involved.","title":"Introduction"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-primer/#use-cases","text":"Verifiable Claims are useful when a person needs to prove that they are: above a certain age, capable of driving a particular motor vehicle, require a particular medication, trained and certified as an electrician, professionally licensed to practice medicine, and cleared to travel internationally. The use cases above are merely a high-level introduction to the problem space. Readers that would like to explore the use cases in more detail are urged to read the Verifiable Claims Working Groups' Use Cases document.","title":"Use Cases"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-primer/#ecosystem","text":"The Verifiable Claims ecosystem is composed of four primary roles: The Issuer , who issues verifiable credentials about a specific Subject . The Holder stores credentials on behalf of a Subject . Holders are typically also the Subject of a credential. The Verifier requests a profile of the Subject . A profile contains a specific set of credentials. The verifier verifies that the credentials provided in the profile are fit-for-purpose. The Identifier Registry is a mechanism that is used to issue identifiers for Subjects . A visual depiction of the ecosystem above is shown below:","title":"Ecosystem"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-primer/#claims-credentials-and-profiles","text":"The ecosystem roles exchange data that enables the realization of the previously mentioned use cases. The data that is exchanged differs based on the roles participating, but is fundamentally composed of Claims, Credentials, and Profiles. A claim is statement about a subject, expressed as a subject-property-value relationship: The data model for claims described above is powerful and can be used to express a large variety of statements. For example, whether or not someone is over the age of 21 may be expressed as follows: These claims may be merged together to express a graph of information about a particular subject. The example below extends the data model above by adding claims that state that Pat knows Sam and that Sam is a student. When an Issuer sends data to a Holder, it bundles a set of claims into a data structure called a credential and digitally signs the data structure: When a Verifier asks for data from a Holder, the Holder typically bundles a set of credentials into a data structure called a profile and digitally signs the data structure: The depictions above are a high-level introduction to the data model and gloss over specifics. Readers that would like to explore the data model in more depth are urged to read the Verifiable Claims Working Groups' Data Model Specification .","title":"Claims, Credentials, and Profiles"},{"location":"rwot5/topics-and-advance-readings/verifiable-claims-primer/#participating","text":"If you would like to participate in shaping this work, there are multiple ways to participate: If you want weekly updates and are NOT a W3C Member, or want to participate in the more experimental work, you should join the Credentials Community Group . The W3C Credentials Community Group holds weekly calls that are open to the public . If you want weekly updates and are a W3C Member, you should join the Verifiable Claims Working Group . The W3C Verifiable Claims Working Group holds weekly calls that are open to W3C Members . We hold bi-yearly face-to-face meetings in the spring and fall at Rebooting Web of Trust and once a year in the fall at the W3C Technical Plenary . The groups are very inclusive and welcome input and participation people from all disciplines and levels of expertise.","title":"Participating"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/","text":"A DID for every thing - Driving Event Data Chain Safer & better mobility through verifiable Driving Event Data Chains. Presented by Dr. Carsten St\u00f6cker (Spherity GmbH), Dr. Michael R\u00fcther (Spherity GmbH) Submitted to Rebooting Web of Trust 8 January 31, 2019, D\u00fcsseldorf, Germany Keywords: decentralized identity, vehicle identity, digital twinning, cryptographically secured data chains, verifiable claims, blockchain, machine learning, agile driving, data chain provenance, audit trails, reputation system Inspiration Driven by technology innovation and ecosystem growth mobility value chains are significantly changing from monolithic, closed to distributed, open systems. Data flows are dynamically defined and stretch across multiple entities. The trustworthiness and accuracy of output data of a distributed mobility value chain is of increasing importance for the safety within a mobility system. This transformation requires new concepts of digital identity and trust for data transactions among non-human entities. Introduction - What our solution does Our submission for RWoT submission describes an application of the previous RWoT whitepaper \"A DID for Everything\" [1] leveraging decentralized autonomic data (DAD) and verifiable data chains for driving event data processing. This enables the verification of a given data flow and the trustworthiness of output data. As the output data are used by several mobility control, risk and business systems it is important that any entity can evaluate the trustworthiness of a given output data set. The decentralized identifier (DID) is a new open standard type of globally unique identifier that offers a model for lifetime scope, portable digital identity that does not depend on any centralized authority and that can never be taken away by third parties. We are using DIDs as a standard identifier in our solution which is supported by the W3C community. Our approach uses cryptographic data structures to link data objects and to establish a method for data flow provenance [1]. By data flow provenance we mean a mechanism for tracing data item content and control through a processing system including any transformation to the data item. This includes flows with multiple sources, collective sensor fusion and processing by machine learning algorithms. Data flow provenance means not just tracing control but also verifying the end-to-end integrity of every data flow including any transformations (additions, deletions, modifications, combinations, and ML processing). When a DID and hence data chain of the resultant data is extended to machines, the provenance chain of the data flow can provide the basis for verifiable claims and attestations about the data flow itself as well as for reputation mechanisms. These novel verifiable data chains and reputation mechanisms allow to asses trustworthiness, reliability or risk metrics of a given output of a data chain. The applications of verifiable data chains stretch across multiple use cases including real-time vehicle value, dangerous driving, road and obstacle mapping, usage-based insurance (UBI), reliable feedback loops into driver assistance system (DAS) and autonomous driving infrastructures, V2V/V2I interactions and cooperative mobility systems. Benefits of data provenance along a digitized value chain include: - reliability of data and ML labels in distributed systems - increased safety - maximized efficiency with less congestion, impact, costs and better services. Our verifiable data chain concept supports the overall goal to demonstrate the working BC/DLT technology components can be used and scaled today for data provenance in digital value chains. General Approach Decentralized identifiers (DIDs) The resulting combinatorics of possible connections between any given set of entities in a mobility system is an impossibly large number. Yet in today's user journeys or business environments, agents (whether human, machine, or software) increasingly need to communicate, access or transact with a diverse group of these interconnected entities to achieve their goals. This requires an interoperable and ubiquitous method to address, verify and connect these elements together. We propose to adopt the open decentralized identifier standard (DID) as an open, interoperable addressing standard and to establish mechanisms to resolve DIDs across multiple central or decentral mobility systems [2]. DIDs are the atomic units of a new layer of decentralized identity infrastructure. DIDs can be extended from identifiers of people, to any entity, thus to identify every thing. We use DIDs to help identify and manage data sets, objects, machines or software agents through their digital twins, to locations, to events, and even to pure data objects. DIDs are derived from public private key pairs. We are using innovative cryptographic solutions for secure key management by fragmenting the private key of a DID that never exists in its entirety. Our key management solution is very effective for providing very secure signing transactions e.g. for smart phones, algorithms or data sets. An integration of the key management technology into embedded devices is on our technology roadmap. A DID has the following required syntax: did:method:idstring We are using the Ethereum Blockchain and DID method ethr for our development work. did:ethr:0x5ed65343eda1c46566dff6774132830b2b821b35 As our technology stack is blockchain agnostic any other DID method based on alternative blockchains can be integrated and used. Verifiable claims DIDs are only the base layer of decentralized identity infrastructure. The next higher layer (where most of the value is unlocked), are verifiable claims [3,4]. This is the technical term for a digitally signed electronic data structure that conforms to the interoperability standards being developed by the W3C Verifiable Credentials Working Group. Verifiable claims can be either self-issued by an entity such as a machine to provide a proof about authenticity and integrity of data or they can be issued by a third party (issuer, e.g. OEM, government, T\u00dcV, service provider, bank). In mobility systems any entity might want to transact with any other entity. This means entities are engaging with each other in a dynamically defined, on-demand way. It is not pre-defined which entities interact among each other. To ensure efficient transactions any new entity in a mobility value chain must be able to independently verify other counter parties. To achieve this objective, we are using the DID approach and are anchoring the verifiable claims on a distributed ledger technology to move the cryptographic root of trust from central systems into a decentral, interoperable infrastructure. Digital twins that are verifiable A digital twin is a digital representation of a biological entity (human, living organism, organization), a physical entity (objects, machines), a digital entity (digital asset, software agent) or any system formed of any combination of individual entities. Digital twins can represent objects and entities as varied as a IoT sensors, ECUs, spare parts, vehicles, traffic lights, access gates, human users, or a city, and everything else in between. More recently they have started to be used to represent intangible entities like services, code, data, processes and knowledge. Digital twin data can consist of any life-cycle attributes and IoT sensor, telematics or compute data. A verifiable digital twin is a digital twin with attributes that are represented by verifiable claims. These attributes such as a birth certificate, authentication proof, a calibration report or sensor data attestations can be independently verified by any third party. This type of digital twin provides verifiable data about its creation, life-cycle, sensor readings, actuator commands or transactions. These verifiable data can be used for audit trails, decision making and for feedback loops in (autonomous) control systems. Verifiable driving event data chain A data chain is a cryptographic data structure that chains signed data objects together and establishes a method for data flow provenance. Data flow provenance allows verifying the end-to-end integrity of every data flow object and its transformations (additions, deletions, modifications, combinations, and machine learning processing). Processing of driving event data is operational in multiple mobility disciplines. Driving event data processing can include multiple data sources, parties, algorithms and processing steps. A human or non-human end-user of driving event data chains needs to be able to validate trustworthiness and accuracy of data chain output data. This requirement becomes of critical importance when the output data is used in safety or security relevant use cases or to make economic decision with significant commercial values involved. For establishing a verifiable data chain, we link signed objects together. The following code snippet is a payload example with a machine learning label (red traffic light), information about the algorithm that created the label (Algorithm 1) and a link to the previous data chain block (previous block ID). HEADER: PAYLOAD TOKEN TYPE & SIGNATURE ALGORITHM { \"typ\": \"JWT\", \"alg\": \"ES256K-R\" } PAYLOAD: DATA { { \"iat\": 1546724123, \"exp\": 1546810523, \"signer\": { \"type\": \"algorithm\", \"name\": \"Algorithm 1\" }, \"data\": { \"claim\": { \"predictionLabel\": \"red traffic light, red traffic signal, stoplight\", \"predictionProb\": \"0.983483\", \"did\": \"did:ethr:0xe405b9ecb83582e4edc546ba27867ee6f46a940d\" }, \"previousBlockId\": \"b86d95d0-1131-11e9-982e-51c29ca1f26e\", \"previousBlockHash\": \"307b817de9b7175db0ded0ea9576027efd64fb21\" }, \"iss\": \"did:ethr:0x5ed65343eda1c46566dff6774132830b2b821b35\" } The data chain object can be verified by validating the signature of the payload. Cryptographic data chains enable users to validate the provenance of entire driving event data processing chains including the authenticity and integrity of the input data, the output data and the provenance of sensing devices and the processing algorithms. We recommend to - establish verifiable data chains for driving event data processing, - provide a DID for every entity and data set, - integrate the data chains with DID registries (e.g. validated list of OEMs). The verifiable digital twins are addressable by their DIDs and providing information about organizations, sensors telematics devices, data sets, external data sources, software algorithms and users involved in the data chain. This approach is of particular value when validation or benchmarking data are available about the sensing devices, vehicles and the algorithms that are processing the driving event data. In combination with a reputation or validation system any user can calculate trustworthiness and accuracy metrics about the output data. As a next step, reputation methods can be integrated for both, individual digital twins and entire data chains. Further standardization work on data chain trustworthiness and accuracy metrics need to be done. Design principles For our digital twin and data chain integration work we are applying the following design principles: Principle Description From VINs to value chains Abstracting the Concept of Identity to a mobility system of vehicles, IoT, road & travel infrastructure, mobility systems, ML agents, driving event data set, autonomous driving/DAS feedback loops, markets and humans. Exchanging data among those entities. E2E data provenance along a value chain. Blockchain-agnostic Use blockchain for anchoring attestations or verifiable claims. Decision on which blockchain to anchor claims based on user preferences or economic metrics such as Tx costs. Use of fiat-backed stable coins for micropayments. Scalable integration Integrated technology stack consisting of off-chain data structures, serverless cloud infrastructure, secure key management, DIDs, ML agents, sensor data, data chain fusion and blockchain connectors. Responsibility Segregation Implementation of this common pattern for micro services design that supports scalability and maintainability of our solution. Standards Use of existing W3C, Industry 4.0 and Automotive data standards and semantic models to ensure adaptability and portability of our solution. Business value Focusing on simple data integrity and authenticity problems within existing value chains. Retrofitting of existing infrastructures to scale adoption. Agile driving event data chain for automotive use cases Dangerous driving events can be divided into two groups: (1) the interaction between a driver\u2019s vehicle and the road environment, and (2) the interaction between a driver\u2019s vehicle and nearby vehicles [5]. Diverse methods for enhancing driving safety have been proposed. Such methods can be roughly classified as passive or active. Passive methods (e.g., seat-belts, airbags, and anti-lock braking systems), which have significantly reduced traffic fatalities, were originally introduced to diminish the degree of injury from an accident. By contrast, active methods are designed to prevent accidents from occurring. Driver assistance systems (DAS) are designed to alert the driver - or an autonomous driving module - as quickly as possible to a potentially dangerous situation. The two classes of driving events may occur simultaneously and lead to certain serious traffic situations. Automotive industry is working on active methods and systems including machine learning algorithms to analyze these two kinds of events and determine dangerous situations from data collected by various sensors and data from external sources. The machine learning output labels about dangerous curves, road obstacles or poor vehicle conditions are fed into control, transaction and risk systems. In distributed mobility systems the trustworthiness and accuracy of the output labels must be independently verifiable. Key question: How can I trust vehicle identity data, 3rd party data and machine learning labels that are created and processed along a distributed mobility value chain? To achieve trustworthiness of output labels we are planning to blend our verifiable data chain technology with historic driving event data and black box algorithms to build a verifiable agile driving solution: - Interoperable decentral identity and verifiable digital twinning protocol - Cryptographically secured and blockchain-enabled data chains - E2E integration of remote sensing (telematics) data and machine learning algorithms Our approach demonstrates how the following trust problems can be addressed with DLT: - Vehicle provenance and configuration - Provenance, verifiability and integrity of the driving event input data (or telematics data) - Integrity and transparency of driving event data chain when multiple 3rd party intermediaries are involved - Credentials about benchmarking of ML algorithms and training data - Aggregated accuracy and trustworthiness of predicted ML labels and attributes Challenges we ran into Over the course of the implementation of our verifiable data chain we ran into the following challenges: 1. Providing DID identity to individual data sets and algorithms 2. No reference implementations for verifiable data chains 3. No common semantic standards for automotive digital twin 4. Lack of validated source code for scalable issuing and anchoring of verifiable claims (>100.000 Tx/s) 5. Getting access to sensible OEM agile driving data and real algorithms 6. Lack of E2E driving event data models and architectures (location, transport, events, processing) We solved (1.) and (2.) with our data chain implementation. We researched W3C vehicle signalling standards and semantic web as well industry 4.0 standards to address (3.). We are testing the sidetree protocol on IPFS that might lead to a solution for (4.). We are planning to benchmark the protocol in the next weeks. We are planning to work with an OEM on (5.). Data model implementation patterns need to be created in order to enable effective implementation of cyber physical systems (CPS). This is a general task to be done regardless of the use of DLT technology. To address (6.) we are working with the German Centre of Artificial Intelligence (DFKI) on a CPS integration methodology, a data modelling methodology and reusable data model implementation patterns for verifiable data chains. Accomplishments that we are proud of We accomplished the development of verifiable data chains including the following technology components: - DID manager - Secure key management - HD identity wallet - Verifiable claims - Deployment on severless cloud functions on AWS - Integration of tensor flow algorithms for image processing - Implementation of corporate requirements for vault/wallet policies We are now planning to integrate historic agile driving data and a black box algorithm with our existing data chain solution on Amazon Web Services. We are planning to demonstarte our solution on RWoT 8. What we learned Our primary leanings are around the following areas of interest: - Abstraction of the DID method to any object - Identity solutions for machine learning agents - Scalability of off-chain data structures - Integration of serverless cloud infrastructure with key management and identity wallets - Verifiability of data chains - Applicability of data chains for above mentioned data-driven business solutions What's next: our broader objective Our broader objective is to establish a scalable digital twin protocol and a technology layer for autonomous things . We call these digital twins autonomous digital twins that are verifiable, semantic and privacy-preserving . We are integrating further W3C and Industry 4.0 standards to establish a semantic digital twin network. Our work integrates further innovation in cryptography and privacy-preserving solutions to achieve GDPR compliance for both, the human and non-human entities. We are looking forward to field test our solution in a complete mobility ecosystem. Addendum - Key Management in accordance to Corporate Requirements We just integrated our Corporate Wallet solution for managing private keys of the entities involved in the drivint event data chain demo. Key management is done via vaults. Vault policies are defined in accordance to compliance requirements of an enterprise. We are using the following set-up for identities and vaults: Account : Entities that require identity(-ies). Accounts are spaces where identities can be managed. An account can be created for human, legal entity, machine or any other physical or virtual object. Inside of accounts, we work with: Participants, Vaults and Sub-Identities. Sub-Identities are represented by wallets (key-pairs) and managed inside vaults. Participant : Participant devices can be created within a particular account. After requesting participant creation, it needs to be activated on a dedicated iOS device or with a cloud software agents (embedded devices to be supported later), to further trace operations from vaults of the corresponding account where participant was specified as a quorum member, so that it can provide approvals for pending operations. In the essence, participant\u2019s device, holds the secret that gives it a possibility to manipulate wallets that exist inside connected Vaults. Vault : Vaults are managed identity groups. Vaults group a single or multiple wallets, and define quorum policies for managing them. At the time of vault creation, there is a need to specify the list of participants that will have a right to approve operations on behalf of wallets identities that can be generated within. Sub-identity/Persona/Wallet : Each vault can have multiple wallets which are sub-identities (or personas) for of vault group or account holder. As an example, let\u2019s consider that some wallet identity is being used to sign some data in order to build a verifiable claim. In this case, after the signing process will start, participants that correspond to a respective vault, will be notified about a pending operation. And after gathering the required number of approvals, the operation will be processed. Vault Quorum Policy : Vault\u2019s Quorum Policy is a rule set. Vault\u2019s Quorum Policy is being defined at vault creation time. It describes the list of participants who participate in a quorum and the number of required approvals that need to be collected from all vault\u2019s participants for progressing a operation (signing or joining a vault). In case you are interested in the above, we would be happy to demonstrate the integration of vaults with legal entities, identity/device management and digital twinning for machine twins at RWoT 8. References [1] A DID for Everything - Rebooting Web of Trust Working Draft [2] W3C DID Specification [3] W3C Verifiable Claims [4] Decentral Identity Foundation [5] Dangerous Driving Event Analysis System by a Cascaded Fuzzy Reasoning Petri Net","title":"A DID for every thing   Agile Driving Data Chain"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#a-did-for-every-thing-driving-event-data-chain","text":"Safer & better mobility through verifiable Driving Event Data Chains. Presented by Dr. Carsten St\u00f6cker (Spherity GmbH), Dr. Michael R\u00fcther (Spherity GmbH) Submitted to Rebooting Web of Trust 8 January 31, 2019, D\u00fcsseldorf, Germany Keywords: decentralized identity, vehicle identity, digital twinning, cryptographically secured data chains, verifiable claims, blockchain, machine learning, agile driving, data chain provenance, audit trails, reputation system","title":"A DID for every thing - Driving Event Data Chain"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#inspiration","text":"Driven by technology innovation and ecosystem growth mobility value chains are significantly changing from monolithic, closed to distributed, open systems. Data flows are dynamically defined and stretch across multiple entities. The trustworthiness and accuracy of output data of a distributed mobility value chain is of increasing importance for the safety within a mobility system. This transformation requires new concepts of digital identity and trust for data transactions among non-human entities.","title":"Inspiration"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#introduction-what-our-solution-does","text":"Our submission for RWoT submission describes an application of the previous RWoT whitepaper \"A DID for Everything\" [1] leveraging decentralized autonomic data (DAD) and verifiable data chains for driving event data processing. This enables the verification of a given data flow and the trustworthiness of output data. As the output data are used by several mobility control, risk and business systems it is important that any entity can evaluate the trustworthiness of a given output data set. The decentralized identifier (DID) is a new open standard type of globally unique identifier that offers a model for lifetime scope, portable digital identity that does not depend on any centralized authority and that can never be taken away by third parties. We are using DIDs as a standard identifier in our solution which is supported by the W3C community. Our approach uses cryptographic data structures to link data objects and to establish a method for data flow provenance [1]. By data flow provenance we mean a mechanism for tracing data item content and control through a processing system including any transformation to the data item. This includes flows with multiple sources, collective sensor fusion and processing by machine learning algorithms. Data flow provenance means not just tracing control but also verifying the end-to-end integrity of every data flow including any transformations (additions, deletions, modifications, combinations, and ML processing). When a DID and hence data chain of the resultant data is extended to machines, the provenance chain of the data flow can provide the basis for verifiable claims and attestations about the data flow itself as well as for reputation mechanisms. These novel verifiable data chains and reputation mechanisms allow to asses trustworthiness, reliability or risk metrics of a given output of a data chain. The applications of verifiable data chains stretch across multiple use cases including real-time vehicle value, dangerous driving, road and obstacle mapping, usage-based insurance (UBI), reliable feedback loops into driver assistance system (DAS) and autonomous driving infrastructures, V2V/V2I interactions and cooperative mobility systems. Benefits of data provenance along a digitized value chain include: - reliability of data and ML labels in distributed systems - increased safety - maximized efficiency with less congestion, impact, costs and better services. Our verifiable data chain concept supports the overall goal to demonstrate the working BC/DLT technology components can be used and scaled today for data provenance in digital value chains.","title":"Introduction - What our solution does"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#general-approach","text":"","title":"General Approach"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#decentralized-identifiers-dids","text":"The resulting combinatorics of possible connections between any given set of entities in a mobility system is an impossibly large number. Yet in today's user journeys or business environments, agents (whether human, machine, or software) increasingly need to communicate, access or transact with a diverse group of these interconnected entities to achieve their goals. This requires an interoperable and ubiquitous method to address, verify and connect these elements together. We propose to adopt the open decentralized identifier standard (DID) as an open, interoperable addressing standard and to establish mechanisms to resolve DIDs across multiple central or decentral mobility systems [2]. DIDs are the atomic units of a new layer of decentralized identity infrastructure. DIDs can be extended from identifiers of people, to any entity, thus to identify every thing. We use DIDs to help identify and manage data sets, objects, machines or software agents through their digital twins, to locations, to events, and even to pure data objects. DIDs are derived from public private key pairs. We are using innovative cryptographic solutions for secure key management by fragmenting the private key of a DID that never exists in its entirety. Our key management solution is very effective for providing very secure signing transactions e.g. for smart phones, algorithms or data sets. An integration of the key management technology into embedded devices is on our technology roadmap. A DID has the following required syntax: did:method:idstring We are using the Ethereum Blockchain and DID method ethr for our development work. did:ethr:0x5ed65343eda1c46566dff6774132830b2b821b35 As our technology stack is blockchain agnostic any other DID method based on alternative blockchains can be integrated and used.","title":"Decentralized identifiers (DIDs)"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#verifiable-claims","text":"DIDs are only the base layer of decentralized identity infrastructure. The next higher layer (where most of the value is unlocked), are verifiable claims [3,4]. This is the technical term for a digitally signed electronic data structure that conforms to the interoperability standards being developed by the W3C Verifiable Credentials Working Group. Verifiable claims can be either self-issued by an entity such as a machine to provide a proof about authenticity and integrity of data or they can be issued by a third party (issuer, e.g. OEM, government, T\u00dcV, service provider, bank). In mobility systems any entity might want to transact with any other entity. This means entities are engaging with each other in a dynamically defined, on-demand way. It is not pre-defined which entities interact among each other. To ensure efficient transactions any new entity in a mobility value chain must be able to independently verify other counter parties. To achieve this objective, we are using the DID approach and are anchoring the verifiable claims on a distributed ledger technology to move the cryptographic root of trust from central systems into a decentral, interoperable infrastructure.","title":"Verifiable claims"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#digital-twins-that-are-verifiable","text":"A digital twin is a digital representation of a biological entity (human, living organism, organization), a physical entity (objects, machines), a digital entity (digital asset, software agent) or any system formed of any combination of individual entities. Digital twins can represent objects and entities as varied as a IoT sensors, ECUs, spare parts, vehicles, traffic lights, access gates, human users, or a city, and everything else in between. More recently they have started to be used to represent intangible entities like services, code, data, processes and knowledge. Digital twin data can consist of any life-cycle attributes and IoT sensor, telematics or compute data. A verifiable digital twin is a digital twin with attributes that are represented by verifiable claims. These attributes such as a birth certificate, authentication proof, a calibration report or sensor data attestations can be independently verified by any third party. This type of digital twin provides verifiable data about its creation, life-cycle, sensor readings, actuator commands or transactions. These verifiable data can be used for audit trails, decision making and for feedback loops in (autonomous) control systems.","title":"Digital twins that are verifiable"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#verifiable-driving-event-data-chain","text":"A data chain is a cryptographic data structure that chains signed data objects together and establishes a method for data flow provenance. Data flow provenance allows verifying the end-to-end integrity of every data flow object and its transformations (additions, deletions, modifications, combinations, and machine learning processing). Processing of driving event data is operational in multiple mobility disciplines. Driving event data processing can include multiple data sources, parties, algorithms and processing steps. A human or non-human end-user of driving event data chains needs to be able to validate trustworthiness and accuracy of data chain output data. This requirement becomes of critical importance when the output data is used in safety or security relevant use cases or to make economic decision with significant commercial values involved. For establishing a verifiable data chain, we link signed objects together. The following code snippet is a payload example with a machine learning label (red traffic light), information about the algorithm that created the label (Algorithm 1) and a link to the previous data chain block (previous block ID). HEADER: PAYLOAD TOKEN TYPE & SIGNATURE ALGORITHM { \"typ\": \"JWT\", \"alg\": \"ES256K-R\" } PAYLOAD: DATA { { \"iat\": 1546724123, \"exp\": 1546810523, \"signer\": { \"type\": \"algorithm\", \"name\": \"Algorithm 1\" }, \"data\": { \"claim\": { \"predictionLabel\": \"red traffic light, red traffic signal, stoplight\", \"predictionProb\": \"0.983483\", \"did\": \"did:ethr:0xe405b9ecb83582e4edc546ba27867ee6f46a940d\" }, \"previousBlockId\": \"b86d95d0-1131-11e9-982e-51c29ca1f26e\", \"previousBlockHash\": \"307b817de9b7175db0ded0ea9576027efd64fb21\" }, \"iss\": \"did:ethr:0x5ed65343eda1c46566dff6774132830b2b821b35\" } The data chain object can be verified by validating the signature of the payload. Cryptographic data chains enable users to validate the provenance of entire driving event data processing chains including the authenticity and integrity of the input data, the output data and the provenance of sensing devices and the processing algorithms. We recommend to - establish verifiable data chains for driving event data processing, - provide a DID for every entity and data set, - integrate the data chains with DID registries (e.g. validated list of OEMs). The verifiable digital twins are addressable by their DIDs and providing information about organizations, sensors telematics devices, data sets, external data sources, software algorithms and users involved in the data chain. This approach is of particular value when validation or benchmarking data are available about the sensing devices, vehicles and the algorithms that are processing the driving event data. In combination with a reputation or validation system any user can calculate trustworthiness and accuracy metrics about the output data. As a next step, reputation methods can be integrated for both, individual digital twins and entire data chains. Further standardization work on data chain trustworthiness and accuracy metrics need to be done.","title":"Verifiable driving event data chain"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#design-principles","text":"For our digital twin and data chain integration work we are applying the following design principles: Principle Description From VINs to value chains Abstracting the Concept of Identity to a mobility system of vehicles, IoT, road & travel infrastructure, mobility systems, ML agents, driving event data set, autonomous driving/DAS feedback loops, markets and humans. Exchanging data among those entities. E2E data provenance along a value chain. Blockchain-agnostic Use blockchain for anchoring attestations or verifiable claims. Decision on which blockchain to anchor claims based on user preferences or economic metrics such as Tx costs. Use of fiat-backed stable coins for micropayments. Scalable integration Integrated technology stack consisting of off-chain data structures, serverless cloud infrastructure, secure key management, DIDs, ML agents, sensor data, data chain fusion and blockchain connectors. Responsibility Segregation Implementation of this common pattern for micro services design that supports scalability and maintainability of our solution. Standards Use of existing W3C, Industry 4.0 and Automotive data standards and semantic models to ensure adaptability and portability of our solution. Business value Focusing on simple data integrity and authenticity problems within existing value chains. Retrofitting of existing infrastructures to scale adoption.","title":"Design principles"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#agile-driving-event-data-chain-for-automotive-use-cases","text":"Dangerous driving events can be divided into two groups: (1) the interaction between a driver\u2019s vehicle and the road environment, and (2) the interaction between a driver\u2019s vehicle and nearby vehicles [5]. Diverse methods for enhancing driving safety have been proposed. Such methods can be roughly classified as passive or active. Passive methods (e.g., seat-belts, airbags, and anti-lock braking systems), which have significantly reduced traffic fatalities, were originally introduced to diminish the degree of injury from an accident. By contrast, active methods are designed to prevent accidents from occurring. Driver assistance systems (DAS) are designed to alert the driver - or an autonomous driving module - as quickly as possible to a potentially dangerous situation. The two classes of driving events may occur simultaneously and lead to certain serious traffic situations. Automotive industry is working on active methods and systems including machine learning algorithms to analyze these two kinds of events and determine dangerous situations from data collected by various sensors and data from external sources. The machine learning output labels about dangerous curves, road obstacles or poor vehicle conditions are fed into control, transaction and risk systems. In distributed mobility systems the trustworthiness and accuracy of the output labels must be independently verifiable. Key question: How can I trust vehicle identity data, 3rd party data and machine learning labels that are created and processed along a distributed mobility value chain? To achieve trustworthiness of output labels we are planning to blend our verifiable data chain technology with historic driving event data and black box algorithms to build a verifiable agile driving solution: - Interoperable decentral identity and verifiable digital twinning protocol - Cryptographically secured and blockchain-enabled data chains - E2E integration of remote sensing (telematics) data and machine learning algorithms Our approach demonstrates how the following trust problems can be addressed with DLT: - Vehicle provenance and configuration - Provenance, verifiability and integrity of the driving event input data (or telematics data) - Integrity and transparency of driving event data chain when multiple 3rd party intermediaries are involved - Credentials about benchmarking of ML algorithms and training data - Aggregated accuracy and trustworthiness of predicted ML labels and attributes","title":"Agile driving event data chain for automotive use cases"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#challenges-we-ran-into","text":"Over the course of the implementation of our verifiable data chain we ran into the following challenges: 1. Providing DID identity to individual data sets and algorithms 2. No reference implementations for verifiable data chains 3. No common semantic standards for automotive digital twin 4. Lack of validated source code for scalable issuing and anchoring of verifiable claims (>100.000 Tx/s) 5. Getting access to sensible OEM agile driving data and real algorithms 6. Lack of E2E driving event data models and architectures (location, transport, events, processing) We solved (1.) and (2.) with our data chain implementation. We researched W3C vehicle signalling standards and semantic web as well industry 4.0 standards to address (3.). We are testing the sidetree protocol on IPFS that might lead to a solution for (4.). We are planning to benchmark the protocol in the next weeks. We are planning to work with an OEM on (5.). Data model implementation patterns need to be created in order to enable effective implementation of cyber physical systems (CPS). This is a general task to be done regardless of the use of DLT technology. To address (6.) we are working with the German Centre of Artificial Intelligence (DFKI) on a CPS integration methodology, a data modelling methodology and reusable data model implementation patterns for verifiable data chains.","title":"Challenges we ran into"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#accomplishments-that-we-are-proud-of","text":"We accomplished the development of verifiable data chains including the following technology components: - DID manager - Secure key management - HD identity wallet - Verifiable claims - Deployment on severless cloud functions on AWS - Integration of tensor flow algorithms for image processing - Implementation of corporate requirements for vault/wallet policies We are now planning to integrate historic agile driving data and a black box algorithm with our existing data chain solution on Amazon Web Services. We are planning to demonstarte our solution on RWoT 8.","title":"Accomplishments that we are proud of"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#what-we-learned","text":"Our primary leanings are around the following areas of interest: - Abstraction of the DID method to any object - Identity solutions for machine learning agents - Scalability of off-chain data structures - Integration of serverless cloud infrastructure with key management and identity wallets - Verifiability of data chains - Applicability of data chains for above mentioned data-driven business solutions","title":"What we learned"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#whats-next-our-broader-objective","text":"Our broader objective is to establish a scalable digital twin protocol and a technology layer for autonomous things . We call these digital twins autonomous digital twins that are verifiable, semantic and privacy-preserving . We are integrating further W3C and Industry 4.0 standards to establish a semantic digital twin network. Our work integrates further innovation in cryptography and privacy-preserving solutions to achieve GDPR compliance for both, the human and non-human entities. We are looking forward to field test our solution in a complete mobility ecosystem.","title":"What's next: our broader objective"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#addendum-key-management-in-accordance-to-corporate-requirements","text":"We just integrated our Corporate Wallet solution for managing private keys of the entities involved in the drivint event data chain demo. Key management is done via vaults. Vault policies are defined in accordance to compliance requirements of an enterprise. We are using the following set-up for identities and vaults: Account : Entities that require identity(-ies). Accounts are spaces where identities can be managed. An account can be created for human, legal entity, machine or any other physical or virtual object. Inside of accounts, we work with: Participants, Vaults and Sub-Identities. Sub-Identities are represented by wallets (key-pairs) and managed inside vaults. Participant : Participant devices can be created within a particular account. After requesting participant creation, it needs to be activated on a dedicated iOS device or with a cloud software agents (embedded devices to be supported later), to further trace operations from vaults of the corresponding account where participant was specified as a quorum member, so that it can provide approvals for pending operations. In the essence, participant\u2019s device, holds the secret that gives it a possibility to manipulate wallets that exist inside connected Vaults. Vault : Vaults are managed identity groups. Vaults group a single or multiple wallets, and define quorum policies for managing them. At the time of vault creation, there is a need to specify the list of participants that will have a right to approve operations on behalf of wallets identities that can be generated within. Sub-identity/Persona/Wallet : Each vault can have multiple wallets which are sub-identities (or personas) for of vault group or account holder. As an example, let\u2019s consider that some wallet identity is being used to sign some data in order to build a verifiable claim. In this case, after the signing process will start, participants that correspond to a respective vault, will be notified about a pending operation. And after gathering the required number of approvals, the operation will be processed. Vault Quorum Policy : Vault\u2019s Quorum Policy is a rule set. Vault\u2019s Quorum Policy is being defined at vault creation time. It describes the list of participants who participate in a quorum and the number of required approvals that need to be collected from all vault\u2019s participants for progressing a operation (signing or joining a vault). In case you are interested in the above, we would be happy to demonstrate the integration of vaults with legal entities, identity/device management and digital twinning for machine twins at RWoT 8.","title":"Addendum - Key Management in accordance to Corporate Requirements"},{"location":"rwot8/topics-and-advance-readings/A-DID-for-every-thing---Agile-Driving-Data-Chain/#references","text":"[1] A DID for Everything - Rebooting Web of Trust Working Draft [2] W3C DID Specification [3] W3C Verifiable Claims [4] Decentral Identity Foundation [5] Dangerous Driving Event Analysis System by a Cascaded Fuzzy Reasoning Petri Net","title":"References"},{"location":"rwot8/topics-and-advance-readings/Aligning-SSI-with-European-Union-Identity-legislation-eIDAS/","text":"Aligning SSI with European Union identity legislation (aka eIDAS Regulation) Authors Nacho Alamillo & Santi Casas Abstract This paper aims to advance the alignment of SSI solutions with the eIDAS Regulation regarding electronic identification. Background Regulation (EU) No 910/2014 of the European Parliament and of the Council of 23 July 2014 on electronic identification and trust services for electronic transactions in the internal market and repealing Directive 1999/93/EC (eIDAS Regulation, avaliable at http://data.europa.eu/eli/reg/2014/910/oj) aims, among other objectives, to facilitate the legal recognition of electronic identification means - used in Member States to access public services - by other Member States. The eIDAS Regulation has been developed, regarding electronic identification, by different implementing acts, notably: Commission Implementing Regulation (EU) 2015/1501 of 8 September 2015 on the interoperability framework pursuant to Article 12(8) of Regulation (EU) No 910/2014 of the European Parliament and of the Council on electronic identification and trust services for electronic transactions in the internal market (eIDAS Interoperability regulation, available at http://data.europa.eu/eli/reg_impl/2015/1501/2015-09-09). Commission Implementing Regulation (EU) 2015/1502 of 8 September 2015 on setting out minimum technical specifications and procedures for assurance levels for electronic identification means pursuant to Article 8(3) of Regulation (EU) No 910/2014 of the European Parliament and of the Council on electronic identification and trust services for electronic transactions in the internal market (eIDAS Assurance Levels regulation, available at http://data.europa.eu/eli/reg_impl/2015/1502/oj). According to eIDAS Regulation Whereas (27), the eIDAS Regulation should be technology-neutral, meaning that the legal effects it grants should be achievable by any technical means provided that the requirements of the Regulation are met. Thus, it should be legally possible to recognise an particular setup of an SSI system under eIDAS, allowing its usage for public services transactions, which are a relevant part of the market. It is also interesting to note that, according to eIDAS Regulation Whereas (17), Member States should encourage the private sector to voluntarily use electronic identification means under a notified scheme for identification purposes when needed for online services or electronic transactions. The possibility to use such electronic identification means would enable the private sector to rely on electronic identification and authentication already largely used in many Member States at least for public services and to make it easier for businesses and citizens to access their online services across borders. One interesting reason to foster the legal acceptance of eIDAS eIDs in the private sector is identification in AML/CFT procedures under Directive (EU) 2015/849 of the European Parliament and of the Council of 20 May 2015 on the prevention of the use of the financial system for the purposes of money laundering or terrorist financing, amending Regulation (EU) No 648/2012 of the European Parliament and of the Council, and repealing Directive 2005/60/EC of the European Parliament and of the Council and Commission Directive 2006/70/EC, amended by Directive (EU) 2018/843 of the European Parliament and of the Council of 30 May 2018 (consolidated text available at http://data.europa.eu/eli/dir/2015/849/2018-07-09). If case one or more Member States of eIDAS Regulation make use of the possibility mentioned in eIDAS Regulation Whereas (17), that would facilitate the full expansion of a SSI system, inheriting the legal value of the eIDAS Regulation recognized identification means. Use cases When analysing the interplay between the eIDAS Regulation and the SSI systems, at least two use cases must be considered. Using eIDAS identification means when creating verifiable claims The first use case considers the utilization of an electronic identification system for the validation of the identity attributes that are to be included in any assertion contained in the DID document. This would be a scenario in which a means of identification recognized in accordance with the eIDAS Regulation is used to verify the information that will be included in a DID document (i.e., using a special kind of oracle that verifies the eIDAS eID to pass the information to the DID creator). eIDAS Interoperability regulation defines minimum data sets for natural persons and for legal persons. For natural persons, the data set includes:current family name(s), current first name(s), date of birth, a unique identifier constructed by the sending Member State in accordance with the technical specifications for the purposes of cross-border identification and which is as persistent as possible in time, first name(s) and family name(s) at birth (optional), place of birth (optional), current address (optional) and gender (optional). For legal persons, the data set includes:current legal name; a unique identifier constructed by the sending Member State in accordance with the technical specifications for the purposes of cross-border identification and which is as persistent as possible in time, current address (optional), VAT registration number (optional), tax reference number (optional), the identifier related to Article 3(1) of Directive 2009/101/EC of the European Parliament and of the Council (optional), Legal Entity Identifier (LEI) referred to in Commission Implementing Regulation (EU) No 1247/2012 (optional), Economic Operator Registration and Identification (EORI) referred to in Commission Implementing Regulation (EU) No 1352/2013 (optional), and excise number provided in Article 2(12) of Council Regulation (EC) No 389/2012 (optional). The main advantage of using this approach is that the DID inherits the level of assurance of the eIDAS electronic identification means, allowing a person with this kind of eID, which is centralized, to get DIDs and leveraging their use in the space of decentralized transactions, gaining real privacy. Using SSI as an eIDAS identification means Although electronic identification under eIDAS Regulation is today clearly aligned with SAML-based infraestructures (see Opinion No. 2/2016 of the Cooperation Network on version 1.1 of the eIDAS Technical specifications, available at https://ec.europa.eu/cefdigital/wiki/pages/viewpage.action?pageId=37750723 and eIDAS eID Profile, available https://ec.europa.eu/cefdigital/wiki/display/CEFDIGITAL/eIDAS+Profile), nothing in the eIDAS or its implementing acts should prevent the usage of a SSI system as an electronic identification means. Thus, the second use case considers a DID as an eIDAS compliant electronic identification means, enabling - at least - transactions with Public Sector authorities and Public Administrations and, if so decided by the DID creator, also with private sector entities. For this purpose, the following challenges are considered: The alignment of the SSI/DID/VC vocabulary with the eIDAS legal concepts. Consideration of some specific DID creators as eIDAS issuers and nodes. Drafting of a SSI eIDAS profile, which could be approved by the eIDAS Cooperation Network.","title":"Aligning SSI with European Union identity legislation (aka eIDAS Regulation)"},{"location":"rwot8/topics-and-advance-readings/Aligning-SSI-with-European-Union-Identity-legislation-eIDAS/#aligning-ssi-with-european-union-identity-legislation-aka-eidas-regulation","text":"","title":"Aligning SSI with European Union identity legislation (aka eIDAS Regulation)"},{"location":"rwot8/topics-and-advance-readings/Aligning-SSI-with-European-Union-Identity-legislation-eIDAS/#authors","text":"Nacho Alamillo & Santi Casas","title":"Authors"},{"location":"rwot8/topics-and-advance-readings/Aligning-SSI-with-European-Union-Identity-legislation-eIDAS/#abstract","text":"This paper aims to advance the alignment of SSI solutions with the eIDAS Regulation regarding electronic identification.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/Aligning-SSI-with-European-Union-Identity-legislation-eIDAS/#background","text":"Regulation (EU) No 910/2014 of the European Parliament and of the Council of 23 July 2014 on electronic identification and trust services for electronic transactions in the internal market and repealing Directive 1999/93/EC (eIDAS Regulation, avaliable at http://data.europa.eu/eli/reg/2014/910/oj) aims, among other objectives, to facilitate the legal recognition of electronic identification means - used in Member States to access public services - by other Member States. The eIDAS Regulation has been developed, regarding electronic identification, by different implementing acts, notably: Commission Implementing Regulation (EU) 2015/1501 of 8 September 2015 on the interoperability framework pursuant to Article 12(8) of Regulation (EU) No 910/2014 of the European Parliament and of the Council on electronic identification and trust services for electronic transactions in the internal market (eIDAS Interoperability regulation, available at http://data.europa.eu/eli/reg_impl/2015/1501/2015-09-09). Commission Implementing Regulation (EU) 2015/1502 of 8 September 2015 on setting out minimum technical specifications and procedures for assurance levels for electronic identification means pursuant to Article 8(3) of Regulation (EU) No 910/2014 of the European Parliament and of the Council on electronic identification and trust services for electronic transactions in the internal market (eIDAS Assurance Levels regulation, available at http://data.europa.eu/eli/reg_impl/2015/1502/oj). According to eIDAS Regulation Whereas (27), the eIDAS Regulation should be technology-neutral, meaning that the legal effects it grants should be achievable by any technical means provided that the requirements of the Regulation are met. Thus, it should be legally possible to recognise an particular setup of an SSI system under eIDAS, allowing its usage for public services transactions, which are a relevant part of the market. It is also interesting to note that, according to eIDAS Regulation Whereas (17), Member States should encourage the private sector to voluntarily use electronic identification means under a notified scheme for identification purposes when needed for online services or electronic transactions. The possibility to use such electronic identification means would enable the private sector to rely on electronic identification and authentication already largely used in many Member States at least for public services and to make it easier for businesses and citizens to access their online services across borders. One interesting reason to foster the legal acceptance of eIDAS eIDs in the private sector is identification in AML/CFT procedures under Directive (EU) 2015/849 of the European Parliament and of the Council of 20 May 2015 on the prevention of the use of the financial system for the purposes of money laundering or terrorist financing, amending Regulation (EU) No 648/2012 of the European Parliament and of the Council, and repealing Directive 2005/60/EC of the European Parliament and of the Council and Commission Directive 2006/70/EC, amended by Directive (EU) 2018/843 of the European Parliament and of the Council of 30 May 2018 (consolidated text available at http://data.europa.eu/eli/dir/2015/849/2018-07-09). If case one or more Member States of eIDAS Regulation make use of the possibility mentioned in eIDAS Regulation Whereas (17), that would facilitate the full expansion of a SSI system, inheriting the legal value of the eIDAS Regulation recognized identification means.","title":"Background"},{"location":"rwot8/topics-and-advance-readings/Aligning-SSI-with-European-Union-Identity-legislation-eIDAS/#use-cases","text":"When analysing the interplay between the eIDAS Regulation and the SSI systems, at least two use cases must be considered.","title":"Use cases"},{"location":"rwot8/topics-and-advance-readings/Aligning-SSI-with-European-Union-Identity-legislation-eIDAS/#using-eidas-identification-means-when-creating-verifiable-claims","text":"The first use case considers the utilization of an electronic identification system for the validation of the identity attributes that are to be included in any assertion contained in the DID document. This would be a scenario in which a means of identification recognized in accordance with the eIDAS Regulation is used to verify the information that will be included in a DID document (i.e., using a special kind of oracle that verifies the eIDAS eID to pass the information to the DID creator). eIDAS Interoperability regulation defines minimum data sets for natural persons and for legal persons. For natural persons, the data set includes:current family name(s), current first name(s), date of birth, a unique identifier constructed by the sending Member State in accordance with the technical specifications for the purposes of cross-border identification and which is as persistent as possible in time, first name(s) and family name(s) at birth (optional), place of birth (optional), current address (optional) and gender (optional). For legal persons, the data set includes:current legal name; a unique identifier constructed by the sending Member State in accordance with the technical specifications for the purposes of cross-border identification and which is as persistent as possible in time, current address (optional), VAT registration number (optional), tax reference number (optional), the identifier related to Article 3(1) of Directive 2009/101/EC of the European Parliament and of the Council (optional), Legal Entity Identifier (LEI) referred to in Commission Implementing Regulation (EU) No 1247/2012 (optional), Economic Operator Registration and Identification (EORI) referred to in Commission Implementing Regulation (EU) No 1352/2013 (optional), and excise number provided in Article 2(12) of Council Regulation (EC) No 389/2012 (optional). The main advantage of using this approach is that the DID inherits the level of assurance of the eIDAS electronic identification means, allowing a person with this kind of eID, which is centralized, to get DIDs and leveraging their use in the space of decentralized transactions, gaining real privacy.","title":"Using eIDAS identification means when creating verifiable claims"},{"location":"rwot8/topics-and-advance-readings/Aligning-SSI-with-European-Union-Identity-legislation-eIDAS/#using-ssi-as-an-eidas-identification-means","text":"Although electronic identification under eIDAS Regulation is today clearly aligned with SAML-based infraestructures (see Opinion No. 2/2016 of the Cooperation Network on version 1.1 of the eIDAS Technical specifications, available at https://ec.europa.eu/cefdigital/wiki/pages/viewpage.action?pageId=37750723 and eIDAS eID Profile, available https://ec.europa.eu/cefdigital/wiki/display/CEFDIGITAL/eIDAS+Profile), nothing in the eIDAS or its implementing acts should prevent the usage of a SSI system as an electronic identification means. Thus, the second use case considers a DID as an eIDAS compliant electronic identification means, enabling - at least - transactions with Public Sector authorities and Public Administrations and, if so decided by the DID creator, also with private sector entities. For this purpose, the following challenges are considered: The alignment of the SSI/DID/VC vocabulary with the eIDAS legal concepts. Consideration of some specific DID creators as eIDAS issuers and nodes. Drafting of a SSI eIDAS profile, which could be approved by the eIDAS Cooperation Network.","title":"Using SSI as an eIDAS identification means"},{"location":"rwot8/topics-and-advance-readings/Anonymous_DIDs/","text":"Staying Anonymous With DIDs Author David Stark david.stark@securekey.com Abstract This paper looks at the ways the different parties in a sharing transaction may wish to stay anonymous. In a simple sharing transaction there are three parties involved: - Requestor - The party who is asking for some information - User - The party who the information is being requested from - Trusted Data Provider - A party who the requestor will trust to provide data for the user If the user is providing non identifiable information (such as age) they may not want to be tracked by the requestor, for example they do not want the requestor to be able to see they visited them on previous occasions. The Problem If a user provides the same DID (associated to a verifiable claim, for example) to multiple parties \u2013 or even the same party multiple times \u2013 those parties, if colluding, could track an individual user's activity. For instance if a user needs to prove they live in a particular country (the claim) to qualify for services at two different companies and provides the same DID associated to the \"country of residence\" claim to prove their eligibility, those parties requesting the information then (if colluding) could confirm that it is in fact the same user/individual. The problem is compounded when multiple anonymous claims (from different sources) need to be combined in the same exchange. How can this be solved If a user needs to issue a claim where they wish to remain anonymous then they can only issue that specific claim once. This would mean that to satisfy a request where the user wishes to stay anonymous then they would need to create a new DID for the purpose of this transaction only and contact thier data provider to provide a new claim for this DID that is not linked in anyway (at least visible to anyone except the data provider and the user) to the users existing DID or claims. Associated issuers, holders and verifiers would need to treat the 'transaction specific' DIDs and claims appropriately by enforcing audience, permissions, and expiry.","title":"Staying Anonymous With DIDs"},{"location":"rwot8/topics-and-advance-readings/Anonymous_DIDs/#staying-anonymous-with-dids","text":"","title":"Staying Anonymous With DIDs"},{"location":"rwot8/topics-and-advance-readings/Anonymous_DIDs/#author","text":"David Stark david.stark@securekey.com","title":"Author"},{"location":"rwot8/topics-and-advance-readings/Anonymous_DIDs/#abstract","text":"This paper looks at the ways the different parties in a sharing transaction may wish to stay anonymous. In a simple sharing transaction there are three parties involved: - Requestor - The party who is asking for some information - User - The party who the information is being requested from - Trusted Data Provider - A party who the requestor will trust to provide data for the user If the user is providing non identifiable information (such as age) they may not want to be tracked by the requestor, for example they do not want the requestor to be able to see they visited them on previous occasions.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/Anonymous_DIDs/#the-problem","text":"If a user provides the same DID (associated to a verifiable claim, for example) to multiple parties \u2013 or even the same party multiple times \u2013 those parties, if colluding, could track an individual user's activity. For instance if a user needs to prove they live in a particular country (the claim) to qualify for services at two different companies and provides the same DID associated to the \"country of residence\" claim to prove their eligibility, those parties requesting the information then (if colluding) could confirm that it is in fact the same user/individual. The problem is compounded when multiple anonymous claims (from different sources) need to be combined in the same exchange.","title":"The Problem"},{"location":"rwot8/topics-and-advance-readings/Anonymous_DIDs/#how-can-this-be-solved","text":"If a user needs to issue a claim where they wish to remain anonymous then they can only issue that specific claim once. This would mean that to satisfy a request where the user wishes to stay anonymous then they would need to create a new DID for the purpose of this transaction only and contact thier data provider to provide a new claim for this DID that is not linked in anyway (at least visible to anyone except the data provider and the user) to the users existing DID or claims. Associated issuers, holders and verifiers would need to treat the 'transaction specific' DIDs and claims appropriately by enforcing audience, permissions, and expiry.","title":"How can this be solved"},{"location":"rwot8/topics-and-advance-readings/Applying_POLA_to_User_Interaction/","text":"Applying the Principle of Least Authority to User Interaction by Bill Tulloh Object capabilities (ocaps) are increasingly recognized as an important tool for achieving the goals of self-sovereign identity. Many of the principles of self-sovereign identity, such as minimization and protection, can best be achieved through the disciplined pursuit of the principle of least authority that ocaps enable. This paper examines how POLA can be extended to better protect users when exercising their self-sovereign identity. Rich Sharing on the Web Perhaps most visibly, ocaps minimize the degree to which systems must rely on identity for access control, by replacing frequent identity checks with bearer instruments representing authority. Ocaps replace identity-based access with authorization-based access. As Marc Stiegler has argued, the choice of access control model has important implications for support for secure cooperation. He identifies six key features, enabled by ocaps, that support rich sharing. : dynnamic, attenuated, chained, cross domain, composable, and accountable. Together, these features ensure that people can share naturally with least authority. For example. chained, the ability to delegate authority, ensures that agents have enough authority, while attenuation, the ability to grant reduced authority, ensures that agents have least authority. Alan Karp illustrates this with the following example: \u201cIn an emergency, Marc asked me to park his car in my garage. I couldn\u2019t do it, so I asked my neighbor to do it for me and told her to get the garage key from my son.\u201d 1. Dynamic. It\u2019s an emergency, so there\u2019s no time to find an administrator to change permissions. 2. Attenuated. Marc did not have to give me all of his permissions, just the car keys. 3. Chained. I could pass Marc\u2019s car keys to my neighbor, further attenuating with just the valet key. 4. Composable. My neighbor combines one permission she got from me, the car key, with one she got from my son, the garage key, to complete the task. 5. Cross-domain. There are three families involved, each with its own policies, yet there is no need to communicate policies to another domain. In this example, I didn\u2019t need to ask Marc to change his policy to grant my neighbor permission to drive his car. 6. Accountable. If Marc finds a new scratch on his car, he knows to ask me to pay for the repair, it\u2019s up to me to collect from my neighbor. Rich sharing demonstrates how the applyinng the principle of least authority enables us to cooperate with those with whom we have only limited trust. POLA enables secure cooperation across trust boundaries. The Powerbox Pattern Ocaps can also play an important role in the control, access, and protection of user identities. For example, ocaps, by providing confinement of authority, helps stop exfiltration of users' data by malicious code. Moreover, as we build identity wallets and user agents, we must be ensure both a high level of security and usability. Ocaps help achieve high security by applying fine-grained POLA to reduce the attack surface. Such fine-grained division of authority may at first glance appear as a usability nightmare. Yet, much interesting work has been done showing that this need not be the case. Ocaps combine designation with authority, which enables user acts of designation - needed anyway - to also control the division of authority. The use of ocaps for secure interaction was pioneered during the early 2000sin the work of Mark Miller and Marc Stiegler on CapDesk and the DarpaBrowser. A major design goal was to put the user, not their applications, in charge. An important step in this direction, was to the invention of the PowerBox pattern. The CapDesk Powerbox was a software module that mediates the granting of authorities to a capability confined application from the user. Individuals necessarily hold many authorities, but there is no need for their applications to do so as well. The Powerbox manages the users authorities and safely allocates them as needed to applications, thus applying the prinicple of least authority. Guidelines for Secure Interaction design Ka-Ping Yee, building on the CapDesk work, identified a set of ten guidelines for secure interaction design. Guidelines well supported by ocap architectures. General principles * Path of least resistance. The most natural way to do a task should also be the safest. * Appropriate boundaries. The interface should draw distinctions among objects and actions along boundaries that matter to the user. Maintaining the Actor-Ability State * Explicit authorization. A User\u2019s authority should only be granted to another actor through an explicit user action understood to imply granting. * Visibility. The interface should let the user easily review any active authority relationships that could affect security decisions. * Revocability. The interface should let users easily revoke authority that the user has granted, whenever revocation is possible. * Expected ability. The interface should not give the user the impression of having authority that the user does not actually have. Communicating with the users * Trusted path. The user\u2019s communication channel to any entity that manipulates authority on the user\u2019s behalf must be unspoofable and free of corruption. * Identifiability. The interface should ensure that identical objects or actions appear different. * Expressiveness. The interface should provide enough expressive power to let users easily express security policies that fit their goals. * Clarity. The effect of any authority-manipulating user action should be clearly apparent to the user before the action take effect. While applying ocaps to secure interaction desgin remains a work in progress, we believe that much can be gained from exending the principle of least authority to the user.","title":"Applying the Principle of Least Authority to User Interaction"},{"location":"rwot8/topics-and-advance-readings/Applying_POLA_to_User_Interaction/#applying-the-principle-of-least-authority-to-user-interaction","text":"by Bill Tulloh Object capabilities (ocaps) are increasingly recognized as an important tool for achieving the goals of self-sovereign identity. Many of the principles of self-sovereign identity, such as minimization and protection, can best be achieved through the disciplined pursuit of the principle of least authority that ocaps enable. This paper examines how POLA can be extended to better protect users when exercising their self-sovereign identity.","title":"Applying the Principle of Least Authority to User Interaction"},{"location":"rwot8/topics-and-advance-readings/Applying_POLA_to_User_Interaction/#rich-sharing-on-the-web","text":"Perhaps most visibly, ocaps minimize the degree to which systems must rely on identity for access control, by replacing frequent identity checks with bearer instruments representing authority. Ocaps replace identity-based access with authorization-based access. As Marc Stiegler has argued, the choice of access control model has important implications for support for secure cooperation. He identifies six key features, enabled by ocaps, that support rich sharing. : dynnamic, attenuated, chained, cross domain, composable, and accountable. Together, these features ensure that people can share naturally with least authority. For example. chained, the ability to delegate authority, ensures that agents have enough authority, while attenuation, the ability to grant reduced authority, ensures that agents have least authority. Alan Karp illustrates this with the following example: \u201cIn an emergency, Marc asked me to park his car in my garage. I couldn\u2019t do it, so I asked my neighbor to do it for me and told her to get the garage key from my son.\u201d 1. Dynamic. It\u2019s an emergency, so there\u2019s no time to find an administrator to change permissions. 2. Attenuated. Marc did not have to give me all of his permissions, just the car keys. 3. Chained. I could pass Marc\u2019s car keys to my neighbor, further attenuating with just the valet key. 4. Composable. My neighbor combines one permission she got from me, the car key, with one she got from my son, the garage key, to complete the task. 5. Cross-domain. There are three families involved, each with its own policies, yet there is no need to communicate policies to another domain. In this example, I didn\u2019t need to ask Marc to change his policy to grant my neighbor permission to drive his car. 6. Accountable. If Marc finds a new scratch on his car, he knows to ask me to pay for the repair, it\u2019s up to me to collect from my neighbor. Rich sharing demonstrates how the applyinng the principle of least authority enables us to cooperate with those with whom we have only limited trust. POLA enables secure cooperation across trust boundaries.","title":"Rich Sharing on the Web"},{"location":"rwot8/topics-and-advance-readings/Applying_POLA_to_User_Interaction/#the-powerbox-pattern","text":"Ocaps can also play an important role in the control, access, and protection of user identities. For example, ocaps, by providing confinement of authority, helps stop exfiltration of users' data by malicious code. Moreover, as we build identity wallets and user agents, we must be ensure both a high level of security and usability. Ocaps help achieve high security by applying fine-grained POLA to reduce the attack surface. Such fine-grained division of authority may at first glance appear as a usability nightmare. Yet, much interesting work has been done showing that this need not be the case. Ocaps combine designation with authority, which enables user acts of designation - needed anyway - to also control the division of authority. The use of ocaps for secure interaction was pioneered during the early 2000sin the work of Mark Miller and Marc Stiegler on CapDesk and the DarpaBrowser. A major design goal was to put the user, not their applications, in charge. An important step in this direction, was to the invention of the PowerBox pattern. The CapDesk Powerbox was a software module that mediates the granting of authorities to a capability confined application from the user. Individuals necessarily hold many authorities, but there is no need for their applications to do so as well. The Powerbox manages the users authorities and safely allocates them as needed to applications, thus applying the prinicple of least authority.","title":"The Powerbox Pattern"},{"location":"rwot8/topics-and-advance-readings/Applying_POLA_to_User_Interaction/#guidelines-for-secure-interaction-design","text":"Ka-Ping Yee, building on the CapDesk work, identified a set of ten guidelines for secure interaction design. Guidelines well supported by ocap architectures. General principles * Path of least resistance. The most natural way to do a task should also be the safest. * Appropriate boundaries. The interface should draw distinctions among objects and actions along boundaries that matter to the user. Maintaining the Actor-Ability State * Explicit authorization. A User\u2019s authority should only be granted to another actor through an explicit user action understood to imply granting. * Visibility. The interface should let the user easily review any active authority relationships that could affect security decisions. * Revocability. The interface should let users easily revoke authority that the user has granted, whenever revocation is possible. * Expected ability. The interface should not give the user the impression of having authority that the user does not actually have. Communicating with the users * Trusted path. The user\u2019s communication channel to any entity that manipulates authority on the user\u2019s behalf must be unspoofable and free of corruption. * Identifiability. The interface should ensure that identical objects or actions appear different. * Expressiveness. The interface should provide enough expressive power to let users easily express security policies that fit their goals. * Clarity. The effect of any authority-manipulating user action should be clearly apparent to the user before the action take effect. While applying ocaps to secure interaction desgin remains a work in progress, we believe that much can be gained from exending the principle of least authority to the user.","title":"Guidelines for Secure Interaction design"},{"location":"rwot8/topics-and-advance-readings/DID-Content-References/","text":"DID Content References 2019-03-02 Drummond Reed and Ken Ebert Motivation A \"naked DID\" by itself identifies a DID subject. A naked DID is also is by itself a valid URL (Uniform Resource Locator), which is one type of URI (Uniform Resource Identifier) as defined by RFC 3986 . As a URL, a naked DID resolves to a DID document that describes the DID subject. We can therefore call a naked DID a DID document reference . By adding additional syntax elements as allowed under RFC 3986, a DID URL can also address other resources besides a DID document. For example: By adding a fragment directly to a naked DID, a DID URL can address a specific component within a DID document (such as a specific public key). We will call this a DID fragment reference . By adding a service ID directly after a naked DID, a DID URL can be dereferenced to a service endpoint and then pass on to it the optional path, query, and/or fragment components of a DID URL. In this way, a DID service reference can serve as a way to persistently address any service endpoint on the Web and to pass to that service endpoint the same components that a conventional URL can pass. More recently a fourth general category of use cases for DIDs has arisen\u2014the use of a DID to provide a persistent, decentralized references to content that is not hosted at a separate service endpoint but is stored at the verifiable data registry itself. A specific example of this use case is the Sovrin community , which stores various persistent content types needed by Sovrin-based credentials on the Sovrin ledger, including schema definitions, credential definitions, and revocation registries. While such persistent content references could be made using a Sovrin-specific content addressing syntax, it would be strongly preferable for these references to use a ledger-neutral DID syntax so they can be shared and reused across other ledgers or anywhere else that DID syntax is supported. We will refer to this fourth category of DID-based references as DID content references . The balance of this document will propose DID ABNF syntax that supports all four types of DID references. Base ABNF Syntax Following is a base ABNF syntax for DIDs and DID URLs that we propose to use as the starting point for this proposal. We note that this syntax is currently under discussion by the W3C Credentials Community Group , so it may change, particularly during the upcoming discussions at Rebooting the Web of Trust #8. For the definitive DID and DID URL ABNF once it is solidified, see the DID spec at https://w3c-ccg.github.io/did-spec/. Any rules not defined in this ABNF are defined in RFC 3986 (for easy reference, a copy of that ABNF is included in the last section of this document). Note: this proposal changes the delimiter character for service references from a semicolon to a dollar sign. While both semicolon and dollar sign are sub-delims characters under RFC 3986 and thus valid delimiters, the dollar sign character is: a) more visually distinguishable, and b) more suggestive of \"service\". did = \"did:\" method \":\" method-specific-idstring method = 1*methodchar methodchar = %x61-7A / DIGIT method-specific-idstring = idstring *( \":\" idstring ) idstring = 1*idchar idchar = ALPHA / DIGIT / \".\" / \"-\" did-url = did [ did-relative-ref ] did-relative-ref = did-fragment-ref / did-service-ref did-fragment-ref = \"#\" fragment did-service-ref = \"$\" service-id [ path-abempty ] [ \"?\" query ] [ \"#\" fragment ] service-id = service-idstring *( \":\" service-idstring ) service-id = 1*uri-safe-char url-safe-char = idchar / \"_\" / pct-encoded did-reference = did-url / did-relative-ref Proposed ABNF Syntax With Support for DID Content References To add support for both types of content references, we only need to add syntax that is parallel to the delimiter syntax used for service references, but which allows for content references in various content referencing formats. The limited character set available for this syntax in a valid URI is defined the sub-delims rule from the URI syntax defined in RFC 3986 . In this ABNF we propose the ! as the delimiter for content references. did = \"did:\" method \":\" method-specific-idstring method = 1*methodchar methodchar = %x61-7A / DIGIT method-specific-idstring = idstring *( \":\" idstring ) idstring = 1*idchar idchar = ALPHA / DIGIT / \".\" / \"-\" did-url = did [ did-relative-ref ] did-relative-ref = did-fragment-ref / did-content-ref / did-service-ref ;added did-content-ref did-fragment-ref = \"#\" fragment did-content-ref = \"!\" content-id ;new content-id = content-idstring *( \":\" content-idstring ) ;new content-idstring = 1*uri-safe-char ;new url-safe-char = idchar / \"_\" / pct-encoded did-service-ref = \"$\" service-id [ path-abempty ] [ \"?\" query ] [ \"#\" fragment ] service-id = service-idstring *( \":\" service-idstring ) service-idstring = 1*uri-safe-char did-reference = did-url / did-relative-ref Content Reference Formats This syntax for content references can support emerging content addressing formats such as Hashlink . Following is an example of a DID URL containing a Hashlink as a content reference to a (fictitious) schema on the Sovrin ledger: did:sov:21tDAKCERh95uGgKbJNHYp!hl:zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e Resolution of DID Content References The intention of this extension to the DID specification is to enable verifiable data registries, such as distributed ledgers or decentralized file systems, to be able to store persistent content natively if they are able. In this case, a DID method specification can be extended to define how a DID resolver can resolve a DID content reference directly to the content object and thus return that as a result of resolution exactly like it would return a DID document as a result of naked DID resolution. RFC 3986 Appendix A (For Reference) Any rules that are not defined in the ABNF above are defined in the ABNF for RFC 3986 . That ABNF is included here for easy reference. Note that we have annotated the syntax path through this ABNF used by the DID and DID URL ABNF. URI = scheme \":\" hier-part [ \"?\" query ] [ \"#\" fragment ] hier-part = \"//\" authority path-abempty / path-absolute / path-rootless ; DID URLs use this rule / path-empty URI-reference = URI / relative-ref absolute-URI = scheme \":\" hier-part [ \"?\" query ] relative-ref = relative-part [ \"?\" query ] [ \"#\" fragment ] relative-part = \"//\" authority path-abempty / path-absolute / path-noscheme / path-empty scheme = ALPHA *( ALPHA / DIGIT / \"+\" / \"-\" / \".\" ) authority = [ userinfo \"@\" ] host [ \":\" port ] userinfo = *( unreserved / pct-encoded / sub-delims / \":\" ) host = IP-literal / IPv4address / reg-name port = *DIGIT IP-literal = \"[\" ( IPv6address / IPvFuture ) \"]\" IPvFuture = \"v\" 1*HEXDIG \".\" 1*( unreserved / sub-delims / \":\" ) IPv6address = 6( h16 \":\" ) ls32 / \"::\" 5( h16 \":\" ) ls32 / [ h16 ] \"::\" 4( h16 \":\" ) ls32 / [ *1( h16 \":\" ) h16 ] \"::\" 3( h16 \":\" ) ls32 / [ *2( h16 \":\" ) h16 ] \"::\" 2( h16 \":\" ) ls32 / [ *3( h16 \":\" ) h16 ] \"::\" h16 \":\" ls32 / [ *4( h16 \":\" ) h16 ] \"::\" ls32 / [ *5( h16 \":\" ) h16 ] \"::\" h16 / [ *6( h16 \":\" ) h16 ] \"::\" h16 = 1*4HEXDIG ls32 = ( h16 \":\" h16 ) / IPv4address IPv4address = dec-octet \".\" dec-octet \".\" dec-octet \".\" dec-octet dec-octet = DIGIT ; 0-9 / %x31-39 DIGIT ; 10-99 / \"1\" 2DIGIT ; 100-199 / \"2\" %x30-34 DIGIT ; 200-249 / \"25\" %x30-35 ; 250-255 reg-name = *( unreserved / pct-encoded / sub-delims ) path = path-abempty ; begins with \"/\" or is empty / path-absolute ; begins with \"/\" but not \"//\" / path-noscheme ; begins with a non-colon segment / path-rootless ; begins with a segment / path-empty ; zero characters path-abempty = *( \"/\" segment ) path-absolute = \"/\" [ segment-nz *( \"/\" segment ) ] path-noscheme = segment-nz-nc *( \"/\" segment ) path-rootless = segment-nz *( \"/\" segment ) ; DID URLs use this rule path-empty = 0<pchar> segment = *pchar segment-nz = 1*pchar ; DID URLs use this rule segment-nz-nc = 1*( unreserved / pct-encoded / sub-delims / \"@\" ) ; non-zero-length segment without any colon \":\" pchar = unreserved / pct-encoded / sub-delims / \":\" / \"@\" ; DID URLs use this rule query = *( pchar / \"/\" / \"?\" ) fragment = *( pchar / \"/\" / \"?\" ) pct-encoded = \"%\" HEXDIG HEXDIG unreserved = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\" reserved = gen-delims / sub-delims gen-delims = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" ; DID URLs use this rule / \"*\" / \"+\" / \",\" / \";\" / \"=\"","title":"DID Content References"},{"location":"rwot8/topics-and-advance-readings/DID-Content-References/#did-content-references","text":"2019-03-02 Drummond Reed and Ken Ebert","title":"DID Content References"},{"location":"rwot8/topics-and-advance-readings/DID-Content-References/#motivation","text":"A \"naked DID\" by itself identifies a DID subject. A naked DID is also is by itself a valid URL (Uniform Resource Locator), which is one type of URI (Uniform Resource Identifier) as defined by RFC 3986 . As a URL, a naked DID resolves to a DID document that describes the DID subject. We can therefore call a naked DID a DID document reference . By adding additional syntax elements as allowed under RFC 3986, a DID URL can also address other resources besides a DID document. For example: By adding a fragment directly to a naked DID, a DID URL can address a specific component within a DID document (such as a specific public key). We will call this a DID fragment reference . By adding a service ID directly after a naked DID, a DID URL can be dereferenced to a service endpoint and then pass on to it the optional path, query, and/or fragment components of a DID URL. In this way, a DID service reference can serve as a way to persistently address any service endpoint on the Web and to pass to that service endpoint the same components that a conventional URL can pass. More recently a fourth general category of use cases for DIDs has arisen\u2014the use of a DID to provide a persistent, decentralized references to content that is not hosted at a separate service endpoint but is stored at the verifiable data registry itself. A specific example of this use case is the Sovrin community , which stores various persistent content types needed by Sovrin-based credentials on the Sovrin ledger, including schema definitions, credential definitions, and revocation registries. While such persistent content references could be made using a Sovrin-specific content addressing syntax, it would be strongly preferable for these references to use a ledger-neutral DID syntax so they can be shared and reused across other ledgers or anywhere else that DID syntax is supported. We will refer to this fourth category of DID-based references as DID content references . The balance of this document will propose DID ABNF syntax that supports all four types of DID references.","title":"Motivation"},{"location":"rwot8/topics-and-advance-readings/DID-Content-References/#base-abnf-syntax","text":"Following is a base ABNF syntax for DIDs and DID URLs that we propose to use as the starting point for this proposal. We note that this syntax is currently under discussion by the W3C Credentials Community Group , so it may change, particularly during the upcoming discussions at Rebooting the Web of Trust #8. For the definitive DID and DID URL ABNF once it is solidified, see the DID spec at https://w3c-ccg.github.io/did-spec/. Any rules not defined in this ABNF are defined in RFC 3986 (for easy reference, a copy of that ABNF is included in the last section of this document). Note: this proposal changes the delimiter character for service references from a semicolon to a dollar sign. While both semicolon and dollar sign are sub-delims characters under RFC 3986 and thus valid delimiters, the dollar sign character is: a) more visually distinguishable, and b) more suggestive of \"service\". did = \"did:\" method \":\" method-specific-idstring method = 1*methodchar methodchar = %x61-7A / DIGIT method-specific-idstring = idstring *( \":\" idstring ) idstring = 1*idchar idchar = ALPHA / DIGIT / \".\" / \"-\" did-url = did [ did-relative-ref ] did-relative-ref = did-fragment-ref / did-service-ref did-fragment-ref = \"#\" fragment did-service-ref = \"$\" service-id [ path-abempty ] [ \"?\" query ] [ \"#\" fragment ] service-id = service-idstring *( \":\" service-idstring ) service-id = 1*uri-safe-char url-safe-char = idchar / \"_\" / pct-encoded did-reference = did-url / did-relative-ref","title":"Base ABNF Syntax"},{"location":"rwot8/topics-and-advance-readings/DID-Content-References/#proposed-abnf-syntax-with-support-for-did-content-references","text":"To add support for both types of content references, we only need to add syntax that is parallel to the delimiter syntax used for service references, but which allows for content references in various content referencing formats. The limited character set available for this syntax in a valid URI is defined the sub-delims rule from the URI syntax defined in RFC 3986 . In this ABNF we propose the ! as the delimiter for content references. did = \"did:\" method \":\" method-specific-idstring method = 1*methodchar methodchar = %x61-7A / DIGIT method-specific-idstring = idstring *( \":\" idstring ) idstring = 1*idchar idchar = ALPHA / DIGIT / \".\" / \"-\" did-url = did [ did-relative-ref ] did-relative-ref = did-fragment-ref / did-content-ref / did-service-ref ;added did-content-ref did-fragment-ref = \"#\" fragment did-content-ref = \"!\" content-id ;new content-id = content-idstring *( \":\" content-idstring ) ;new content-idstring = 1*uri-safe-char ;new url-safe-char = idchar / \"_\" / pct-encoded did-service-ref = \"$\" service-id [ path-abempty ] [ \"?\" query ] [ \"#\" fragment ] service-id = service-idstring *( \":\" service-idstring ) service-idstring = 1*uri-safe-char did-reference = did-url / did-relative-ref","title":"Proposed ABNF Syntax With Support for DID Content References"},{"location":"rwot8/topics-and-advance-readings/DID-Content-References/#content-reference-formats","text":"This syntax for content references can support emerging content addressing formats such as Hashlink . Following is an example of a DID URL containing a Hashlink as a content reference to a (fictitious) schema on the Sovrin ledger: did:sov:21tDAKCERh95uGgKbJNHYp!hl:zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e","title":"Content Reference Formats"},{"location":"rwot8/topics-and-advance-readings/DID-Content-References/#resolution-of-did-content-references","text":"The intention of this extension to the DID specification is to enable verifiable data registries, such as distributed ledgers or decentralized file systems, to be able to store persistent content natively if they are able. In this case, a DID method specification can be extended to define how a DID resolver can resolve a DID content reference directly to the content object and thus return that as a result of resolution exactly like it would return a DID document as a result of naked DID resolution.","title":"Resolution of DID Content References"},{"location":"rwot8/topics-and-advance-readings/DID-Content-References/#rfc-3986-appendix-a-for-reference","text":"Any rules that are not defined in the ABNF above are defined in the ABNF for RFC 3986 . That ABNF is included here for easy reference. Note that we have annotated the syntax path through this ABNF used by the DID and DID URL ABNF. URI = scheme \":\" hier-part [ \"?\" query ] [ \"#\" fragment ] hier-part = \"//\" authority path-abempty / path-absolute / path-rootless ; DID URLs use this rule / path-empty URI-reference = URI / relative-ref absolute-URI = scheme \":\" hier-part [ \"?\" query ] relative-ref = relative-part [ \"?\" query ] [ \"#\" fragment ] relative-part = \"//\" authority path-abempty / path-absolute / path-noscheme / path-empty scheme = ALPHA *( ALPHA / DIGIT / \"+\" / \"-\" / \".\" ) authority = [ userinfo \"@\" ] host [ \":\" port ] userinfo = *( unreserved / pct-encoded / sub-delims / \":\" ) host = IP-literal / IPv4address / reg-name port = *DIGIT IP-literal = \"[\" ( IPv6address / IPvFuture ) \"]\" IPvFuture = \"v\" 1*HEXDIG \".\" 1*( unreserved / sub-delims / \":\" ) IPv6address = 6( h16 \":\" ) ls32 / \"::\" 5( h16 \":\" ) ls32 / [ h16 ] \"::\" 4( h16 \":\" ) ls32 / [ *1( h16 \":\" ) h16 ] \"::\" 3( h16 \":\" ) ls32 / [ *2( h16 \":\" ) h16 ] \"::\" 2( h16 \":\" ) ls32 / [ *3( h16 \":\" ) h16 ] \"::\" h16 \":\" ls32 / [ *4( h16 \":\" ) h16 ] \"::\" ls32 / [ *5( h16 \":\" ) h16 ] \"::\" h16 / [ *6( h16 \":\" ) h16 ] \"::\" h16 = 1*4HEXDIG ls32 = ( h16 \":\" h16 ) / IPv4address IPv4address = dec-octet \".\" dec-octet \".\" dec-octet \".\" dec-octet dec-octet = DIGIT ; 0-9 / %x31-39 DIGIT ; 10-99 / \"1\" 2DIGIT ; 100-199 / \"2\" %x30-34 DIGIT ; 200-249 / \"25\" %x30-35 ; 250-255 reg-name = *( unreserved / pct-encoded / sub-delims ) path = path-abempty ; begins with \"/\" or is empty / path-absolute ; begins with \"/\" but not \"//\" / path-noscheme ; begins with a non-colon segment / path-rootless ; begins with a segment / path-empty ; zero characters path-abempty = *( \"/\" segment ) path-absolute = \"/\" [ segment-nz *( \"/\" segment ) ] path-noscheme = segment-nz-nc *( \"/\" segment ) path-rootless = segment-nz *( \"/\" segment ) ; DID URLs use this rule path-empty = 0<pchar> segment = *pchar segment-nz = 1*pchar ; DID URLs use this rule segment-nz-nc = 1*( unreserved / pct-encoded / sub-delims / \"@\" ) ; non-zero-length segment without any colon \":\" pchar = unreserved / pct-encoded / sub-delims / \":\" / \"@\" ; DID URLs use this rule query = *( pchar / \"/\" / \"?\" ) fragment = *( pchar / \"/\" / \"?\" ) pct-encoded = \"%\" HEXDIG HEXDIG unreserved = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\" reserved = gen-delims / sub-delims gen-delims = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\" sub-delims = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\" ; DID URLs use this rule / \"*\" / \"+\" / \",\" / \";\" / \"=\"","title":"RFC 3986 Appendix A (For Reference)"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/","text":"Digital Identity for the Homeless Authors Mike Varley Matthew Wong Abstract This paper explores the possibilities and practicalities of leveraging a digital decentralized identity for helping homeless, distressed, or under-serviced populations. Some points that will be covered: * homeless does not mean 'disconnected'. * reliable identity is still essential for providing services to help these people. * the valuable services which need to made available to this group. * we don't need 100% converage of affected people to have value. * how decentralized ID networks support the creation of online communities with real-world capabilities. Background Although this background is Toronto Ontario Canada specific, it provides a context for a global issue. Problem Statement Mass homelessness in Canada emerged in the 1980s, following a massive disinvestment in affordable housing, structural shifts in the economy and reduced spending on social supports. Since then stakeholders across the country have tried and tested solutions to address the issue. These responses, largely based on the provision of emergency services, have prevented meaningful progress. Fortunately, there are many signs that we are entering a new phase -- one that will lead to an end to homelessness in Canada. Why Decentralize or Web of Trust? Modern services constantly require users to provide a verified, government identity. IDs are needed, not just to drive a car or enter a bar, but to register for school, open a bank account and access many government benefits. Individuals who do not have verification are locked out from vast expanses of the modern economy. Decentralized / Web of Trust based user-managed identity is a necessity for the millions of people around the world without identification or documentation. These \u201clost citizens\u201d have no access to social service, no ability to transact, and no clear citizenship. Decentralized solution is needed to solve this problem for a few reasons, one; blockchain will decrease the chances of fraud and stolen identities being used. Second, for individuals that actually don\u2019t have physical proof for their ID this elevates the need to register and request for physical identification which can take up months. Third, this can give people more proactive control over their data and make it more difficult for unauthorized users to exploit it. Lastly losing physical ID\u2019s is one of the most common problems amongst those that are experiencing homelessness, they often get their wallets, ID\u2019s, entire luggage or bag stolen so they always have to start over. Having their identities on the blockchain will mean they won\u2019t have to start over each time as long as they have their master key they will always be able to have their ID\u2019s with them wherever they go regardless if they lose their belongings or not. What existing solutions do we have? There are ID clinics across Canada that help homeless individuals re-apply to get their driver\u2019s license, health card, and SIN cards back. They help clients individually fill out forms to get their ID\u2019s back however the challenge is the bureaucracy and hoops they have to jump through to acquire it as well as the time it takes to wait for government to replace it. The problem that always occurs after is individuals losing it due to circumstance and then having to start all over again in the obtaining process. Pilots to provide identity services for the homeless are emerging, notably with two examples. New York with the Fummi app distributed on Android phones and recently a ploit in Holland (https://arxiv.org/pdf/1806.01926.pdF). Other solutions are proposed to different vulnerable groups with similar identity needs, like refugees with solutions like Taqanu (https://www.taqanu.com) What are we aiming to do? The focus of this topic is how to apply existing decentralized digital ID solution to help individuals experiencing homelessness in the city (Toronto). E.g. how to reserve services such as overnight shelters with digital ID, keep track of certificate offered by free public organized re-training programs, and built credibility via repeat use the the same digital ID with varies services run by non-profit organizations in the city etc. The current digital ID solution is mature enough to tackle this issue, however how to properly design and deploy this solution in a city wide scale without major government backing is still challenging -- how to make a digital ID more than just a digital file and become a tool to built trust on for the individual experiencing homelessness. Homeless does not means disconnected homeless does not mean no access to a smartphone homeless does not mean you have no root of trust (DL, Bank account, ...) homeless does not mean they are alone, or isolated (but it is easy for them to feel this way) Why a reliable Identity is valuable to this community Social workers in this community learn who the people are. Making this identity claim 'portable' across the city/locality/region as people move about. Services requiring identity exist for the homeless, including reserving overnight shelters, re-training / education programs, employment services Why Decentralized Digital Identity By its nature, the disposessed / homeless commnity is already decentralized. The value lies in empowering a member of this community to leverage an identity and community relationship when moving across regions, or being able to have an 'identity' when attempting to re-enter 'normal' society. The social workers working with homeless can provide in-person ID services and provide an 'on-boarding' for a reliable, portable ID, and the advent of a digital trust framework means the identity claims are immiately recognizable and useable across supporting agencies and services. Example use case: Joe takes advantage of a bed at a shelter. The intake shelter worker is able to provide Joe with a digital ID, that gives Joe a portable 'service ID'. When Joe decides to stay at a different shelter, he can use his serviceID. When Joe registers for a course, he can use his serviceID. when Joe completes the training/course, now Joe has a linked claim to his serviceID. The principles of the decentralized digital identity allow for privacy respecting exchanges as well - so Joe does not have to reveal that he stayed at any particular shelter, or can be tracked for the number of courses he registered for but did not complete.","title":"Digital Identity for the Homeless"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#digital-identity-for-the-homeless","text":"","title":"Digital Identity for the Homeless"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#authors","text":"Mike Varley Matthew Wong","title":"Authors"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#abstract","text":"This paper explores the possibilities and practicalities of leveraging a digital decentralized identity for helping homeless, distressed, or under-serviced populations. Some points that will be covered: * homeless does not mean 'disconnected'. * reliable identity is still essential for providing services to help these people. * the valuable services which need to made available to this group. * we don't need 100% converage of affected people to have value. * how decentralized ID networks support the creation of online communities with real-world capabilities.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#background","text":"Although this background is Toronto Ontario Canada specific, it provides a context for a global issue.","title":"Background"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#problem-statement","text":"Mass homelessness in Canada emerged in the 1980s, following a massive disinvestment in affordable housing, structural shifts in the economy and reduced spending on social supports. Since then stakeholders across the country have tried and tested solutions to address the issue. These responses, largely based on the provision of emergency services, have prevented meaningful progress. Fortunately, there are many signs that we are entering a new phase -- one that will lead to an end to homelessness in Canada.","title":"Problem Statement"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#why-decentralize-or-web-of-trust","text":"Modern services constantly require users to provide a verified, government identity. IDs are needed, not just to drive a car or enter a bar, but to register for school, open a bank account and access many government benefits. Individuals who do not have verification are locked out from vast expanses of the modern economy. Decentralized / Web of Trust based user-managed identity is a necessity for the millions of people around the world without identification or documentation. These \u201clost citizens\u201d have no access to social service, no ability to transact, and no clear citizenship. Decentralized solution is needed to solve this problem for a few reasons, one; blockchain will decrease the chances of fraud and stolen identities being used. Second, for individuals that actually don\u2019t have physical proof for their ID this elevates the need to register and request for physical identification which can take up months. Third, this can give people more proactive control over their data and make it more difficult for unauthorized users to exploit it. Lastly losing physical ID\u2019s is one of the most common problems amongst those that are experiencing homelessness, they often get their wallets, ID\u2019s, entire luggage or bag stolen so they always have to start over. Having their identities on the blockchain will mean they won\u2019t have to start over each time as long as they have their master key they will always be able to have their ID\u2019s with them wherever they go regardless if they lose their belongings or not.","title":"Why Decentralize or Web of Trust?"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#what-existing-solutions-do-we-have","text":"There are ID clinics across Canada that help homeless individuals re-apply to get their driver\u2019s license, health card, and SIN cards back. They help clients individually fill out forms to get their ID\u2019s back however the challenge is the bureaucracy and hoops they have to jump through to acquire it as well as the time it takes to wait for government to replace it. The problem that always occurs after is individuals losing it due to circumstance and then having to start all over again in the obtaining process. Pilots to provide identity services for the homeless are emerging, notably with two examples. New York with the Fummi app distributed on Android phones and recently a ploit in Holland (https://arxiv.org/pdf/1806.01926.pdF). Other solutions are proposed to different vulnerable groups with similar identity needs, like refugees with solutions like Taqanu (https://www.taqanu.com)","title":"What existing solutions do we have?"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#what-are-we-aiming-to-do","text":"The focus of this topic is how to apply existing decentralized digital ID solution to help individuals experiencing homelessness in the city (Toronto). E.g. how to reserve services such as overnight shelters with digital ID, keep track of certificate offered by free public organized re-training programs, and built credibility via repeat use the the same digital ID with varies services run by non-profit organizations in the city etc. The current digital ID solution is mature enough to tackle this issue, however how to properly design and deploy this solution in a city wide scale without major government backing is still challenging -- how to make a digital ID more than just a digital file and become a tool to built trust on for the individual experiencing homelessness.","title":"What are we aiming to do?"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#homeless-does-not-means-disconnected","text":"homeless does not mean no access to a smartphone homeless does not mean you have no root of trust (DL, Bank account, ...) homeless does not mean they are alone, or isolated (but it is easy for them to feel this way)","title":"Homeless does not means disconnected"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#why-a-reliable-identity-is-valuable-to-this-community","text":"Social workers in this community learn who the people are. Making this identity claim 'portable' across the city/locality/region as people move about. Services requiring identity exist for the homeless, including reserving overnight shelters, re-training / education programs, employment services","title":"Why a reliable Identity is valuable to this community"},{"location":"rwot8/topics-and-advance-readings/Digital-Identity-for-the-Homeless/#why-decentralized-digital-identity","text":"By its nature, the disposessed / homeless commnity is already decentralized. The value lies in empowering a member of this community to leverage an identity and community relationship when moving across regions, or being able to have an 'identity' when attempting to re-enter 'normal' society. The social workers working with homeless can provide in-person ID services and provide an 'on-boarding' for a reliable, portable ID, and the advent of a digital trust framework means the identity claims are immiately recognizable and useable across supporting agencies and services. Example use case: Joe takes advantage of a bed at a shelter. The intake shelter worker is able to provide Joe with a digital ID, that gives Joe a portable 'service ID'. When Joe decides to stay at a different shelter, he can use his serviceID. When Joe registers for a course, he can use his serviceID. when Joe completes the training/course, now Joe has a linked claim to his serviceID. The principles of the decentralized digital identity allow for privacy respecting exchanges as well - so Joe does not have to reveal that he stayed at any particular shelter, or can be tracked for the number of courses he registered for but did not complete.","title":"Why Decentralized Digital Identity"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/","text":"Digital Trust Protocol C. Keutmann & T. Pastoor DigitalTrustProtocol.org postmaster@digitaltrustprotocol.org Abstract The Digital Trust Protocol (DTP) is a solution for the handling of trust in the digital space. The protocol is broadly designed to work with all aspects of trust; this includes identity, reputation, and security. The protocol is designed to be independent of specific systems and is very minimalistic. The intention is to extend the protocol with layers above rather than direct implementations. The protocol claim message data is designed to be self-provable of authenticity, by use of private/public key algorithms and the blockchain timestamping feature. This enables the protocol to be decentralized without a need for central authority, as servers can use a peer-to-peer communication system and verify all the data it receives individually, without having to rely on the sender. The Protocol allows anyone, including automated software, to issue their own cryptographic identities, for the use of trust and reputation and be able to verify those of others, without the need for a trusted third party. Users issue claims to other identities, and this way build a personal web of trust network. Users can search for claims on subjects by use of the user's trust network. The search only include results from trusted sources, claims from untrusted sources are not returned. The system is resistant to attacks like Sybil and bot networks as they are usually not a part of the uses networks and therefore not considered when evaluating searches. The system works on the incentive of the user\u2019s reputation and established trust, to be a good actor within their networks, since no one will listen to them if no one trusts them. It is possible to trust anything digitally, and therefore the protocol suitable to be used for anything that requires some degree of trust shared between systems related or unrelated. The DTP protocol is ideal for identity, reputation and security management that needs to be used in a broader sense without a central authority and furthermore it can augment existing systems already in place. Introduction Trust in the digital world of today are mostly centralized and siloed. Many reputation systems only work on specific platforms. There are usually no sharing of trust and reputation between the platforms, and this makes it hard to keeping track of identities reputation across systems. Many platforms do even not have any form of reputation system in place, like many forums, news sites and so on. Therefore keeping a personal record of each identity on many different contents providing platforms becomes a time consuming and near impossible task, as the identity of the same person may differ from platform to platform. Many existing solutions only offer a specific kind of trust, like reputation and have no possibility for diversity in expressing the kind of trust a user may have of an identity or item. The Digital Trust Protocol (DTP) is designed to provide an open protocol for the handling of trust of any kind in the digital space, in a decentralized way. DTP is a decentralized web of trust protocol. The DTP system works on the assumption that it is harder to gain trust than to losing trust. This is the basis for the system to function properly, as it creates an incentive for a user to act appropriately to maintain the reputation. Steadily losing trust will most likely result in loss of attention and influence. A simple definition of trust is it is an expression of a belief or experience that a certain identity will act in a repeatable or predictable pattern. Simply DTP defines trust as proof of a predictable, repeatable pattern, and that is subjective to the observer. This simple pattern enables development of relatively simple rules that are easy to implement into autonomous programs, and by these rules, they can issue trust on their own. Identity in DTP is based on trust. Identity is something that is internal and external. The internal identity is about self-realization and is not the subject of the DTP protocol. The external identity is properties like name, age, birthplace, citizenship, and other recognizable properties. They are all claims given by others. Usually, a state authority recognizes its citizens by name, address, identity number, and other credentials. Verifying the identity of a person is to use a trusted authority as a source for the claims, and not only to rely on the subject. In DTP external identity properties are provided by claims from others, not from oneself. Just like it is not possible to issue a valid driver license to yourself at home without any authority involvement. In DTP an authority can be any entity, like the government, companies, organizations, and friends. However, the authority still has to be recognized by others to have any relevance when asking for trust. Security is claims given to an entity. Therefore the DTP can act as an access control system for identities across platforms. A claim can act as a simple ticket claim for specific an event, issued to a specific identity, thereby preventing reselling. Claims also include time properties to handle when the claims are active. Rating is claims specifying a specific value for an entity. It is possible to issue rating claims to specific items like a car or printer. However, cars and printers are not able to issue a public identity for themselves. Therefore the name or a serial number of the item can be used to generate the identity address of the item. Because there was no private/public key generation involved in the process of generating the identity of the item, nobody can issue claims on behalf of an item. It is possible to trust anything digitally and therefore the protocol suitable to be used for anything that requires some degree of trust shared between systems not related and without the need for any central authority. DTP protocol defines a simple piece of data called a claim message. The claim message data is designed to be self-provable of authenticity by use of private/public key algorithms and the blockchain timestamping feature, and this ensures that nobody else will be able to issue the same claim message without the private key used to sign the message. The timestamp feature ensures that the claim can prove its existence at a specific time, thereby preventing backdating claim messages. The self-provable authenticity feature of a claim message enables decentralization without the need for central authority, as servers can use peer-to-peer communication and still be able to verify all the data they receive, without having to trust the sender. The DTP servers that host the DTP web of trust networks are based on the claim messages. They serve users requests to solve the queries on different subjects and targets. The servers can be public or private and still share data with anyone. The DTP servers can focus on specific claim types or host broader networks. The protocol for sharing of data between servers is not a part of the DTP protocol as this is considered a layer on top of the DTP protocol. Because the claim message is self-provable authentic, the servers can be sure that no matter how they got the data, it is verifiable against tampering. Per default the claim messages are not encrypted, only signed by the issuer, making the claim readable by anyone. When claim messages need to be private and secret, they do not need to be encrypted, as the claims are stored on closed servers and not publish however, others can still trust the public key of a closed network, enabling self-sovereign identity management, as it makes it possible to prove chain of trusts without exposing more details than needed. Architecture The DTP protocol specifies generic data structure and a rule set for searching on these data in a specific way. The data structure does not specify the algorithms for calculating the private/public keys, signing scripts and hash values for ID\u2019s, this is up for the implementation of the protocol. The system consists of a client-server model, together with a clearly defined data structure. The generation of cryptographic key-pairs, and signing messages usually happens on the client. The messages are sent to a DTP server of choice, that timestamps each message and constructs a trust graph from all collected messages. The DTP protocol focuses on defining a common data structure and how to search on the data, but does not limit the types of trust and algorithms used to sign the trust. Event Sourcing is the base data pattern of the DTP protocol, where each claim defines a property change. The claim only defines a single atomic value for a single target from a single source, and are extended with a number of properties like scope and time. The principle of only changing of atomic values makes it easy to maintain the data in a decentralized way because it does not require previous knowledge of the state of an object. Bundling multiple claim values like Name, address, email into a single claim, introduces a higher complexity of implementation without any significant benefits. The claims are always bundled together into a package with the possibility of signing all the claims in one go before submitting it to the DTP server. Claim A claim message is a relatively small piece of data with a flat data structure that is self-provable authentic and defines a single value issued to a subject. After a claim message is created, a hash ID is calculated and then signed by the issuer. Usually, the timestamp is added by the DTP server. A claim message needs to contain at least the issuer signature and a timestamp, for other DTP servers to verify that the received claim message is valid. Claim messages can be of any type, and therefore the DTP protocol specifies a basic set of claim types for standardization purposes. When issuing and signing claims, the signature can be base on a simple private/public key algorithm principle, or it is also possible to provide a script with multiple signatures for the claim. It is also possible to provide any custom non-standard script/type if needed, but then this has to be supported by any DTP server that serves the claim in its web of trust graph. The value part of the claim defines a statement towards a subject. The value is what the issuer thinks of a subject. For simplifications, it is not possible to add more than one value per claim. Issuing multiple values to the same subject is done by issuing multiple claim messages. The single value per claim enables the system to update or replace individual claims with new ones as the need occurs. Updating or removing claims happens by issuing new claims messages to the same target with a new value or an empty value to simulate removal. The package that contains the claims, features techniques to enable single signing and templating for mitigating the bloat of having to issue multiple claims of the same type towards the same subject. It's possible to extract individual claims out of the package and add them into new updated packages and still keep the proof of authenticity with each claim. This way old replaced claims can be discarded and existing and new claims a combined into new packages before sharing. It is possible to add a signature to the subject part of the claim message, proving that the issuer is also in control of the subject. This enables the DTP servers to change the behavior of the search algorithms used for finding trusts and enable features like tunneling for key management. The claim message can have an activation and expiration date, enabling them only to be active at certain times. A use case would be that the trust is used as a ticket system or to provide secure access to systems for a limited time. Claim messages contain a scope property for limiting the context of the claim, as the claim given to specific web platforms may not be relevant on other platforms. This is very beneficial to limit the amount of trust hosted on a DTP server. A claim message is hosted in a package when submitted or transported around. Example of a claim package { \"algorithm\": \"double256.merkle.dtp1\", \"id\": \"xSJLFeSNTYn4AHjvbz4rCh7AFz1ZJyFwZW1M8GJy3g4=\", \"created\": 1548757308, \"claims\": [ { \"id\": \"Lp2+ebZ9U0LRv/Z/xnx8pPVhbkRdoyscsdcY+n+StaI=\", \"created\": 1548757166, \"issuer\": { \"type\": \"secp256k1-pkh\", \"id\": \"1LRNDp2HfVKc1FsCJwbJwMhtssXL8fdTx2\", \"signature\": \"H0+9PhNSxm6ySMShlkuC1ZCTuYdbxSHquP1KCmb/M4LxZwOSFoHUy+fHcZiiI/zjheVJvrQbM/1X4Eg6+wTx3N0=\" }, \"subject\": { \"type\": \"name\", \"id\": \"TrustProtocol\" }, \"type\": \"binary.trust.dtp1\", \"value\": \"true\", \"scope\": \"twitter.com\" }, { \"id\": \"3atbj7U7RMSfL8OQFaGDzYlpGRbpnm3kHItMzFXLjMk=\", \"created\": 1548757166, \"issuer\": { \"type\": \"secp256k1-pkh\", \"id\": \"1LRNDp2HfVKc1FsCJwbJwMhtssXL8fdTx2\", \"signature\": \"ICWN+jH6Gn8+c7FJ0rSk3SPRmRI97FiXVtEdnm2S0utJOpZaE+WqNDII1RhHwLDlgVxlyNFpkp7w3K+WCBJXhPg=\" }, \"subject\": { \"type\": \"id\", \"id\": \"1NKtqyDndSvQ87ANAHsWR5XGL66pXkZxc8\" }, \"type\": \"binary.trust.dtp1\", \"value\": \"true\", \"scope\": \"\" }, { \"id\": \"QlbmdW4R1Ctth4Ah4xIVNzcflhlyr4oFaZFtIASi9HA=\", \"created\": 1548757169, \"issuer\": { \"type\": \"secp256k1-pkh\", \"id\": \"1LRNDp2HfVKc1FsCJwbJwMhtssXL8fdTx2\", \"signature\": \"IPe6xOj4ofy1QWSB646AHvKPa7a7mk3ko6D/dKdk15jkVZ4wLt5ci1eCJrj2j4uWSM+JYFlVy+Z1Sa8LHMDNK8U=\" }, \"subject\": { \"type\": \"id\", \"id\": \"1NKtqyDndSvQ87ANAHsWR5XGL66pXkZxc8\" }, \"type\": \"alias.identity.dtp1\", \"value\": \"Digital Trust Protocol\", \"scope\": \"twitter.com\" } ], \"timestamps\": [ { \"blockchain\": \"btctest\", \"algorithm\": \"secp256k1-double256.merkle.dtp1\", \"source\": \"xSJLFeSNTYn4AHjvbz4rCh7AFz1ZJyFwZW1M8GJy3g4=\", \"registered\": 1548757308 } ], \"server\": { \"type\": \"secp256k1-pkh\", \"id\": \"1LCGKp8zkmU3jBSdsRNfLzqsJH3qSzSxyk\", \"signature\": \"IBtutLOpiB5rcpgvSxdNbvB5OpVM5yp8t1dO3FVWvgPWdX+EvM3eQrfyBNeOQYil7K/xuzsgXYNYE7XjR/byhBo=\" } } In the example above, three claims have been created and added to a package. The claims trust a subject named \u201cTrustProtocol\u201d and a subject with an ID \u201c1NKtqyDndSvQ87ANAHsWR5XGL66pXkZxc8\u201d and its alias of \u201cDigital Trust Protocol\u201d. These three claims are trusting the same entity on Twitter, where the entity has the public key \u201c1NKtqyDndSvQ87ANAHsWR5XGL66pXkZxc8\u201d, but also the twitter name \u201cTrustProtocol\u201d is trusted. Finally, an alias claim issued to the public key, connecting the key with an alias. Note that the scope property is empty for the claim that trusts the ID, this is because the trust is global and not limited to a scope. The server property in the package is created and signed by the DTP server receiving the claims from the client. This way other DTP servers can identify the source of the package. Lastly, the DTP server timestamps the claims by use of a publicly available blockchain to ensure that claims can prove their time of existence. Package When DTP server shares claim messages with other DTP servers, they use a package to contain the claim messages; this enables them to create an array of claim they have received from users and timestamp them all at once by using a Merkle tree algorithm. DTP servers also sign the package with their own identity, enabling other DTP servers to be able to verify the origin of the package. By having DTP packaging and signing claims, it helps to confirm the existence of the claims, further enhancing the proof of the existence of the claims, because the claim message has now been seen by at least one DTP server and shared in a package signed by the server. The package includes a template property to reduce storage needed by providing shared templates for claims to use. Even key signing is supported within the template. It is possible to add multiple claims to a single package and sign the package once. The signature is based on the id calculated from a Merkle tree of all the id\u2019s from the claims, and the timestamp works in the same way. This enables claims to be separated from their original package and added to other newer packages, but still be able to be self-provable authentic. The Merkle tree id feature enables repackaging of the claims into new packages as individual claims to get replaced, avoiding having to keep old replaced claims data with still active claims. DTP server The DTP can have multiple roles in the DTP protocol, as they can act as a timestamp server and as a search engine for the DTP web of trust graph. The timestamp role is to ensure that claim messages can prove their existence at a specific time. Furthermore, the server can package up the claim messages into packages that the server signs on its own, then sharing the package with other DTP servers, improving the claims proof of existence. The DTP server stores the claim messages in a regular database and not on a blockchain. No blockchain is needed for storage as there is no need for a consensus of the state of the ledger because it is not possible to re-spent the same claim given to a subject on to other subjects; therefore the amount of trust that can be issued are endless. Only the timestamp feature uses a blockchain, and by use of the Merkle tree algorithm, it is possible to timestamp a large number of claim messages in one single transaction on a blockchain. Web of Trust The DTP web of trust graph role of the DTP server is to serve user requests on the user personal trust network. The claim message in itself is a plain value carrier, and therefore a graph structure is needed to be able to search through relevant trust based on a users perspective. A DTP server collects claim messages that are relevant for its users and build up a graph structure that is searchable. The DTP protocol defines a standard search and result structure for search and result, and it also defines a standard set of rules on how to search through the web of trust graph regarding the perspective of the user. A tunneling claim is where the issuer of a claim have signed both the issuer key and the subject key, thereby proving control of both keys. A tunneling claim do not enforce an incremental count of degree when calculating the search, but just follow through. It enables a deeper level of search beyond three degrees, and as the issuer usually only creates a few tunneling claims, it will limiting the number of claims to search through. A tunneling claim is suited for key management as a public identity with a private key highly secured and rarely used, can delegate trust to a daily key, by the tunneling claim. It makes it easy to have high security with convenient key management; this enables organizations to have a hierarchy of keys for divisions, departments, and employees. Trust issued by an employee can be linked all the way back to the organization's public key by a chain of claims, and therefore only the organizations public key has to be trusted when validating the proof of credentials, even that is was an employee that issued the credentials. This also makes it easy to replace keys when employees change positions or keys gets compromised. A simple scenario of issuing trust and searching for it. Client A issue trust to B and B issue trust to Client C Client A searches on Client C and gets the result from the DTP search server. The result contains the trust from A to B and the trust from B to C. It is up to the client to decide how to interpret the search result as the may be multiple results on from different identities having different opinions. Therefore the server presents the data, and the client handles it from there on. Some basic rules of the search engine are that searches never go further than 3 degrees away from the user, as this quickly become irrelevant the further out in the network the search goes. Another rule is that the only result from the same degree is returned and the search does not continue further onto the next degree when a match is found. Only trusted identities on the network are followed and distrusted identities are ignored. The chains of trust are the basics of the system, as they enable for verification of proof of identity credentials by following the trust chains generated by authorities. The network of DTP servers and scalability The usability and value of a web of trust network increase with more users join the system. The DTP system is designed to scale up to a very high number of identities and claim messages supporting a global implementation. The nature of the claim message is that they are self-provable authentic and therefore the DTP servers do not need to rely on a central authority but can share the data directly in a peer to peer network. The DTP protocol does not specify the communications between servers as this is regarded as a layer on top of the DTP protocol. The DTP server may offer different scopes for trust services; therefore the trust messages include a type and scope properties to help to filter and limiting the trust before it is verified and included in a servers DTP web of trust graph. A claim message is designed to be very small usually under one kilobyte; therefore the properties of the claim naturally have a limited size. The small size helps claim messages propagate quickly through the system and limit attack surfaces. It is estimated that it is possible to have DTP search servers with reasonably low cost that can handle web of trust graphs containing millions of identities and trusts with effective memory management. Servers can become more specialized and limiting the type and scope of trusts as the networks get larger. It is possible that the clients will have a list of dedicated DTP search server for different scopes of trust and choose the right server in the right context. The client The client issues trust and interpret the search results from the DTP server. It is up to each client to decide how to read the search result as this may have a different meaning in different scenarios. When using the DTP protocol, the client does not need to provide an identity to be able to use the web of trust graph. Anonymous users can rely on other established identities and use their networks, this could be friends or well-known entities like online search engines, and security firms specialized in web security. However, without a personal DTP identity, it is not possible for others to trust the identity and establish a personal web of trust network. Key management The issuer of trust uses a private/public key algorithms to generate an identity in the form of a public key and to sign the claim messages. However, it is also possible to use a script like languages for multi signatures for proving ownership. After a daily session of issuing claims, they would be bundled into a package and signed by a possible hardware device before submitting to a DTP server. This gives a very high degree of security of the private key and avoids having to sign every claim individually. The subject of a claim message can be signed as well, enabling optimization of the DTP search engine. For strong key management, a subject signing technique is used for replaceable identities. A person can create a highly secure key and publish the public key for others to trust. Then create a less secure private key for daily use, that is trusted by the highly secure key. The claim message contains a signature for both the issuer (high secure key) and subject (daily key). A DTP search server regards this as a tunneling claim and channel through it without increasing the degree. This way if the daily key is compromised, it can be replaced by a new daily key and trust network copied. This happens by distrusting the old daily key and trusting the new daily key. After propagation, all DTP search servers ignore the old daily key and use the new daily key. From others perspective, it looks that the web of trust network is unchanged as they use the highly secure public key, that routes through to the new daily key. Identity The identities on the DTP system is based on private/public key algorithms, creating a pseudonymous identity. Connecting a users personal information to that identity is done by others issuing claim message to the identity. One can claim information about oneself, but this may not be considered in the DTP search rules and therefore not show up in search results. Because claims about one's identity are given by others, like the government, company or friends, it is possible to present proof of identity without revealing more information than needed. A government issues trust to an identity with the claims like name and birthday. This trust is however not shared publicly but kept securely private on government servers. However, the government can issue proofs of the trust, like over a certain age and so on. The government has published a general public key, that others can trust in their networks. So now when for example proof of age has to be presented, then a search on the identity with the query for a certain age can be issued. The result is a chain of proof from the government to the identity, proving that the chain is valid. Anyone that trusts the government public key can now verify and accept the proof of a certain age because the chain of claims is valid all the way from the verifier to the subject, though the government. A subject can choose to store the chain of proof locally and present this whenever needed, avoiding any search on a government server in the case of verification. Multiple different proofs can be stored locally and only present the necessary ones without exposing more data than needed. As the proofs only state a fact and not the value itself, then the concerns for storing the data securely can be more relaxed. Searches on information on identities held by the government can be limited to only allowing the identities to make queries about them self, prevent information phishing. Items do not have an identity and therefore cannot issue trust by themselves. Trusting items can be done by using a unique value of the item, as a source of identity. As the identity is generated from a hash value of the unique value source, it is not possible to issue trust on behalf of that item, only receiving trust is possible. Incentive The system assumes it is generally harder to gain trust than to lose trust, both for humans and machines. This gives the users an incentive to guard their trust highly, as they otherwise may lose attention, influence, and opportunities. The trust network is based on a subjective viewpoint, as each subject has individual preferences of trust in others and items. By assigning trust to everything that is possible to represent digitally; it becomes possible to leverage on the viewpoints and opinions on everything from trusted sources, this enables users to get information about entities that will help to make decisions about that entity. Imagine news articles flagged for their trustworthiness, products in a supermarket trusted for their quality, and the car mechanic rated for services, all within a single users personal trust network. For an entity to keep trust already gained, creates an incentive to avoid bad behavior and keep up its predictability, this also applies to DTP servers to avoid being selective in hosting and sharing trust for uses, as this otherwise leads to distrust of the server and ultimate losing its customer base. Therefore the distribution, sharing, and searching for trust, rely on this incentive. Potential Attacks The strength of the system is that it is subjective and the trust is self-provable authentic. This makes it hard to perform attacks where a massive number of claim messages are issued from a bot network, trying to influence a specific subject. Because it only has a minimal effect for the users of the system besides taking up resources, as nobody trusts the spam in the first place and therefore the spam will no influence the user's networks of trust. Entities trusting the spam can quickly be identified and distrusted if necessary to close of the spam from the personal network. The prediction attack is from users trying to make predictions by creating a million \u2018guessing\u2019 messages and not / barely share them, to make it seems that the issuer has created a correct trust when presented in the future scenarios. This can be countered by looking at packages containing the trust when trusts are published on a DTP server, they are time stamped and package into a Trust packages that in return are signed by the server and then shared with other servers. If no packages with the trust in it, can be found on any servers, then the trust may seem not to be shared publicly and therefore may have limited historical value. Furthermore, if the identity behind the prediction has a minimal history one may assume that it has been fabricated to this scenario. The excluding trust attack is when a DTP search server selectively chooses not to include a specific trust for some reason, and therefore the search result still returns old or no claims on purpose. In this case, because of openly sharing of claims, enables them to be stored by multiple servers, and therefore reasonably easy to detect if a server has not included the latest claims by comparing data from different servers. When a server does not in a reasonably timely manner demonstrate a willingness to include specific claims, it can be distrusted by other servers and users, effectively render the server useless in a broader sense as nobody trust it anymore. Conclusion With the rise of the Internet, the possibility to efficiently extend the contact surface with the rest of the world was a reality. However, this presented some problems as there is no general way to transform the trust from a near physical environment onto the digital space in a broader digital form. Merely keeping track of the trust of everybody is an overwhelming task. The solution is to leverage the trust of others trusted parties subjectively chosen and stored digitally. The DTP system aims to solve this problem by defining a protocol for sharing and searching on trust within subjects own space. The DTP system is designed to be very simple, lightweight, decentralized and open source. It is possible to extend already established systems without changing them because the DTP protocol is fundamentally a metadata system. The DTP system is intended to be used as the foundation for trust, identity, and security. The DTP system is a tool for the users to navigate in the sea of information and be able to selectively choose information relevant based the users own trusted networks and for users to do self-sovereign identity management without the need to store credential documentation privately. References [1] Verifiable Claims Data Model and Representations https://www.w3.org/TR/verifiable-claims-data-model/ [2] Trust (emotion). https://en.wikipedia.org/wiki/Trust_(emotion) [3] Trust. https://ldapwiki.com/wiki/Trust [4] Identity is an Edge Protocol. https://static1.squarespace.com/static/55f73743e4b051cfcc0b02cf/t/59009d56f5e23188266086e0/1493212506000/Identity%2Bis%2Ban%2BEdge%2BProtocol+2.pdf [5] SPKI/SDSI Certificates. http://theworld.com/~cme/spki.txt [6] Decentralized Public Key Infrastructure. https://danubetech.com/download/dpki.pdf [7] Bitcoin: A Peer-to-Peer Electronic Cash System. https://bitcoin.org/bitcoin.pdf [8] Event Sourcing. https://martinfowler.com/eaaDev/EventSourcing.html Copyright \u00a9 2019 Digital Trust Protocol Developers Permission is hereby granted, free of charge, to any person obtaining a copy of this document and software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Digital Trust Protocol"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#digital-trust-protocol","text":"C. Keutmann & T. Pastoor DigitalTrustProtocol.org postmaster@digitaltrustprotocol.org","title":"Digital Trust Protocol"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#abstract","text":"The Digital Trust Protocol (DTP) is a solution for the handling of trust in the digital space. The protocol is broadly designed to work with all aspects of trust; this includes identity, reputation, and security. The protocol is designed to be independent of specific systems and is very minimalistic. The intention is to extend the protocol with layers above rather than direct implementations. The protocol claim message data is designed to be self-provable of authenticity, by use of private/public key algorithms and the blockchain timestamping feature. This enables the protocol to be decentralized without a need for central authority, as servers can use a peer-to-peer communication system and verify all the data it receives individually, without having to rely on the sender. The Protocol allows anyone, including automated software, to issue their own cryptographic identities, for the use of trust and reputation and be able to verify those of others, without the need for a trusted third party. Users issue claims to other identities, and this way build a personal web of trust network. Users can search for claims on subjects by use of the user's trust network. The search only include results from trusted sources, claims from untrusted sources are not returned. The system is resistant to attacks like Sybil and bot networks as they are usually not a part of the uses networks and therefore not considered when evaluating searches. The system works on the incentive of the user\u2019s reputation and established trust, to be a good actor within their networks, since no one will listen to them if no one trusts them. It is possible to trust anything digitally, and therefore the protocol suitable to be used for anything that requires some degree of trust shared between systems related or unrelated. The DTP protocol is ideal for identity, reputation and security management that needs to be used in a broader sense without a central authority and furthermore it can augment existing systems already in place.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#introduction","text":"Trust in the digital world of today are mostly centralized and siloed. Many reputation systems only work on specific platforms. There are usually no sharing of trust and reputation between the platforms, and this makes it hard to keeping track of identities reputation across systems. Many platforms do even not have any form of reputation system in place, like many forums, news sites and so on. Therefore keeping a personal record of each identity on many different contents providing platforms becomes a time consuming and near impossible task, as the identity of the same person may differ from platform to platform. Many existing solutions only offer a specific kind of trust, like reputation and have no possibility for diversity in expressing the kind of trust a user may have of an identity or item. The Digital Trust Protocol (DTP) is designed to provide an open protocol for the handling of trust of any kind in the digital space, in a decentralized way. DTP is a decentralized web of trust protocol. The DTP system works on the assumption that it is harder to gain trust than to losing trust. This is the basis for the system to function properly, as it creates an incentive for a user to act appropriately to maintain the reputation. Steadily losing trust will most likely result in loss of attention and influence. A simple definition of trust is it is an expression of a belief or experience that a certain identity will act in a repeatable or predictable pattern. Simply DTP defines trust as proof of a predictable, repeatable pattern, and that is subjective to the observer. This simple pattern enables development of relatively simple rules that are easy to implement into autonomous programs, and by these rules, they can issue trust on their own. Identity in DTP is based on trust. Identity is something that is internal and external. The internal identity is about self-realization and is not the subject of the DTP protocol. The external identity is properties like name, age, birthplace, citizenship, and other recognizable properties. They are all claims given by others. Usually, a state authority recognizes its citizens by name, address, identity number, and other credentials. Verifying the identity of a person is to use a trusted authority as a source for the claims, and not only to rely on the subject. In DTP external identity properties are provided by claims from others, not from oneself. Just like it is not possible to issue a valid driver license to yourself at home without any authority involvement. In DTP an authority can be any entity, like the government, companies, organizations, and friends. However, the authority still has to be recognized by others to have any relevance when asking for trust. Security is claims given to an entity. Therefore the DTP can act as an access control system for identities across platforms. A claim can act as a simple ticket claim for specific an event, issued to a specific identity, thereby preventing reselling. Claims also include time properties to handle when the claims are active. Rating is claims specifying a specific value for an entity. It is possible to issue rating claims to specific items like a car or printer. However, cars and printers are not able to issue a public identity for themselves. Therefore the name or a serial number of the item can be used to generate the identity address of the item. Because there was no private/public key generation involved in the process of generating the identity of the item, nobody can issue claims on behalf of an item. It is possible to trust anything digitally and therefore the protocol suitable to be used for anything that requires some degree of trust shared between systems not related and without the need for any central authority. DTP protocol defines a simple piece of data called a claim message. The claim message data is designed to be self-provable of authenticity by use of private/public key algorithms and the blockchain timestamping feature, and this ensures that nobody else will be able to issue the same claim message without the private key used to sign the message. The timestamp feature ensures that the claim can prove its existence at a specific time, thereby preventing backdating claim messages. The self-provable authenticity feature of a claim message enables decentralization without the need for central authority, as servers can use peer-to-peer communication and still be able to verify all the data they receive, without having to trust the sender. The DTP servers that host the DTP web of trust networks are based on the claim messages. They serve users requests to solve the queries on different subjects and targets. The servers can be public or private and still share data with anyone. The DTP servers can focus on specific claim types or host broader networks. The protocol for sharing of data between servers is not a part of the DTP protocol as this is considered a layer on top of the DTP protocol. Because the claim message is self-provable authentic, the servers can be sure that no matter how they got the data, it is verifiable against tampering. Per default the claim messages are not encrypted, only signed by the issuer, making the claim readable by anyone. When claim messages need to be private and secret, they do not need to be encrypted, as the claims are stored on closed servers and not publish however, others can still trust the public key of a closed network, enabling self-sovereign identity management, as it makes it possible to prove chain of trusts without exposing more details than needed.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#architecture","text":"The DTP protocol specifies generic data structure and a rule set for searching on these data in a specific way. The data structure does not specify the algorithms for calculating the private/public keys, signing scripts and hash values for ID\u2019s, this is up for the implementation of the protocol. The system consists of a client-server model, together with a clearly defined data structure. The generation of cryptographic key-pairs, and signing messages usually happens on the client. The messages are sent to a DTP server of choice, that timestamps each message and constructs a trust graph from all collected messages. The DTP protocol focuses on defining a common data structure and how to search on the data, but does not limit the types of trust and algorithms used to sign the trust. Event Sourcing is the base data pattern of the DTP protocol, where each claim defines a property change. The claim only defines a single atomic value for a single target from a single source, and are extended with a number of properties like scope and time. The principle of only changing of atomic values makes it easy to maintain the data in a decentralized way because it does not require previous knowledge of the state of an object. Bundling multiple claim values like Name, address, email into a single claim, introduces a higher complexity of implementation without any significant benefits. The claims are always bundled together into a package with the possibility of signing all the claims in one go before submitting it to the DTP server.","title":"Architecture"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#claim","text":"A claim message is a relatively small piece of data with a flat data structure that is self-provable authentic and defines a single value issued to a subject. After a claim message is created, a hash ID is calculated and then signed by the issuer. Usually, the timestamp is added by the DTP server. A claim message needs to contain at least the issuer signature and a timestamp, for other DTP servers to verify that the received claim message is valid. Claim messages can be of any type, and therefore the DTP protocol specifies a basic set of claim types for standardization purposes. When issuing and signing claims, the signature can be base on a simple private/public key algorithm principle, or it is also possible to provide a script with multiple signatures for the claim. It is also possible to provide any custom non-standard script/type if needed, but then this has to be supported by any DTP server that serves the claim in its web of trust graph. The value part of the claim defines a statement towards a subject. The value is what the issuer thinks of a subject. For simplifications, it is not possible to add more than one value per claim. Issuing multiple values to the same subject is done by issuing multiple claim messages. The single value per claim enables the system to update or replace individual claims with new ones as the need occurs. Updating or removing claims happens by issuing new claims messages to the same target with a new value or an empty value to simulate removal. The package that contains the claims, features techniques to enable single signing and templating for mitigating the bloat of having to issue multiple claims of the same type towards the same subject. It's possible to extract individual claims out of the package and add them into new updated packages and still keep the proof of authenticity with each claim. This way old replaced claims can be discarded and existing and new claims a combined into new packages before sharing. It is possible to add a signature to the subject part of the claim message, proving that the issuer is also in control of the subject. This enables the DTP servers to change the behavior of the search algorithms used for finding trusts and enable features like tunneling for key management. The claim message can have an activation and expiration date, enabling them only to be active at certain times. A use case would be that the trust is used as a ticket system or to provide secure access to systems for a limited time. Claim messages contain a scope property for limiting the context of the claim, as the claim given to specific web platforms may not be relevant on other platforms. This is very beneficial to limit the amount of trust hosted on a DTP server. A claim message is hosted in a package when submitted or transported around. Example of a claim package { \"algorithm\": \"double256.merkle.dtp1\", \"id\": \"xSJLFeSNTYn4AHjvbz4rCh7AFz1ZJyFwZW1M8GJy3g4=\", \"created\": 1548757308, \"claims\": [ { \"id\": \"Lp2+ebZ9U0LRv/Z/xnx8pPVhbkRdoyscsdcY+n+StaI=\", \"created\": 1548757166, \"issuer\": { \"type\": \"secp256k1-pkh\", \"id\": \"1LRNDp2HfVKc1FsCJwbJwMhtssXL8fdTx2\", \"signature\": \"H0+9PhNSxm6ySMShlkuC1ZCTuYdbxSHquP1KCmb/M4LxZwOSFoHUy+fHcZiiI/zjheVJvrQbM/1X4Eg6+wTx3N0=\" }, \"subject\": { \"type\": \"name\", \"id\": \"TrustProtocol\" }, \"type\": \"binary.trust.dtp1\", \"value\": \"true\", \"scope\": \"twitter.com\" }, { \"id\": \"3atbj7U7RMSfL8OQFaGDzYlpGRbpnm3kHItMzFXLjMk=\", \"created\": 1548757166, \"issuer\": { \"type\": \"secp256k1-pkh\", \"id\": \"1LRNDp2HfVKc1FsCJwbJwMhtssXL8fdTx2\", \"signature\": \"ICWN+jH6Gn8+c7FJ0rSk3SPRmRI97FiXVtEdnm2S0utJOpZaE+WqNDII1RhHwLDlgVxlyNFpkp7w3K+WCBJXhPg=\" }, \"subject\": { \"type\": \"id\", \"id\": \"1NKtqyDndSvQ87ANAHsWR5XGL66pXkZxc8\" }, \"type\": \"binary.trust.dtp1\", \"value\": \"true\", \"scope\": \"\" }, { \"id\": \"QlbmdW4R1Ctth4Ah4xIVNzcflhlyr4oFaZFtIASi9HA=\", \"created\": 1548757169, \"issuer\": { \"type\": \"secp256k1-pkh\", \"id\": \"1LRNDp2HfVKc1FsCJwbJwMhtssXL8fdTx2\", \"signature\": \"IPe6xOj4ofy1QWSB646AHvKPa7a7mk3ko6D/dKdk15jkVZ4wLt5ci1eCJrj2j4uWSM+JYFlVy+Z1Sa8LHMDNK8U=\" }, \"subject\": { \"type\": \"id\", \"id\": \"1NKtqyDndSvQ87ANAHsWR5XGL66pXkZxc8\" }, \"type\": \"alias.identity.dtp1\", \"value\": \"Digital Trust Protocol\", \"scope\": \"twitter.com\" } ], \"timestamps\": [ { \"blockchain\": \"btctest\", \"algorithm\": \"secp256k1-double256.merkle.dtp1\", \"source\": \"xSJLFeSNTYn4AHjvbz4rCh7AFz1ZJyFwZW1M8GJy3g4=\", \"registered\": 1548757308 } ], \"server\": { \"type\": \"secp256k1-pkh\", \"id\": \"1LCGKp8zkmU3jBSdsRNfLzqsJH3qSzSxyk\", \"signature\": \"IBtutLOpiB5rcpgvSxdNbvB5OpVM5yp8t1dO3FVWvgPWdX+EvM3eQrfyBNeOQYil7K/xuzsgXYNYE7XjR/byhBo=\" } } In the example above, three claims have been created and added to a package. The claims trust a subject named \u201cTrustProtocol\u201d and a subject with an ID \u201c1NKtqyDndSvQ87ANAHsWR5XGL66pXkZxc8\u201d and its alias of \u201cDigital Trust Protocol\u201d. These three claims are trusting the same entity on Twitter, where the entity has the public key \u201c1NKtqyDndSvQ87ANAHsWR5XGL66pXkZxc8\u201d, but also the twitter name \u201cTrustProtocol\u201d is trusted. Finally, an alias claim issued to the public key, connecting the key with an alias. Note that the scope property is empty for the claim that trusts the ID, this is because the trust is global and not limited to a scope. The server property in the package is created and signed by the DTP server receiving the claims from the client. This way other DTP servers can identify the source of the package. Lastly, the DTP server timestamps the claims by use of a publicly available blockchain to ensure that claims can prove their time of existence.","title":"Claim"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#package","text":"When DTP server shares claim messages with other DTP servers, they use a package to contain the claim messages; this enables them to create an array of claim they have received from users and timestamp them all at once by using a Merkle tree algorithm. DTP servers also sign the package with their own identity, enabling other DTP servers to be able to verify the origin of the package. By having DTP packaging and signing claims, it helps to confirm the existence of the claims, further enhancing the proof of the existence of the claims, because the claim message has now been seen by at least one DTP server and shared in a package signed by the server. The package includes a template property to reduce storage needed by providing shared templates for claims to use. Even key signing is supported within the template. It is possible to add multiple claims to a single package and sign the package once. The signature is based on the id calculated from a Merkle tree of all the id\u2019s from the claims, and the timestamp works in the same way. This enables claims to be separated from their original package and added to other newer packages, but still be able to be self-provable authentic. The Merkle tree id feature enables repackaging of the claims into new packages as individual claims to get replaced, avoiding having to keep old replaced claims data with still active claims.","title":"Package"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#dtp-server","text":"The DTP can have multiple roles in the DTP protocol, as they can act as a timestamp server and as a search engine for the DTP web of trust graph. The timestamp role is to ensure that claim messages can prove their existence at a specific time. Furthermore, the server can package up the claim messages into packages that the server signs on its own, then sharing the package with other DTP servers, improving the claims proof of existence. The DTP server stores the claim messages in a regular database and not on a blockchain. No blockchain is needed for storage as there is no need for a consensus of the state of the ledger because it is not possible to re-spent the same claim given to a subject on to other subjects; therefore the amount of trust that can be issued are endless. Only the timestamp feature uses a blockchain, and by use of the Merkle tree algorithm, it is possible to timestamp a large number of claim messages in one single transaction on a blockchain.","title":"DTP server"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#web-of-trust","text":"The DTP web of trust graph role of the DTP server is to serve user requests on the user personal trust network. The claim message in itself is a plain value carrier, and therefore a graph structure is needed to be able to search through relevant trust based on a users perspective. A DTP server collects claim messages that are relevant for its users and build up a graph structure that is searchable. The DTP protocol defines a standard search and result structure for search and result, and it also defines a standard set of rules on how to search through the web of trust graph regarding the perspective of the user. A tunneling claim is where the issuer of a claim have signed both the issuer key and the subject key, thereby proving control of both keys. A tunneling claim do not enforce an incremental count of degree when calculating the search, but just follow through. It enables a deeper level of search beyond three degrees, and as the issuer usually only creates a few tunneling claims, it will limiting the number of claims to search through. A tunneling claim is suited for key management as a public identity with a private key highly secured and rarely used, can delegate trust to a daily key, by the tunneling claim. It makes it easy to have high security with convenient key management; this enables organizations to have a hierarchy of keys for divisions, departments, and employees. Trust issued by an employee can be linked all the way back to the organization's public key by a chain of claims, and therefore only the organizations public key has to be trusted when validating the proof of credentials, even that is was an employee that issued the credentials. This also makes it easy to replace keys when employees change positions or keys gets compromised. A simple scenario of issuing trust and searching for it. Client A issue trust to B and B issue trust to Client C Client A searches on Client C and gets the result from the DTP search server. The result contains the trust from A to B and the trust from B to C. It is up to the client to decide how to interpret the search result as the may be multiple results on from different identities having different opinions. Therefore the server presents the data, and the client handles it from there on. Some basic rules of the search engine are that searches never go further than 3 degrees away from the user, as this quickly become irrelevant the further out in the network the search goes. Another rule is that the only result from the same degree is returned and the search does not continue further onto the next degree when a match is found. Only trusted identities on the network are followed and distrusted identities are ignored. The chains of trust are the basics of the system, as they enable for verification of proof of identity credentials by following the trust chains generated by authorities. The network of DTP servers and scalability The usability and value of a web of trust network increase with more users join the system. The DTP system is designed to scale up to a very high number of identities and claim messages supporting a global implementation. The nature of the claim message is that they are self-provable authentic and therefore the DTP servers do not need to rely on a central authority but can share the data directly in a peer to peer network. The DTP protocol does not specify the communications between servers as this is regarded as a layer on top of the DTP protocol. The DTP server may offer different scopes for trust services; therefore the trust messages include a type and scope properties to help to filter and limiting the trust before it is verified and included in a servers DTP web of trust graph. A claim message is designed to be very small usually under one kilobyte; therefore the properties of the claim naturally have a limited size. The small size helps claim messages propagate quickly through the system and limit attack surfaces. It is estimated that it is possible to have DTP search servers with reasonably low cost that can handle web of trust graphs containing millions of identities and trusts with effective memory management. Servers can become more specialized and limiting the type and scope of trusts as the networks get larger. It is possible that the clients will have a list of dedicated DTP search server for different scopes of trust and choose the right server in the right context.","title":"Web of Trust"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#the-client","text":"The client issues trust and interpret the search results from the DTP server. It is up to each client to decide how to read the search result as this may have a different meaning in different scenarios. When using the DTP protocol, the client does not need to provide an identity to be able to use the web of trust graph. Anonymous users can rely on other established identities and use their networks, this could be friends or well-known entities like online search engines, and security firms specialized in web security. However, without a personal DTP identity, it is not possible for others to trust the identity and establish a personal web of trust network. Key management The issuer of trust uses a private/public key algorithms to generate an identity in the form of a public key and to sign the claim messages. However, it is also possible to use a script like languages for multi signatures for proving ownership. After a daily session of issuing claims, they would be bundled into a package and signed by a possible hardware device before submitting to a DTP server. This gives a very high degree of security of the private key and avoids having to sign every claim individually. The subject of a claim message can be signed as well, enabling optimization of the DTP search engine. For strong key management, a subject signing technique is used for replaceable identities. A person can create a highly secure key and publish the public key for others to trust. Then create a less secure private key for daily use, that is trusted by the highly secure key. The claim message contains a signature for both the issuer (high secure key) and subject (daily key). A DTP search server regards this as a tunneling claim and channel through it without increasing the degree. This way if the daily key is compromised, it can be replaced by a new daily key and trust network copied. This happens by distrusting the old daily key and trusting the new daily key. After propagation, all DTP search servers ignore the old daily key and use the new daily key. From others perspective, it looks that the web of trust network is unchanged as they use the highly secure public key, that routes through to the new daily key.","title":"The client"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#identity","text":"The identities on the DTP system is based on private/public key algorithms, creating a pseudonymous identity. Connecting a users personal information to that identity is done by others issuing claim message to the identity. One can claim information about oneself, but this may not be considered in the DTP search rules and therefore not show up in search results. Because claims about one's identity are given by others, like the government, company or friends, it is possible to present proof of identity without revealing more information than needed. A government issues trust to an identity with the claims like name and birthday. This trust is however not shared publicly but kept securely private on government servers. However, the government can issue proofs of the trust, like over a certain age and so on. The government has published a general public key, that others can trust in their networks. So now when for example proof of age has to be presented, then a search on the identity with the query for a certain age can be issued. The result is a chain of proof from the government to the identity, proving that the chain is valid. Anyone that trusts the government public key can now verify and accept the proof of a certain age because the chain of claims is valid all the way from the verifier to the subject, though the government. A subject can choose to store the chain of proof locally and present this whenever needed, avoiding any search on a government server in the case of verification. Multiple different proofs can be stored locally and only present the necessary ones without exposing more data than needed. As the proofs only state a fact and not the value itself, then the concerns for storing the data securely can be more relaxed. Searches on information on identities held by the government can be limited to only allowing the identities to make queries about them self, prevent information phishing. Items do not have an identity and therefore cannot issue trust by themselves. Trusting items can be done by using a unique value of the item, as a source of identity. As the identity is generated from a hash value of the unique value source, it is not possible to issue trust on behalf of that item, only receiving trust is possible.","title":"Identity"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#incentive","text":"The system assumes it is generally harder to gain trust than to lose trust, both for humans and machines. This gives the users an incentive to guard their trust highly, as they otherwise may lose attention, influence, and opportunities. The trust network is based on a subjective viewpoint, as each subject has individual preferences of trust in others and items. By assigning trust to everything that is possible to represent digitally; it becomes possible to leverage on the viewpoints and opinions on everything from trusted sources, this enables users to get information about entities that will help to make decisions about that entity. Imagine news articles flagged for their trustworthiness, products in a supermarket trusted for their quality, and the car mechanic rated for services, all within a single users personal trust network. For an entity to keep trust already gained, creates an incentive to avoid bad behavior and keep up its predictability, this also applies to DTP servers to avoid being selective in hosting and sharing trust for uses, as this otherwise leads to distrust of the server and ultimate losing its customer base. Therefore the distribution, sharing, and searching for trust, rely on this incentive. Potential Attacks The strength of the system is that it is subjective and the trust is self-provable authentic. This makes it hard to perform attacks where a massive number of claim messages are issued from a bot network, trying to influence a specific subject. Because it only has a minimal effect for the users of the system besides taking up resources, as nobody trusts the spam in the first place and therefore the spam will no influence the user's networks of trust. Entities trusting the spam can quickly be identified and distrusted if necessary to close of the spam from the personal network. The prediction attack is from users trying to make predictions by creating a million \u2018guessing\u2019 messages and not / barely share them, to make it seems that the issuer has created a correct trust when presented in the future scenarios. This can be countered by looking at packages containing the trust when trusts are published on a DTP server, they are time stamped and package into a Trust packages that in return are signed by the server and then shared with other servers. If no packages with the trust in it, can be found on any servers, then the trust may seem not to be shared publicly and therefore may have limited historical value. Furthermore, if the identity behind the prediction has a minimal history one may assume that it has been fabricated to this scenario. The excluding trust attack is when a DTP search server selectively chooses not to include a specific trust for some reason, and therefore the search result still returns old or no claims on purpose. In this case, because of openly sharing of claims, enables them to be stored by multiple servers, and therefore reasonably easy to detect if a server has not included the latest claims by comparing data from different servers. When a server does not in a reasonably timely manner demonstrate a willingness to include specific claims, it can be distrusted by other servers and users, effectively render the server useless in a broader sense as nobody trust it anymore.","title":"Incentive"},{"location":"rwot8/topics-and-advance-readings/DigitalTrustProtocol/#conclusion","text":"With the rise of the Internet, the possibility to efficiently extend the contact surface with the rest of the world was a reality. However, this presented some problems as there is no general way to transform the trust from a near physical environment onto the digital space in a broader digital form. Merely keeping track of the trust of everybody is an overwhelming task. The solution is to leverage the trust of others trusted parties subjectively chosen and stored digitally. The DTP system aims to solve this problem by defining a protocol for sharing and searching on trust within subjects own space. The DTP system is designed to be very simple, lightweight, decentralized and open source. It is possible to extend already established systems without changing them because the DTP protocol is fundamentally a metadata system. The DTP system is intended to be used as the foundation for trust, identity, and security. The DTP system is a tool for the users to navigate in the sea of information and be able to selectively choose information relevant based the users own trusted networks and for users to do self-sovereign identity management without the need to store credential documentation privately. References [1] Verifiable Claims Data Model and Representations https://www.w3.org/TR/verifiable-claims-data-model/ [2] Trust (emotion). https://en.wikipedia.org/wiki/Trust_(emotion) [3] Trust. https://ldapwiki.com/wiki/Trust [4] Identity is an Edge Protocol. https://static1.squarespace.com/static/55f73743e4b051cfcc0b02cf/t/59009d56f5e23188266086e0/1493212506000/Identity%2Bis%2Ban%2BEdge%2BProtocol+2.pdf [5] SPKI/SDSI Certificates. http://theworld.com/~cme/spki.txt [6] Decentralized Public Key Infrastructure. https://danubetech.com/download/dpki.pdf [7] Bitcoin: A Peer-to-Peer Electronic Cash System. https://bitcoin.org/bitcoin.pdf [8] Event Sourcing. https://martinfowler.com/eaaDev/EventSourcing.html Copyright \u00a9 2019 Digital Trust Protocol Developers Permission is hereby granted, free of charge, to any person obtaining a copy of this document and software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Conclusion"},{"location":"rwot8/topics-and-advance-readings/P2P-DID/","text":"Peer to Peer DIDs by Brent Zundel (with much input from Daniel Hardman) Abstract Multiple DID methods are under development, each based on a different support structure (e.g. Veres One, Sovrin, IPFS, etc). During development of standard DID method specifications, and while exploring certain DID-related use cases, there has arisen the notion of a \"ledgerless\" DID, i.e. a DID that is not anchored to any infrastructure. We call these peer to peer (P2P) DIDs because the first use cases that explored the need for ledgerless DID were peer to peer messaging scenarios. Proposal Work on a DID Method spec for P2P DIDs as a special subset of DIDs that are not universally resolvable on some ledger or central storage infrastructure, but only within the group where they are used. Draft of P2P DID Method Spec. Show how a P2P DID could be added/converted to an existing DID method. Requirements P2P DIDs should be probabilistically unique among the set of all possible P2P DIDs. A P2P DID may be later made public by anchoring on infrastructure and converting into a DID that resolves according to the DID method specification for that infrastructure. A P2P DID may be anchored to multiple infrastructures simultaneously.","title":"Peer to Peer DIDs"},{"location":"rwot8/topics-and-advance-readings/P2P-DID/#peer-to-peer-dids","text":"by Brent Zundel (with much input from Daniel Hardman)","title":"Peer to Peer DIDs"},{"location":"rwot8/topics-and-advance-readings/P2P-DID/#abstract","text":"Multiple DID methods are under development, each based on a different support structure (e.g. Veres One, Sovrin, IPFS, etc). During development of standard DID method specifications, and while exploring certain DID-related use cases, there has arisen the notion of a \"ledgerless\" DID, i.e. a DID that is not anchored to any infrastructure. We call these peer to peer (P2P) DIDs because the first use cases that explored the need for ledgerless DID were peer to peer messaging scenarios.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/P2P-DID/#proposal","text":"Work on a DID Method spec for P2P DIDs as a special subset of DIDs that are not universally resolvable on some ledger or central storage infrastructure, but only within the group where they are used. Draft of P2P DID Method Spec. Show how a P2P DID could be added/converted to an existing DID method.","title":"Proposal"},{"location":"rwot8/topics-and-advance-readings/P2P-DID/#requirements","text":"P2P DIDs should be probabilistically unique among the set of all possible P2P DIDs. A P2P DID may be later made public by anchoring on infrastructure and converting into a DID that resolves according to the DID method specification for that infrastructure. A P2P DID may be anchored to multiple infrastructures simultaneously.","title":"Requirements"},{"location":"rwot8/topics-and-advance-readings/Proof_of_Key_Ownership_with_OpenID_Connect_Self-Issued_Identities/","text":"Proof of Key Ownership with OpenID Connect Self-Issued Identities Proving ownership of a DID requires proving ownership of a private key corresponding to a public key for the DID. Of course, this could be done with a new DID-specific protocol. However, there already exist standard protocols for proving ownership of a public/private key pair. The OpenID Connect specification defines Self-Issued Identities in Section 7. A self-issued identity is represented by a public/private key pair. Logging in with the self-issued identity proves control of the private key. Microsoft is experimenting with using self-issued identities to prove ownership of DIDs. This is straightforward. The DID key is used as the key for the self-issued identity. The self-issued identity is validated in the standard way. In addition, a \"did\" claim containing the DID identifier is added to the self-issued ID Token. This enables relying parties that understand that the self-issued identity is also a DID to perform DID operations after control of the key has been verified using OpenID Connect. I look forward to discussing this potential DID validation approach with the workshop participants. Author: Michael B. Jones https://self-issued.info/ , @selfissued","title":"Proof of Key Ownership with OpenID Connect Self-Issued Identities"},{"location":"rwot8/topics-and-advance-readings/Proof_of_Key_Ownership_with_OpenID_Connect_Self-Issued_Identities/#proof-of-key-ownership-with-openid-connect-self-issued-identities","text":"Proving ownership of a DID requires proving ownership of a private key corresponding to a public key for the DID. Of course, this could be done with a new DID-specific protocol. However, there already exist standard protocols for proving ownership of a public/private key pair. The OpenID Connect specification defines Self-Issued Identities in Section 7. A self-issued identity is represented by a public/private key pair. Logging in with the self-issued identity proves control of the private key. Microsoft is experimenting with using self-issued identities to prove ownership of DIDs. This is straightforward. The DID key is used as the key for the self-issued identity. The self-issued identity is validated in the standard way. In addition, a \"did\" claim containing the DID identifier is added to the self-issued ID Token. This enables relying parties that understand that the self-issued identity is also a DID to perform DID operations after control of the key has been verified using OpenID Connect. I look forward to discussing this potential DID validation approach with the workshop participants. Author: Michael B. Jones https://self-issued.info/ , @selfissued","title":"Proof of Key Ownership with OpenID Connect Self-Issued Identities"},{"location":"rwot8/topics-and-advance-readings/Providing_Decentralized_Identity_to_the_most_vulnerable_populations/","text":"Providing Decentralized Identity to the most vulnerable populations Draft: Rebooting Web of Trust VIII Topic Paper Authors Jeremi Joslin jeremi@idpass.org Greg Martel - greg@idpass.org Hailey Park - hailey@idpass.org Problem Decentralized identity solution has been mainly designed for the developed world and most implementation requires the need for expensive devices such as smartphones. However, by 2021, it is estimated that only 40% of the world\u2019s population will own one. In contrast, there are one billion invisible people without an identity and a large majority still remains under the poverty line. Without a verifiable identity, one-seventh of the global population lacks the rights to access essential services such as Citizenship, Education, Justice, Health Care, Banking and Insurance. Moreover, in order to provide humanitarian aid to this population biometric is commonly captured to identify individuals. Although, from our experience, digital identity and biometric verification is often achieved by using a centralized identity management system which increases privacy concerns and requires an internet connection. Idea This is the reason why we are proposing a smart card based solution that leverages on W3C standards of Decentralized Identifiers and Verifiable Credentials. The card will be used to sign transaction and securely store the private keys and Verifiable Credentials. Access Management One of the drawbacks of using a smart card is the lack of a user interface on the card, making it more difficult to selectively protect access to features and credentials stored on the card. The following are authentication mechanism that could be used for our smart card: - PIN code: one of the most common authentication methods in developed countries. However, based on our experiences, it was not considered as the preferred solutions due to low literacy and numeracy levels in some of the remote areas. - Biometrics match-on-card: eliminates the need for a central database by both storing and matching fingerprint, iris and face biometrics data directly on a smartcard. - Cryptography: authenticate the Point of Services. - Time Delay: can be used to differentiated between a simple tap, multi-tap or long-tap of the card. Through the use and combination of these authentication processes, two possible solutions could be implemented: Use of trusted devices like in the case of the credit card network where the user trust the terminal shows them the amount being deducted from the card and executes the action. This can be achieved by using cryptography and having the terminal authenticate to the card. The main advantage of this would be making it easier for the user to interact with the card, but to access their own data, they need to use a certified terminal. Use of a combination of authentication processes such as multiple PIN codes, fingerprint match or a delay to unlock access to different information. The advantage of this solution is that the user has increased control over their data as any device can be used to access it, but things such as keeping track of multiple PIN codes might be inconvenient for some. These methods are not exclusive, and can actually be used in conjunction with others depending on how they are used.","title":"Providing Decentralized Identity to the most vulnerable populations"},{"location":"rwot8/topics-and-advance-readings/Providing_Decentralized_Identity_to_the_most_vulnerable_populations/#providing-decentralized-identity-to-the-most-vulnerable-populations","text":"","title":"Providing Decentralized Identity to the most vulnerable populations"},{"location":"rwot8/topics-and-advance-readings/Providing_Decentralized_Identity_to_the_most_vulnerable_populations/#draft-rebooting-web-of-trust-viii-topic-paper","text":"","title":"Draft: Rebooting Web of Trust VIII Topic Paper"},{"location":"rwot8/topics-and-advance-readings/Providing_Decentralized_Identity_to_the_most_vulnerable_populations/#authors","text":"Jeremi Joslin jeremi@idpass.org Greg Martel - greg@idpass.org Hailey Park - hailey@idpass.org","title":"Authors"},{"location":"rwot8/topics-and-advance-readings/Providing_Decentralized_Identity_to_the_most_vulnerable_populations/#problem","text":"Decentralized identity solution has been mainly designed for the developed world and most implementation requires the need for expensive devices such as smartphones. However, by 2021, it is estimated that only 40% of the world\u2019s population will own one. In contrast, there are one billion invisible people without an identity and a large majority still remains under the poverty line. Without a verifiable identity, one-seventh of the global population lacks the rights to access essential services such as Citizenship, Education, Justice, Health Care, Banking and Insurance. Moreover, in order to provide humanitarian aid to this population biometric is commonly captured to identify individuals. Although, from our experience, digital identity and biometric verification is often achieved by using a centralized identity management system which increases privacy concerns and requires an internet connection.","title":"Problem"},{"location":"rwot8/topics-and-advance-readings/Providing_Decentralized_Identity_to_the_most_vulnerable_populations/#idea","text":"This is the reason why we are proposing a smart card based solution that leverages on W3C standards of Decentralized Identifiers and Verifiable Credentials. The card will be used to sign transaction and securely store the private keys and Verifiable Credentials.","title":"Idea"},{"location":"rwot8/topics-and-advance-readings/Providing_Decentralized_Identity_to_the_most_vulnerable_populations/#access-management","text":"One of the drawbacks of using a smart card is the lack of a user interface on the card, making it more difficult to selectively protect access to features and credentials stored on the card. The following are authentication mechanism that could be used for our smart card: - PIN code: one of the most common authentication methods in developed countries. However, based on our experiences, it was not considered as the preferred solutions due to low literacy and numeracy levels in some of the remote areas. - Biometrics match-on-card: eliminates the need for a central database by both storing and matching fingerprint, iris and face biometrics data directly on a smartcard. - Cryptography: authenticate the Point of Services. - Time Delay: can be used to differentiated between a simple tap, multi-tap or long-tap of the card. Through the use and combination of these authentication processes, two possible solutions could be implemented: Use of trusted devices like in the case of the credit card network where the user trust the terminal shows them the amount being deducted from the card and executes the action. This can be achieved by using cryptography and having the terminal authenticate to the card. The main advantage of this would be making it easier for the user to interact with the card, but to access their own data, they need to use a certified terminal. Use of a combination of authentication processes such as multiple PIN codes, fingerprint match or a delay to unlock access to different information. The advantage of this solution is that the user has increased control over their data as any device can be used to access it, but things such as keeping track of multiple PIN codes might be inconvenient for some. These methods are not exclusive, and can actually be used in conjunction with others depending on how they are used.","title":"Access Management"},{"location":"rwot8/topics-and-advance-readings/RWOT-local-chapters/","text":"RWOT local chapters Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: short_topic_paper_to_open_discussion, local communities, face to face meetings IMHO, RWOT community does very interesting stuff - theoritical explorations and practical implementaions of cool things that has to do with how people cooperate, built on the new ever-appearing \"hard\" IT technologies and ever-deeper \"soft\" understandings (game theory, economics, evolutionary psychology). It is great to have this community as a global phenomenon, but the lack of face-to-face regular communication decreases productivity, potential output, number of people willing to participate in it. What if we had regular RWOT-like weekly/monthly meetings locally? Something like tech meetups, but not limited to a particular technologies. A place where people can meet and talk/explore/do cool things that has to do with human cooperation. Questions: Who would participate in this meetings? How would they agree on the common topics to discuss, things to do? Is it not what is being done in academia already? Do we need a tutorial on how to organize, conduct such meetings?","title":"RWOT local chapters"},{"location":"rwot8/topics-and-advance-readings/RWOT-local-chapters/#rwot-local-chapters","text":"Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: short_topic_paper_to_open_discussion, local communities, face to face meetings IMHO, RWOT community does very interesting stuff - theoritical explorations and practical implementaions of cool things that has to do with how people cooperate, built on the new ever-appearing \"hard\" IT technologies and ever-deeper \"soft\" understandings (game theory, economics, evolutionary psychology). It is great to have this community as a global phenomenon, but the lack of face-to-face regular communication decreases productivity, potential output, number of people willing to participate in it. What if we had regular RWOT-like weekly/monthly meetings locally? Something like tech meetups, but not limited to a particular technologies. A place where people can meet and talk/explore/do cool things that has to do with human cooperation.","title":"RWOT local chapters"},{"location":"rwot8/topics-and-advance-readings/RWOT-local-chapters/#questions","text":"Who would participate in this meetings? How would they agree on the common topics to discuss, things to do? Is it not what is being done in academia already? Do we need a tutorial on how to organize, conduct such meetings?","title":"Questions:"},{"location":"rwot8/topics-and-advance-readings/SSI-FrameworkProposal/","text":"Self-Sovereign Identity Framework/Thought Model Rieks Joosten ( rieks.joosten@tno.nl ) TNO's SSI-group has been constructing a thought model around SSI that we need (and already use) to determine whether or not we have a common understanding about SSI (related topics) and efficiently straighten out any misunderstandings. This not only facilitates our work as we avoid the (endless) terminological discussions, but also helps us to deepen our understanding of what we are actually doing. This model, which we refer to as a Self-Sovereign Identity Framework or SSIF, is a work that is still in progress, but where the basics seem to have become stable. In this two-pager, we introduce the purpose for the model, its use, and a (new) definition of the term 'definition', which we need to make it all work. Additionally, a draft text is made available in which most of our current thinking about SSI is expressed. I would like to discuss this approach and what we might do to make it beneficial to all of us. Purpose / Context The SSIF provides a thought model and corresponding terminology that allows us to think in a precise manner about SSI within the context of its purpose(s) - which for us is the generic enablement of electronic support for (administrative) business transactions. The ability to conceptually talk about governance-, process- and technology-related topics, has proven beneficial, e.g. in discussions that compare different technologies, or discussions in which we determine of some technology is fit for the purpose we want to use it for. Construction As in mathematics, we construct the model from a set of assumptions (axioms) that we use, and use them to further develop the concepts that we need in order to ensure that the result is fit for (our) purpose, as stated above. When developing the terminology, we try to specify definitions in terms of criteria that English speaking people that are interested in SSI are expected to evaluate in the same way, which allows participants in the discussions that we have to determine easily whether or not they are making the same distinctions, i.e. talk about the same thing, and to reconcile terminology disagreements. A nice consequence of this focus on criteria rather than the name we use for it, is that we generally do not object to using another name for a concept (as long as there's agreement to the criteria that we use for it). Use A (thought) model provides you with a specific perspective on the model's subject. The quality of a thought model can be measured by the ease it helps you solve problems, or understand issues. Being fixed at using traditional thought models can be very counterproductive, and introducing new ones very dangerous, as is easily demonstrated by the example of the introduction of the heliocentric model for computing planetary trajectories by Galileo, Copernic and others: some were excommunicated (perhaps even burnt at the stake?) for having this other perspective, even though it made computing planetary trajectories so much easier. We won't be surprised to find opposition to thoughts we may introduce regarding SSI related stuff, but would like to request that the merits of what we do is judged by the effects it has on understanding SSI, and to resolve issues amongst people that agree to use the model/framework. If you decided to use the model, please do so very consciously. Become aware of when you use the model, and equally important: when you revert to more traditional ways of thinking. The latter isn\u2019t bad (see where it has brought you), it is just different. This awareness will help you to evaluate the usefulness of the model as well as of your own thinking. It may also inspire you to propose enhancements to this model (which we solicit). Definitions in the model One of the main characteristics of this model is its use of terminology and definitions. For terms whose meaning is generally not disputed, e.g. because there is an undisputed description in a dictionary, we use the definition as provided there. However, whenever discussions arise \u2013 often continuously \u2013 about the meaning of some term, another approach is called for. The approach we use aims to guarantee, to the maximum possible extent, that different people will (come to) have the same understanding of such terms, and also that this understanding is relevant within the discussions they have. For us, this means: discussions that are somehow related to self-sovereign identity. What we do is specify one or more criteria by which we can distinguish between what is, and what is not, an instance of the term we like to define. Participants in the discussion can then come up with use-case. Then, every participant applies the criteria to the use-case, and together they can see whether or not they use the criteria in the same way. If they don\u2019t, the criteria need to be rephrased so that every participant uses it in the same way. When they use it in the same way, they can agree on the term or phrase they will be using in their discussions to refer to what satisfies the criteria, and what does not. This method basically reverses the way definitions are used: rather than using a term and (dis)agreeing on its meaning, we look for the meaning we want to have consensus about, and then tag a name or phrase to it. Also, we \u2018scope\u2019 our definitions, i.e. limit their use to the discussions/contexts where there is consensus about the criteria. As an example, the terms defined in this chapter are valid for this chapter, but not necessarily for the other chapters in this book. Doing so allows for terms to be defined differently in different contexts. Where's the Model? The model is still under construction. However, we do have some text that provide the basics as we see it, which you can find here .","title":"Self-Sovereign Identity Framework/Thought Model"},{"location":"rwot8/topics-and-advance-readings/SSI-FrameworkProposal/#self-sovereign-identity-frameworkthought-model","text":"Rieks Joosten ( rieks.joosten@tno.nl ) TNO's SSI-group has been constructing a thought model around SSI that we need (and already use) to determine whether or not we have a common understanding about SSI (related topics) and efficiently straighten out any misunderstandings. This not only facilitates our work as we avoid the (endless) terminological discussions, but also helps us to deepen our understanding of what we are actually doing. This model, which we refer to as a Self-Sovereign Identity Framework or SSIF, is a work that is still in progress, but where the basics seem to have become stable. In this two-pager, we introduce the purpose for the model, its use, and a (new) definition of the term 'definition', which we need to make it all work. Additionally, a draft text is made available in which most of our current thinking about SSI is expressed. I would like to discuss this approach and what we might do to make it beneficial to all of us.","title":"Self-Sovereign Identity Framework/Thought Model"},{"location":"rwot8/topics-and-advance-readings/SSI-FrameworkProposal/#purpose-context","text":"The SSIF provides a thought model and corresponding terminology that allows us to think in a precise manner about SSI within the context of its purpose(s) - which for us is the generic enablement of electronic support for (administrative) business transactions. The ability to conceptually talk about governance-, process- and technology-related topics, has proven beneficial, e.g. in discussions that compare different technologies, or discussions in which we determine of some technology is fit for the purpose we want to use it for.","title":"Purpose / Context"},{"location":"rwot8/topics-and-advance-readings/SSI-FrameworkProposal/#construction","text":"As in mathematics, we construct the model from a set of assumptions (axioms) that we use, and use them to further develop the concepts that we need in order to ensure that the result is fit for (our) purpose, as stated above. When developing the terminology, we try to specify definitions in terms of criteria that English speaking people that are interested in SSI are expected to evaluate in the same way, which allows participants in the discussions that we have to determine easily whether or not they are making the same distinctions, i.e. talk about the same thing, and to reconcile terminology disagreements. A nice consequence of this focus on criteria rather than the name we use for it, is that we generally do not object to using another name for a concept (as long as there's agreement to the criteria that we use for it).","title":"Construction"},{"location":"rwot8/topics-and-advance-readings/SSI-FrameworkProposal/#use","text":"A (thought) model provides you with a specific perspective on the model's subject. The quality of a thought model can be measured by the ease it helps you solve problems, or understand issues. Being fixed at using traditional thought models can be very counterproductive, and introducing new ones very dangerous, as is easily demonstrated by the example of the introduction of the heliocentric model for computing planetary trajectories by Galileo, Copernic and others: some were excommunicated (perhaps even burnt at the stake?) for having this other perspective, even though it made computing planetary trajectories so much easier. We won't be surprised to find opposition to thoughts we may introduce regarding SSI related stuff, but would like to request that the merits of what we do is judged by the effects it has on understanding SSI, and to resolve issues amongst people that agree to use the model/framework. If you decided to use the model, please do so very consciously. Become aware of when you use the model, and equally important: when you revert to more traditional ways of thinking. The latter isn\u2019t bad (see where it has brought you), it is just different. This awareness will help you to evaluate the usefulness of the model as well as of your own thinking. It may also inspire you to propose enhancements to this model (which we solicit).","title":"Use"},{"location":"rwot8/topics-and-advance-readings/SSI-FrameworkProposal/#definitions-in-the-model","text":"One of the main characteristics of this model is its use of terminology and definitions. For terms whose meaning is generally not disputed, e.g. because there is an undisputed description in a dictionary, we use the definition as provided there. However, whenever discussions arise \u2013 often continuously \u2013 about the meaning of some term, another approach is called for. The approach we use aims to guarantee, to the maximum possible extent, that different people will (come to) have the same understanding of such terms, and also that this understanding is relevant within the discussions they have. For us, this means: discussions that are somehow related to self-sovereign identity. What we do is specify one or more criteria by which we can distinguish between what is, and what is not, an instance of the term we like to define. Participants in the discussion can then come up with use-case. Then, every participant applies the criteria to the use-case, and together they can see whether or not they use the criteria in the same way. If they don\u2019t, the criteria need to be rephrased so that every participant uses it in the same way. When they use it in the same way, they can agree on the term or phrase they will be using in their discussions to refer to what satisfies the criteria, and what does not. This method basically reverses the way definitions are used: rather than using a term and (dis)agreeing on its meaning, we look for the meaning we want to have consensus about, and then tag a name or phrase to it. Also, we \u2018scope\u2019 our definitions, i.e. limit their use to the discussions/contexts where there is consensus about the criteria. As an example, the terms defined in this chapter are valid for this chapter, but not necessarily for the other chapters in this book. Doing so allows for terms to be defined differently in different contexts.","title":"Definitions in the model"},{"location":"rwot8/topics-and-advance-readings/SSI-FrameworkProposal/#wheres-the-model","text":"The model is still under construction. However, we do have some text that provide the basics as we see it, which you can find here .","title":"Where's the Model?"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/","text":"The Untimely Death of SSI Author Michael Shea Abstract To perform a strategic analysis on the Self-Sovereign Identity sector to identify gaps in the approach that should be addressed. Introduction This paper is not intended as a competitive analysis of the individual players in the sector but as an analysis of the SSI community at large, the competitive landscape and the market place threats that would prevent SSI coming to maturity. In many aspects the SSI sector is attempting to create a \u2018Blue Ocean\u2019 strategy to shift the competitive landscape on identity in the digital realm to redefine digital identity to one of personal control. In doing so, it creates as potential competitors not only the traditional identity software providers, but also commercial enterprises that have tied control of identity into their business models (Google, Facebook). The SSI community it not some recent creation, but has evolved out of a group of individuals that have been long active in the realm of Internet identity. With the creation of Blockchain and Distributed Ledger Technologies (DLTs) a fundamental technology has come into the market that so far appears to enable a degree of decentralization and control that has heretofore not been achievable. As a relatively newcomer to the digital identity space, what has struck me is how passionate and tight knit this community is; over the past six months I have read, observed and spoken with many different members of different SSI projects, and the striking feature is that there exists a strong ethos of cooperation while seeking the strongest technical solutions for the sector. There are strong passions and opinions around the right and wrong way to describe or do things, but these differences are always expressed in a manner conducive to creative dissonance with the objective of creating something much better. Trigger In the past six weeks I have observed a couple of episodes that are the source of this paper topic. The first occurred shortly after the Global Hyperledger conference in Basel, Switzerland and the second more recently with the application to the W3C for DID WG status. Just after the Hyperledger conference Michael Herman opened up many issues (10-20?) in Github around the language and construction of the DID specification. These issues resulted in many email/Zoom/Rocket Chat conversations. What was surprising (to me anyways) was, as a specification that appeared to be advanced in implementation and commitment of people and financial resources the level of gap. The second episode, was around the state of the DID Use Case document that was submitted to W3C with the WG application. From the comments within the email channel, this oversight could have had very serious repercussions on all the solid work that had been done to date. Again given the level of engagement of the community I found this event surprising and lead me to wonder if, while deep into all the technology aspects of SSI, that it would be useful and important to take a step back and get \u2018on the balcony\u2019 to try and see the larger picture. SWOT Analysis There are many kinds of strategic analysis frameworks, but in this case, I am proposing that Strengths, Weaknesses, Opportunities and Threats (SWOT) analysis be conducted on the SSI community and sector. While SWOT analyses are typically done in commercial or marketing related activities it is also appropriate for community organizing, with the hoped outcome to be a summary paper and input into a plan of action to counter the threats, leverage the opportunities, build up the weaknesses, and maintain the strengths. Strengths Dedication and engagement of SSI technical community Depth of knowledge and experience on digital identity Engagement and commitment of community Weaknesses Size of and breadth community Economic model of underlying Blockchain or DLT technology Interoperability Immaturity of underlying Blockchain/DLT technology Missing research into communicating highly technical SSI ideas to non-technical business, government, and general publics Missing research into user experience and user interfaces for SSI Missing centralized or coordinated advocacy and rapid responders to active threats to SSI. Who are SSI's funded lawyers and lobbyists? Sovereignty is hard for most people to spell and most people are unclear on what it means The politics and values surrounding SSI are not universal The language of \"owning personal data\" obstructs the move from data exploitation to data personhood and human rights Opportunities Rapid technology changes occurring in the base technologies Elimination of centralized identity repository honey pots Threats Fragmenting of the internet Failure to reach critical mass Entrenchment of centralized identity solutions within market Centralized identity systems controlled by State actors Consent Theater lowers demand for vs. informed consent It is becoming harder to persuade key adopters that the problems are solvable with so much identity theft, cyberwar, and incompetent infosec The language of \"owning personal data\" conflicts with extant business and civil rights laws Branding association with \"blockchain\" may increase perceived risk Summary A danger in all SWOT analyses is the reduction of everything to bullet points, I accept this risk right now, with the intent of developing these further with more input and conversation. In preparing this paper, I have created an initial list based on my observations, conversations and readings from the last six months. I look forward to an engaged conversation to develop and expand these with those of you that are interested in this topic.","title":"The Untimely Death of SSI"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#the-untimely-death-of-ssi","text":"","title":"The Untimely Death of SSI"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#author","text":"Michael Shea","title":"Author"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#abstract","text":"To perform a strategic analysis on the Self-Sovereign Identity sector to identify gaps in the approach that should be addressed.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#introduction","text":"This paper is not intended as a competitive analysis of the individual players in the sector but as an analysis of the SSI community at large, the competitive landscape and the market place threats that would prevent SSI coming to maturity. In many aspects the SSI sector is attempting to create a \u2018Blue Ocean\u2019 strategy to shift the competitive landscape on identity in the digital realm to redefine digital identity to one of personal control. In doing so, it creates as potential competitors not only the traditional identity software providers, but also commercial enterprises that have tied control of identity into their business models (Google, Facebook). The SSI community it not some recent creation, but has evolved out of a group of individuals that have been long active in the realm of Internet identity. With the creation of Blockchain and Distributed Ledger Technologies (DLTs) a fundamental technology has come into the market that so far appears to enable a degree of decentralization and control that has heretofore not been achievable. As a relatively newcomer to the digital identity space, what has struck me is how passionate and tight knit this community is; over the past six months I have read, observed and spoken with many different members of different SSI projects, and the striking feature is that there exists a strong ethos of cooperation while seeking the strongest technical solutions for the sector. There are strong passions and opinions around the right and wrong way to describe or do things, but these differences are always expressed in a manner conducive to creative dissonance with the objective of creating something much better.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#trigger","text":"In the past six weeks I have observed a couple of episodes that are the source of this paper topic. The first occurred shortly after the Global Hyperledger conference in Basel, Switzerland and the second more recently with the application to the W3C for DID WG status. Just after the Hyperledger conference Michael Herman opened up many issues (10-20?) in Github around the language and construction of the DID specification. These issues resulted in many email/Zoom/Rocket Chat conversations. What was surprising (to me anyways) was, as a specification that appeared to be advanced in implementation and commitment of people and financial resources the level of gap. The second episode, was around the state of the DID Use Case document that was submitted to W3C with the WG application. From the comments within the email channel, this oversight could have had very serious repercussions on all the solid work that had been done to date. Again given the level of engagement of the community I found this event surprising and lead me to wonder if, while deep into all the technology aspects of SSI, that it would be useful and important to take a step back and get \u2018on the balcony\u2019 to try and see the larger picture.","title":"Trigger"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#swot-analysis","text":"There are many kinds of strategic analysis frameworks, but in this case, I am proposing that Strengths, Weaknesses, Opportunities and Threats (SWOT) analysis be conducted on the SSI community and sector. While SWOT analyses are typically done in commercial or marketing related activities it is also appropriate for community organizing, with the hoped outcome to be a summary paper and input into a plan of action to counter the threats, leverage the opportunities, build up the weaknesses, and maintain the strengths.","title":"SWOT Analysis"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#strengths","text":"Dedication and engagement of SSI technical community Depth of knowledge and experience on digital identity Engagement and commitment of community","title":"Strengths"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#weaknesses","text":"Size of and breadth community Economic model of underlying Blockchain or DLT technology Interoperability Immaturity of underlying Blockchain/DLT technology Missing research into communicating highly technical SSI ideas to non-technical business, government, and general publics Missing research into user experience and user interfaces for SSI Missing centralized or coordinated advocacy and rapid responders to active threats to SSI. Who are SSI's funded lawyers and lobbyists? Sovereignty is hard for most people to spell and most people are unclear on what it means The politics and values surrounding SSI are not universal The language of \"owning personal data\" obstructs the move from data exploitation to data personhood and human rights","title":"Weaknesses"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#opportunities","text":"Rapid technology changes occurring in the base technologies Elimination of centralized identity repository honey pots","title":"Opportunities"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#threats","text":"Fragmenting of the internet Failure to reach critical mass Entrenchment of centralized identity solutions within market Centralized identity systems controlled by State actors Consent Theater lowers demand for vs. informed consent It is becoming harder to persuade key adopters that the problems are solvable with so much identity theft, cyberwar, and incompetent infosec The language of \"owning personal data\" conflicts with extant business and civil rights laws Branding association with \"blockchain\" may increase perceived risk","title":"Threats"},{"location":"rwot8/topics-and-advance-readings/The-Untimely-Death-Of-SSI/#summary","text":"A danger in all SWOT analyses is the reduction of everything to bullet points, I accept this risk right now, with the intent of developing these further with more input and conversation. In preparing this paper, I have created an initial list based on my observations, conversations and readings from the last six months. I look forward to an engaged conversation to develop and expand these with those of you that are interested in this topic.","title":"Summary"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/","text":"Universal DID Operations Markus Sabadello (markus@danubetech.com), Nader Helmy (nader.helmy@danubetech.com), Vienna, 8th February 2019 Introduction Decentralized Identifiers (DIDs) have seen increasing adoption across a wide number of distributed ledger ecosystems and blockchains. This is in large part due to our ability to effectively communicate by resolving these DIDs. The process of obtaining a DID Document associated with a particular DID is outlined in the DID Resolution spec. A blockchain-agnostic implementation of the spec is hosted at the Decentralized Identity Foundation and is fully open-sourced. The Universal Resolver can be found at its website and also runs locally or remotely through an API. It currently supports DIDs on Sovrin, BTCR, uPort, Jolocom, Veres One, ERC-725, Blockstack, IPFS, and DNS via a number of community-contributed drivers built on top of the Universal Resolver. Now that we can universally resolve a DID, how can we do the same with the entire DID lifecycle? All DID methods commonly share 4 operations: Create, Resolve, Update, and Revoke. We can envision a counterpart to the Resolver, called a Registrar, that contains these additional DID operations. DID Resolution is relatively straightforward because there is no authentication required and thus no keys involved. In addition, it is an atomic operation and can easily be done over the web. Creating a DID, updating its DID Document and revoking a DID's secret will require the same abstraction layer. Abstract Interface create() create(method, options) -> state, metadata create(method, options, did-doc) -> state, metadata create(method, options, did-doc, wallet) -> state, metadata To create a DID, we specify where we want it created, with optional parameters for registering a DID Document and storing keys. method * sov, btcr, v1, ... options * mainnet or testnet or other... * seed did-doc * entire new DID Document wallet * storage for generated private keys * storage of existing keys * e.g. text file, wallet API endpoint, local wallet, etc. update() update(identifier, options, wallet, did-doc) -> state, metadata update(identifier, options, wallet, did-doc-operation) -> state, metadata To update a DID with a new DID Document, we verify ownership over the DID and submit any requested changes. did-doc * entire new DID Document, to replace the previous one did-doc-operation * incremental update to the existing DID Document, e.g.: * add-service * remove-service * add-publickey * remove-publickey revoke() revoke(identifier, options, wallet) -> state, metadata To revoke a DID, we verify ownership over the DID. checkOperation() checkOperation(jobid) -> state, metadata checkOperation(identifier) -> state, metadata At any time, we can query the Registrar to find the state of a previous DID operation, or the last known state of the identifier. jobid * returned as part of the state object * generated by the Registrar state state finished DID wallet {optional} action jobid actiontype (e.g. send tokens to wallet) wait jobid waittype (e.g. wait for confirmation on chain) fail error message metadata metadata operation metadata duration method metadata method-specific hash token balance The Registrar has four states: finished, failed, wait, and action. This flow diagram indicates the responses a typical user can expect. Architecture In order to implement a library or tool that supports the above interfaces for creating, updating, and revoking DIDs in a method-agnostic way, we can imagine a similar architecture as we have built for the Universal Resolver , i.e. using a set of drivers that perform method-specific operations. Accordingly, we can call this library or tool a Universal Registrar . Some architectural questions that apply to the resolve() operation also apply to other operations, e.g.: Is the abstract interface implemented as a library that can be integrated locally into an application or service, or is the abstract interface exposed by a remote service and used via HTTP or another binding? How do method-specific drivers interact with the DID's target system? For example, do they have direct access to a blockchain full node? What are implications of the above questions for trust and security? Unlike the resolve() operation however, the other operations create() , update() , and revoke() are more challenging and therefore raise additional architectural questions, since they typically involve the use of secrets such as private keys, and write operations to the DID's target system: Where are secrets generated? Are a DID's private keys generated by the driver, or by the client that uses the Universal Registrar? Where are secrets stored? Are a DID's private keys stored in a wallet held by the driver, or by the client that uses the Universal Registrar? Where are the identifiers generated? Does the client generate the identifier (the DID) that gets created, or does this happen entirely inside the driver? Note that e.g. in the \"btcr\" DID method, the DID only becomes known at the end of the creation process, not at the beginning. The Universal Registrar interface can be configured to generate & store keys in various locations, e.g. in the client, in each method-specific driver, or in the Registrar's cloud wallet. HTTP Binding The abstract interface above can be implemented and deployed in the form of bindings to different protocols, such as simple HTTP POST operations, with inputs and outputs encoded as JSON. For example, the operations above can be deployed at the following endpoints: https://uniregistrar.io/1.0/create https://uniregistrar.io/1.0/update https://uniregistrar.io/1.0/revoke Examples create() did:sov REQUEST create(\"did:sov\", { \"network\": \"stn\" }) RESPONSE state { \"state\": \"finished\", \"identifier\": \"did:sov:stn:888G8onFVhEP3kVCipXvey\", \"wallet\": { \"seed\": \"ceiusFJbi5z1Fs3vOj7HKIGcCblb84pl\" } } metadata { \"network\": \"stn\", \"poolVersion\": 2, \"submitterDid\": \"WRfXPg8dantKVubE3HX8pw\" } create() did:btcr REQUEST create(\"did:btcr\", { \"network\": \"stn\" }) RESPONSE state { \"state\": \"wait\", \"jobId\": \"cd86ca7a-4ae5-40ed-8187-99b5484415e3\", \"wait\": \"confirmingtransaction\", \"waitTime\": \"1800000\" } metadata { \"chain\": \"TESTNET\", \"transactionHash\": \"42e74f2530c452cae0fac7495d4143fffac8784dec1f00f22a4b2196b28fa4da\", \"balance\": 0.31018803, \"changeAddress\": \"n3mR6awpt4D1yfCwkVReKXbbpPosfz569r\" } WAIT 30 MINUTES ... REQUEST checkOperation(\"cd86ca7a-4ae5-40ed-8187-99b5484415e3\") RESPONSE state { \"state\": \"finished\", \"identifier\": \"did:btcr:xk7m-czu9-qq8c-djqs\", \"wallet\": { \"privateKeyWif\": \"cSaQH1A2v9b56DTTtMobTvxLJ3Z4yun2urNhVYLTnn7jRi3wtaBZ\", \"privateKeyHex\": \"95086c356343a6cb56f186f3cbb5791a2acd2cd4a34063416a94d01fd51af768\" } } metadata { \"chain\": \"TESTNET\", \"transactionHash\": \"1810eb9000a43eee466af6f159fad9d8423fb4b8912e55d9fc155de388b66cd3\", \"blockHeight\": 1456107, \"blockIndex\": 47 } Other Topics and Future Ideas Indy A2A binding to communicate with the Universal Resolver / Registrar With further development of Indy Agents and other interoperable Agent protocols there will be new opportunities for utilization of the DID infrastructure included in the Resolver & Registrar. These APIs could be used in a broader set of consumer-facing services and applications, integrated within development environments, and/or built into decentralized production systems. Special considerations about certain DID methods (e.g. peer) Off-ledger DIDs or public key DIDs should be supported as they become more defined protocols. The abstract interface is compatible with those types of DIDs, and additional drivers can be built to support their functionality. Wallet Developments As implementations of Identity Wallets are developed and created, there will be new extensions and interfaces to improve on security and usability. These developments are theoretically compatible with the abstract interface outlined here, with few if any modifications. The Universal Registrar should be updated as browser extensions, hardware wallets, cloud wallets, etc. become a reality.","title":"Universal DID Operations"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#universal-did-operations","text":"Markus Sabadello (markus@danubetech.com), Nader Helmy (nader.helmy@danubetech.com), Vienna, 8th February 2019","title":"Universal DID Operations"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#introduction","text":"Decentralized Identifiers (DIDs) have seen increasing adoption across a wide number of distributed ledger ecosystems and blockchains. This is in large part due to our ability to effectively communicate by resolving these DIDs. The process of obtaining a DID Document associated with a particular DID is outlined in the DID Resolution spec. A blockchain-agnostic implementation of the spec is hosted at the Decentralized Identity Foundation and is fully open-sourced. The Universal Resolver can be found at its website and also runs locally or remotely through an API. It currently supports DIDs on Sovrin, BTCR, uPort, Jolocom, Veres One, ERC-725, Blockstack, IPFS, and DNS via a number of community-contributed drivers built on top of the Universal Resolver. Now that we can universally resolve a DID, how can we do the same with the entire DID lifecycle? All DID methods commonly share 4 operations: Create, Resolve, Update, and Revoke. We can envision a counterpart to the Resolver, called a Registrar, that contains these additional DID operations. DID Resolution is relatively straightforward because there is no authentication required and thus no keys involved. In addition, it is an atomic operation and can easily be done over the web. Creating a DID, updating its DID Document and revoking a DID's secret will require the same abstraction layer.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#abstract-interface","text":"","title":"Abstract Interface"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#create","text":"create(method, options) -> state, metadata create(method, options, did-doc) -> state, metadata create(method, options, did-doc, wallet) -> state, metadata To create a DID, we specify where we want it created, with optional parameters for registering a DID Document and storing keys. method * sov, btcr, v1, ... options * mainnet or testnet or other... * seed did-doc * entire new DID Document wallet * storage for generated private keys * storage of existing keys * e.g. text file, wallet API endpoint, local wallet, etc.","title":"create()"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#update","text":"update(identifier, options, wallet, did-doc) -> state, metadata update(identifier, options, wallet, did-doc-operation) -> state, metadata To update a DID with a new DID Document, we verify ownership over the DID and submit any requested changes. did-doc * entire new DID Document, to replace the previous one did-doc-operation * incremental update to the existing DID Document, e.g.: * add-service * remove-service * add-publickey * remove-publickey","title":"update()"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#revoke","text":"revoke(identifier, options, wallet) -> state, metadata To revoke a DID, we verify ownership over the DID.","title":"revoke()"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#checkoperation","text":"checkOperation(jobid) -> state, metadata checkOperation(identifier) -> state, metadata At any time, we can query the Registrar to find the state of a previous DID operation, or the last known state of the identifier. jobid * returned as part of the state object * generated by the Registrar","title":"checkOperation()"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#state","text":"state finished DID wallet {optional} action jobid actiontype (e.g. send tokens to wallet) wait jobid waittype (e.g. wait for confirmation on chain) fail error message","title":"state"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#metadata","text":"metadata operation metadata duration method metadata method-specific hash token balance The Registrar has four states: finished, failed, wait, and action. This flow diagram indicates the responses a typical user can expect.","title":"metadata"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#architecture","text":"In order to implement a library or tool that supports the above interfaces for creating, updating, and revoking DIDs in a method-agnostic way, we can imagine a similar architecture as we have built for the Universal Resolver , i.e. using a set of drivers that perform method-specific operations. Accordingly, we can call this library or tool a Universal Registrar . Some architectural questions that apply to the resolve() operation also apply to other operations, e.g.: Is the abstract interface implemented as a library that can be integrated locally into an application or service, or is the abstract interface exposed by a remote service and used via HTTP or another binding? How do method-specific drivers interact with the DID's target system? For example, do they have direct access to a blockchain full node? What are implications of the above questions for trust and security? Unlike the resolve() operation however, the other operations create() , update() , and revoke() are more challenging and therefore raise additional architectural questions, since they typically involve the use of secrets such as private keys, and write operations to the DID's target system: Where are secrets generated? Are a DID's private keys generated by the driver, or by the client that uses the Universal Registrar? Where are secrets stored? Are a DID's private keys stored in a wallet held by the driver, or by the client that uses the Universal Registrar? Where are the identifiers generated? Does the client generate the identifier (the DID) that gets created, or does this happen entirely inside the driver? Note that e.g. in the \"btcr\" DID method, the DID only becomes known at the end of the creation process, not at the beginning. The Universal Registrar interface can be configured to generate & store keys in various locations, e.g. in the client, in each method-specific driver, or in the Registrar's cloud wallet.","title":"Architecture"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#http-binding","text":"The abstract interface above can be implemented and deployed in the form of bindings to different protocols, such as simple HTTP POST operations, with inputs and outputs encoded as JSON. For example, the operations above can be deployed at the following endpoints: https://uniregistrar.io/1.0/create https://uniregistrar.io/1.0/update https://uniregistrar.io/1.0/revoke","title":"HTTP Binding"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#examples","text":"","title":"Examples"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#create-didsov","text":"","title":"create() did:sov"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#request","text":"create(\"did:sov\", { \"network\": \"stn\" })","title":"REQUEST"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#response","text":"state { \"state\": \"finished\", \"identifier\": \"did:sov:stn:888G8onFVhEP3kVCipXvey\", \"wallet\": { \"seed\": \"ceiusFJbi5z1Fs3vOj7HKIGcCblb84pl\" } } metadata { \"network\": \"stn\", \"poolVersion\": 2, \"submitterDid\": \"WRfXPg8dantKVubE3HX8pw\" }","title":"RESPONSE"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#create-didbtcr","text":"","title":"create() did:btcr"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#request_1","text":"create(\"did:btcr\", { \"network\": \"stn\" })","title":"REQUEST"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#response_1","text":"state { \"state\": \"wait\", \"jobId\": \"cd86ca7a-4ae5-40ed-8187-99b5484415e3\", \"wait\": \"confirmingtransaction\", \"waitTime\": \"1800000\" } metadata { \"chain\": \"TESTNET\", \"transactionHash\": \"42e74f2530c452cae0fac7495d4143fffac8784dec1f00f22a4b2196b28fa4da\", \"balance\": 0.31018803, \"changeAddress\": \"n3mR6awpt4D1yfCwkVReKXbbpPosfz569r\" }","title":"RESPONSE"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#wait-30-minutes","text":"","title":"WAIT 30 MINUTES ..."},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#request_2","text":"checkOperation(\"cd86ca7a-4ae5-40ed-8187-99b5484415e3\")","title":"REQUEST"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#response_2","text":"state { \"state\": \"finished\", \"identifier\": \"did:btcr:xk7m-czu9-qq8c-djqs\", \"wallet\": { \"privateKeyWif\": \"cSaQH1A2v9b56DTTtMobTvxLJ3Z4yun2urNhVYLTnn7jRi3wtaBZ\", \"privateKeyHex\": \"95086c356343a6cb56f186f3cbb5791a2acd2cd4a34063416a94d01fd51af768\" } } metadata { \"chain\": \"TESTNET\", \"transactionHash\": \"1810eb9000a43eee466af6f159fad9d8423fb4b8912e55d9fc155de388b66cd3\", \"blockHeight\": 1456107, \"blockIndex\": 47 }","title":"RESPONSE"},{"location":"rwot8/topics-and-advance-readings/Universal-DID-Operations/#other-topics-and-future-ideas","text":"Indy A2A binding to communicate with the Universal Resolver / Registrar With further development of Indy Agents and other interoperable Agent protocols there will be new opportunities for utilization of the DID infrastructure included in the Resolver & Registrar. These APIs could be used in a broader set of consumer-facing services and applications, integrated within development environments, and/or built into decentralized production systems. Special considerations about certain DID methods (e.g. peer) Off-ledger DIDs or public key DIDs should be supported as they become more defined protocols. The abstract interface is compatible with those types of DIDs, and additional drivers can be built to support their functionality. Wallet Developments As implementations of Identity Wallets are developed and created, there will be new extensions and interfaces to improve on security and usability. These developments are theoretically compatible with the abstract interface outlined here, with few if any modifications. The Universal Registrar should be updated as browser extensions, hardware wallets, cloud wallets, etc. become a reality.","title":"Other Topics and Future Ideas"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/","text":"Using Immutable Data Objects to Define Verifiable Credential Data Ken Ebert, Sovrin Foundation, ken@sovrin.org Abstract Verifiable Credentials are strengthened by providing immutable data objects that provide a full definition of the data being signed. This is particularly true for objects with ZKP style signatures, where a more granular description of the data is required in order to support disclosure and predicate proofs on a per-property basis. Control Objects Control objects, such as DIDs, revocation registries, and payment addresses, contain data required to establish control. It is expected that these objects need to change over time. Keys need to be rotated, credentials need to be revoked, and balances need to be updated. Data Objects Data objects contain information that describes semantic meaning of verifiable credentials. In order to establish a cryptographic assurance of what is being signed in a verifiable credential, the definition of the meaning must not change after a credential is issued. Immutable data objects that provide this definition of meaning include: Verifiable Credential A verifiable credential is a set of signed claims about a subject. The issuer and claims in the credential about the subject can be cryptographically verified to confirm that the issuer made those claims. The meaning of the claims is represented by other data objects. Verifiable credentials are stored off-ledger to preserve privacy. Verifiable credential definitions are specified in JSON-LD. Credential Definition The credential definition identifies the issuer, an associated mapping, and public keys for this type of credential. Credential definitions are immutably stored on-ledger to provide access to the materials used to verify any of the credentials issued using the definition. Credential definitions are specified in JSON-LD. Mapping A mapping contains an ordered selection of properties from the source schema. For each property a corresponding encoding is specified to prepare the data for ZKP signatures. The primary objective of the mapping is to provide a sequence of numbers to sign in a ZKP style proof. Mappings are immutably stored on-ledger to provide the holder with the definition of each property and how that property was transformed into a number included in the ZKP proof. Mappings can be shared between issuers to promote interoperability among trust frameworks established by communities of trust; multiple issuers can refer the same mapping from their respective credential definitions. Others outside the issuer\u2019s trust framework may choose to trust credentials from the framework. Mappings are specified in JSON-LD. Schema A schema defines the structure and type of data for claim data. Schemas can contain sub-schemas. Schemas can represent complex data objects. Schemas include properties such as type, label, and description. Standard schemas can be extended to incorporate definitions of properties not previously included. Many schemas already exist, such as those available at schema.org. Many schemas available on the internet do not have the property of immutability that can provide an unchanging definition of the properties signed in a verifiable credential. Schemas are immutably stored on-ledger to provide the holder with the definition of each property. Mappings refer to the properties defined in the schema to select properties for inclusion by an issuer in a verifiable credential. Schemas can be shared between mappings to promote interoperability. Verifiers\u2019 presentation requests also refer to the schemas used in a verifiable credential to specify which properties from the verifiable credential should be included in a derived presentation created by a holder. Multiple presentations can share schemas. Schemas are specified in JSON-LD. Context A context is a collection of shortcut term definitions. Contexts establish definitions that promote efficiency while preserving accuracy. Contexts are immutably store on-ledger to provide the ecosystem with term definitions associated with verifiable credentials, credential definitions, mappings, encodings, presentation requests, presentations, etc. Changing a context could change the meaning of a signed verifiable credential. Contexts are specified in JSON-LD. Encoding An encoding specifies the source data type and conversion algorithm used to transform a claim property to an attribute that can be signed using a ZKP style signature. ZKP signatures require an array of integer attributes. Some source data types can be converted to an integer in a way that does not support predicate proofs, such as \u201cless than\u201d or \u201cgreater than or equal\u201d. For example, a source data type of string (stored as UTF-8) could be converted to an integer using a hashing algorithm, such as SHA-256. This converted attribute can be signed. However, in a presentation the value can only be disclosed or not disclosed; predicate proofs are not possible. Other encodings, such as dates, could be converted into seconds since 1970. This would allow the value to be disclosed or not disclosed; predicate proofs are also enabled. A proof such as \u201cAge over 18\u201d could be constructed to determine if an individual is an adult without revealing the individual\u2019s age. Encoding definitions are stored immutably on the ledger. Corresponding encoding algorithms are coded in approved and signed libraries. Encoding definitions are specified in JSON-LD. Presentation Request Presentation Requests describe to a holder the set of properties, types of proofs, and issuers that are acceptable to a verifier. Presentation requests can specify one or more sources for a presentation data element. Graph paths are used to define the specific property from a source credential to be used in a derived credential, which will be included in the presentation. Proof types indicate whether the attribute value will be revealed or a predicate which must be satisfied. A list of acceptable issuers for a presentation data element is also specified. Presentation requests can be stored immutably on the ledger in cases where reuse is important. However, a presentation request can be specific to a pair-wise relationship, in which case, the presentation request can be stored off-ledger. Presentation requests are specified in JSON-LD. Presentation Presentations are a special case of a verifiable credential. Presentations contain derived claims from verifiable credentials. Presentations also contain cryptographic material for the proof of the derived claims. In addition, where more than one verifiable credential was used to create the derived claims, the presentation must contain cryptographic material for proof that the source credentials are held by the same entity. In some instances a presentation may include self-asserted data not based on derived data from a verifiable credential. The verifier analyzes the presentation to substantiate claims using the cryptographic material. Presentations are stored off the ledger by the verifier or deleted after use. Presentations are specified in JSON-LD. Conclusion Once a verifiable credential is issued it cannot be changed without breaking the cryptographic chain of trust. Similarly, once a verifiable credential is issued, the immutable data objects that provide a full definition of the data being signed cannot be changed without breaking the chain of trust.","title":"Using Immutable Data Objects to Define Verifiable Credential Data"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#using-immutable-data-objects-to-define-verifiable-credential-data","text":"Ken Ebert, Sovrin Foundation, ken@sovrin.org","title":"Using Immutable Data Objects to Define Verifiable Credential Data"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#abstract","text":"Verifiable Credentials are strengthened by providing immutable data objects that provide a full definition of the data being signed. This is particularly true for objects with ZKP style signatures, where a more granular description of the data is required in order to support disclosure and predicate proofs on a per-property basis.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#control-objects","text":"Control objects, such as DIDs, revocation registries, and payment addresses, contain data required to establish control. It is expected that these objects need to change over time. Keys need to be rotated, credentials need to be revoked, and balances need to be updated.","title":"Control Objects"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#data-objects","text":"Data objects contain information that describes semantic meaning of verifiable credentials. In order to establish a cryptographic assurance of what is being signed in a verifiable credential, the definition of the meaning must not change after a credential is issued. Immutable data objects that provide this definition of meaning include:","title":"Data Objects"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#verifiable-credential","text":"A verifiable credential is a set of signed claims about a subject. The issuer and claims in the credential about the subject can be cryptographically verified to confirm that the issuer made those claims. The meaning of the claims is represented by other data objects. Verifiable credentials are stored off-ledger to preserve privacy. Verifiable credential definitions are specified in JSON-LD.","title":"Verifiable Credential"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#credential-definition","text":"The credential definition identifies the issuer, an associated mapping, and public keys for this type of credential. Credential definitions are immutably stored on-ledger to provide access to the materials used to verify any of the credentials issued using the definition. Credential definitions are specified in JSON-LD.","title":"Credential Definition"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#mapping","text":"A mapping contains an ordered selection of properties from the source schema. For each property a corresponding encoding is specified to prepare the data for ZKP signatures. The primary objective of the mapping is to provide a sequence of numbers to sign in a ZKP style proof. Mappings are immutably stored on-ledger to provide the holder with the definition of each property and how that property was transformed into a number included in the ZKP proof. Mappings can be shared between issuers to promote interoperability among trust frameworks established by communities of trust; multiple issuers can refer the same mapping from their respective credential definitions. Others outside the issuer\u2019s trust framework may choose to trust credentials from the framework. Mappings are specified in JSON-LD.","title":"Mapping"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#schema","text":"A schema defines the structure and type of data for claim data. Schemas can contain sub-schemas. Schemas can represent complex data objects. Schemas include properties such as type, label, and description. Standard schemas can be extended to incorporate definitions of properties not previously included. Many schemas already exist, such as those available at schema.org. Many schemas available on the internet do not have the property of immutability that can provide an unchanging definition of the properties signed in a verifiable credential. Schemas are immutably stored on-ledger to provide the holder with the definition of each property. Mappings refer to the properties defined in the schema to select properties for inclusion by an issuer in a verifiable credential. Schemas can be shared between mappings to promote interoperability. Verifiers\u2019 presentation requests also refer to the schemas used in a verifiable credential to specify which properties from the verifiable credential should be included in a derived presentation created by a holder. Multiple presentations can share schemas. Schemas are specified in JSON-LD.","title":"Schema"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#context","text":"A context is a collection of shortcut term definitions. Contexts establish definitions that promote efficiency while preserving accuracy. Contexts are immutably store on-ledger to provide the ecosystem with term definitions associated with verifiable credentials, credential definitions, mappings, encodings, presentation requests, presentations, etc. Changing a context could change the meaning of a signed verifiable credential. Contexts are specified in JSON-LD.","title":"Context"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#encoding","text":"An encoding specifies the source data type and conversion algorithm used to transform a claim property to an attribute that can be signed using a ZKP style signature. ZKP signatures require an array of integer attributes. Some source data types can be converted to an integer in a way that does not support predicate proofs, such as \u201cless than\u201d or \u201cgreater than or equal\u201d. For example, a source data type of string (stored as UTF-8) could be converted to an integer using a hashing algorithm, such as SHA-256. This converted attribute can be signed. However, in a presentation the value can only be disclosed or not disclosed; predicate proofs are not possible. Other encodings, such as dates, could be converted into seconds since 1970. This would allow the value to be disclosed or not disclosed; predicate proofs are also enabled. A proof such as \u201cAge over 18\u201d could be constructed to determine if an individual is an adult without revealing the individual\u2019s age. Encoding definitions are stored immutably on the ledger. Corresponding encoding algorithms are coded in approved and signed libraries. Encoding definitions are specified in JSON-LD.","title":"Encoding"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#presentation-request","text":"Presentation Requests describe to a holder the set of properties, types of proofs, and issuers that are acceptable to a verifier. Presentation requests can specify one or more sources for a presentation data element. Graph paths are used to define the specific property from a source credential to be used in a derived credential, which will be included in the presentation. Proof types indicate whether the attribute value will be revealed or a predicate which must be satisfied. A list of acceptable issuers for a presentation data element is also specified. Presentation requests can be stored immutably on the ledger in cases where reuse is important. However, a presentation request can be specific to a pair-wise relationship, in which case, the presentation request can be stored off-ledger. Presentation requests are specified in JSON-LD.","title":"Presentation Request"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#presentation","text":"Presentations are a special case of a verifiable credential. Presentations contain derived claims from verifiable credentials. Presentations also contain cryptographic material for the proof of the derived claims. In addition, where more than one verifiable credential was used to create the derived claims, the presentation must contain cryptographic material for proof that the source credentials are held by the same entity. In some instances a presentation may include self-asserted data not based on derived data from a verifiable credential. The verifier analyzes the presentation to substantiate claims using the cryptographic material. Presentations are stored off the ledger by the verifier or deleted after use. Presentations are specified in JSON-LD.","title":"Presentation"},{"location":"rwot8/topics-and-advance-readings/Using-Immutable-Data-Objects/#conclusion","text":"Once a verifiable credential is issued it cannot be changed without breaking the cryptographic chain of trust. Similarly, once a verifiable credential is issued, the immutable data objects that provide a full definition of the data being signed cannot be changed without breaking the chain of trust.","title":"Conclusion"},{"location":"rwot8/topics-and-advance-readings/Zero-knowledge-Proofs/","text":"Zero-knowledge proofs in identity systems by Jordi Baylina < jordi@iden3.io > and David Suarez < david@iden3.io > Privacy is key to identity systems, and Zero-knowledge proofs (ZKP) are core to maintain confidentiality over user data, but still being able to transact by receiving claims and proving these to a third party. Model of a ZKP We start with the description of a model of as a deterministic program or logic circuit (from now circuit) where there is a private input (secret) and a public output. Such a system is able to generate a proof of the private input which is a function of the private input itself and the circuit. Using these variables, a verification function can be constructed such as depending on the circuit, the output and the proof (but still not the private input), a validation can be performed in terms of true or false, conforming the ZKP schema. Example We could illustrate that in an example, based on SHA-256 hashing of a private input r . We can proof that we know the r that matches a given without revealing it. A proof can be constructed over these two signals such as Proof = F(SHA-256, r) and the verification function would check the validity of the proof if V(SHA-256, H(r), Proof) = true . Merkle tree structures Merkle trees are the fundamental data structure for the iden3 technology to store and validate the claims for each identity. The dependencies and the hierarchy model allows to generate proofs in a very efficient way. For example, in a votation use case, we could have a merkle tree like this one: Merkle proof In the leafs of the merkle tree there is the claim for each user enabling to participate in the votation, and the middle levels of the tree (and the root) are hashes of the combined data of the tree at the lower level. So, a proof for a single user can be generated from the specific claim and the corresponding siblings (which are the nodes of the tree needed for the upwards recalculation of the tree) of the merkle tree such as that: Proof validator circuit So, in the ZKP model previuosly described, we could be able to design such a circuit which recreates the calculation model of the root of a merkle tree to verify the received proofs by recalculating the root of the merkle tree. This root will be written in the public Ethereum blockchain so it can be compared and the proof considered as valid. To illustrate this validator abstraction, let's take the basic part with the circuit that generates a public key of an identity (which is known) from the private key (which is not known): This circuit can be included into a more complex one where the whole merkle tree is calculated to verify the proof and to check if a specific identity belongs to a census and so is enabled to vote. In this scenario, it might be necessary to control that each identity can only vote once, so an additional calculation can be included which the application of voting is able to register if a unique identity has been participating before but not having to store any identifier which can be associated with the identity. This is a nullifier function. 3-party of claim issuance, validation and ZKP The three party model (identity, claim issuer and verifier) in combination with ZKP is the core of privacy of identity attestations since a user can answer to any requirement as long as the claim exists and there is a trust relationship between the verifier and the issuer. And the most important thing, no information is revealed in this process. The user is providing compliance proofs which are enough for the validator to be accepted. Non-reusable proofs In addition to the 3-party model, since the proofs include some kind of information of validation, it's important that the proofs generated for a verifier identity are not reusable to another identity and so maintain privacy for the users. This mecanism is implemented with a specific circuit that gives validity of the proof to the original recipient but invalidates the proof if this identity would try to forward it, because the proof is only valid if the sender does not know the private key of the validator. Since the initial recipient knows his own private key, this proof is not valid to be forwarded.","title":"Zero-knowledge proofs in identity systems"},{"location":"rwot8/topics-and-advance-readings/Zero-knowledge-Proofs/#zero-knowledge-proofs-in-identity-systems","text":"by Jordi Baylina < jordi@iden3.io > and David Suarez < david@iden3.io > Privacy is key to identity systems, and Zero-knowledge proofs (ZKP) are core to maintain confidentiality over user data, but still being able to transact by receiving claims and proving these to a third party.","title":"Zero-knowledge proofs in identity systems"},{"location":"rwot8/topics-and-advance-readings/Zero-knowledge-Proofs/#model-of-a-zkp","text":"We start with the description of a model of as a deterministic program or logic circuit (from now circuit) where there is a private input (secret) and a public output. Such a system is able to generate a proof of the private input which is a function of the private input itself and the circuit. Using these variables, a verification function can be constructed such as depending on the circuit, the output and the proof (but still not the private input), a validation can be performed in terms of true or false, conforming the ZKP schema.","title":"Model of a ZKP"},{"location":"rwot8/topics-and-advance-readings/Zero-knowledge-Proofs/#example","text":"We could illustrate that in an example, based on SHA-256 hashing of a private input r . We can proof that we know the r that matches a given without revealing it. A proof can be constructed over these two signals such as Proof = F(SHA-256, r) and the verification function would check the validity of the proof if V(SHA-256, H(r), Proof) = true .","title":"Example"},{"location":"rwot8/topics-and-advance-readings/Zero-knowledge-Proofs/#merkle-tree-structures","text":"Merkle trees are the fundamental data structure for the iden3 technology to store and validate the claims for each identity. The dependencies and the hierarchy model allows to generate proofs in a very efficient way. For example, in a votation use case, we could have a merkle tree like this one:","title":"Merkle tree structures"},{"location":"rwot8/topics-and-advance-readings/Zero-knowledge-Proofs/#merkle-proof","text":"In the leafs of the merkle tree there is the claim for each user enabling to participate in the votation, and the middle levels of the tree (and the root) are hashes of the combined data of the tree at the lower level. So, a proof for a single user can be generated from the specific claim and the corresponding siblings (which are the nodes of the tree needed for the upwards recalculation of the tree) of the merkle tree such as that:","title":"Merkle proof"},{"location":"rwot8/topics-and-advance-readings/Zero-knowledge-Proofs/#proof-validator-circuit","text":"So, in the ZKP model previuosly described, we could be able to design such a circuit which recreates the calculation model of the root of a merkle tree to verify the received proofs by recalculating the root of the merkle tree. This root will be written in the public Ethereum blockchain so it can be compared and the proof considered as valid. To illustrate this validator abstraction, let's take the basic part with the circuit that generates a public key of an identity (which is known) from the private key (which is not known): This circuit can be included into a more complex one where the whole merkle tree is calculated to verify the proof and to check if a specific identity belongs to a census and so is enabled to vote. In this scenario, it might be necessary to control that each identity can only vote once, so an additional calculation can be included which the application of voting is able to register if a unique identity has been participating before but not having to store any identifier which can be associated with the identity. This is a nullifier function.","title":"Proof validator circuit"},{"location":"rwot8/topics-and-advance-readings/Zero-knowledge-Proofs/#3-party-of-claim-issuance-validation-and-zkp","text":"The three party model (identity, claim issuer and verifier) in combination with ZKP is the core of privacy of identity attestations since a user can answer to any requirement as long as the claim exists and there is a trust relationship between the verifier and the issuer. And the most important thing, no information is revealed in this process. The user is providing compliance proofs which are enough for the validator to be accepted.","title":"3-party of claim issuance, validation and ZKP"},{"location":"rwot8/topics-and-advance-readings/Zero-knowledge-Proofs/#non-reusable-proofs","text":"In addition to the 3-party model, since the proofs include some kind of information of validation, it's important that the proofs generated for a verifier identity are not reusable to another identity and so maintain privacy for the users. This mecanism is implemented with a specific circuit that gives validity of the proof to the original recipient but invalidates the proof if this identity would try to forward it, because the proof is only valid if the sender does not know the private key of the validator. Since the initial recipient knows his own private key, this proof is not valid to be forwarded.","title":"Non-reusable proofs"},{"location":"rwot8/topics-and-advance-readings/bootstrap_web-of-trust_reliance-lifecycle/","text":"How do we bootstrap the web of trust in Verifiable Claims by Matt Stone, Brightlink and Dan Burnett, Consensys In the world of Verifiable Credentials, it is essential that Verifiers can trust Issuers. To this end, there must be a common understanding of the \u201cfunctional identity\u201d of the Issuer.[joe\u2019s paper]. How do humans establish the appropriate level understanding to trust the artifact with conviction, i.e. how does one link \u201cthis key\u201d to \u201cthat real world entity (person, company, etc)\u201d Significant work has been done on this topic to date, but what\u2019s true now, and what\u2019s handwaving to solve later? As chairs of the VCWG the authors have been close to this topic for a while now, but it is still unclear how all the pieces fit together, primarily when it comes to how humans in the world will recognize and become comfortable with this process online. Situation For example, any individual or company can register a DID and Issue a VC to any individual. How does a Verifier really understand and validate that the issuer is the actual \u201cDepartment of X\u201d or \u201cCompany Y\u201d? Challenges The challenges are: In the beginning it\u2019s all \u201cuntrustworthy b/c noone has a reputation\u201d How do we get a critical mass of understanding so Issuers and Verifiers know what\u2019s trustworthy? We have great technology, how to you jump-start grassroots adoption? Ideas work group process goes here... Decisions/Approach Workgroup deliverable will go here :) Evidence of Success What are the metrics that indicate we\u2019re growing towards a critical mass of both understanding and reliance? How do we measure where we are on a \u201creliance life cycle\u201d. Consider a lifecycle that indicates how heavily a solution is relied upon: New capabilities move from \u201ccool, new, exciting\u201d through \u201cuseful in the marketplace\u201d to \u201cessential for everyday life\u201d. Where are we in the life cycle and are we progressing to the next phase of reliance? Consider how reliant we\u2019ve become on GPS as an analogy","title":"How do we bootstrap the web of trust in Verifiable Claims"},{"location":"rwot8/topics-and-advance-readings/bootstrap_web-of-trust_reliance-lifecycle/#how-do-we-bootstrap-the-web-of-trust-in-verifiable-claims","text":"","title":"How do we bootstrap the web of trust in Verifiable Claims"},{"location":"rwot8/topics-and-advance-readings/bootstrap_web-of-trust_reliance-lifecycle/#by-matt-stone-brightlink-and-dan-burnett-consensys","text":"In the world of Verifiable Credentials, it is essential that Verifiers can trust Issuers. To this end, there must be a common understanding of the \u201cfunctional identity\u201d of the Issuer.[joe\u2019s paper]. How do humans establish the appropriate level understanding to trust the artifact with conviction, i.e. how does one link \u201cthis key\u201d to \u201cthat real world entity (person, company, etc)\u201d Significant work has been done on this topic to date, but what\u2019s true now, and what\u2019s handwaving to solve later? As chairs of the VCWG the authors have been close to this topic for a while now, but it is still unclear how all the pieces fit together, primarily when it comes to how humans in the world will recognize and become comfortable with this process online.","title":"by Matt Stone, Brightlink and Dan Burnett, Consensys"},{"location":"rwot8/topics-and-advance-readings/bootstrap_web-of-trust_reliance-lifecycle/#situation","text":"For example, any individual or company can register a DID and Issue a VC to any individual. How does a Verifier really understand and validate that the issuer is the actual \u201cDepartment of X\u201d or \u201cCompany Y\u201d?","title":"Situation"},{"location":"rwot8/topics-and-advance-readings/bootstrap_web-of-trust_reliance-lifecycle/#challenges","text":"The challenges are: In the beginning it\u2019s all \u201cuntrustworthy b/c noone has a reputation\u201d How do we get a critical mass of understanding so Issuers and Verifiers know what\u2019s trustworthy? We have great technology, how to you jump-start grassroots adoption?","title":"Challenges"},{"location":"rwot8/topics-and-advance-readings/bootstrap_web-of-trust_reliance-lifecycle/#ideas","text":"work group process goes here...","title":"Ideas"},{"location":"rwot8/topics-and-advance-readings/bootstrap_web-of-trust_reliance-lifecycle/#decisionsapproach","text":"Workgroup deliverable will go here :)","title":"Decisions/Approach"},{"location":"rwot8/topics-and-advance-readings/bootstrap_web-of-trust_reliance-lifecycle/#evidence-of-success","text":"What are the metrics that indicate we\u2019re growing towards a critical mass of both understanding and reliance? How do we measure where we are on a \u201creliance life cycle\u201d. Consider a lifecycle that indicates how heavily a solution is relied upon: New capabilities move from \u201ccool, new, exciting\u201d through \u201cuseful in the marketplace\u201d to \u201cessential for everyday life\u201d. Where are we in the life cycle and are we progressing to the next phase of reliance? Consider how reliant we\u2019ve become on GPS as an analogy","title":"Evidence of Success"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/","text":"Bringing the Dependencies of a BTCR Wallet to the Swift Ecosystem Wolf McNally for Blockchain Commons The Swift Ecosystem Swift is a programming language introduced by Apple in 2014 as the new official programming language for writing iOS and macOS apps. In 2015 Apple released Swift under the Apache open source license and also seeded a build of Swift for Linux. A growing community of developers headquartered at Swift.org are extending the language in various ways, including Google's high-performance numerical computation library Swift for TensorFlow and server-side application frameworks such as Vapor . IBM has launched an initiative to promote server-side Swift, including Kitura , its own Swift server framework. Swift has a number of traits that make it attractive for use on the desktop, on mobile devices and cloud services. Swift is a modern, multi-paradigm language designed to be safe, fast, and expressive . In this document, the phrase \"Apple platforms\" refers collectively to iOS, macOS, and tvOS. The use of Swift across Apple platforms and Linux is what this document refers to as the Swift ecosystem . Bridging BTCR Dependencies to Swift While all the code to implement something as complex as a DID resolver or registrar could be written entirely in Swift, it makes sense to leverage code already written in other languages, and use Swift as a top-level language used to tie heterogenous modules into a unified application, whether it be a mobile application or server. This document surveys programing languages and technologies of interest, discusses issues of interoperating with Swift, and lists software packages of note. The author invites additions and clarifications from the reader. Programming Languages C C is the grandfather of most modern programming languages, and is still valued for its simplicity, speed, and interoperability with other languages. The whole Swift ecosystem can call C functions directly.. Packages of interest written in C include: Libsecp256k1 \u2014 Optimized C library for EC operations on curve secp256k1. Used by: Bitcoin Core and Libbitcoin Nettle \u2014 A low-level cryptographic library that is designed to fit easily in more or less any context. Used by: Bread Wallet Breadwallet Core \u2014 An implementation of SPV. Used by: Bread Wallet C++ C++ is an object-oriented dialect of C, and is one of the main workhorse languages in use today. Swift cannot call C++ directly, but we can get around this by writing a C-based shim\u2014 Swift calls the C API and the C API calls C++. This technique works under the entire Swift ecosystem. Packages of interest written in C++ include: Libbitcoin \u2014 Libbitcoin is a multipurpose bitcoin library targeted towards high end use. An ideal backend to build fast implementations on top: mobile apps, desktop clients and server API's. The library places a heavy focus around asychronicity, speed and availability. Blockchain Commons have already released a framework for bridging from Libbitcoin to Swift written by the author. Bitcoin Core \u2014 A free and open-source application that serves as a bitcoin node and provides a bitcoin wallet which fully verifies payments. It is considered to be bitcoin's reference implementation. Boost \u2014 A set of libraries that provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing. It contains over eighty individual libraries. Used by Libbitcoin. V8 \u2014 Google\u2019s open source high-performance JavaScript and WebAssembly engine. It is used in Chrome and in Node.js, among others. There are already available bridges from Swift to V8. Javascript JavaScript is a high-level, interpreted programming language. It is a language that is also characterized as dynamic, weakly typed, prototype-based and multi-paradigm. Alongside HTML and CSS, JavaScript is one of the three core technologies of the World Wide Web. Apple platforms include JavaScriptCore , a built-in framework used by the WebKit engine in the Safari Browser, but also made available directly to Apple platforms developers for evaluating JavaScript programs. On Linux, Swift applications can integrate Javascript by using an execution engine like V8 . Packages of interest written in JavaScript include: jsonld.js \u2014 An implementation of the JSON-LD specification in JavaScript. jsonld-signatures \u2014 An implementation of the Linked Data Signatures specification for JSON-LD. Webcoin \u2014 A Bitcoin SPV client that works in Node.js and the browser. Go Go (AKA Golang) is a statically typed, compiled programming language designed at Google. Swift running under iOS can bridge to Go using Go Mobile . Under Linux, Go modules can be compiled to export C functions that can be called by Swift. Packages of interest written in Go include: btcd \u2014 A full node bitcoin implementation. One key difference between btcd and Bitcoin Core is that btcd does NOT include wallet functionality and this was a very intentional design decision. That functionality is provided by the btcwallet project. btcwallet \u2014 A daemon handling bitcoin wallet functionality for a single user. spvwallet \u2014 Lightweight p2p SPV wallet and library in Go. It connects directly to the bitcoin p2p network to fetch headers, merkle blocks, and transactions. Other Technologies JSON JSON, as specified in RFC7159 , is a simple data description language for representing objects on the Web. The Swift Foundation framework , available on Apple platforms and Linux, includes APIs for working with JSON: The Codable protocol, which works in concert with the JSONEncoder and JSONDecoder classes to serialize statically-typed data structures. The JSONSerialization class, which can be used to structure and navigate dynamic JSON objects. JSON-LD JSON-LD is an extension of JSON designed as a light-weight syntax that can be used to express Linked Data, and is specifically used in the Decentralized Identifiers (DID) specification for constructing DID Documents . A JSON-LD implementation will usually provide the functionality described in JSON-LD 1.1 Processing Algorithms and API . Swift can integrate JSON-LD by bridging to the jsonld.js package listed above. DID Document Hosting The process of resolving, creating, updating, or revoking a DID implies that the target DID Document itself is stored somewhere under the control of its owner. A DID wallet will need to be able to store, retrieve, and update DID documents via a hosting service or protocol. Services and protocols of note include: GitHub \u2014 A web-based hosting service for version control using Git . It is mostly used for computer code, but can host data of any type. It offers all of the distributed version control and source code management (SCM) functionality of Git as well as adding its own features. DropBox \u2014 A file hosting service that offers cloud storage, file synchronization, personal cloud, and client software. Amazon S3 \u2014 A \"simple storage service\" offered by Amazon Web Services (AWS) that provides object storage through a web service interface. IPFS \u2014 A protocol and network designed to create a content-addressable , peer-to-peer method of storing and sharing hypermedia in a distributed file system. Users typically run an IPFS node that stores their own files, which are replicated upon demand to other IPFS nodes. Data placed into IPFS is public by default, and must be encrypted by the user if it is to remain controlled. Notably, once data is placed into IPFS, it needs to be considered indelible . Storj \u2014 A decentralized, distributed, encrypted file system. Storj data is end-to-end encrypted and can only be accessed by its owners by default, but Storj can also be used to host public buckets as a CDN . Linked Data Signatures Linked Data Signatures describes a mechanism for ensuring the authenticity and integrity of Linked Data documents using digital signatures. It can be applied to Linked Data formats such as JSON-LD or RDF. This is how DID documents are signed. Swift can integrate Linked Data Signatures by bridging to the jsonld-signatures package listed above. REST REST is acronym for REpresentational State Transfer . RESTful web services (RWS), provide interoperability between computer systems on the Internet. The Swift ecosystem uses the Foundation URLSession API to perform HTTP REST requests. Packages like Alamofire build more elegant conveniences on top of this. REST APIs of note include: GitHub API \u2014 Used to manage GitHub repositories and commit data such as DID Documents. Dropbox API \u2014 The Dropbox API allows developers to work with files in Dropbox, including advanced functionality like sharing. Amazon S3 API \u2014 You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web. IPFS API \u2014 When an IPFS node is running as a daemon, it exposes an HTTP API that allows you to control the node and run the same commands you can from the command line. Storj API \u2014 Access the Storj network using a simple REST API. SPV Simple Payment Verification (SPV) is a technique described in Satoshi Nakamoto\u2019s paper . SPV allows a lightweight client (like a mobile app) to verify that a transaction is included in the Bitcoin blockchain, without downloading the entire blockchain. The SPV client only needs download the block headers directly from the Bitcoin network, which are much smaller than the full blocks. To verify that a transaction is in a block, a SPV client requests a proof of inclusion, in the form of a Merkle branch . SPV clients offer more security than web wallets, because they do not need to trust the servers with the information they send. Packages that provide SPV services that can be called from Swift include: Webcoin (see above) BreadWallet Core (see above) spvwallet (see above) BreadWallet Bread Wallet (AKA BreadWallet or BRD) is an open source SPV wallet for iOS and Android that supports a variety of cryptocurrencies, including Bitcoin-based altcoins and Ethereum tokens. Bread Wallet is under active development, hosted on GitHub and released under the MIT open source license. The iOS version has been forked 228 times as of this writing. The iOS and Android versions of Bread Wallet share the same C-based BreadWallet Core (see above) to provide SPV functionality. The current iOS version is written in Swift. One potential path for bringing a BTCR wallet to iOS would be to fork Bread Wallet and then extend it to include BTCR DID capabilities. Benefits of this approach include leveraging a mature and well-tested SPV codebase and shipping app. Possible drawbacks include dealing with possible (TBD) mismatches between the assumptions and requirements of Bread Wallet and those of a BTCR wallet. In any case, incorporating any of the necessary technologies listed above (e.g., JSON-LD) should be possible via the bridging techniques described.","title":"Bringing the Dependencies of a BTCR Wallet to the Swift Ecosystem"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#bringing-the-dependencies-of-a-btcr-wallet-to-the-swift-ecosystem","text":"","title":"Bringing the Dependencies of a BTCR Wallet to the Swift Ecosystem"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#wolf-mcnally-for-blockchain-commons","text":"","title":"Wolf McNally for Blockchain Commons"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#the-swift-ecosystem","text":"Swift is a programming language introduced by Apple in 2014 as the new official programming language for writing iOS and macOS apps. In 2015 Apple released Swift under the Apache open source license and also seeded a build of Swift for Linux. A growing community of developers headquartered at Swift.org are extending the language in various ways, including Google's high-performance numerical computation library Swift for TensorFlow and server-side application frameworks such as Vapor . IBM has launched an initiative to promote server-side Swift, including Kitura , its own Swift server framework. Swift has a number of traits that make it attractive for use on the desktop, on mobile devices and cloud services. Swift is a modern, multi-paradigm language designed to be safe, fast, and expressive . In this document, the phrase \"Apple platforms\" refers collectively to iOS, macOS, and tvOS. The use of Swift across Apple platforms and Linux is what this document refers to as the Swift ecosystem .","title":"The Swift Ecosystem"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#bridging-btcr-dependencies-to-swift","text":"While all the code to implement something as complex as a DID resolver or registrar could be written entirely in Swift, it makes sense to leverage code already written in other languages, and use Swift as a top-level language used to tie heterogenous modules into a unified application, whether it be a mobile application or server. This document surveys programing languages and technologies of interest, discusses issues of interoperating with Swift, and lists software packages of note. The author invites additions and clarifications from the reader.","title":"Bridging BTCR Dependencies to Swift"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#programming-languages","text":"","title":"Programming Languages"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#c","text":"C is the grandfather of most modern programming languages, and is still valued for its simplicity, speed, and interoperability with other languages. The whole Swift ecosystem can call C functions directly.. Packages of interest written in C include: Libsecp256k1 \u2014 Optimized C library for EC operations on curve secp256k1. Used by: Bitcoin Core and Libbitcoin Nettle \u2014 A low-level cryptographic library that is designed to fit easily in more or less any context. Used by: Bread Wallet Breadwallet Core \u2014 An implementation of SPV. Used by: Bread Wallet","title":"C"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#c_1","text":"C++ is an object-oriented dialect of C, and is one of the main workhorse languages in use today. Swift cannot call C++ directly, but we can get around this by writing a C-based shim\u2014 Swift calls the C API and the C API calls C++. This technique works under the entire Swift ecosystem. Packages of interest written in C++ include: Libbitcoin \u2014 Libbitcoin is a multipurpose bitcoin library targeted towards high end use. An ideal backend to build fast implementations on top: mobile apps, desktop clients and server API's. The library places a heavy focus around asychronicity, speed and availability. Blockchain Commons have already released a framework for bridging from Libbitcoin to Swift written by the author. Bitcoin Core \u2014 A free and open-source application that serves as a bitcoin node and provides a bitcoin wallet which fully verifies payments. It is considered to be bitcoin's reference implementation. Boost \u2014 A set of libraries that provide support for tasks and structures such as linear algebra, pseudorandom number generation, multithreading, image processing, regular expressions, and unit testing. It contains over eighty individual libraries. Used by Libbitcoin. V8 \u2014 Google\u2019s open source high-performance JavaScript and WebAssembly engine. It is used in Chrome and in Node.js, among others. There are already available bridges from Swift to V8.","title":"C++"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#javascript","text":"JavaScript is a high-level, interpreted programming language. It is a language that is also characterized as dynamic, weakly typed, prototype-based and multi-paradigm. Alongside HTML and CSS, JavaScript is one of the three core technologies of the World Wide Web. Apple platforms include JavaScriptCore , a built-in framework used by the WebKit engine in the Safari Browser, but also made available directly to Apple platforms developers for evaluating JavaScript programs. On Linux, Swift applications can integrate Javascript by using an execution engine like V8 . Packages of interest written in JavaScript include: jsonld.js \u2014 An implementation of the JSON-LD specification in JavaScript. jsonld-signatures \u2014 An implementation of the Linked Data Signatures specification for JSON-LD. Webcoin \u2014 A Bitcoin SPV client that works in Node.js and the browser.","title":"Javascript"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#go","text":"Go (AKA Golang) is a statically typed, compiled programming language designed at Google. Swift running under iOS can bridge to Go using Go Mobile . Under Linux, Go modules can be compiled to export C functions that can be called by Swift. Packages of interest written in Go include: btcd \u2014 A full node bitcoin implementation. One key difference between btcd and Bitcoin Core is that btcd does NOT include wallet functionality and this was a very intentional design decision. That functionality is provided by the btcwallet project. btcwallet \u2014 A daemon handling bitcoin wallet functionality for a single user. spvwallet \u2014 Lightweight p2p SPV wallet and library in Go. It connects directly to the bitcoin p2p network to fetch headers, merkle blocks, and transactions.","title":"Go"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#other-technologies","text":"","title":"Other Technologies"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#json","text":"JSON, as specified in RFC7159 , is a simple data description language for representing objects on the Web. The Swift Foundation framework , available on Apple platforms and Linux, includes APIs for working with JSON: The Codable protocol, which works in concert with the JSONEncoder and JSONDecoder classes to serialize statically-typed data structures. The JSONSerialization class, which can be used to structure and navigate dynamic JSON objects.","title":"JSON"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#json-ld","text":"JSON-LD is an extension of JSON designed as a light-weight syntax that can be used to express Linked Data, and is specifically used in the Decentralized Identifiers (DID) specification for constructing DID Documents . A JSON-LD implementation will usually provide the functionality described in JSON-LD 1.1 Processing Algorithms and API . Swift can integrate JSON-LD by bridging to the jsonld.js package listed above.","title":"JSON-LD"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#did-document-hosting","text":"The process of resolving, creating, updating, or revoking a DID implies that the target DID Document itself is stored somewhere under the control of its owner. A DID wallet will need to be able to store, retrieve, and update DID documents via a hosting service or protocol. Services and protocols of note include: GitHub \u2014 A web-based hosting service for version control using Git . It is mostly used for computer code, but can host data of any type. It offers all of the distributed version control and source code management (SCM) functionality of Git as well as adding its own features. DropBox \u2014 A file hosting service that offers cloud storage, file synchronization, personal cloud, and client software. Amazon S3 \u2014 A \"simple storage service\" offered by Amazon Web Services (AWS) that provides object storage through a web service interface. IPFS \u2014 A protocol and network designed to create a content-addressable , peer-to-peer method of storing and sharing hypermedia in a distributed file system. Users typically run an IPFS node that stores their own files, which are replicated upon demand to other IPFS nodes. Data placed into IPFS is public by default, and must be encrypted by the user if it is to remain controlled. Notably, once data is placed into IPFS, it needs to be considered indelible . Storj \u2014 A decentralized, distributed, encrypted file system. Storj data is end-to-end encrypted and can only be accessed by its owners by default, but Storj can also be used to host public buckets as a CDN .","title":"DID Document Hosting"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#linked-data-signatures","text":"Linked Data Signatures describes a mechanism for ensuring the authenticity and integrity of Linked Data documents using digital signatures. It can be applied to Linked Data formats such as JSON-LD or RDF. This is how DID documents are signed. Swift can integrate Linked Data Signatures by bridging to the jsonld-signatures package listed above.","title":"Linked Data Signatures"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#rest","text":"REST is acronym for REpresentational State Transfer . RESTful web services (RWS), provide interoperability between computer systems on the Internet. The Swift ecosystem uses the Foundation URLSession API to perform HTTP REST requests. Packages like Alamofire build more elegant conveniences on top of this. REST APIs of note include: GitHub API \u2014 Used to manage GitHub repositories and commit data such as DID Documents. Dropbox API \u2014 The Dropbox API allows developers to work with files in Dropbox, including advanced functionality like sharing. Amazon S3 API \u2014 You can use Amazon S3 to store and retrieve any amount of data at any time, from anywhere on the web. IPFS API \u2014 When an IPFS node is running as a daemon, it exposes an HTTP API that allows you to control the node and run the same commands you can from the command line. Storj API \u2014 Access the Storj network using a simple REST API.","title":"REST"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#spv","text":"Simple Payment Verification (SPV) is a technique described in Satoshi Nakamoto\u2019s paper . SPV allows a lightweight client (like a mobile app) to verify that a transaction is included in the Bitcoin blockchain, without downloading the entire blockchain. The SPV client only needs download the block headers directly from the Bitcoin network, which are much smaller than the full blocks. To verify that a transaction is in a block, a SPV client requests a proof of inclusion, in the form of a Merkle branch . SPV clients offer more security than web wallets, because they do not need to trust the servers with the information they send. Packages that provide SPV services that can be called from Swift include: Webcoin (see above) BreadWallet Core (see above) spvwallet (see above)","title":"SPV"},{"location":"rwot8/topics-and-advance-readings/bringing-dependencies-btcr-wallet-to-swift-ecosystem/#breadwallet","text":"Bread Wallet (AKA BreadWallet or BRD) is an open source SPV wallet for iOS and Android that supports a variety of cryptocurrencies, including Bitcoin-based altcoins and Ethereum tokens. Bread Wallet is under active development, hosted on GitHub and released under the MIT open source license. The iOS version has been forked 228 times as of this writing. The iOS and Android versions of Bread Wallet share the same C-based BreadWallet Core (see above) to provide SPV functionality. The current iOS version is written in Swift. One potential path for bringing a BTCR wallet to iOS would be to fork Bread Wallet and then extend it to include BTCR DID capabilities. Benefits of this approach include leveraging a mature and well-tested SPV codebase and shipping app. Possible drawbacks include dealing with possible (TBD) mismatches between the assumptions and requirements of Bread Wallet and those of a BTCR wallet. In any case, incorporating any of the necessary technologies listed above (e.g., JSON-LD) should be possible via the bridging techniques described.","title":"BreadWallet"},{"location":"rwot8/topics-and-advance-readings/contextual_nondiscrete_identity/","text":"Contextual Non-discrete Identity By Siu Kei Chung, Clearmatics Technologies cc@clearmatics.com Introduction The idea of an identity, a constant unit defining one as a unique entity, seems to be an agreed-upon concept. Universal identity and the ability to uniquely identify any entity across any system identically is what we all strive for. As we begin to make attempts at defining an identity and to encompass the characteristics of an entity we start to find that the boundaries between truth and perception become blurred. Identity starts to become a spectrum, a continuous array of possible descriptions of your subject. The granularity of your definition is called into question. If I were to be identified where would you start? Suddenly it becomes clear that identity is almost always a description of an entity within a context. Is Bob a hard-working employee or is Alice a trust-worthy friend? Each description may also carry different connotations in different contexts yet we still immediately reach for our vocabulary book without understanding how identities should be defined and for what purpose. In this short paper I question the notion of a single discrete identity and the continuous nature of identity. Trust in Interactions In any interaction between two individuals, whether online or offline, we are subject to the evaluation by the counterparty. Our movements and actions contribute to a mostly subconcious process that determines how we are perceived by the other side. Given an interaction that involves a transaction, this becomes a very conscious process, deeming whether our counterparty will fulfill their end of an agreement or not. A lot of the game theory of interactions in open systems such as social encounters hinges on the correct perception of another's motivations. Thus trust becomes a function of both the perception of an identity and that in the context of the interaction. What is trust? Given rational participants, two individuals will only participate in a system or interaction to the extent to which they believe their counterparty will not be malicious. Malicious in this context could mean anything interpreted by 'betrayal of trust' but for simplicity we can imagine a simple game of giving money and having it returned to you. If Alice absolutely trusts Bob, she will give him any amount of money and expect it to be returned. If he is as trustworthy, that is what will happen. If Alice does not trust Bob, she will never give him any money as she expects to always be robbed. Most interactions lie in between with a sea of complexities resulting from an perceived aggregate of incentives of each individual by their respective counterparts to calculate the level of trust they have for each other as a measure of how much they will engage in the interaction. Most times they reduce to emotional choices. Trust therefore becomes an individual-specific measureable extent to which they will participate in an interaction. In more complex scenarios, a simplification of trust would simply be that one individual will only put at risk, in any given interaction, value that is less than what they perceive their counterparty would lose if they were malicious. That becomes the measure of trust. The redundancies of self-identity As conscious individuals we form our own ideas of self. To ourselves we are characterised mostly by our thoughts and the way we perceive them. We observe our own actions and determine a description set that matches that of who we are, or so we believe. We do well to judge ourselves by our intentions as we measure ourselves by our own values and ideals, but we measure others by their actions as they are all that are exhibited. These two poles don't usually coincide and we are usually bad judges of ourselves. Self-identity is an important component of identity as identities are continuously evolving, however, how we choose to see ourselves makes no impact upon the way the world interfaces with us. Until our actions define our character and provide a window into our psychology, our ideas of self-identity have no tangible effect on us as an identity. Arguably, identity as an isolated concept does not exist and can only exist in the context of other identities and how they interact. In effect, self-identity is simply an isolated idea of an individual's own perception of self and plays no direct part in interactions. Contextual Identity In distributed, decentralised systems designed to become an engine of some function, usually involving people, that may eventually encode entire economies or social structures, identity becomes paramount. The context in which identities operate dictate how the system will function and how incentive mechanisms uphold the correct functioning of the system via governance protocols. All systems that involve people have varying degrees of incentivisation depending on the purpose that the system has and as such each individual's participation is dependent on their on self-interest bounded by the possible benefit in their participation. In a system of participants who all freely interact, all possess unique perceptions of the identity of each other to varying degrees. Those degrees reduce down depending on the relevance in the interaction between any two individuals which then become measurements in each participant's calculation of how best to interact (to maximise gain; whatever that means). The identity of each entity is never absolute. It becomes the union of all possible perceptions of a single entity which result in an intangible probability of the subject possessing certain traits. The complexities of unbounded identities produce an impossibility for practical usage. The measurement of perception, too, is an impossible task. How can we encode enough about identity to allow further future systems to be built without the boundaries of current identity models? Incentives and trust are intertwined concepts that dictate how individuals participate in a system. Will discrete identities suffice for all possible systems? Could we build a more complex system of identities around contextual definitions that take into account an individual's prior involvement in any given system?","title":"Contextual Non-discrete Identity"},{"location":"rwot8/topics-and-advance-readings/contextual_nondiscrete_identity/#contextual-non-discrete-identity","text":"By Siu Kei Chung, Clearmatics Technologies cc@clearmatics.com","title":"Contextual Non-discrete Identity"},{"location":"rwot8/topics-and-advance-readings/contextual_nondiscrete_identity/#introduction","text":"The idea of an identity, a constant unit defining one as a unique entity, seems to be an agreed-upon concept. Universal identity and the ability to uniquely identify any entity across any system identically is what we all strive for. As we begin to make attempts at defining an identity and to encompass the characteristics of an entity we start to find that the boundaries between truth and perception become blurred. Identity starts to become a spectrum, a continuous array of possible descriptions of your subject. The granularity of your definition is called into question. If I were to be identified where would you start? Suddenly it becomes clear that identity is almost always a description of an entity within a context. Is Bob a hard-working employee or is Alice a trust-worthy friend? Each description may also carry different connotations in different contexts yet we still immediately reach for our vocabulary book without understanding how identities should be defined and for what purpose. In this short paper I question the notion of a single discrete identity and the continuous nature of identity.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/contextual_nondiscrete_identity/#trust-in-interactions","text":"In any interaction between two individuals, whether online or offline, we are subject to the evaluation by the counterparty. Our movements and actions contribute to a mostly subconcious process that determines how we are perceived by the other side. Given an interaction that involves a transaction, this becomes a very conscious process, deeming whether our counterparty will fulfill their end of an agreement or not. A lot of the game theory of interactions in open systems such as social encounters hinges on the correct perception of another's motivations. Thus trust becomes a function of both the perception of an identity and that in the context of the interaction. What is trust? Given rational participants, two individuals will only participate in a system or interaction to the extent to which they believe their counterparty will not be malicious. Malicious in this context could mean anything interpreted by 'betrayal of trust' but for simplicity we can imagine a simple game of giving money and having it returned to you. If Alice absolutely trusts Bob, she will give him any amount of money and expect it to be returned. If he is as trustworthy, that is what will happen. If Alice does not trust Bob, she will never give him any money as she expects to always be robbed. Most interactions lie in between with a sea of complexities resulting from an perceived aggregate of incentives of each individual by their respective counterparts to calculate the level of trust they have for each other as a measure of how much they will engage in the interaction. Most times they reduce to emotional choices. Trust therefore becomes an individual-specific measureable extent to which they will participate in an interaction. In more complex scenarios, a simplification of trust would simply be that one individual will only put at risk, in any given interaction, value that is less than what they perceive their counterparty would lose if they were malicious. That becomes the measure of trust.","title":"Trust in Interactions"},{"location":"rwot8/topics-and-advance-readings/contextual_nondiscrete_identity/#the-redundancies-of-self-identity","text":"As conscious individuals we form our own ideas of self. To ourselves we are characterised mostly by our thoughts and the way we perceive them. We observe our own actions and determine a description set that matches that of who we are, or so we believe. We do well to judge ourselves by our intentions as we measure ourselves by our own values and ideals, but we measure others by their actions as they are all that are exhibited. These two poles don't usually coincide and we are usually bad judges of ourselves. Self-identity is an important component of identity as identities are continuously evolving, however, how we choose to see ourselves makes no impact upon the way the world interfaces with us. Until our actions define our character and provide a window into our psychology, our ideas of self-identity have no tangible effect on us as an identity. Arguably, identity as an isolated concept does not exist and can only exist in the context of other identities and how they interact. In effect, self-identity is simply an isolated idea of an individual's own perception of self and plays no direct part in interactions.","title":"The redundancies of self-identity"},{"location":"rwot8/topics-and-advance-readings/contextual_nondiscrete_identity/#contextual-identity","text":"In distributed, decentralised systems designed to become an engine of some function, usually involving people, that may eventually encode entire economies or social structures, identity becomes paramount. The context in which identities operate dictate how the system will function and how incentive mechanisms uphold the correct functioning of the system via governance protocols. All systems that involve people have varying degrees of incentivisation depending on the purpose that the system has and as such each individual's participation is dependent on their on self-interest bounded by the possible benefit in their participation. In a system of participants who all freely interact, all possess unique perceptions of the identity of each other to varying degrees. Those degrees reduce down depending on the relevance in the interaction between any two individuals which then become measurements in each participant's calculation of how best to interact (to maximise gain; whatever that means). The identity of each entity is never absolute. It becomes the union of all possible perceptions of a single entity which result in an intangible probability of the subject possessing certain traits. The complexities of unbounded identities produce an impossibility for practical usage. The measurement of perception, too, is an impossible task. How can we encode enough about identity to allow further future systems to be built without the boundaries of current identity models? Incentives and trust are intertwined concepts that dictate how individuals participate in a system. Will discrete identities suffice for all possible systems? Could we build a more complex system of identities around contextual definitions that take into account an individual's prior involvement in any given system?","title":"Contextual Identity"},{"location":"rwot8/topics-and-advance-readings/credentials-and-correlation/","text":"Credentials and Correlation A submission to Rebooting the Web of Trust #8 Jack Poole, jack.w.poole@gmail.com Background Within SSI frameworks, selective disclosure using zero-knowledge proofs and verifiable credentials enables users to prove specific attributes about themselves, while keeping other attributes private. This could allow a user to prove that that are a graduate of University X or are a current employee of Company Y without revealing their name or other aspects of their identity. In scenarios where a user wishes to disclose multiple attributes about their identity, whether from a single credential or from a combination of multiple credentials, it\u2019s important for users to understand that disclosing multiple attributes about one\u2019s identity could expose them to unwanted correlation. Problem While data can be anonymized or de-identified to protect user privacy, de-anonymization attacks are possible when different data sets can be linked together. For instance, when multiple de-identified datasets are analyzed together, such as mobile phone logs and transit trips [1], correlation can expose individual identities. Selective disclosure workflows allow SSI users to share attributes about their identity in a privacy-respecting way, but only if they share attributes that can\u2019t be correlated. How can users be educated of the privacy implications of such behaviors, and what role can software agents have? Goal It should be clear to SSI users that utilizing selective disclosure technologies can expose them to correlation attacks. Similar to SSL warnings, users should be guided away from vulnerable situations with explanations and clear guidance of safe practices. I would like to explore the potential and limitations that software agents can have in protecting user privacy. How can agents assess a user\u2019s degree of anonymity when sharing multiple attributes? What anonymity set is considered safe for certain applications? References: [1] http://news.mit.edu/2018/privacy-risks-mobility-data-1207","title":"**Credentials and Correlation**"},{"location":"rwot8/topics-and-advance-readings/credentials-and-correlation/#credentials-and-correlation","text":"A submission to Rebooting the Web of Trust #8 Jack Poole, jack.w.poole@gmail.com","title":"Credentials and Correlation"},{"location":"rwot8/topics-and-advance-readings/credentials-and-correlation/#background","text":"Within SSI frameworks, selective disclosure using zero-knowledge proofs and verifiable credentials enables users to prove specific attributes about themselves, while keeping other attributes private. This could allow a user to prove that that are a graduate of University X or are a current employee of Company Y without revealing their name or other aspects of their identity. In scenarios where a user wishes to disclose multiple attributes about their identity, whether from a single credential or from a combination of multiple credentials, it\u2019s important for users to understand that disclosing multiple attributes about one\u2019s identity could expose them to unwanted correlation.","title":"Background"},{"location":"rwot8/topics-and-advance-readings/credentials-and-correlation/#problem","text":"While data can be anonymized or de-identified to protect user privacy, de-anonymization attacks are possible when different data sets can be linked together. For instance, when multiple de-identified datasets are analyzed together, such as mobile phone logs and transit trips [1], correlation can expose individual identities. Selective disclosure workflows allow SSI users to share attributes about their identity in a privacy-respecting way, but only if they share attributes that can\u2019t be correlated. How can users be educated of the privacy implications of such behaviors, and what role can software agents have?","title":"Problem"},{"location":"rwot8/topics-and-advance-readings/credentials-and-correlation/#goal","text":"It should be clear to SSI users that utilizing selective disclosure technologies can expose them to correlation attacks. Similar to SSL warnings, users should be guided away from vulnerable situations with explanations and clear guidance of safe practices. I would like to explore the potential and limitations that software agents can have in protecting user privacy. How can agents assess a user\u2019s degree of anonymity when sharing multiple attributes? What anonymity set is considered safe for certain applications? References: [1] http://news.mit.edu/2018/privacy-risks-mobility-data-1207","title":"Goal"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/","text":"Data Value Realisation: Control, Commons & Ownership by Katryna Dow Up until now the power to capture, analyse and profit from personal data has resided with business, government and social networks. What if you and I had the same power? - Meeco Manifesto, 2012 Introduction As our lives become ever more digital, we are faced with new challenges around the economic models, methods of service delivery and how to equitably distribute the exchange of value. As the amount of data we generate and share accumulates, it tells a rich story through the generation of a \u201cdigital twin\u201d. Access to follow, track and interrogate this digital twin is of increasing value to interested parties. Unfortunately, it also leaves individuals vulnerable to cyber threats such as identity theft. Increasingly, we are faced with solving data and identity issues to support a dual physical and digital existence. Data about an individual (data subject) may even be generated prior to physical birth and continue to be generated after physical death. Therefore, the methods of access, control, delegation and consent are foundational to the legal, commercial and technological rights of data subjects. This is also the specific area of solutions focus for Meeco. In a 2017 publication, industry thought-leaders collaborated to define the need to address the asymmetry of the \u201cfree data\u201d market and look towards a model whereby individuals could be compensated for generating the data that fuels the current centralised, ad-driven, commercial models (Ibarra et. al., 2017). The authors assert that \u201cthe internet economy largely began with a venture-capital fuelled bubble that chased usage with little sense for a business model\u201d (Ibarra et. al., 2017). Since data is becoming a key measure of whether a company will remain relevant through the digital revolution (Opher et. al., 2016), new approaches and business models are required to meet the needs of the changing marketplace (Opher et. al., 2016). To address these increasing data value issues, a number of data-value models and frameworks are emerging, including: Data as Labor Data as Property Data Commons Open Data This paper explores the aforementioned frameworks in the context of the emerging human-centred data economy. The Evolution of Data Sharing The evolution of data sharing has resulted in a common observation; \u201cif it is free, then you\u2019re the product\u201d. Companies such as Microsoft and Apple are making public arguments in a bid to be positioned on the right side of history, advocating for the introduction of ethical data practices through to acknowledging the data rights of customers through open source or user-controlled technologies, such as the Microsoft Decentralised Identity project (2018). Figure 1. (below) demonstrates the two predominate business models that have emerged through data collection over the past two decades. These commercial practices have positioned companies to either make products and services that help customers, or deliver services as a mechanism to make their customers into the product. Data subjects are becoming increasingly aware of the implications of sharing their data. Future proof organisations will be those that succeed to gain and maintain the confidence and trust of their customers. A possible model described by Experian in 2017, was the promise of better value services that are not only secure but easier to consume as a result of customers sharing their data. However, if the value exchange is neither lawful or equitable, then this promise is unlikely to result in better outcomes across health, education, travel and financial services. Conversely, if the right security, privacy and economic models can emerge, the potential for personalised services and expediential growth will fuel the next generation of digital services. Companies who make their customers into the product have been labelled in a category known as \u201cSurveillance Capitalism\u201d. The rise of this phenomenon has been documented by Shoshana Zuboff in her recently published book; The Age of Surveillance Capitalism. It describes the use of data to derive a \u201cbehavioural futures markets,\u201d where predictions about our behaviour are bought and sold to the highest bidder. Zuboff affirms that society is at a critical point in history where our decisions regarding how we use technology, under what terms and who the beneficiaries are, will largely define the rest of the twenty-first century (Zuboff, 2019) Figure 1: Evolution of business models resulting from data sharing. 1. Companies that make their customers into products. 2. Companies that make products to help their customers (\u00a9 Meeco, 2015) Data Regulation Over the past 12 months, regulatory regimes have begun to address the rights of access, portability and control. Some of this regulation has been passed to specifically address the asymmetry and power of surveillance capitalism. The regulation is also trying to provide recourse in parallel to the rapid development and deployment of technologies such as artificial intelligence (AI) inclusive of the data required for training AI. Some of the key regulatory and legal mechanisms include: The General Data Protection Regulation (GDPR) \u2013 Europe, May 2018 Payment Services Directive 2 (PSD2) \u2013 European Union, 13th January 2018 Open Banking (European Union & UK - 13th January 2018, Australia -1st February 2020) Personal Information Protection Act (PIPA) \u2013 Japan (30th May 2017) California Consumer Privacy Act (28th June 2018) The primary objective of these regulatory changes is to address the asymmetry of power exercised by centralised data controllers. More often these are multinational social networks, e-commerce platforms and multinational enterprises. The aim is to protect the rights of individuals in step with the development of technology. However, it is evident that the rate of technology evolution is moving much faster than the ability for regulators to keep pace. As a result, regulation alone won\u2019t bring the necessary changes for a more equitable data economy. The Data Economy In 2012 when Meeco was founded, the preliminary research projects focussed on the value of data. Key lines of enquiry included questions such as: Can data actually be owned, or does the nature of it mean there are always multiple parties of interest, e.g. data creator, subject, observer, issuer etc? Is data control a more likely mechanism to derive value than ownership? What could be enabled at an individual, family or community group through data consolidation and collective control? How might society at large; government and commercial ventures create net additional benefit for through data control mechanisms, such as data commons, trusts or mutual benefit structures? What would be the potential benefits to society and who be the beneficiaries? Reports published by the World Economic Forum painted a picture of an emerging data and identity economy, with the promise of increased agency if trust, transparency and fair value could be established. New Models for the Realisation of Value Regardless of the mechanisms for regulation or technology acceptance, there are some basic fundamentals that drive the creation and realisation of value. These are consistent in society and have been applied to property, financial instruments, gold, art and intellectual property protected artefacts such as music and literature. The question is whether this can now be can be applied to data, such that the data subject is able to achieve fair value for participation in the data economy. In a 2015 article on emerging data rights, Katryna Dow described these universal value mechanisms applied to data as follows: Data is an asset Assets generate value Whoever controls the asset derives the value Markets are built on trading and exchanging value to the advantage of the controller Individuals generate data (assets) but don\u2019t have mechanisms to participate in the value chain, therefore control of the asset needs to lawfully shift to the individual in order for that value to be realised by the individual Mechanisms for individuals to achieve this would be required to enable individuals to be part of the value chain Being part of the value chain creates parity and parity leads to flatter structures and cost reduction, giving rise to peer-2-peer services and the potential to remove intermediaries Cost reduction creates new forms of value available for distribution between the participants New forms of distribution enable new business models which drive innovation Innovation supports the evolution of society and often means the things of the past are no longer acceptable, such as the centralised commercial control of personal data through social networks. Figure 2: Data as an asset unlocks new value in the data marketplace \u00a9 Meeco 2015 Emerging Legal and Commercial Models Over the past decade organisations have been able to increase the value of their asset base through the collection and control of customer data, e.g. loyalty programs and big data repositories. Additionally, organisations have been able to insure data and record its value as a tangible asset to increase the value of balance sheets, stock prices and market position. If companies can do this, then it follows that it should only be a matter of time before the same could be true for individuals. However, without the legal, economic or technological means to derive value from data, it is unlikely that individuals will benefit directly from participating in a data marketplace. More so, without the acknowledgement that the ethical, legal and control mechanisms of the current connected world are not in service to humanity, we risk compounding the economic benefits exploited by relatively few actors through centralised privately held commercial and social platforms. Left to their own devices these platforms are increasingly removing agency in both the physical world and digital transactions. \u201cEvery child born in today\u2019s digital world will live a dual physical and digital life. How we enable privacy and security for their digital twin in as important as the rights afforded to them in the physical world. Protecting their identity, enabling trusted authentication and authorisation of services is the key challenge for us to solve in an increasingly connected world. If we don\u2019t get this right, we risk a generation born into digital slavery rather than delivering on the promise of empowerment through technology\u201d - Katryna Dow, World Government Summit, 2019 To address these increasing data value issues a number of models are emerging, including Data as Labor (DaL), Data as Property (DaP), Data Commons and Open Data. In order to evaluate the merits of these proposed frameworks, it\u2019s important to understand the underlying ethos and where they draw on or seek to apply existing legal statues. Data as Labor The prevailing model of treating personal data is essentially a barter system \u2013 consumers enjoy free services in exchange for their data, which is gathered by service providers at no charge. This model is called Data as Capital (DaC). An alternative offered is DaL (Ibarra et. al., 2017). This offers incentives to consumers to contribute data. DaL would provide benefits such as Increased quality and quantity of user data, especially in the context of AI and Machine Learning, which requires large amounts of different data Curbing the activity of trans-national companies as sole \u201cbuyers\u201d of user data, and the creation of a fair and vibrant market for data labor Increased engagement in their own data by individuals, and creation of \u201cdigital dignity\u201d even to the point of assisting Machine Learning in tackling sophisticated tasks, with commensurate reward While pointing out that DaC and DaL can cohabit in the market, the authors acknowledge the difficulties in creating this new \u201cRadical Market\u201d and data economy. They include: The necessity of enlisting very large organisations in the change. There are different business models among the large commercial data players (Facebook and Google are most reliant on DaC), but all have relatively stable, profitable ` businesses. To pivot to a different form of data economy will be challenging for them, despite the appeal of access to more and different data for use in AI and Machine Learning activities. The need for users\u2019 to participate in numbers and be prepared to bargain collectively (which implies membership of a form of union or membership association). Finally, governments, despite mandating significant new access to user data (e.g. GDPR), have not created labour laws that are compatible with the DaL economy. Apart from these issues, the DaL model does not contemplate data ownership. DaL seeks to return value for users in exchange for their data; once that exchange is complete, it is commonplace that ownership of the relevant data passes to the \u201cbuyer\u201d. The early thinking around DaL formed the basis for Eric Posner and Glen Weyl to extend the concepts of \u201cRadical Markets\u201d into the publication of the book by the same tile in 2018. Radical Markets sets out to demonstrate why private property is inherently monopolistic, and how society would all be better served if private ownership were converted into a public auction for public benefit. They show how the principle of one person, one vote inhibits democracy, suggesting instead an ingenious way for voters to effectively influence the issues that matter most to them. They argue that every citizen of a host country should benefit from immigration\u2014not just migrants and their capitalist employers. They propose leveraging antitrust laws to liberate markets from the grip of institutional investors and creating a data labor movement to force digital monopolies to compensate people for their electronic data. \u201cOnly by radically expanding the scope of markets can we reduce inequality, restore robust economic growth, and resolve political conflicts. But to do that, we must replace our most sacred institutions with truly free and open competition\u201d \u2014 Radical Markets Data as Property Personal data is generated between the data subject and the data controller. This is distinct from intellectual property which is generated by the data subject (Schermer, 2015). As Schermer points out, this fact alone questions whether the data subject owns their personal data as they did not autonomously generate it and suggests that personal data is the opinion that a data controller holds about a person (Schermer, 2015). Schermer examines The Right to Be Forgotten as an application of this theory. Data subjects have a right to be forgotten, but society at large has a right to form and record an opinion about a person, for example for criminal proceedings. If personal data is treated as private property, the right to be forgotten will always win and may inhibit what is best for society (e.g. prosecuting a criminal to protect community safety). Ritter and Mayer (2018) propose a property rules construct that clearly defines a right to own digital information arises upon creation (whether by keystroke or machine) and suggest when and how that right attaches to specific data though the exercise of technological controls. They tout possible benefits such as efficient adaptations of new data asset rules, and new methods for regulating and enforcing rights regarding personal information (Ritter and Mayer, 2018). In 2018, Elizabeth Renieris and Dazza Greenwood co-authored an article on property rights, with the title posing the question: \u201cDo we really want to \u2018sell\u2019 ourselves? The risks of a Property law paradigm for personal data ownership In the article the authors highlight that while \u201cownership implies a property law model of our data, they argue that the legal framework for identity-related data must also consider constitutional or human rights laws rather than mere property law rules. This question speaks to the core of the issue about data ownership. Does moving from corporations trading data to individuals trading data (based on their property rights) move society forward, or risk creating a greater digital divide, and/or making privacy a luxury item out of the reach of much of the population? More so, is the average person aware of the risks, consequences and downstream impacts of trading their data, for example possible exclusion from insurance or employment. If data has the potential to unlock expediential value in terms of personalisation of health, education, finance and employment \u2013 access is critical, but the mechanism of trading may actually inhibit the benefits along with mitigating personal risks. \u201cWe must find the balance that will enable personalisation without further asymmetry in the market\u201d \u2013 Katryna Dow 2019 As outlined by Renieris and Greenwood, under common law, ownership in property is a collective of five rights: Of possession Control Exclusion Enjoyment Disposition Whilst title ownership of real property or goods relates to the evidence of a form of deed, bill of lading, receipt \u2013 essentially proof of commercial transaction or purchase. Also, property rights are granted and as such enable property or goods to be transferred moving ownership and control to another party. However, as more and more our digital twins are a mirror for our physical selves, the issue of our fundamental and (existing) human rights are also bound with the notion of privacy, security and rights to protect oneself and be kept from societal harm. Essentially it will not serve humanity or society if people need to \u201csell\u201d parts of their digital self for commercial gain \u2013 in the same way the law does not permit people to sell body parts. Which leaves us with the tension between the protections afforded by control, e.g. exclusion and enjoyment (imagine if you could limit what Facebook does with your data) and at the same time, property law in its entirety doesn\u2019t account for the phenomenon that data is and exoskeleton (location, action, device) virtual layer around our physical self, and therefore requires protections akin to our physical self. Data Commons Whilst Data commons is not a new concept, it is beginning to gain more attention as the issues of regulation and ownership are debated. Taking a \u2018Commons\u2019 approach has proved to enable other disciplines, e.g. Creative Commons to flourish with fair use and acknowledgment of contributors for the collective good. Critical to a Commons approach is the clarity of purpose, use and application. For data this is particularly challenging because there exists the: data created by individuals (data subject) data that is created about the data created (metadata) data the observer creates about what the data subject created or is derived, enhanced or used from the metadata or data processing. And so, it goes, on between the data subject (or creator) and the observer, collector, processor etc. More often there is more than one party of interest or desired control in any data created. A current example is the data created on social networks. Whilst data subjects may have the \u201crights\u201d to request data created (posts, media, comments), many networks consider the meta-data, insights and inferences to belong to the processor and not the subject. Data Commons provides a framework of co-locating data and data resources and sharing data to create an interoperable resource for the wider community to access in order to conduct research (for example, Genomic Data Commons). Frame works for alternative business models should be tested based on the principles of the commons (Vercellone et. al., 2018). Christopher Allen (2015) summarises the principles of the commons in the following 10 points: Define Boundaries: There are clearly defined boundaries around the common resources of a system from the larger environment. Define Legitimate Users: There is a clearly defined community of legitimate user of those resources. Adapt Locally: Rule for use of resources are adapted to local needs and conditions. Decide Inclusively: Those using resources are included in decision making. Monitor Effectively: There exists effective monitoring of the system by accountable monitors. Share Knowledge: All parties share knowledge of local conditions of the system. Hold Accountable: Have graduated sanctions for those who violate community rules. Offer Mediation: Offer cheap and easy access to conflict resolution. Govern Locally: Community self-determination is recognised by higher level authorities. Don\u2019t Externalise Costs: Resource systems embedded in other resource systems are organised in and accountable to multiple layers of nested communities. Whilst there are many benefits made possible through a Commons approach, the rights of value derived from control of a data asset are not as clear and open to exploitation by entities that have the means to do so. Open Data Open data is the theory that data should be readily available to the public to access, analyse and republish as they wish, without restrictions from copyright, patents or other mechanisms of control (Open Data Institute). For the purpose of this paper, it is to be assumed that data such as weather, pollution, traffic flows, transportation, town-planning and health warnings must stay open in the interest of the greater good. However, the ability for a data subject to combine their own personal data (PII) with open data and social data, results in an amplified data set where more context and intent may be derived, thus increasing the value of the data. The ability to bring personal, proprietary and open data together provides a significant advantage for organisation with the computing power to industrialise this, e.g. Facebook. It is for this reason that the German antitrust watchdog has just launched a legal challenge to order Facebook to stop gathering data from source outside the platform and using the data to track people who are not member or users of Facebook. Conversely there are emerging projects, such as DEcentralised Citizens Owned Data Ecosystem (DECODE) in Spain, whose aim is to provide rights above GDPR in response to people\u2019s concerns about a loss of control over their personal information on the internet (Vercellone et. al., 2018). DECODE aims to build a data-centric digital economy where data that is generated and gathered by citizens, the Internet of Things (IoT), and sensor networks can be available for broader communal use, with appropriate privacy protections. Outcomes will demonstrate the wider social value that comes with individuals being given the power to take control of their personal data and given the means to share their data on their terms (Vercellone et. al., 2018). The Argument Against Data Rights Despite the range of emerging frameworks, there are also strong arguments against the rights of data subjects, in particular for use-cases that benefit the common good, such as; the collection of census, pollution, voting, health and transport data. These data sets are vital to planning critical infrastructure such as hospitals, schools, places of worship, airports and managing environmental impact. Modelling longevity, rates of disease, increase in population and mortality trends are increasing important as we are both living longer and the population size of the ageing \u201cBaby Boomer\u201d generation. Whilst ever commercial actors with centralised data control power can justify the cost of regulatory fines, it is easier for them to continue with existing data practices and build fines into the cost of sale, therefore there is motivation to campaign against individual data rights. Under changes such as Open Banking, it\u2019s likely financial intuitions will argue for the minimum compliance in order to protect what they consider as \u201cowning the customer\u201d. Conversely, neo banks and early adopters may take the lead in providing increased customer utility and better customer experiences by embracing more open and inclusive data eco-systems. CONCLUSION In summary many of these concepts have merit; the potential to blend data as labor (for effort reward) with data as property (rights of contol and enjoyment) with data commons (fair use) together with open data (collective good) represent an ideal legally protected outcome. Harmonising these perspectives will no doubt result in trade-offs and challenges in jurisdictions around the world. As we approach 2020 we are locked in debates around the application of 20th century laws and statutes to technologies that will be ubiquitous over the next 20-100 years. In many ways this is like trying to regulate how to put an engine in a horse rather than accept that the paradigm has completely changed. Historically, we have seen once accepted behaviours such a not wearing a seat-belt, smoking on aircrafts, or denying the right to vote as dangerous and archaic. Laws and norms have evolved to make these practices unlawful. It is our opinion that we will see the same evolution of rights with respect to the lawful access, control, delegation and consent of personal data. However, without a clear legal framework inclusive of liability and recourse, it will continue to be difficult to develop new commercial models and methods of value which can drive the adoption of trusted data marketplaces. This is why open standards, decentralised solutions and interoperability of personal data exchange mechanisms are vital to maintaining a free and democratic society; one that can harness the power of data for both the individual and collective good. References Allen C., (2015) 10 Design Principles for Governing the Commons. Available: http://www.lifewithalacrity.com/2015/03/10-design-principles-for-governing-the-commons.html Allen C., \u201cSelf-Sovereign Identity Principles 1.0,\u201d GitHub, 23 October 2016. [Online]. Available: https://github.com/ChristopherA/self-sovereign-identity/blob/master/self-sovereign-identity- principles.md. [Accessed 13 May 2018]. DECODE - DEcentralised Citizens Owned Data Ecosystem, Spain https://decodeproject.eu Dow K., \u201cMeeco Manifesto\u201d 2012 https://meeco.me/manifesto.html Dow K., Seat Belts \u2013 Cigarettes - Data Blog (October 2015) https://katrynadow.me/seat-belts-cigarettes-data/#more-303 Haslingden R., (2017) Open Banking \u2013 Creating a New Era of Data Sharing. Experian. Available: https://www.experian.co.uk/assets/resources/white-papers/open-banking-whitepaper-2017.pdf Ibarra I. A., Goff L., Hern\u00e1ndez D. J., Laier J., Weyl E. G., (2017) Should We Treat Data as Labor? Moving Beyond \u201cFree\u201d American Economic Association Papers & Proceedings, Vol 1 (1). Facebook \u2013 Germany Antitrust Watchdog to Act Against Facebook https://www.reuters.com/article/us-facebook-germany-antitrust/german-antitrust-watchdog-to-act-against-facebook-report-idUSKCN1P70KO Microsoft: Decentralised Identity \u2013 Own and Control Your Identity https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE2DjfY Opher A., Chou A., Onda A., and Sounderrajan K., (2016) The Rise of the Data Economy: Driving Value through Internet of Things Data Monetization. IBM. Available: https://www.ibm.com/downloads/cas/4JROLDQ7 accessed 21 February 2019. Posner E. A., and Weyl E. G., (2018) Radical Markets. Uprooting Capitalism and Democracy for a Just Society. Princeton University Press. Renieris E. M. and Greenwood D., (2018) Do we really want to \u201csell\u201d ourselves? https://medium.com/@hackylawyER/do-we-really-want-to-sell-ourselves-the-risks-of-a-property-law-paradigm-for-data-ownership-b217e42edffa Ritter J., and Mayer A., (2018) Regulating Data as Property: A New Construct for Moving Forward, 16 Duke Law & Technology Review 220-277 Schermer B., (2015). Privacy and property: do you really own your personal data? Universiteit Leiden Law Blog. Available: https://leidenlawblog.nl/articles/privacy-and-property-do-you-really-own-your-personal-data Vercellone C., Brancaccio F., Giuliani A., Puletti F., Rocchi G., and Vattimo P., (2018) DEcentralised Citizens Owned Data Ecosystem (DECODE) D2.4 Data driven disruptive commons-based models. Zuboff S., (2019) The Age of Surveillance Capitalism. The Fight for the Future at the New Frontier of Power, Allen & Unwin. Forthcoming.","title":"Data Value Realisation: Control, Commons & Ownership"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#data-value-realisation-control-commons-ownership","text":"","title":"Data Value Realisation: Control, Commons &amp; Ownership"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#by-katryna-dow","text":"Up until now the power to capture, analyse and profit from personal data has resided with business, government and social networks. What if you and I had the same power? - Meeco Manifesto, 2012","title":"by Katryna Dow"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#introduction","text":"As our lives become ever more digital, we are faced with new challenges around the economic models, methods of service delivery and how to equitably distribute the exchange of value. As the amount of data we generate and share accumulates, it tells a rich story through the generation of a \u201cdigital twin\u201d. Access to follow, track and interrogate this digital twin is of increasing value to interested parties. Unfortunately, it also leaves individuals vulnerable to cyber threats such as identity theft. Increasingly, we are faced with solving data and identity issues to support a dual physical and digital existence. Data about an individual (data subject) may even be generated prior to physical birth and continue to be generated after physical death. Therefore, the methods of access, control, delegation and consent are foundational to the legal, commercial and technological rights of data subjects. This is also the specific area of solutions focus for Meeco. In a 2017 publication, industry thought-leaders collaborated to define the need to address the asymmetry of the \u201cfree data\u201d market and look towards a model whereby individuals could be compensated for generating the data that fuels the current centralised, ad-driven, commercial models (Ibarra et. al., 2017). The authors assert that \u201cthe internet economy largely began with a venture-capital fuelled bubble that chased usage with little sense for a business model\u201d (Ibarra et. al., 2017). Since data is becoming a key measure of whether a company will remain relevant through the digital revolution (Opher et. al., 2016), new approaches and business models are required to meet the needs of the changing marketplace (Opher et. al., 2016). To address these increasing data value issues, a number of data-value models and frameworks are emerging, including: Data as Labor Data as Property Data Commons Open Data This paper explores the aforementioned frameworks in the context of the emerging human-centred data economy.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#the-evolution-of-data-sharing","text":"The evolution of data sharing has resulted in a common observation; \u201cif it is free, then you\u2019re the product\u201d. Companies such as Microsoft and Apple are making public arguments in a bid to be positioned on the right side of history, advocating for the introduction of ethical data practices through to acknowledging the data rights of customers through open source or user-controlled technologies, such as the Microsoft Decentralised Identity project (2018). Figure 1. (below) demonstrates the two predominate business models that have emerged through data collection over the past two decades. These commercial practices have positioned companies to either make products and services that help customers, or deliver services as a mechanism to make their customers into the product. Data subjects are becoming increasingly aware of the implications of sharing their data. Future proof organisations will be those that succeed to gain and maintain the confidence and trust of their customers. A possible model described by Experian in 2017, was the promise of better value services that are not only secure but easier to consume as a result of customers sharing their data. However, if the value exchange is neither lawful or equitable, then this promise is unlikely to result in better outcomes across health, education, travel and financial services. Conversely, if the right security, privacy and economic models can emerge, the potential for personalised services and expediential growth will fuel the next generation of digital services. Companies who make their customers into the product have been labelled in a category known as \u201cSurveillance Capitalism\u201d. The rise of this phenomenon has been documented by Shoshana Zuboff in her recently published book; The Age of Surveillance Capitalism. It describes the use of data to derive a \u201cbehavioural futures markets,\u201d where predictions about our behaviour are bought and sold to the highest bidder. Zuboff affirms that society is at a critical point in history where our decisions regarding how we use technology, under what terms and who the beneficiaries are, will largely define the rest of the twenty-first century (Zuboff, 2019) Figure 1: Evolution of business models resulting from data sharing. 1. Companies that make their customers into products. 2. Companies that make products to help their customers (\u00a9 Meeco, 2015)","title":"The Evolution of Data Sharing"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#data-regulation","text":"Over the past 12 months, regulatory regimes have begun to address the rights of access, portability and control. Some of this regulation has been passed to specifically address the asymmetry and power of surveillance capitalism. The regulation is also trying to provide recourse in parallel to the rapid development and deployment of technologies such as artificial intelligence (AI) inclusive of the data required for training AI. Some of the key regulatory and legal mechanisms include: The General Data Protection Regulation (GDPR) \u2013 Europe, May 2018 Payment Services Directive 2 (PSD2) \u2013 European Union, 13th January 2018 Open Banking (European Union & UK - 13th January 2018, Australia -1st February 2020) Personal Information Protection Act (PIPA) \u2013 Japan (30th May 2017) California Consumer Privacy Act (28th June 2018) The primary objective of these regulatory changes is to address the asymmetry of power exercised by centralised data controllers. More often these are multinational social networks, e-commerce platforms and multinational enterprises. The aim is to protect the rights of individuals in step with the development of technology. However, it is evident that the rate of technology evolution is moving much faster than the ability for regulators to keep pace. As a result, regulation alone won\u2019t bring the necessary changes for a more equitable data economy.","title":"Data Regulation"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#the-data-economy","text":"In 2012 when Meeco was founded, the preliminary research projects focussed on the value of data. Key lines of enquiry included questions such as: Can data actually be owned, or does the nature of it mean there are always multiple parties of interest, e.g. data creator, subject, observer, issuer etc? Is data control a more likely mechanism to derive value than ownership? What could be enabled at an individual, family or community group through data consolidation and collective control? How might society at large; government and commercial ventures create net additional benefit for through data control mechanisms, such as data commons, trusts or mutual benefit structures? What would be the potential benefits to society and who be the beneficiaries? Reports published by the World Economic Forum painted a picture of an emerging data and identity economy, with the promise of increased agency if trust, transparency and fair value could be established.","title":"The Data Economy"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#new-models-for-the-realisation-of-value","text":"Regardless of the mechanisms for regulation or technology acceptance, there are some basic fundamentals that drive the creation and realisation of value. These are consistent in society and have been applied to property, financial instruments, gold, art and intellectual property protected artefacts such as music and literature. The question is whether this can now be can be applied to data, such that the data subject is able to achieve fair value for participation in the data economy. In a 2015 article on emerging data rights, Katryna Dow described these universal value mechanisms applied to data as follows: Data is an asset Assets generate value Whoever controls the asset derives the value Markets are built on trading and exchanging value to the advantage of the controller Individuals generate data (assets) but don\u2019t have mechanisms to participate in the value chain, therefore control of the asset needs to lawfully shift to the individual in order for that value to be realised by the individual Mechanisms for individuals to achieve this would be required to enable individuals to be part of the value chain Being part of the value chain creates parity and parity leads to flatter structures and cost reduction, giving rise to peer-2-peer services and the potential to remove intermediaries Cost reduction creates new forms of value available for distribution between the participants New forms of distribution enable new business models which drive innovation Innovation supports the evolution of society and often means the things of the past are no longer acceptable, such as the centralised commercial control of personal data through social networks. Figure 2: Data as an asset unlocks new value in the data marketplace \u00a9 Meeco 2015","title":"New Models for the Realisation of Value"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#emerging-legal-and-commercial-models","text":"Over the past decade organisations have been able to increase the value of their asset base through the collection and control of customer data, e.g. loyalty programs and big data repositories. Additionally, organisations have been able to insure data and record its value as a tangible asset to increase the value of balance sheets, stock prices and market position. If companies can do this, then it follows that it should only be a matter of time before the same could be true for individuals. However, without the legal, economic or technological means to derive value from data, it is unlikely that individuals will benefit directly from participating in a data marketplace. More so, without the acknowledgement that the ethical, legal and control mechanisms of the current connected world are not in service to humanity, we risk compounding the economic benefits exploited by relatively few actors through centralised privately held commercial and social platforms. Left to their own devices these platforms are increasingly removing agency in both the physical world and digital transactions. \u201cEvery child born in today\u2019s digital world will live a dual physical and digital life. How we enable privacy and security for their digital twin in as important as the rights afforded to them in the physical world. Protecting their identity, enabling trusted authentication and authorisation of services is the key challenge for us to solve in an increasingly connected world. If we don\u2019t get this right, we risk a generation born into digital slavery rather than delivering on the promise of empowerment through technology\u201d - Katryna Dow, World Government Summit, 2019 To address these increasing data value issues a number of models are emerging, including Data as Labor (DaL), Data as Property (DaP), Data Commons and Open Data. In order to evaluate the merits of these proposed frameworks, it\u2019s important to understand the underlying ethos and where they draw on or seek to apply existing legal statues.","title":"Emerging Legal and Commercial Models"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#data-as-labor","text":"The prevailing model of treating personal data is essentially a barter system \u2013 consumers enjoy free services in exchange for their data, which is gathered by service providers at no charge. This model is called Data as Capital (DaC). An alternative offered is DaL (Ibarra et. al., 2017). This offers incentives to consumers to contribute data. DaL would provide benefits such as Increased quality and quantity of user data, especially in the context of AI and Machine Learning, which requires large amounts of different data Curbing the activity of trans-national companies as sole \u201cbuyers\u201d of user data, and the creation of a fair and vibrant market for data labor Increased engagement in their own data by individuals, and creation of \u201cdigital dignity\u201d even to the point of assisting Machine Learning in tackling sophisticated tasks, with commensurate reward While pointing out that DaC and DaL can cohabit in the market, the authors acknowledge the difficulties in creating this new \u201cRadical Market\u201d and data economy. They include: The necessity of enlisting very large organisations in the change. There are different business models among the large commercial data players (Facebook and Google are most reliant on DaC), but all have relatively stable, profitable ` businesses. To pivot to a different form of data economy will be challenging for them, despite the appeal of access to more and different data for use in AI and Machine Learning activities. The need for users\u2019 to participate in numbers and be prepared to bargain collectively (which implies membership of a form of union or membership association). Finally, governments, despite mandating significant new access to user data (e.g. GDPR), have not created labour laws that are compatible with the DaL economy. Apart from these issues, the DaL model does not contemplate data ownership. DaL seeks to return value for users in exchange for their data; once that exchange is complete, it is commonplace that ownership of the relevant data passes to the \u201cbuyer\u201d. The early thinking around DaL formed the basis for Eric Posner and Glen Weyl to extend the concepts of \u201cRadical Markets\u201d into the publication of the book by the same tile in 2018. Radical Markets sets out to demonstrate why private property is inherently monopolistic, and how society would all be better served if private ownership were converted into a public auction for public benefit. They show how the principle of one person, one vote inhibits democracy, suggesting instead an ingenious way for voters to effectively influence the issues that matter most to them. They argue that every citizen of a host country should benefit from immigration\u2014not just migrants and their capitalist employers. They propose leveraging antitrust laws to liberate markets from the grip of institutional investors and creating a data labor movement to force digital monopolies to compensate people for their electronic data. \u201cOnly by radically expanding the scope of markets can we reduce inequality, restore robust economic growth, and resolve political conflicts. But to do that, we must replace our most sacred institutions with truly free and open competition\u201d \u2014 Radical Markets","title":"Data as Labor"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#data-as-property","text":"Personal data is generated between the data subject and the data controller. This is distinct from intellectual property which is generated by the data subject (Schermer, 2015). As Schermer points out, this fact alone questions whether the data subject owns their personal data as they did not autonomously generate it and suggests that personal data is the opinion that a data controller holds about a person (Schermer, 2015). Schermer examines The Right to Be Forgotten as an application of this theory. Data subjects have a right to be forgotten, but society at large has a right to form and record an opinion about a person, for example for criminal proceedings. If personal data is treated as private property, the right to be forgotten will always win and may inhibit what is best for society (e.g. prosecuting a criminal to protect community safety). Ritter and Mayer (2018) propose a property rules construct that clearly defines a right to own digital information arises upon creation (whether by keystroke or machine) and suggest when and how that right attaches to specific data though the exercise of technological controls. They tout possible benefits such as efficient adaptations of new data asset rules, and new methods for regulating and enforcing rights regarding personal information (Ritter and Mayer, 2018). In 2018, Elizabeth Renieris and Dazza Greenwood co-authored an article on property rights, with the title posing the question: \u201cDo we really want to \u2018sell\u2019 ourselves? The risks of a Property law paradigm for personal data ownership In the article the authors highlight that while \u201cownership implies a property law model of our data, they argue that the legal framework for identity-related data must also consider constitutional or human rights laws rather than mere property law rules. This question speaks to the core of the issue about data ownership. Does moving from corporations trading data to individuals trading data (based on their property rights) move society forward, or risk creating a greater digital divide, and/or making privacy a luxury item out of the reach of much of the population? More so, is the average person aware of the risks, consequences and downstream impacts of trading their data, for example possible exclusion from insurance or employment. If data has the potential to unlock expediential value in terms of personalisation of health, education, finance and employment \u2013 access is critical, but the mechanism of trading may actually inhibit the benefits along with mitigating personal risks. \u201cWe must find the balance that will enable personalisation without further asymmetry in the market\u201d \u2013 Katryna Dow 2019 As outlined by Renieris and Greenwood, under common law, ownership in property is a collective of five rights: Of possession Control Exclusion Enjoyment Disposition Whilst title ownership of real property or goods relates to the evidence of a form of deed, bill of lading, receipt \u2013 essentially proof of commercial transaction or purchase. Also, property rights are granted and as such enable property or goods to be transferred moving ownership and control to another party. However, as more and more our digital twins are a mirror for our physical selves, the issue of our fundamental and (existing) human rights are also bound with the notion of privacy, security and rights to protect oneself and be kept from societal harm. Essentially it will not serve humanity or society if people need to \u201csell\u201d parts of their digital self for commercial gain \u2013 in the same way the law does not permit people to sell body parts. Which leaves us with the tension between the protections afforded by control, e.g. exclusion and enjoyment (imagine if you could limit what Facebook does with your data) and at the same time, property law in its entirety doesn\u2019t account for the phenomenon that data is and exoskeleton (location, action, device) virtual layer around our physical self, and therefore requires protections akin to our physical self.","title":"Data as Property"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#data-commons","text":"Whilst Data commons is not a new concept, it is beginning to gain more attention as the issues of regulation and ownership are debated. Taking a \u2018Commons\u2019 approach has proved to enable other disciplines, e.g. Creative Commons to flourish with fair use and acknowledgment of contributors for the collective good. Critical to a Commons approach is the clarity of purpose, use and application. For data this is particularly challenging because there exists the: data created by individuals (data subject) data that is created about the data created (metadata) data the observer creates about what the data subject created or is derived, enhanced or used from the metadata or data processing. And so, it goes, on between the data subject (or creator) and the observer, collector, processor etc. More often there is more than one party of interest or desired control in any data created. A current example is the data created on social networks. Whilst data subjects may have the \u201crights\u201d to request data created (posts, media, comments), many networks consider the meta-data, insights and inferences to belong to the processor and not the subject. Data Commons provides a framework of co-locating data and data resources and sharing data to create an interoperable resource for the wider community to access in order to conduct research (for example, Genomic Data Commons). Frame works for alternative business models should be tested based on the principles of the commons (Vercellone et. al., 2018). Christopher Allen (2015) summarises the principles of the commons in the following 10 points: Define Boundaries: There are clearly defined boundaries around the common resources of a system from the larger environment. Define Legitimate Users: There is a clearly defined community of legitimate user of those resources. Adapt Locally: Rule for use of resources are adapted to local needs and conditions. Decide Inclusively: Those using resources are included in decision making. Monitor Effectively: There exists effective monitoring of the system by accountable monitors. Share Knowledge: All parties share knowledge of local conditions of the system. Hold Accountable: Have graduated sanctions for those who violate community rules. Offer Mediation: Offer cheap and easy access to conflict resolution. Govern Locally: Community self-determination is recognised by higher level authorities. Don\u2019t Externalise Costs: Resource systems embedded in other resource systems are organised in and accountable to multiple layers of nested communities. Whilst there are many benefits made possible through a Commons approach, the rights of value derived from control of a data asset are not as clear and open to exploitation by entities that have the means to do so.","title":"Data Commons"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#open-data","text":"Open data is the theory that data should be readily available to the public to access, analyse and republish as they wish, without restrictions from copyright, patents or other mechanisms of control (Open Data Institute). For the purpose of this paper, it is to be assumed that data such as weather, pollution, traffic flows, transportation, town-planning and health warnings must stay open in the interest of the greater good. However, the ability for a data subject to combine their own personal data (PII) with open data and social data, results in an amplified data set where more context and intent may be derived, thus increasing the value of the data. The ability to bring personal, proprietary and open data together provides a significant advantage for organisation with the computing power to industrialise this, e.g. Facebook. It is for this reason that the German antitrust watchdog has just launched a legal challenge to order Facebook to stop gathering data from source outside the platform and using the data to track people who are not member or users of Facebook. Conversely there are emerging projects, such as DEcentralised Citizens Owned Data Ecosystem (DECODE) in Spain, whose aim is to provide rights above GDPR in response to people\u2019s concerns about a loss of control over their personal information on the internet (Vercellone et. al., 2018). DECODE aims to build a data-centric digital economy where data that is generated and gathered by citizens, the Internet of Things (IoT), and sensor networks can be available for broader communal use, with appropriate privacy protections. Outcomes will demonstrate the wider social value that comes with individuals being given the power to take control of their personal data and given the means to share their data on their terms (Vercellone et. al., 2018).","title":"Open Data"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#the-argument-against-data-rights","text":"Despite the range of emerging frameworks, there are also strong arguments against the rights of data subjects, in particular for use-cases that benefit the common good, such as; the collection of census, pollution, voting, health and transport data. These data sets are vital to planning critical infrastructure such as hospitals, schools, places of worship, airports and managing environmental impact. Modelling longevity, rates of disease, increase in population and mortality trends are increasing important as we are both living longer and the population size of the ageing \u201cBaby Boomer\u201d generation. Whilst ever commercial actors with centralised data control power can justify the cost of regulatory fines, it is easier for them to continue with existing data practices and build fines into the cost of sale, therefore there is motivation to campaign against individual data rights. Under changes such as Open Banking, it\u2019s likely financial intuitions will argue for the minimum compliance in order to protect what they consider as \u201cowning the customer\u201d. Conversely, neo banks and early adopters may take the lead in providing increased customer utility and better customer experiences by embracing more open and inclusive data eco-systems.","title":"The Argument Against Data Rights"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#conclusion","text":"In summary many of these concepts have merit; the potential to blend data as labor (for effort reward) with data as property (rights of contol and enjoyment) with data commons (fair use) together with open data (collective good) represent an ideal legally protected outcome. Harmonising these perspectives will no doubt result in trade-offs and challenges in jurisdictions around the world. As we approach 2020 we are locked in debates around the application of 20th century laws and statutes to technologies that will be ubiquitous over the next 20-100 years. In many ways this is like trying to regulate how to put an engine in a horse rather than accept that the paradigm has completely changed. Historically, we have seen once accepted behaviours such a not wearing a seat-belt, smoking on aircrafts, or denying the right to vote as dangerous and archaic. Laws and norms have evolved to make these practices unlawful. It is our opinion that we will see the same evolution of rights with respect to the lawful access, control, delegation and consent of personal data. However, without a clear legal framework inclusive of liability and recourse, it will continue to be difficult to develop new commercial models and methods of value which can drive the adoption of trusted data marketplaces. This is why open standards, decentralised solutions and interoperability of personal data exchange mechanisms are vital to maintaining a free and democratic society; one that can harness the power of data for both the individual and collective good.","title":"CONCLUSION"},{"location":"rwot8/topics-and-advance-readings/data-vala-realization/#references","text":"Allen C., (2015) 10 Design Principles for Governing the Commons. Available: http://www.lifewithalacrity.com/2015/03/10-design-principles-for-governing-the-commons.html Allen C., \u201cSelf-Sovereign Identity Principles 1.0,\u201d GitHub, 23 October 2016. [Online]. Available: https://github.com/ChristopherA/self-sovereign-identity/blob/master/self-sovereign-identity- principles.md. [Accessed 13 May 2018]. DECODE - DEcentralised Citizens Owned Data Ecosystem, Spain https://decodeproject.eu Dow K., \u201cMeeco Manifesto\u201d 2012 https://meeco.me/manifesto.html Dow K., Seat Belts \u2013 Cigarettes - Data Blog (October 2015) https://katrynadow.me/seat-belts-cigarettes-data/#more-303 Haslingden R., (2017) Open Banking \u2013 Creating a New Era of Data Sharing. Experian. Available: https://www.experian.co.uk/assets/resources/white-papers/open-banking-whitepaper-2017.pdf Ibarra I. A., Goff L., Hern\u00e1ndez D. J., Laier J., Weyl E. G., (2017) Should We Treat Data as Labor? Moving Beyond \u201cFree\u201d American Economic Association Papers & Proceedings, Vol 1 (1). Facebook \u2013 Germany Antitrust Watchdog to Act Against Facebook https://www.reuters.com/article/us-facebook-germany-antitrust/german-antitrust-watchdog-to-act-against-facebook-report-idUSKCN1P70KO Microsoft: Decentralised Identity \u2013 Own and Control Your Identity https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE2DjfY Opher A., Chou A., Onda A., and Sounderrajan K., (2016) The Rise of the Data Economy: Driving Value through Internet of Things Data Monetization. IBM. Available: https://www.ibm.com/downloads/cas/4JROLDQ7 accessed 21 February 2019. Posner E. A., and Weyl E. G., (2018) Radical Markets. Uprooting Capitalism and Democracy for a Just Society. Princeton University Press. Renieris E. M. and Greenwood D., (2018) Do we really want to \u201csell\u201d ourselves? https://medium.com/@hackylawyER/do-we-really-want-to-sell-ourselves-the-risks-of-a-property-law-paradigm-for-data-ownership-b217e42edffa Ritter J., and Mayer A., (2018) Regulating Data as Property: A New Construct for Moving Forward, 16 Duke Law & Technology Review 220-277 Schermer B., (2015). Privacy and property: do you really own your personal data? Universiteit Leiden Law Blog. Available: https://leidenlawblog.nl/articles/privacy-and-property-do-you-really-own-your-personal-data Vercellone C., Brancaccio F., Giuliani A., Puletti F., Rocchi G., and Vattimo P., (2018) DEcentralised Citizens Owned Data Ecosystem (DECODE) D2.4 Data driven disruptive commons-based models. Zuboff S., (2019) The Age of Surveillance Capitalism. The Fight for the Future at the New Frontier of Power, Allen & Unwin. Forthcoming.","title":"References"},{"location":"rwot8/topics-and-advance-readings/decentralized-digital-identity-book/","text":"Decentralized Digital Identity The advent of Self-Sovereign Identity with Blockchain Alex Preukschat and Drummond Reed Background Alex Preukschat and Drummond Reed are preparing a book about decentralized digital identity in which many people from the global identity and blockchain communities are participating. It is expected that this book will become a reference piece about decentralized digital identity and the self-sovereign identity ecosystem around the world. It will be published in the autumn of 2019. Structure of the book The objective of the book is to be as evergreen as possible in highlighting the importance of self-sovereign identity for the world, sharing use cases in plenty of different industries, exploring the concept of decentralization, explaining the basic architecture and technology principles for SSI and the legal frameworks that will support (and be enabled by) SSI. One-third of any revenue generated by the book will be donated to RWOT, one third to IIW and one third to SSIMeetup.org. The authors will not receive any monetary compensation for the book. Anthology Format The book is designed to be an anthology composed of chapters contributed by many different authors in the space. The first three chapters, by Alex, Drummond, and Christopher Allen, will set the stage and provide the big picture of SSI. Then there will be major sections as shown in the TOC below. About Alex Preukschat Alex is a best selling author for \"Blockchain: the industrial revolution of the internet\". Published May 2017 with 7 editions/8000 prints in 15 months. TOP 10 Amazon.es best seller for two weeks and TOP 1 best seller for the business and technology section for one month. Most popular and sold Spanish speaking Blockchain book. 4 editions in 4 months and 5000 printed copies. Published also in Colombia, Peru, Ecuador, and Argentina. Creator and Autor of the first Bitcoin graphic novel of the world: \"Bitcoin: The hunt for Satoshi Nakamoto\". Published in English, Russian, Korean, Brazilian Portuguese and Spanish in 2014 that became a global best seller in the global Bitcoin community. Coordinating author of the Spanish translation of the \u201cThe book of Satosi\u201d available here libroblockchain.com/satoshi and co-creator of Blockchain Espa\u00f1a ( Blockchain Espana.org: one of the main Spanish Blockchain communities) and Alianza Blockchain Iberoam\u00e9rica (AlianzaBlockchain.org: an alliance of Blockchain communities from Argentina, Chile, Colombia, Panama, Mexico, and Spain). Creator of SSIMeetup.org, a global Self-Sovereign Identity (SSI) community, to popularize interest around the principles of Decentralized Digital Identity. About Drummond Reed Drummond has spent over two decades in Internet identity, security, privacy, and trust frameworks. He joined Evernym as Chief Trust Officer after Evernym\u2019s acquisition of Respect Network, where he was CEO, co-founder, and co-author of the Respect Trust Framework, which was honored with the Privacy Award at the 2011 European Identity Conference. Drummond is a Trustee and Secretary of the Sovrin Foundation, where he serves as chair of the Sovrin Trust Framework Working Group. He has served as co-chair of the OASIS XDI Technical Committee since 2004, the semantic data interchange protocol that implements Privacy by Design. Prior to starting Respect Network, Drummond was Executive Director of two industry foundations: the Information Card Foundation and the Open Identity Exchange. He has also served as a founding board member of the OpenID Foundation, ISTPA, XDI.org, and Identity Commons. In 2002 he was a recipient of the Digital Identity Pioneer Award from Digital ID World, and in 2013 he was honored as an OASIS Distinguished Contributor. Draft Table of Contents Decentralized Digital Identity: The Advent of Self-Sovereign Identity with Blockchain Part A Decentralized Digital Identity: What Is It? 1 Decentralized digital identity: principles & fundamentals 2 The path to Self-Sovereign Identity (SSI) 3 Concepts of Decentralized Digital Identity Part B How the Identity Revolution will Change your Business 4 Digital Banking just got faster and more trustworthy with Decentralized Identity 5 Decentralized financial products decrease wait times when powered by Decentralized Identity 6 Insurance firms reinvent themselves with Decentralized Digital Identity 7 Telecommunications providers find profound ways to fill the missing gap of the Internet 8 The energy sector gets a new relationship model 9 Decentralized Digital Identity in Healthcare: The foundations of a new era of trusted relationships between individuals, organizations and clinical things 10 The next Media revolution is here thanks to Decentralized Digital Identity 11 NGOs and financial inclusion powered by Decentralized Identity 12 Governments get on board to enhance the public good with Decentralized Digital Identity 13 Canada: The meaning of trust for Identity 14 Law enforcement and legal professionals streamline crime fighting once and for all 15 Decentralized Digital Identity squashes multiple squabbles in family court 16 Educational facilities turn to Decentralized Digital Identity to keep students and staff safer 17 Animal care, adoption, and accountability just became crystal clear 18 The Coworking and office space industry with decentralized identity 19 The Recruitment industry with decentralized identity Part C Cross-Industry Use Cases of Blockchain 18 Open democracy and electronic voting 19 Smart cities with Decentralized Identity 20 Music, images and a fairer concept of intellectual property 21 Decentralized identity, Decentralized File Systems, and the Decentralized Web 22 The opportunity of e-commerce with Identity 23 Smart contracts and Decentralized Digital Identity 24 The Self-Sovereign Expansion of the Digital \u201cI\u201d and \u201cWe\u201d 25 Supply-chain management powered by Decentralized Digital Identity in Pharma Part D Decentralization as a model for life 26 Hacktivism, cypherpunks and the birth of Decentralized Digital Identity 28 Decentralization for a peaceful society 29 Decentralization as a model for life 30 The three dimensions of Decentralized Identity 31 Centralization vs decentralization believers Part E Blockchain & Decentralized Identity Technology 32 Basic Cryptography principles for Decentralized Digital Identity 33 Free software, open source in the Decentralized Digital Identity world 34 Decentralized Digital Identity Architectures 35 Verifiable Credentials 36 DIDs \u2013 the basic ingredients 37 Decentralized Identity Graphs 38 Identity wallets are not the same as crypto wallets 39 Making Smart Contracts smart with Decentralized Digital Identity 40 Public Identities in the Ethereum ecosystem Part F Decentralized Identity and the Law 41 Legal questions from Decentralized Digital Identity to Governance Frameworks Part G Identity, Investment and Money 42 Decentralized Digital Identity protocol business models 43 Identity is money Part H Closing 44 A vision of the future based on Decentralized Digital Identity 45 Evolution of the Decentralized Digital Identity Community","title":"Decentralized Digital Identity"},{"location":"rwot8/topics-and-advance-readings/decentralized-digital-identity-book/#decentralized-digital-identity","text":"","title":"Decentralized Digital Identity"},{"location":"rwot8/topics-and-advance-readings/decentralized-digital-identity-book/#the-advent-of-self-sovereign-identity-with-blockchain","text":"Alex Preukschat and Drummond Reed Background Alex Preukschat and Drummond Reed are preparing a book about decentralized digital identity in which many people from the global identity and blockchain communities are participating. It is expected that this book will become a reference piece about decentralized digital identity and the self-sovereign identity ecosystem around the world. It will be published in the autumn of 2019. Structure of the book The objective of the book is to be as evergreen as possible in highlighting the importance of self-sovereign identity for the world, sharing use cases in plenty of different industries, exploring the concept of decentralization, explaining the basic architecture and technology principles for SSI and the legal frameworks that will support (and be enabled by) SSI. One-third of any revenue generated by the book will be donated to RWOT, one third to IIW and one third to SSIMeetup.org. The authors will not receive any monetary compensation for the book. Anthology Format The book is designed to be an anthology composed of chapters contributed by many different authors in the space. The first three chapters, by Alex, Drummond, and Christopher Allen, will set the stage and provide the big picture of SSI. Then there will be major sections as shown in the TOC below. About Alex Preukschat Alex is a best selling author for \"Blockchain: the industrial revolution of the internet\". Published May 2017 with 7 editions/8000 prints in 15 months. TOP 10 Amazon.es best seller for two weeks and TOP 1 best seller for the business and technology section for one month. Most popular and sold Spanish speaking Blockchain book. 4 editions in 4 months and 5000 printed copies. Published also in Colombia, Peru, Ecuador, and Argentina. Creator and Autor of the first Bitcoin graphic novel of the world: \"Bitcoin: The hunt for Satoshi Nakamoto\". Published in English, Russian, Korean, Brazilian Portuguese and Spanish in 2014 that became a global best seller in the global Bitcoin community. Coordinating author of the Spanish translation of the \u201cThe book of Satosi\u201d available here libroblockchain.com/satoshi and co-creator of Blockchain Espa\u00f1a ( Blockchain Espana.org: one of the main Spanish Blockchain communities) and Alianza Blockchain Iberoam\u00e9rica (AlianzaBlockchain.org: an alliance of Blockchain communities from Argentina, Chile, Colombia, Panama, Mexico, and Spain). Creator of SSIMeetup.org, a global Self-Sovereign Identity (SSI) community, to popularize interest around the principles of Decentralized Digital Identity. About Drummond Reed Drummond has spent over two decades in Internet identity, security, privacy, and trust frameworks. He joined Evernym as Chief Trust Officer after Evernym\u2019s acquisition of Respect Network, where he was CEO, co-founder, and co-author of the Respect Trust Framework, which was honored with the Privacy Award at the 2011 European Identity Conference. Drummond is a Trustee and Secretary of the Sovrin Foundation, where he serves as chair of the Sovrin Trust Framework Working Group. He has served as co-chair of the OASIS XDI Technical Committee since 2004, the semantic data interchange protocol that implements Privacy by Design. Prior to starting Respect Network, Drummond was Executive Director of two industry foundations: the Information Card Foundation and the Open Identity Exchange. He has also served as a founding board member of the OpenID Foundation, ISTPA, XDI.org, and Identity Commons. In 2002 he was a recipient of the Digital Identity Pioneer Award from Digital ID World, and in 2013 he was honored as an OASIS Distinguished Contributor. Draft Table of Contents Decentralized Digital Identity: The Advent of Self-Sovereign Identity with Blockchain Part A Decentralized Digital Identity: What Is It? 1 Decentralized digital identity: principles & fundamentals 2 The path to Self-Sovereign Identity (SSI) 3 Concepts of Decentralized Digital Identity Part B How the Identity Revolution will Change your Business 4 Digital Banking just got faster and more trustworthy with Decentralized Identity 5 Decentralized financial products decrease wait times when powered by Decentralized Identity 6 Insurance firms reinvent themselves with Decentralized Digital Identity 7 Telecommunications providers find profound ways to fill the missing gap of the Internet 8 The energy sector gets a new relationship model 9 Decentralized Digital Identity in Healthcare: The foundations of a new era of trusted relationships between individuals, organizations and clinical things 10 The next Media revolution is here thanks to Decentralized Digital Identity 11 NGOs and financial inclusion powered by Decentralized Identity 12 Governments get on board to enhance the public good with Decentralized Digital Identity 13 Canada: The meaning of trust for Identity 14 Law enforcement and legal professionals streamline crime fighting once and for all 15 Decentralized Digital Identity squashes multiple squabbles in family court 16 Educational facilities turn to Decentralized Digital Identity to keep students and staff safer 17 Animal care, adoption, and accountability just became crystal clear 18 The Coworking and office space industry with decentralized identity 19 The Recruitment industry with decentralized identity Part C Cross-Industry Use Cases of Blockchain 18 Open democracy and electronic voting 19 Smart cities with Decentralized Identity 20 Music, images and a fairer concept of intellectual property 21 Decentralized identity, Decentralized File Systems, and the Decentralized Web 22 The opportunity of e-commerce with Identity 23 Smart contracts and Decentralized Digital Identity 24 The Self-Sovereign Expansion of the Digital \u201cI\u201d and \u201cWe\u201d 25 Supply-chain management powered by Decentralized Digital Identity in Pharma Part D Decentralization as a model for life 26 Hacktivism, cypherpunks and the birth of Decentralized Digital Identity 28 Decentralization for a peaceful society 29 Decentralization as a model for life 30 The three dimensions of Decentralized Identity 31 Centralization vs decentralization believers Part E Blockchain & Decentralized Identity Technology 32 Basic Cryptography principles for Decentralized Digital Identity 33 Free software, open source in the Decentralized Digital Identity world 34 Decentralized Digital Identity Architectures 35 Verifiable Credentials 36 DIDs \u2013 the basic ingredients 37 Decentralized Identity Graphs 38 Identity wallets are not the same as crypto wallets 39 Making Smart Contracts smart with Decentralized Digital Identity 40 Public Identities in the Ethereum ecosystem Part F Decentralized Identity and the Law 41 Legal questions from Decentralized Digital Identity to Governance Frameworks Part G Identity, Investment and Money 42 Decentralized Digital Identity protocol business models 43 Identity is money Part H Closing 44 A vision of the future based on Decentralized Digital Identity 45 Evolution of the Decentralized Digital Identity Community","title":"The advent of Self-Sovereign Identity with Blockchain"},{"location":"rwot8/topics-and-advance-readings/did-fee-market-using-micropayments/","text":"Re-inventing Incentive Structures Using DID and Microtransactions by Yancy Ribbens Microtransactions and DID (Decentralized Identifiers) can enable a user to curate their online identity, request payment or mark data private. This article proposes allowing a fee market to develop around what information a user chooses to reveal about their identity in the context of an earned credential. Introduction A DID can be anchored to a public blockchain which acts as a registry. Interrogating a public blockchain allows a user to find, reference, revoke or update a DID. Only the user possessing the cryptographic material to a DID can make state changes and prove ownership. Services that implement DID infrastructure move ownership of identifiers from third parties to individuals, thereby allowing people ownership of their digital identifiers. Earners When an individual earns a credential, they may choose to advertise or reveal data about the achievement. However, the individual may not wish details of his/her Personally Identifiable Information (PII) to be surfaced and associated with a credential. Issuer An issuer is an entity that issues a credential to an earner. Typically this issuer knows the person by their first name, last name or other forms of PII which the issuer uses as identifiers. By using a DID in lieu of other PII, the issuer could be limited to only the PII that is relevant to issue the credential and nothing more. For example, the issuer may require cryptographic proof that has been signed by a government to show citizenship. When the issuer issues a credential because of a passing exam, for example, they only need to associate the passing marks with a unique DID that has citizenship. Other attributes of the earner such as race, age, name and gender can all be hidden by the owner of the DID because they are irrelevant to the achievement. Credential Example credentials such as Blockcerts and OBI have data attributes for both issuer and earner (credential recipient). By augmenting a credential to use an issuer DID and a recipient DID, both issuer and earner can have greater control over the data that is revealed and associated with the credential, assuming no other PII is collected except the DID pubkey. Issuing Authority The issuing authority is often a third party entity contracted out by the issuer and registered with by the earner. This authority may possess details of PII associated with the issuer as well as the earner enabling the issuing authority to be an intermediary for PII data. DID Semantics BTCR (Bitcoin Resolver), secures an individual's identifiers in the same way as currency. The owner of the identifier possesses a public key which can be known to anyone, and a private key known only by the identifier owner (or trusted interface). This public key can reference one or more outputs which point to an external resource under the control of an individual. Example outputs could be, a trusted website, the IP address of their own personal computer or an IPFS address. If the identifier owner loses control of the end-point their DID points to, the user's private key can be used to revoke the DID. Furthermore, if the identity owner loses the private key, decentralized key recovery systems using Shamir secret sharing are possible. Developing a fee market Microtransactions, for example, the lightning network, allow paywalls to be constructed, which reveal information in exchange for small amounts of currency. Every request for data pertaining to a DID could be serviced only if the requester meets the payment requested by the individual. Instead of receiving a microtransaction, the DID owner may choose to make a DID available to external entities for free. Furthermore, an algorithm could be constructed to parse arbitrary blockchains in search of a DID with matching criteria. This could be done by identifying which addresses are associated with DIDs and either follow the public information or negotiate payment.","title":"Re-inventing Incentive Structures Using DID and Microtransactions"},{"location":"rwot8/topics-and-advance-readings/did-fee-market-using-micropayments/#re-inventing-incentive-structures-using-did-and-microtransactions","text":"by Yancy Ribbens Microtransactions and DID (Decentralized Identifiers) can enable a user to curate their online identity, request payment or mark data private. This article proposes allowing a fee market to develop around what information a user chooses to reveal about their identity in the context of an earned credential.","title":"Re-inventing Incentive Structures Using DID and Microtransactions"},{"location":"rwot8/topics-and-advance-readings/did-fee-market-using-micropayments/#introduction","text":"A DID can be anchored to a public blockchain which acts as a registry. Interrogating a public blockchain allows a user to find, reference, revoke or update a DID. Only the user possessing the cryptographic material to a DID can make state changes and prove ownership. Services that implement DID infrastructure move ownership of identifiers from third parties to individuals, thereby allowing people ownership of their digital identifiers.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/did-fee-market-using-micropayments/#earners","text":"When an individual earns a credential, they may choose to advertise or reveal data about the achievement. However, the individual may not wish details of his/her Personally Identifiable Information (PII) to be surfaced and associated with a credential.","title":"Earners"},{"location":"rwot8/topics-and-advance-readings/did-fee-market-using-micropayments/#issuer","text":"An issuer is an entity that issues a credential to an earner. Typically this issuer knows the person by their first name, last name or other forms of PII which the issuer uses as identifiers. By using a DID in lieu of other PII, the issuer could be limited to only the PII that is relevant to issue the credential and nothing more. For example, the issuer may require cryptographic proof that has been signed by a government to show citizenship. When the issuer issues a credential because of a passing exam, for example, they only need to associate the passing marks with a unique DID that has citizenship. Other attributes of the earner such as race, age, name and gender can all be hidden by the owner of the DID because they are irrelevant to the achievement.","title":"Issuer"},{"location":"rwot8/topics-and-advance-readings/did-fee-market-using-micropayments/#credential","text":"Example credentials such as Blockcerts and OBI have data attributes for both issuer and earner (credential recipient). By augmenting a credential to use an issuer DID and a recipient DID, both issuer and earner can have greater control over the data that is revealed and associated with the credential, assuming no other PII is collected except the DID pubkey.","title":"Credential"},{"location":"rwot8/topics-and-advance-readings/did-fee-market-using-micropayments/#issuing-authority","text":"The issuing authority is often a third party entity contracted out by the issuer and registered with by the earner. This authority may possess details of PII associated with the issuer as well as the earner enabling the issuing authority to be an intermediary for PII data.","title":"Issuing Authority"},{"location":"rwot8/topics-and-advance-readings/did-fee-market-using-micropayments/#did-semantics","text":"BTCR (Bitcoin Resolver), secures an individual's identifiers in the same way as currency. The owner of the identifier possesses a public key which can be known to anyone, and a private key known only by the identifier owner (or trusted interface). This public key can reference one or more outputs which point to an external resource under the control of an individual. Example outputs could be, a trusted website, the IP address of their own personal computer or an IPFS address. If the identifier owner loses control of the end-point their DID points to, the user's private key can be used to revoke the DID. Furthermore, if the identity owner loses the private key, decentralized key recovery systems using Shamir secret sharing are possible.","title":"DID Semantics"},{"location":"rwot8/topics-and-advance-readings/did-fee-market-using-micropayments/#developing-a-fee-market","text":"Microtransactions, for example, the lightning network, allow paywalls to be constructed, which reveal information in exchange for small amounts of currency. Every request for data pertaining to a DID could be serviced only if the requester meets the payment requested by the individual. Instead of receiving a microtransaction, the DID owner may choose to make a DID available to external entities for free. Furthermore, an algorithm could be constructed to parse arbitrary blockchains in search of a DID with matching criteria. This could be done by identifying which addresses are associated with DIDs and either follow the public information or negotiate payment.","title":"Developing a fee market"},{"location":"rwot8/topics-and-advance-readings/did-namespace-records/","text":"DID Namespace Records -- Drummond Reed , V1 2019-01-31 Introduction & Motivation DIDs are a unique new class of globally unique identifiers with four primary properties: 1. Persistent \u2014assigned once and never change. 1. Resolvable \u2014associated with standard metadata describing the identified entity 1. Cryptographically-verifiable \u2014the DID controller can prove ownership using private keys 1. Decentralized \u2014no centralized registration authority needed. DIDs can be supported on any modern blockchain or distributed ledger by creating a DID method for that target system. Each DID method operates in its own DID namespace . DID method names can be nested within a namespace just like other hierarchical name systems, e.g., DNS. For example: did:example: did:example:foo: did:example:bar: did:example:bar:baz: However the DID spec (as of 2019-01-31) does not yet include a mechanism to enable DID namespaces to \"point\" to one another\u2014similar to how DNS nameservers point to one another\u2014in a manner that can be discovered and cryptographically verified. The lack of this feature is not surprising since every DID method name represents an entire decentralized network, and it was originally thought that the \"flat\" nature of DIDs meant nesting DID namespaces would have minimal value. But as DID usage has grown, new motivations have emerged for linking DID namespaces. Besides discoverability of different decentralized networks, DID namespace linking also enables building bridges between different governance frameworks and trust assurance models\u2014key building blocks of a global web of trust . DID Namespace Records This gap can easily be filled with a simple addition to the DID specification: DID namespace records . The vocabulary for DID namespace records could be defined either by: 1. Adding it to the base JSON-LD context for all DID documents, or 1. Specifying it in a separate JSON-LD context for DID namespace records. Which approach is better is a judgement that can be made by the forthcoming DID Working Group. In this proposal, we will assume the former approach, i.e., that the vocabulary for DID namespace records have been included in the base JSON-LD context for DID documents. A DID namespace record is very simple because its address is just the fully qualified name of the DID method for that namespace . In other words, to resolve a DID namespace record, a DID resolver simply makes a resolution request for the fully qualified DID method name using that DID method. For example, to request the DID namespace record for did:example:foo: , a resolver would resolve the DID method name did:example:foo: using the DID method :did:example: . The DID document returned MAY contain one or more DID namespace records. Each DID namespace record is a JSON object in the following form: { \"children\": [\"did:example:foo:\", \"did:example:bar:\"], \"remote\": [\"did:abc:\", \"did:xyz:\"] } Children are child namespaces of the parent DID namespace. Remote are pointers to other associated DID namespaces that are not children of the parent. In this specification, no semantics are defined for this pointer. Such semantics MAY be specified by a DID method specification, by the governance framework for a DID namespace, or other mechanisms. Example DID Document for a DID Namespace Record The following DID document could be maintained by the authority for the did:example: namespace. { \"@context\": [\"https://w3id.org/did/v1\"], \"id\": \"did:example:\", \"publicKey\": [{ \"id\": \"did:example:123456789abcdefghi#keys-1\", \"type\": \"RsaVerificationKey2018\", \"controller\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }], \"children\": [\"did:example:foo:\", \"did:example:bar:\"], \"remote\": [\"did:abc:\", \"did:xyz:\"] } Value Propositions for Linked DID Namespaces Unlike DNS, where resolution of all domain names uses the same protocol, resolving a DID from a specific DID namespace requires a DID resolver to understand the associated DID method. If a resolver does not support that DID method, it will not be able to resolve that DID. However DIDs are a very different type of identifier than a domain name, so linking DID namespaces solve a different problems that delegating domain names. The two primary use cases as of this writing are: 1. Discovery of child networks within a decentralized network of networks. If a group of decentralized networks wanted to explicitly (and verifiably) express an affiliation, they can do it with DID namespace records. The children of the :did:example: namespace shown above are an example. There are two potential motivations: 1. Sharing the same parent DID method \u2014the networks could agree to all share the same DID method definition across all the namespaces while allowing each network to have its own governance. 1. Inheriting from the same parent governance framework \u2014the networks could all agree to the same base trust guarantees. 1. Discovery of affiliated remote networks. Although these remote networks do not share the same DID method, an explicit link could express some shared policies or trust guarantees\u2014particularly if it was bilateral. Next Steps After review and discussion at Rebooting the Web of Trust #8, the intent is to submit this proposal as a feature request for the main DID specification.","title":"DID Namespace Records"},{"location":"rwot8/topics-and-advance-readings/did-namespace-records/#did-namespace-records","text":"-- Drummond Reed , V1 2019-01-31","title":"DID Namespace Records"},{"location":"rwot8/topics-and-advance-readings/did-namespace-records/#introduction-motivation","text":"DIDs are a unique new class of globally unique identifiers with four primary properties: 1. Persistent \u2014assigned once and never change. 1. Resolvable \u2014associated with standard metadata describing the identified entity 1. Cryptographically-verifiable \u2014the DID controller can prove ownership using private keys 1. Decentralized \u2014no centralized registration authority needed. DIDs can be supported on any modern blockchain or distributed ledger by creating a DID method for that target system. Each DID method operates in its own DID namespace . DID method names can be nested within a namespace just like other hierarchical name systems, e.g., DNS. For example: did:example: did:example:foo: did:example:bar: did:example:bar:baz: However the DID spec (as of 2019-01-31) does not yet include a mechanism to enable DID namespaces to \"point\" to one another\u2014similar to how DNS nameservers point to one another\u2014in a manner that can be discovered and cryptographically verified. The lack of this feature is not surprising since every DID method name represents an entire decentralized network, and it was originally thought that the \"flat\" nature of DIDs meant nesting DID namespaces would have minimal value. But as DID usage has grown, new motivations have emerged for linking DID namespaces. Besides discoverability of different decentralized networks, DID namespace linking also enables building bridges between different governance frameworks and trust assurance models\u2014key building blocks of a global web of trust .","title":"Introduction &amp; Motivation"},{"location":"rwot8/topics-and-advance-readings/did-namespace-records/#did-namespace-records_1","text":"This gap can easily be filled with a simple addition to the DID specification: DID namespace records . The vocabulary for DID namespace records could be defined either by: 1. Adding it to the base JSON-LD context for all DID documents, or 1. Specifying it in a separate JSON-LD context for DID namespace records. Which approach is better is a judgement that can be made by the forthcoming DID Working Group. In this proposal, we will assume the former approach, i.e., that the vocabulary for DID namespace records have been included in the base JSON-LD context for DID documents. A DID namespace record is very simple because its address is just the fully qualified name of the DID method for that namespace . In other words, to resolve a DID namespace record, a DID resolver simply makes a resolution request for the fully qualified DID method name using that DID method. For example, to request the DID namespace record for did:example:foo: , a resolver would resolve the DID method name did:example:foo: using the DID method :did:example: . The DID document returned MAY contain one or more DID namespace records. Each DID namespace record is a JSON object in the following form: { \"children\": [\"did:example:foo:\", \"did:example:bar:\"], \"remote\": [\"did:abc:\", \"did:xyz:\"] } Children are child namespaces of the parent DID namespace. Remote are pointers to other associated DID namespaces that are not children of the parent. In this specification, no semantics are defined for this pointer. Such semantics MAY be specified by a DID method specification, by the governance framework for a DID namespace, or other mechanisms.","title":"DID Namespace Records"},{"location":"rwot8/topics-and-advance-readings/did-namespace-records/#example-did-document-for-a-did-namespace-record","text":"The following DID document could be maintained by the authority for the did:example: namespace. { \"@context\": [\"https://w3id.org/did/v1\"], \"id\": \"did:example:\", \"publicKey\": [{ \"id\": \"did:example:123456789abcdefghi#keys-1\", \"type\": \"RsaVerificationKey2018\", \"controller\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }], \"children\": [\"did:example:foo:\", \"did:example:bar:\"], \"remote\": [\"did:abc:\", \"did:xyz:\"] }","title":"Example DID Document for a DID Namespace Record"},{"location":"rwot8/topics-and-advance-readings/did-namespace-records/#value-propositions-for-linked-did-namespaces","text":"Unlike DNS, where resolution of all domain names uses the same protocol, resolving a DID from a specific DID namespace requires a DID resolver to understand the associated DID method. If a resolver does not support that DID method, it will not be able to resolve that DID. However DIDs are a very different type of identifier than a domain name, so linking DID namespaces solve a different problems that delegating domain names. The two primary use cases as of this writing are: 1. Discovery of child networks within a decentralized network of networks. If a group of decentralized networks wanted to explicitly (and verifiably) express an affiliation, they can do it with DID namespace records. The children of the :did:example: namespace shown above are an example. There are two potential motivations: 1. Sharing the same parent DID method \u2014the networks could agree to all share the same DID method definition across all the namespaces while allowing each network to have its own governance. 1. Inheriting from the same parent governance framework \u2014the networks could all agree to the same base trust guarantees. 1. Discovery of affiliated remote networks. Although these remote networks do not share the same DID method, an explicit link could express some shared policies or trust guarantees\u2014particularly if it was bilateral.","title":"Value Propositions for Linked DID Namespaces"},{"location":"rwot8/topics-and-advance-readings/did-namespace-records/#next-steps","text":"After review and discussion at Rebooting the Web of Trust #8, the intent is to submit this proposal as a feature request for the main DID specification.","title":"Next Steps"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/","text":"DID Primer Spring 2018 This is a community document maintained by co-editors Drummond Reed and Manu Sporny and other contributors and implementers of the Decentralized Identifier 1.0 specification . Introduction At a superficial level, a decentralized identifier ( DID ) is simply a new type of globally unique identifier with special features designed for blockchains. But at a deeper level, DIDs are the core component of an entirely new layer of decentralized digital identity and public key infrastructure (PKI) for the Internet. This decentralized public key infrastructure (DPKI) could have as much impact on global cybersecurity and cyberprivacy as the development of the SSL/TLS protocol for encrypted Web traffic (now the largest PKI in the world). This primer is designed to give newcomers to DID architecture the background they need to understand not just the DID specification, but the overall architecture for decentralized identity represented by the family of DID-related specifications currently under development. It covers: Background on the origin of DIDs and the DID specification. How DIDs differ from other globally-unique identifiers. How the syntax of DIDs can be adapted to work with any modern blockchain. How DIDs resolve to DID documents containing public keys and service endpoints. The key role that DID methods play in the implementation of DID infrastructure. Privacy considerations for the use of DIDs. How DID infrastructure lays the foundation for verifiable claims . Setting the Stage: The Origin of DIDs In the history of the Internet, every identifier that is both globally unique and globally resolvable -- meaning you can look it up and obtain metadata about the resource it identifies -- has required some type of centralized administration. For example, both IP (Internet Protocol) addresses and DNS (Domain Name System) names -- the foundations for the Internet and the Web -- require centralized registries and registrars. Although these centralized systems are very efficient, this architecture has long been recognized as both a single point of control (and thus potential censorship) and a single point of failure. So, in the last few years, several groups began independently investigating decentralized alternatives. In chronological order: The W3C Web Payments Working Group and W3C Verifiable Claims Task Force , led by Manu Sporny and David Longley of Digital Bazaar, recognized that truly portable digital credentials for individuals would require a new type of identifier that was not dependent on a third-party for registration or resolution. The XDI.org Registry Working Group, led by OASIS XDI Technical Committee co-chairs Drummond Reed and Markus Sabadello and Internet Identity Workshop (IIW) co-founder Phil Windley, began looking for a decentralized solution for identifying participants in a global peer-to-peer XDI semantic data interchange network. The Rebooting the Web of Trust (RWOT) community, led by Christopher Allen, began exploring how blockchain technology could be used to enable the decentralized digital identity and trust network originally envisioned by Phil Zimmermann for PGP . The U.S. Department of Homeland Security (DHS) Science & Technology Directorate (S&T), led by Identity and Data Privacy Program Manager Anil John, began researching how blockchain technology could be used for privacy-respecting decentralized identity management . In the spring of 2016, all four groups converged on the concept of DIDs, a term originally coined by the W3C Verifiable Claims Task Force. Thanks in part to R&D funding provided by DHS S&T, work on the first Decentralized Identifier 1.0 specification began in earnest at RWOT #2 in May 2016. The draft spec underwent review at RWOT #3 and IIW #23 in October 2016, and was published as Implementer\u2019s Draft 01 on 21 November 2016. After the W3C Verifiable Claims Working Group was approved in March 2017, in July 2017 the DID specification was contributed to the W3C Credentials Community Group. Work on several related specifications (see below) is continuing at RWOT and IIW events held every six months, as well as other industry events and conferences. See Appendix A for a list of resources and ways to become involved in the DID family of specifications. How DIDs Differ from Other Globally Unique Identifiers The need for globally unique identifiers that do not require a centralized registration authority is not new. UUIDs (Universally Unique Identifiers, also called GUIDs, Globally Unique Identifiers) were developed for this purpose in the 1980s and standardized first by the Open Software Foundation and then by IETF RFC 4122 . The need for persistent identifiers (identifiers that can be assigned once to an entity and never need to change) is also not new. This class of identifiers was standardized as URNs (Uniform Resource Names) first by IETF RFC 2141 and more recently by RFC 8141 . As a rule, however, UUIDs are not globally resolvable and URNs -- if resolvable -- require a centralized registration authority. In addition, neither UUIDs or URNs inherently address a third characteristic -- the ability to cryptographically verify ownership of the identifier . For blockchain identity -- and more specifically self-sovereign identity , which can be defined as a lifetime portable digital identity that does not depend on any centralized authority -- we need a new class of identifier that fulfills all four requirements: persistence, global resolvability, cryptographic verifiability, and decentralization. The Format of a DID In 2016 the developers of the DID specification agreed with a suggestion from Christopher Allen that DIDs could be adapted to work with multiple blockchains by following the same basic pattern as the URN specification: The key difference is that with DIDs the namespace component identifies a DID method , and a DID method specification specifies the format of the method-specific identifier. DID methods (further explained below) define how DIDs work with a specific blockchain. All DID method specs must define the format and generation of the method-specific identifier. Note that the method specific identifier string must be unique in the namespace of that DID method. For example the DID above uses the Sovrin DID method in which the method-specific identifier is generated by base-56-encoding the first half of an Ed25519 verification key. DID Documents DID infrastructure can be thought of as a global key-value database in which the database is all DID-compatible blockchains, distributed ledgers, or decentralized networks. In this virtual database, the key is a DID, and the value is a DID document . The purpose of the DID document is to describe the public keys, authentication protocols, and service endpoints necessary to bootstrap cryptographically-verifiable interactions with the identified entity. A DID document is a valid JSON-LD object that uses the DID context (the RDF vocabulary of property names) defined in the DID specification. This includes six components (all optional): The DID itself , so the DID document is fully self-describing. A set of public keys or other proofs that can be used for authentication or interaction with DID subject. A set of authentication protocols for authenticating the DID subject. A set of service endpoints that describe where and how to interact with the DID subject. Timestamps for auditing. A optional JSON-LD signature if needed to verify the integrity of the DID document. See the DID specification for several examples of DID documents. DID Methods DIDs and DID documents can be adapted to any modern blockchain, distributed ledger, or other decentralized network capable of resolving a unique key into a unique value. It does not matter whether the blockchain is public, private, permissionless, or permissioned. Defining how a DID and DID document are created, resolved, and managed on a specific blockchain or \"target system\" is the role of a DID method specification . DID method specifications are to the generic DID specification as URN namespace specifications (UUID, ISBN, OID, LSID, etc.) are to the generic IETF URN specification ( RFC 8141 ). A DID method specification must define the following: The DID method name. The ABNF structure of the method-specific identifier. How the method-specific identifier is generated or derived. How the CRUD operations are performed on a DID and DID document: a. Creating a new DID. b. Reading (resolving) a DID document. c. Updating a DID document. d. Deleting (revoking) a DID. It is these CRUD operations that may vary the most across different DID methods. For example: Create. Some DID methods may generate a DID directly from a cryptographic key pair. Others may use the address of a transaction or a smart contract on the blockchain itself. Read. Some DID methods use blockchains that can store DID documents directly on the blockchain. Others may instruct DID resolvers to construct them dynamically based on attributes of a blockchain record. Still others may store a pointer on the blockchain to a DID document stored in one or more parts on other decentralized storage networks such as IPFS or STORJ . Update. The update operation is the most critical from a security standpoint because control of a DID document represents control of the public keys or proofs necessary to authenticate an entity (and therefore for an attacker to impersonate the entity). Since verification of DID document update permissions can only be enforced by the target blockchain, the DID method specification must define precisely how authentication and authorization are performed for any update operation. Delete. DID entries on a blockchain are by definition immutable, so they can never be \u201cdeleted\u201d in the conventional database sense. However they can be revoked in the cryptographic sense. A DID method specification must define how this termination is performed, e.g., by writing a null DID document. Related Specifications DIDs are the atomic unit of a new layer of decentralized identity infrastructure. This is a list of the other specifications in the DID family that are currently under development. DKMS (Decentralized Key Management System) DIDs are only possible with public/private key cryptography; the ability to generate, write, and update a DID and DID document to a blockchain without any intermediary requires control of the associated private key. This key management cannot itself rely on centralized authorities or it would defeat the whole purpose. In short, decentralized identity requires decentralized key management . The purpose of the DKMS (Decentralized Key Management System) specification is to specify and interoperable protocol for managing the lifecycle of private keys and other private metadata associated with a DID in a way that is interoperable across different blockchains, apps, and vendors. This spec is being developed under a grant from the Science & Technology Directorate of the U.S. Department of Homeland Security based on the requirements and best practices set forth in NIST Special Publication 800-130 , \u201cA Framework for Designing Key Management Systems\u201d. DID TLS Today's TLS infrastructure uses X.509 certs based on traditional hierarchical PKI, where certificate authorities (CAs) follow standardized best practices in order to qualify as trust roots that will be recognized by browser vendors. DID TLS will decentralize this process by enabling the standard X.509 cert elements required to establish a TLS session to be generated dynamically from any DID and DID document that conforms to the DID spec. The DID TLS specification will enable encrypted, peer-to-peer connections to be negotiated in real time between any two DID-identified entities (people, organizations, things). This will radically expand the protections of the TLS protocol and could potentially turn them into the default for all nearly all forms of Internet communication. DID Names The DID specification is intentionally limited to machine-generated decentralized identifiers that are completely lacking in human memorability or usability. However there are many use cases it is desirable to be able to discover a DID using a human-friendly semantic name. Such a naming service would look like a flat, cryptographically-verifiable version of DNS. The big difference, of course, is that a DID naming service needs to be fully decentralized, i.e., not depend on centralized registries and registrars. Registration of DID names would be made directly by identity owners to the blockchain itself using the same cryptographic verification as DID transactions. The goal of the DID Names specification is to standardize how an interoperable decentralized naming layer can operate directly on top of the DID layer. A DID name is mapped to a DID the same way a DID is mapped to a DID document. DID names will be an optional feature of a DID method, so the governance and economics of a DID namespace can be specified by the same community that defines the associated DID method. DID Auth A common goal of all blockchain identity systems is the cryptographic authentication of an identity owner. The various protocols all use some type of cryptographic challenge/response similar to the SQRL protocol originally proposed by Steve Gibson and the Web Authentication protocol currently being standardized by W3C. In these protocols, a one-time challenge is issued by the relying party, signed by the identity owner's private key, and then verified by the relying party using the identity owner\u2019s public key. Whereas SQRL and Web Authentication use pairwise public keys that cannot be externally verified, DIDs will enable verification of the public key against the blockchain identified by the DID method. The DID Auth specification will standardize this cryptographic challenge/response authentication protocol so it can be used with any DID that supports it. DID Auth endpoints would then become one of the standard DID identity services than can be discovered via a DID document. DIDs and Privacy by Design Privacy is an essential component of any identity management solution; it is especially critical for a global identity system that uses immutable public blockchains. Thankfully DID architecture can incorporate Privacy by Design at the very lowest levels of infrastructure and thus become a powerful, new, privacy-preserving technology if deployed using best practices such as: Pairwise-pseudonymous DIDs. While DIDs can be used as well-known public identifiers, they can also be used as private identifiers issued on a per-relationship basis. So rather than a person having a single DID, like a cell phone number or national ID number, she can have thousands of pairwise-unique DIDs that cannot be correlated without her consent, yet can still be managed as easily as an address book. Off-chain private data. Storing any type of PII on a public blockchain, even encrypted or hashed, is dangerous for two reasons: 1) the encrypted or hashed data is a global correlation point when the data is shared with multiple parties, and 2) if the encryption is eventually broken (e.g., quantum computing ), the data will be forever accessible on an immutable public ledger. So the best practice is to store all private data off-chain and exchange it only over encrypted, private, peer-to-peer connections. Selective disclosure. The decentralized PKI (DPKI) that DIDs make possible opens the door to individuals gaining greater control over their personal data in two ways. First, it enables it to be shared using encrypted digital credentials (see below). Second, these credentials can use zero-knowledge proof cryptography for data minimization , e.g., you can disclose that you are over a certain age without disclosing your exact birthdate. DIDs and Verifiable Claims DIDs are only the base layer of decentralized identity infrastructure. The next higher layer -- where most of the value is unlocked -- is verifiable claims . This is the technical term for a digitally signed electronic credential that conforms to the interoperability standards being developed by the W3C Verifiable Claims Working Group . For a complete introduction, please see the Verifiable Claims Primer . The diagram below (from Manu Sporny) illustrates the three primary roles in the verifiable claims ecosystem: issuers sign claims and give them to holders (identity owners) who present them to verifiers who verify the signatures in order to grant access to resources. Note that in all three cases, the parties interact with the DID layer to register DIDs as persistent identifiers for issuers or holders, and to resolve those DIDs to obtain the public keys needed to verify the signature of an issuer or holder. Since any issuer may provide claims to any holder who may present them to any verifier, this results in set of rich, interlocking trust relationships that do not need to conform to any pre-established hierarchy -- a web of trust . Appendix A: DID Community Resources Besides the links throughout this primer, these additional resources are available to anyone interested in joining the DID community. W3C Verifiable Claims Working Group mailing list W3C Credentials Community Group DID specification issues list Rebooting the Web of Trust event (held every six months) Internet Identity Workshop event (held every six months)","title":"DID Primer Spring 2018"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#did-primer-spring-2018","text":"This is a community document maintained by co-editors Drummond Reed and Manu Sporny and other contributors and implementers of the Decentralized Identifier 1.0 specification .","title":"DID Primer Spring 2018"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#introduction","text":"At a superficial level, a decentralized identifier ( DID ) is simply a new type of globally unique identifier with special features designed for blockchains. But at a deeper level, DIDs are the core component of an entirely new layer of decentralized digital identity and public key infrastructure (PKI) for the Internet. This decentralized public key infrastructure (DPKI) could have as much impact on global cybersecurity and cyberprivacy as the development of the SSL/TLS protocol for encrypted Web traffic (now the largest PKI in the world). This primer is designed to give newcomers to DID architecture the background they need to understand not just the DID specification, but the overall architecture for decentralized identity represented by the family of DID-related specifications currently under development. It covers: Background on the origin of DIDs and the DID specification. How DIDs differ from other globally-unique identifiers. How the syntax of DIDs can be adapted to work with any modern blockchain. How DIDs resolve to DID documents containing public keys and service endpoints. The key role that DID methods play in the implementation of DID infrastructure. Privacy considerations for the use of DIDs. How DID infrastructure lays the foundation for verifiable claims .","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#setting-the-stage-the-origin-of-dids","text":"In the history of the Internet, every identifier that is both globally unique and globally resolvable -- meaning you can look it up and obtain metadata about the resource it identifies -- has required some type of centralized administration. For example, both IP (Internet Protocol) addresses and DNS (Domain Name System) names -- the foundations for the Internet and the Web -- require centralized registries and registrars. Although these centralized systems are very efficient, this architecture has long been recognized as both a single point of control (and thus potential censorship) and a single point of failure. So, in the last few years, several groups began independently investigating decentralized alternatives. In chronological order: The W3C Web Payments Working Group and W3C Verifiable Claims Task Force , led by Manu Sporny and David Longley of Digital Bazaar, recognized that truly portable digital credentials for individuals would require a new type of identifier that was not dependent on a third-party for registration or resolution. The XDI.org Registry Working Group, led by OASIS XDI Technical Committee co-chairs Drummond Reed and Markus Sabadello and Internet Identity Workshop (IIW) co-founder Phil Windley, began looking for a decentralized solution for identifying participants in a global peer-to-peer XDI semantic data interchange network. The Rebooting the Web of Trust (RWOT) community, led by Christopher Allen, began exploring how blockchain technology could be used to enable the decentralized digital identity and trust network originally envisioned by Phil Zimmermann for PGP . The U.S. Department of Homeland Security (DHS) Science & Technology Directorate (S&T), led by Identity and Data Privacy Program Manager Anil John, began researching how blockchain technology could be used for privacy-respecting decentralized identity management . In the spring of 2016, all four groups converged on the concept of DIDs, a term originally coined by the W3C Verifiable Claims Task Force. Thanks in part to R&D funding provided by DHS S&T, work on the first Decentralized Identifier 1.0 specification began in earnest at RWOT #2 in May 2016. The draft spec underwent review at RWOT #3 and IIW #23 in October 2016, and was published as Implementer\u2019s Draft 01 on 21 November 2016. After the W3C Verifiable Claims Working Group was approved in March 2017, in July 2017 the DID specification was contributed to the W3C Credentials Community Group. Work on several related specifications (see below) is continuing at RWOT and IIW events held every six months, as well as other industry events and conferences. See Appendix A for a list of resources and ways to become involved in the DID family of specifications.","title":"Setting the Stage: The Origin of DIDs"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#how-dids-differ-from-other-globally-unique-identifiers","text":"The need for globally unique identifiers that do not require a centralized registration authority is not new. UUIDs (Universally Unique Identifiers, also called GUIDs, Globally Unique Identifiers) were developed for this purpose in the 1980s and standardized first by the Open Software Foundation and then by IETF RFC 4122 . The need for persistent identifiers (identifiers that can be assigned once to an entity and never need to change) is also not new. This class of identifiers was standardized as URNs (Uniform Resource Names) first by IETF RFC 2141 and more recently by RFC 8141 . As a rule, however, UUIDs are not globally resolvable and URNs -- if resolvable -- require a centralized registration authority. In addition, neither UUIDs or URNs inherently address a third characteristic -- the ability to cryptographically verify ownership of the identifier . For blockchain identity -- and more specifically self-sovereign identity , which can be defined as a lifetime portable digital identity that does not depend on any centralized authority -- we need a new class of identifier that fulfills all four requirements: persistence, global resolvability, cryptographic verifiability, and decentralization.","title":"How DIDs Differ from Other Globally Unique Identifiers"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#the-format-of-a-did","text":"In 2016 the developers of the DID specification agreed with a suggestion from Christopher Allen that DIDs could be adapted to work with multiple blockchains by following the same basic pattern as the URN specification: The key difference is that with DIDs the namespace component identifies a DID method , and a DID method specification specifies the format of the method-specific identifier. DID methods (further explained below) define how DIDs work with a specific blockchain. All DID method specs must define the format and generation of the method-specific identifier. Note that the method specific identifier string must be unique in the namespace of that DID method. For example the DID above uses the Sovrin DID method in which the method-specific identifier is generated by base-56-encoding the first half of an Ed25519 verification key.","title":"The Format of a DID"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#did-documents","text":"DID infrastructure can be thought of as a global key-value database in which the database is all DID-compatible blockchains, distributed ledgers, or decentralized networks. In this virtual database, the key is a DID, and the value is a DID document . The purpose of the DID document is to describe the public keys, authentication protocols, and service endpoints necessary to bootstrap cryptographically-verifiable interactions with the identified entity. A DID document is a valid JSON-LD object that uses the DID context (the RDF vocabulary of property names) defined in the DID specification. This includes six components (all optional): The DID itself , so the DID document is fully self-describing. A set of public keys or other proofs that can be used for authentication or interaction with DID subject. A set of authentication protocols for authenticating the DID subject. A set of service endpoints that describe where and how to interact with the DID subject. Timestamps for auditing. A optional JSON-LD signature if needed to verify the integrity of the DID document. See the DID specification for several examples of DID documents.","title":"DID Documents"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#did-methods","text":"DIDs and DID documents can be adapted to any modern blockchain, distributed ledger, or other decentralized network capable of resolving a unique key into a unique value. It does not matter whether the blockchain is public, private, permissionless, or permissioned. Defining how a DID and DID document are created, resolved, and managed on a specific blockchain or \"target system\" is the role of a DID method specification . DID method specifications are to the generic DID specification as URN namespace specifications (UUID, ISBN, OID, LSID, etc.) are to the generic IETF URN specification ( RFC 8141 ). A DID method specification must define the following: The DID method name. The ABNF structure of the method-specific identifier. How the method-specific identifier is generated or derived. How the CRUD operations are performed on a DID and DID document: a. Creating a new DID. b. Reading (resolving) a DID document. c. Updating a DID document. d. Deleting (revoking) a DID. It is these CRUD operations that may vary the most across different DID methods. For example: Create. Some DID methods may generate a DID directly from a cryptographic key pair. Others may use the address of a transaction or a smart contract on the blockchain itself. Read. Some DID methods use blockchains that can store DID documents directly on the blockchain. Others may instruct DID resolvers to construct them dynamically based on attributes of a blockchain record. Still others may store a pointer on the blockchain to a DID document stored in one or more parts on other decentralized storage networks such as IPFS or STORJ . Update. The update operation is the most critical from a security standpoint because control of a DID document represents control of the public keys or proofs necessary to authenticate an entity (and therefore for an attacker to impersonate the entity). Since verification of DID document update permissions can only be enforced by the target blockchain, the DID method specification must define precisely how authentication and authorization are performed for any update operation. Delete. DID entries on a blockchain are by definition immutable, so they can never be \u201cdeleted\u201d in the conventional database sense. However they can be revoked in the cryptographic sense. A DID method specification must define how this termination is performed, e.g., by writing a null DID document.","title":"DID Methods"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#related-specifications","text":"DIDs are the atomic unit of a new layer of decentralized identity infrastructure. This is a list of the other specifications in the DID family that are currently under development.","title":"Related Specifications"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#dkms-decentralized-key-management-system","text":"DIDs are only possible with public/private key cryptography; the ability to generate, write, and update a DID and DID document to a blockchain without any intermediary requires control of the associated private key. This key management cannot itself rely on centralized authorities or it would defeat the whole purpose. In short, decentralized identity requires decentralized key management . The purpose of the DKMS (Decentralized Key Management System) specification is to specify and interoperable protocol for managing the lifecycle of private keys and other private metadata associated with a DID in a way that is interoperable across different blockchains, apps, and vendors. This spec is being developed under a grant from the Science & Technology Directorate of the U.S. Department of Homeland Security based on the requirements and best practices set forth in NIST Special Publication 800-130 , \u201cA Framework for Designing Key Management Systems\u201d.","title":"DKMS (Decentralized Key Management System)"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#did-tls","text":"Today's TLS infrastructure uses X.509 certs based on traditional hierarchical PKI, where certificate authorities (CAs) follow standardized best practices in order to qualify as trust roots that will be recognized by browser vendors. DID TLS will decentralize this process by enabling the standard X.509 cert elements required to establish a TLS session to be generated dynamically from any DID and DID document that conforms to the DID spec. The DID TLS specification will enable encrypted, peer-to-peer connections to be negotiated in real time between any two DID-identified entities (people, organizations, things). This will radically expand the protections of the TLS protocol and could potentially turn them into the default for all nearly all forms of Internet communication.","title":"DID TLS"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#did-names","text":"The DID specification is intentionally limited to machine-generated decentralized identifiers that are completely lacking in human memorability or usability. However there are many use cases it is desirable to be able to discover a DID using a human-friendly semantic name. Such a naming service would look like a flat, cryptographically-verifiable version of DNS. The big difference, of course, is that a DID naming service needs to be fully decentralized, i.e., not depend on centralized registries and registrars. Registration of DID names would be made directly by identity owners to the blockchain itself using the same cryptographic verification as DID transactions. The goal of the DID Names specification is to standardize how an interoperable decentralized naming layer can operate directly on top of the DID layer. A DID name is mapped to a DID the same way a DID is mapped to a DID document. DID names will be an optional feature of a DID method, so the governance and economics of a DID namespace can be specified by the same community that defines the associated DID method.","title":"DID Names"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#did-auth","text":"A common goal of all blockchain identity systems is the cryptographic authentication of an identity owner. The various protocols all use some type of cryptographic challenge/response similar to the SQRL protocol originally proposed by Steve Gibson and the Web Authentication protocol currently being standardized by W3C. In these protocols, a one-time challenge is issued by the relying party, signed by the identity owner's private key, and then verified by the relying party using the identity owner\u2019s public key. Whereas SQRL and Web Authentication use pairwise public keys that cannot be externally verified, DIDs will enable verification of the public key against the blockchain identified by the DID method. The DID Auth specification will standardize this cryptographic challenge/response authentication protocol so it can be used with any DID that supports it. DID Auth endpoints would then become one of the standard DID identity services than can be discovered via a DID document.","title":"DID Auth"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#dids-and-privacy-by-design","text":"Privacy is an essential component of any identity management solution; it is especially critical for a global identity system that uses immutable public blockchains. Thankfully DID architecture can incorporate Privacy by Design at the very lowest levels of infrastructure and thus become a powerful, new, privacy-preserving technology if deployed using best practices such as: Pairwise-pseudonymous DIDs. While DIDs can be used as well-known public identifiers, they can also be used as private identifiers issued on a per-relationship basis. So rather than a person having a single DID, like a cell phone number or national ID number, she can have thousands of pairwise-unique DIDs that cannot be correlated without her consent, yet can still be managed as easily as an address book. Off-chain private data. Storing any type of PII on a public blockchain, even encrypted or hashed, is dangerous for two reasons: 1) the encrypted or hashed data is a global correlation point when the data is shared with multiple parties, and 2) if the encryption is eventually broken (e.g., quantum computing ), the data will be forever accessible on an immutable public ledger. So the best practice is to store all private data off-chain and exchange it only over encrypted, private, peer-to-peer connections. Selective disclosure. The decentralized PKI (DPKI) that DIDs make possible opens the door to individuals gaining greater control over their personal data in two ways. First, it enables it to be shared using encrypted digital credentials (see below). Second, these credentials can use zero-knowledge proof cryptography for data minimization , e.g., you can disclose that you are over a certain age without disclosing your exact birthdate.","title":"DIDs and Privacy by Design"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#dids-and-verifiable-claims","text":"DIDs are only the base layer of decentralized identity infrastructure. The next higher layer -- where most of the value is unlocked -- is verifiable claims . This is the technical term for a digitally signed electronic credential that conforms to the interoperability standards being developed by the W3C Verifiable Claims Working Group . For a complete introduction, please see the Verifiable Claims Primer . The diagram below (from Manu Sporny) illustrates the three primary roles in the verifiable claims ecosystem: issuers sign claims and give them to holders (identity owners) who present them to verifiers who verify the signatures in order to grant access to resources. Note that in all three cases, the parties interact with the DID layer to register DIDs as persistent identifiers for issuers or holders, and to resolve those DIDs to obtain the public keys needed to verify the signature of an issuer or holder. Since any issuer may provide claims to any holder who may present them to any verifier, this results in set of rich, interlocking trust relationships that do not need to conform to any pre-established hierarchy -- a web of trust .","title":"DIDs and Verifiable Claims"},{"location":"rwot8/topics-and-advance-readings/did-primer-extended/#appendix-a-did-community-resources","text":"Besides the links throughout this primer, these additional resources are available to anyone interested in joining the DID community. W3C Verifiable Claims Working Group mailing list W3C Credentials Community Group DID specification issues list Rebooting the Web of Trust event (held every six months) Internet Identity Workshop event (held every six months)","title":"Appendix A: DID Community Resources"},{"location":"rwot8/topics-and-advance-readings/did-primer/","text":"A Short Primer for Decentralized Identifiers This is a community document maintained by co-editors Drummond Reed and Manu Sporny and other contributors and implementers of the Decentralized Identifier 1.0 specification . Introduction At a superficial level, a decentralized identifier ( DID ) is simply a new type of globally unique identifier. But at a deeper level, DIDs are the core component of an entirely new layer of decentralized digital identity and public key infrastructure (PKI) for the Internet. This decentralized public key infrastructure (DPKI) could have as much impact on global cybersecurity and cyberprivacy as the development of the SSL/TLS protocol for encrypted Web traffic (now the largest PKI in the world). This primer is designed to give newcomers to DID architecture the background they need to understand not just the DID specification, but the overall architecture for decentralized identity represented by the family of DID-related specifications currently under development. It covers: Background on the origin of DIDs and the DID specification. How DIDs differ from other globally-unique identifiers. How the syntax of DIDs can be adapted to work with decentralized networks. How DIDs resolve to DID Documents containing public keys and service endpoints. The key role that DID Methods play in the implementation of DID infrastructure. Privacy considerations for the use of DIDs. How DID infrastructure lays the foundation for verifiable credentials . How DIDs Differ from Other Globally Unique Identifiers The need for globally unique identifiers that do not require a centralized registration authority is not new. UUIDs (Universally Unique Identifiers, also called GUIDs, Globally Unique Identifiers) were developed for this purpose in the 1980s and standardized first by the Open Software Foundation and then by IETF RFC 4122 . The need for persistent identifiers (identifiers that can be assigned once to an entity and never need to change) is also not new. This class of identifiers was standardized as URNs (Uniform Resource Names) first by IETF RFC 2141 and more recently by RFC 8141 . As a rule, however, UUIDs are not globally resolvable and URNs -- if resolvable -- require a centralized registration authority. In addition, neither UUIDs or URNs inherently address a third characteristic -- the ability to cryptographically verify ownership of the identifier . For self-sovereign identity , which can be defined as a lifetime portable digital identity that does not depend on any centralized authority, we need a new class of identifier that fulfills all four requirements: persistence, global resolvability, cryptographic verifiability, and decentralization. The Format of a DID In 2016 the developers of the DID specification agreed with a suggestion from Christopher Allen that DIDs could be adapted to work with multiple blockchains by following the same basic pattern as the URN specification: The key difference is that with DIDs the namespace component identifies a DID method , and a DID method specification specifies the format of the method-specific identifier. DID methods (further explained below) define how DIDs work with a specific blockchain. All DID method specs must define the format and generation of the method-specific identifier. Note that the method specific identifier string must be unique in the namespace of that DID method. For example, in the Sovrin DID method ( did:sov: ), the method-specific identifier is generated by base-56-encoding the first half of an Ed25519 verification key. DID Documents DID infrastructure can be thought of as a global key-value database in which the database is all DID-compatible blockchains, distributed ledgers, or decentralized networks. In this virtual database, the key is a DID, and the value is a DID document . The purpose of the DID document is to describe the public keys, authentication protocols, and service endpoints necessary to bootstrap cryptographically-verifiable interactions with the identified entity. A DID document is a valid JSON-LD object that uses the DID context (the RDF vocabulary of property names) defined in the DID specification. This includes six components (all optional): The DID itself , so the DID document is fully self-describing. A set of cryptographic material , such as public keys, that can be used for authentication or interaction with the DID subject. A set of cryptographic protocols for interacting with the DID subject, such as authentication and capability delegation. A set of service endpoints that describe where and how to interact with the DID subject. Timestamps for auditing. A optional JSON-LD signature if needed to verify the integrity of the DID document. See the DID specification for several examples of DID documents. DID Methods DIDs and DID documents can be adapted to any modern blockchain, distributed ledger, or other decentralized network capable of resolving a unique key into a unique value. It does not matter whether the blockchain is public, private, permissionless, or permissioned. Defining how a DID and DID document are created, resolved, and managed on a specific blockchain or \"target system\" is the role of a DID method specification . DID method specifications are to the generic DID specification as URN namespace specifications (UUID, ISBN, OID, LSID, etc.) are to the generic IETF URN specification ( RFC 8141 ). DID method specifications typically define at least the following operations for a particular target system: Create. Some DID methods may generate a DID directly from a cryptographic key pair. Others may use the address of a transaction or a smart contract on the blockchain itself. Read. Some DID methods use blockchains that can store DID documents directly on the blockchain. Others may instruct DID resolvers to construct them dynamically based on attributes of a blockchain record. Still others may store a pointer on the blockchain to a DID document stored in one or more parts on other decentralized storage networks such as IPFS or STORJ . Update. The update operation is the most critical from a security standpoint because control of a DID document represents control of the public keys or proofs necessary to authenticate an entity (and therefore for an attacker to impersonate the entity). Since verification of DID document update permissions can only be enforced by the target blockchain, the DID method specification must define precisely how authentication and authorization are performed for any update operation. Delete. DID entries on a blockchain are by definition immutable, so they can never be \u201cdeleted\u201d in the conventional database sense. However they can be revoked in the cryptographic sense. A DID method specification must define how this termination is performed, e.g., by writing a null DID document. See the DID Method Registry for a complete list of all known DID Method specifications. DIDs and Privacy by Design Privacy is an essential component of any identity management solution; it is especially critical for a global identity system that uses immutable public blockchains. Thankfully DID architecture can incorporate Privacy by Design at the very lowest levels of infrastructure and thus become a powerful, new, privacy-preserving technology if deployed using best practices such as: Pairwise-pseudonymous DIDs. While DIDs can be used as well-known public identifiers, they can also be used as private identifiers issued on a per-relationship basis. So rather than a person having a single DID, like a cell phone number or national ID number, she can have thousands of pairwise-unique DIDs that cannot be correlated without her consent, yet can still be managed as easily as an address book. Off-chain private data. Storing any type of PII on a public blockchain, even encrypted or hashed, is dangerous for two reasons: 1) the encrypted or hashed data is a global correlation point when the data is shared with multiple parties, and 2) if the encryption is eventually broken (e.g., quantum computing ), the data will be forever accessible on an immutable public ledger. So the best practice is to store all private data off-chain and exchange it only over encrypted, private, peer-to-peer connections. Selective disclosure. The decentralized PKI (DPKI) that DIDs make possible opens the door to individuals gaining greater control over their personal data in two ways. First, it enables it to be shared using encrypted digital credentials (see below). Second, these credentials can use zero-knowledge proof cryptography for data minimization , e.g., you can disclose that you are over a certain age without disclosing your exact birthdate. DIDs and Verifiable Credentials DIDs are only the base layer of decentralized identity infrastructure. The next higher layer -- where most of the value is unlocked -- is verifiable credentials . This is the technical term for a digitally signed electronic credential that conforms to the interoperability standards being developed by the W3C Verifiable Claims Working Group . DIDs can be used to identify various entities in the Verifiable Credentials ecosystem such as issuers, holders, subjects, and verifiers. More generally, DIDs can be used as identifiers for people, devices, and organizations. See the Verifiable Credentials Primer for a brief introduction to the topic. Appendix A: DID Community Resources Besides the links throughout this primer, these additional resources are available to anyone interested in joining the communities that are actively developing specifications, experiments, and pilot projects. W3C Verifiable Claims Working Group mailing list W3C Credentials Community Group DID specification issues list Rebooting the Web of Trust event (held every six months) Internet Identity Workshop event (held every six months)","title":"A Short Primer for Decentralized Identifiers"},{"location":"rwot8/topics-and-advance-readings/did-primer/#a-short-primer-for-decentralized-identifiers","text":"This is a community document maintained by co-editors Drummond Reed and Manu Sporny and other contributors and implementers of the Decentralized Identifier 1.0 specification .","title":"A Short Primer for Decentralized Identifiers"},{"location":"rwot8/topics-and-advance-readings/did-primer/#introduction","text":"At a superficial level, a decentralized identifier ( DID ) is simply a new type of globally unique identifier. But at a deeper level, DIDs are the core component of an entirely new layer of decentralized digital identity and public key infrastructure (PKI) for the Internet. This decentralized public key infrastructure (DPKI) could have as much impact on global cybersecurity and cyberprivacy as the development of the SSL/TLS protocol for encrypted Web traffic (now the largest PKI in the world). This primer is designed to give newcomers to DID architecture the background they need to understand not just the DID specification, but the overall architecture for decentralized identity represented by the family of DID-related specifications currently under development. It covers: Background on the origin of DIDs and the DID specification. How DIDs differ from other globally-unique identifiers. How the syntax of DIDs can be adapted to work with decentralized networks. How DIDs resolve to DID Documents containing public keys and service endpoints. The key role that DID Methods play in the implementation of DID infrastructure. Privacy considerations for the use of DIDs. How DID infrastructure lays the foundation for verifiable credentials .","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/did-primer/#how-dids-differ-from-other-globally-unique-identifiers","text":"The need for globally unique identifiers that do not require a centralized registration authority is not new. UUIDs (Universally Unique Identifiers, also called GUIDs, Globally Unique Identifiers) were developed for this purpose in the 1980s and standardized first by the Open Software Foundation and then by IETF RFC 4122 . The need for persistent identifiers (identifiers that can be assigned once to an entity and never need to change) is also not new. This class of identifiers was standardized as URNs (Uniform Resource Names) first by IETF RFC 2141 and more recently by RFC 8141 . As a rule, however, UUIDs are not globally resolvable and URNs -- if resolvable -- require a centralized registration authority. In addition, neither UUIDs or URNs inherently address a third characteristic -- the ability to cryptographically verify ownership of the identifier . For self-sovereign identity , which can be defined as a lifetime portable digital identity that does not depend on any centralized authority, we need a new class of identifier that fulfills all four requirements: persistence, global resolvability, cryptographic verifiability, and decentralization.","title":"How DIDs Differ from Other Globally Unique Identifiers"},{"location":"rwot8/topics-and-advance-readings/did-primer/#the-format-of-a-did","text":"In 2016 the developers of the DID specification agreed with a suggestion from Christopher Allen that DIDs could be adapted to work with multiple blockchains by following the same basic pattern as the URN specification: The key difference is that with DIDs the namespace component identifies a DID method , and a DID method specification specifies the format of the method-specific identifier. DID methods (further explained below) define how DIDs work with a specific blockchain. All DID method specs must define the format and generation of the method-specific identifier. Note that the method specific identifier string must be unique in the namespace of that DID method. For example, in the Sovrin DID method ( did:sov: ), the method-specific identifier is generated by base-56-encoding the first half of an Ed25519 verification key.","title":"The Format of a DID"},{"location":"rwot8/topics-and-advance-readings/did-primer/#did-documents","text":"DID infrastructure can be thought of as a global key-value database in which the database is all DID-compatible blockchains, distributed ledgers, or decentralized networks. In this virtual database, the key is a DID, and the value is a DID document . The purpose of the DID document is to describe the public keys, authentication protocols, and service endpoints necessary to bootstrap cryptographically-verifiable interactions with the identified entity. A DID document is a valid JSON-LD object that uses the DID context (the RDF vocabulary of property names) defined in the DID specification. This includes six components (all optional): The DID itself , so the DID document is fully self-describing. A set of cryptographic material , such as public keys, that can be used for authentication or interaction with the DID subject. A set of cryptographic protocols for interacting with the DID subject, such as authentication and capability delegation. A set of service endpoints that describe where and how to interact with the DID subject. Timestamps for auditing. A optional JSON-LD signature if needed to verify the integrity of the DID document. See the DID specification for several examples of DID documents.","title":"DID Documents"},{"location":"rwot8/topics-and-advance-readings/did-primer/#did-methods","text":"DIDs and DID documents can be adapted to any modern blockchain, distributed ledger, or other decentralized network capable of resolving a unique key into a unique value. It does not matter whether the blockchain is public, private, permissionless, or permissioned. Defining how a DID and DID document are created, resolved, and managed on a specific blockchain or \"target system\" is the role of a DID method specification . DID method specifications are to the generic DID specification as URN namespace specifications (UUID, ISBN, OID, LSID, etc.) are to the generic IETF URN specification ( RFC 8141 ). DID method specifications typically define at least the following operations for a particular target system: Create. Some DID methods may generate a DID directly from a cryptographic key pair. Others may use the address of a transaction or a smart contract on the blockchain itself. Read. Some DID methods use blockchains that can store DID documents directly on the blockchain. Others may instruct DID resolvers to construct them dynamically based on attributes of a blockchain record. Still others may store a pointer on the blockchain to a DID document stored in one or more parts on other decentralized storage networks such as IPFS or STORJ . Update. The update operation is the most critical from a security standpoint because control of a DID document represents control of the public keys or proofs necessary to authenticate an entity (and therefore for an attacker to impersonate the entity). Since verification of DID document update permissions can only be enforced by the target blockchain, the DID method specification must define precisely how authentication and authorization are performed for any update operation. Delete. DID entries on a blockchain are by definition immutable, so they can never be \u201cdeleted\u201d in the conventional database sense. However they can be revoked in the cryptographic sense. A DID method specification must define how this termination is performed, e.g., by writing a null DID document. See the DID Method Registry for a complete list of all known DID Method specifications.","title":"DID Methods"},{"location":"rwot8/topics-and-advance-readings/did-primer/#dids-and-privacy-by-design","text":"Privacy is an essential component of any identity management solution; it is especially critical for a global identity system that uses immutable public blockchains. Thankfully DID architecture can incorporate Privacy by Design at the very lowest levels of infrastructure and thus become a powerful, new, privacy-preserving technology if deployed using best practices such as: Pairwise-pseudonymous DIDs. While DIDs can be used as well-known public identifiers, they can also be used as private identifiers issued on a per-relationship basis. So rather than a person having a single DID, like a cell phone number or national ID number, she can have thousands of pairwise-unique DIDs that cannot be correlated without her consent, yet can still be managed as easily as an address book. Off-chain private data. Storing any type of PII on a public blockchain, even encrypted or hashed, is dangerous for two reasons: 1) the encrypted or hashed data is a global correlation point when the data is shared with multiple parties, and 2) if the encryption is eventually broken (e.g., quantum computing ), the data will be forever accessible on an immutable public ledger. So the best practice is to store all private data off-chain and exchange it only over encrypted, private, peer-to-peer connections. Selective disclosure. The decentralized PKI (DPKI) that DIDs make possible opens the door to individuals gaining greater control over their personal data in two ways. First, it enables it to be shared using encrypted digital credentials (see below). Second, these credentials can use zero-knowledge proof cryptography for data minimization , e.g., you can disclose that you are over a certain age without disclosing your exact birthdate.","title":"DIDs and Privacy by Design"},{"location":"rwot8/topics-and-advance-readings/did-primer/#dids-and-verifiable-credentials","text":"DIDs are only the base layer of decentralized identity infrastructure. The next higher layer -- where most of the value is unlocked -- is verifiable credentials . This is the technical term for a digitally signed electronic credential that conforms to the interoperability standards being developed by the W3C Verifiable Claims Working Group . DIDs can be used to identify various entities in the Verifiable Credentials ecosystem such as issuers, holders, subjects, and verifiers. More generally, DIDs can be used as identifiers for people, devices, and organizations. See the Verifiable Credentials Primer for a brief introduction to the topic.","title":"DIDs and Verifiable Credentials"},{"location":"rwot8/topics-and-advance-readings/did-primer/#appendix-a-did-community-resources","text":"Besides the links throughout this primer, these additional resources are available to anyone interested in joining the communities that are actively developing specifications, experiments, and pilot projects. W3C Verifiable Claims Working Group mailing list W3C Credentials Community Group DID specification issues list Rebooting the Web of Trust event (held every six months) Internet Identity Workshop event (held every six months)","title":"Appendix A: DID Community Resources"},{"location":"rwot8/topics-and-advance-readings/did-spec-current-status/","text":"The Current Status of the DID Specification By Amy Guy <amy@rhiaro.co.uk> and Dmitri Zagidulin <dzagidulin@gmail.com>, Digital Bazaar Work on the Decentralized Identifier 1.0 specification began at RWOT #2 in May 2016, and a draft was published on 21 November 2016. Work continues under the custody of the W3C Credentials Community Group , a group of 237 members, who contribute by taking part in weekly teleconference calls , engaging in discussions on the mailing list , and raising issues on the spec on GitHub . This document serves as a summary of the current state of work. It includes a rough categorization of the current open issues, with the goal of identifying actions which can be taken quickly to move the spec forward, as well as topics which need extended discussion amongst the community. Issues (and sometimes PRs) are categorized as follows: clarify : Clarification or disambiguation of concepts the community already has consensus on. discuss : Topics which need more discussion to reach consensus about. editorial : Updates to informative language, without changes to meaning or conformance criteria. elsewhere : Issues relating to other documents (specs, namespaces, etc). question : Questions about the spec or implementation guidance. If you're new to DIDs, or want a refresher on the background and core concepts of the DID spec, you will find it useful to read the DID Primer (or extended DID Primer ) first. This document will be updated as work on the DID specification progresses, so information will be current at the time of RWOT8. Clarification needed Some topics have consensus amongst members of the Community Group, or are intuitively understood by people who have been deeply embedded in the work for a while, but remain unclear with regards to text in the specification. For these, the spec needs new text, or reworking of existing text, to make sure concepts are communicated clearly and unambiguously to new readers. Issues are labeled ' clarify ' and include: The scope and purpose of the DID specification ( #157 , #151 , #121 , #138 ). What is a DID? ( #130 , #156 , #155 , #141 , #140 , #125 , #124 , #123 , #122 , #115 ). Use of '[distributed] ledger' ( #150 , #149 ). Questions about keys ( #147 , #143 , #105 ). ABNF rules ( #136 , #135 , #131 ). Decentralization and practicality ( #133 , #120 ). Service endpoints ( #104 ). Content types ( #84 , #82 ). Action: The CG should agree definitive definitions or statements for each of these, and then work to integrate them into the spec text. Bigger discussion points Some topics need deeper discussion to ensure common understanding amongst the CG and the wider community, or more technical work to make sure they are resolved properly. Issues are labeled ' discuss ' and include: Do we need to explicitly name the thing that the DID identifies? (Or, consolidate casual references to the thing the DID identifies?) Eg. 'referent', 'entity', 'subject', .. ( #154 , #148 , #145 , #130 , #139 ). Current consensus: The thing that the DID identifies is a DID Document, no other terms are required or needed. DID URI structure, and Resolving DIDs into DID Documents ( #97 , #90 , #85 , #80 ). Key revocation ( #96 ). DID method discovery ( #83 ). DID controllers ( #153 ). Action: discuss at RWOT and on CCG calls, resolve misunderstandings, and where applicable break issues down into manageable changes that can be made to the spec. Smaller todos Editorial issues, such as grammatical fixes or reworking sentences without changing the meaning, and perhaps structural changes like section naming or ordering, are labeled ' editorial ' and include: Clearer language / small correction requested: #144 , #137 , #129 , #128 , #127 , #126 , #119 , #118 , #117 , #116 , #112 , #81 Additional explanation requested: #134 . Action: PRs please! External documents Some aspects of the DID work are eventually extracted into separate specifications. There are other external documents which are connected to the spec. Issues relating to these are labeled ' elsewhere ' and include: JSON-LD context(s): #152 . Related to DID Resolution: #97 , #64 , #17 . Signatures: #60 , #56 , #39 , #38 , #37 , #29 . Action: Open issues or PRs on external documents, update references or summaries in the DID spec if applicable. Other Discussion Points And if that wasn't enough, there are other topics that are of interest to the CG which haven't (yet) been raised as issues. Cryptographic Hyperlinks (Non-normative) Discussion about Cryptonyms and \"Off-ledger\" DIDs (related: #113 )","title":"The Current Status of the DID Specification"},{"location":"rwot8/topics-and-advance-readings/did-spec-current-status/#the-current-status-of-the-did-specification","text":"By Amy Guy <amy@rhiaro.co.uk> and Dmitri Zagidulin <dzagidulin@gmail.com>, Digital Bazaar Work on the Decentralized Identifier 1.0 specification began at RWOT #2 in May 2016, and a draft was published on 21 November 2016. Work continues under the custody of the W3C Credentials Community Group , a group of 237 members, who contribute by taking part in weekly teleconference calls , engaging in discussions on the mailing list , and raising issues on the spec on GitHub . This document serves as a summary of the current state of work. It includes a rough categorization of the current open issues, with the goal of identifying actions which can be taken quickly to move the spec forward, as well as topics which need extended discussion amongst the community. Issues (and sometimes PRs) are categorized as follows: clarify : Clarification or disambiguation of concepts the community already has consensus on. discuss : Topics which need more discussion to reach consensus about. editorial : Updates to informative language, without changes to meaning or conformance criteria. elsewhere : Issues relating to other documents (specs, namespaces, etc). question : Questions about the spec or implementation guidance. If you're new to DIDs, or want a refresher on the background and core concepts of the DID spec, you will find it useful to read the DID Primer (or extended DID Primer ) first. This document will be updated as work on the DID specification progresses, so information will be current at the time of RWOT8.","title":"The Current Status of the DID Specification"},{"location":"rwot8/topics-and-advance-readings/did-spec-current-status/#clarification-needed","text":"Some topics have consensus amongst members of the Community Group, or are intuitively understood by people who have been deeply embedded in the work for a while, but remain unclear with regards to text in the specification. For these, the spec needs new text, or reworking of existing text, to make sure concepts are communicated clearly and unambiguously to new readers. Issues are labeled ' clarify ' and include: The scope and purpose of the DID specification ( #157 , #151 , #121 , #138 ). What is a DID? ( #130 , #156 , #155 , #141 , #140 , #125 , #124 , #123 , #122 , #115 ). Use of '[distributed] ledger' ( #150 , #149 ). Questions about keys ( #147 , #143 , #105 ). ABNF rules ( #136 , #135 , #131 ). Decentralization and practicality ( #133 , #120 ). Service endpoints ( #104 ). Content types ( #84 , #82 ). Action: The CG should agree definitive definitions or statements for each of these, and then work to integrate them into the spec text.","title":"Clarification needed"},{"location":"rwot8/topics-and-advance-readings/did-spec-current-status/#bigger-discussion-points","text":"Some topics need deeper discussion to ensure common understanding amongst the CG and the wider community, or more technical work to make sure they are resolved properly. Issues are labeled ' discuss ' and include: Do we need to explicitly name the thing that the DID identifies? (Or, consolidate casual references to the thing the DID identifies?) Eg. 'referent', 'entity', 'subject', .. ( #154 , #148 , #145 , #130 , #139 ). Current consensus: The thing that the DID identifies is a DID Document, no other terms are required or needed. DID URI structure, and Resolving DIDs into DID Documents ( #97 , #90 , #85 , #80 ). Key revocation ( #96 ). DID method discovery ( #83 ). DID controllers ( #153 ). Action: discuss at RWOT and on CCG calls, resolve misunderstandings, and where applicable break issues down into manageable changes that can be made to the spec.","title":"Bigger discussion points"},{"location":"rwot8/topics-and-advance-readings/did-spec-current-status/#smaller-todos","text":"Editorial issues, such as grammatical fixes or reworking sentences without changing the meaning, and perhaps structural changes like section naming or ordering, are labeled ' editorial ' and include: Clearer language / small correction requested: #144 , #137 , #129 , #128 , #127 , #126 , #119 , #118 , #117 , #116 , #112 , #81 Additional explanation requested: #134 . Action: PRs please!","title":"Smaller todos"},{"location":"rwot8/topics-and-advance-readings/did-spec-current-status/#external-documents","text":"Some aspects of the DID work are eventually extracted into separate specifications. There are other external documents which are connected to the spec. Issues relating to these are labeled ' elsewhere ' and include: JSON-LD context(s): #152 . Related to DID Resolution: #97 , #64 , #17 . Signatures: #60 , #56 , #39 , #38 , #37 , #29 . Action: Open issues or PRs on external documents, update references or summaries in the DID spec if applicable.","title":"External documents"},{"location":"rwot8/topics-and-advance-readings/did-spec-current-status/#other-discussion-points","text":"And if that wasn't enough, there are other topics that are of interest to the CG which haven't (yet) been raised as issues. Cryptographic Hyperlinks (Non-normative) Discussion about Cryptonyms and \"Off-ledger\" DIDs (related: #113 )","title":"Other Discussion Points"},{"location":"rwot8/topics-and-advance-readings/did-ux/","text":"Where's the UX? By Alberto Elias hi@albertoelias.me DIDs provide a safer way to identify any entity (person, organization, software..) against any other entity in such a way that each one is the real controller of their identity. DIDs are a global identifier with no centralized authority involved. They are really powerful but they're still not usable. I've been doing some user testing with both developers and non-technical folks and every single one of them has complained about the authentication and registration flows. We all agree that this work is for nothing if people don't end up using them . This community is definitely human-focused, but initial DID implementations haven't nailed the user experience yet. We're battling usernames and passwords. It's not a strong contender in neither user experience nor security. It's a pain to have a password for hundreds of sites/apps and having to remember them and writing them in all your devices, specially on smartphones and XR HMDs (VR/AR Head Mounted Displays). This also means that people use bad passwords, and use the same one for every service. We end up being hacked all the time. DIDs are more secure and censorship resistant than passwords, specially thanks to some amazing work by the community: key recovery , social recovery , capability based system etc. But even if the identity systems we created were just as (in)secure as passwords but user-owned , that would already be a substantial step in the right direction. The thing is that, even though privacy awareness is on the raise, people still won't care enough about security and data ownership if the UX isn't at least as good as what they're currently using. There is a lot to gain if get the UX right. The good thing is that DID UX can be way better than passwords . First things first. Users MUST never see DIDs or interact with DID Documents directly. I know the community is already completely on board on this one, but I still see several implementations exposing this data quite visibly. The first impression people will get is during the registration flow. It should only require a button press which would generate the necessary keys and store them in an encrypted storage. As long as the code that generates the keys is open source and happens locally so it's provable that it wasn't shared, we're good. The key store needs to be encrypted so that, for example, on the Web other loaded scripts can't access the keys. The most common flow is authentication . It should also consist of 1-2 button presses depending if the service requires more information from the person. We can even provide the option to auto-login when the application doesn't require any extra information. This would be specially useful with virtual worlds as you could navigate from world to world with a persistent avatar. Many see Verified Claims as the API for DIDs. They are a very powerful API that allow our DID based identities to tie in very nicely into all aspect of our lives. They should also only require a button press to read or write data. The UI could be something similar to OAuth where your identity app shows you the request and you can Confirm or Reject . A crucial aspect for DIDs is that they need to be universal and work everywhere , including new devices. XR is a whole new platform which opens the door to change. We can use it as an opportunity to do things differently. The XR market is expected to grow significantly in 2019 due to the launch of standalone HMDs that reduce the friction to use them greatly. These headsets aren't connected to anything, neither a PC nor a phone. The computer is incorporated into the HMD, and the only interface available is the VR one. Current DID implementations just don't work in XR as they require scanning a QR code or accessing a private key in another device that doesn't connect nicely to the headset. DIDs are a really powerful way to identify any kind of entity. We still have a lot of things to figure out and we haven't covered all the security holes, but the underlying technology is already in a good place. I believe it is time to start getting users . For that we need to prioritise UX and DIDs allow us to offer a better UX than passwords, with increased security and giving people back their data. Let's ship it.","title":"Where's the UX?"},{"location":"rwot8/topics-and-advance-readings/did-ux/#wheres-the-ux","text":"By Alberto Elias hi@albertoelias.me DIDs provide a safer way to identify any entity (person, organization, software..) against any other entity in such a way that each one is the real controller of their identity. DIDs are a global identifier with no centralized authority involved. They are really powerful but they're still not usable. I've been doing some user testing with both developers and non-technical folks and every single one of them has complained about the authentication and registration flows. We all agree that this work is for nothing if people don't end up using them . This community is definitely human-focused, but initial DID implementations haven't nailed the user experience yet. We're battling usernames and passwords. It's not a strong contender in neither user experience nor security. It's a pain to have a password for hundreds of sites/apps and having to remember them and writing them in all your devices, specially on smartphones and XR HMDs (VR/AR Head Mounted Displays). This also means that people use bad passwords, and use the same one for every service. We end up being hacked all the time. DIDs are more secure and censorship resistant than passwords, specially thanks to some amazing work by the community: key recovery , social recovery , capability based system etc. But even if the identity systems we created were just as (in)secure as passwords but user-owned , that would already be a substantial step in the right direction. The thing is that, even though privacy awareness is on the raise, people still won't care enough about security and data ownership if the UX isn't at least as good as what they're currently using. There is a lot to gain if get the UX right. The good thing is that DID UX can be way better than passwords . First things first. Users MUST never see DIDs or interact with DID Documents directly. I know the community is already completely on board on this one, but I still see several implementations exposing this data quite visibly. The first impression people will get is during the registration flow. It should only require a button press which would generate the necessary keys and store them in an encrypted storage. As long as the code that generates the keys is open source and happens locally so it's provable that it wasn't shared, we're good. The key store needs to be encrypted so that, for example, on the Web other loaded scripts can't access the keys. The most common flow is authentication . It should also consist of 1-2 button presses depending if the service requires more information from the person. We can even provide the option to auto-login when the application doesn't require any extra information. This would be specially useful with virtual worlds as you could navigate from world to world with a persistent avatar. Many see Verified Claims as the API for DIDs. They are a very powerful API that allow our DID based identities to tie in very nicely into all aspect of our lives. They should also only require a button press to read or write data. The UI could be something similar to OAuth where your identity app shows you the request and you can Confirm or Reject . A crucial aspect for DIDs is that they need to be universal and work everywhere , including new devices. XR is a whole new platform which opens the door to change. We can use it as an opportunity to do things differently. The XR market is expected to grow significantly in 2019 due to the launch of standalone HMDs that reduce the friction to use them greatly. These headsets aren't connected to anything, neither a PC nor a phone. The computer is incorporated into the HMD, and the only interface available is the VR one. Current DID implementations just don't work in XR as they require scanning a QR code or accessing a private key in another device that doesn't connect nicely to the headset. DIDs are a really powerful way to identify any kind of entity. We still have a lot of things to figure out and we haven't covered all the security holes, but the underlying technology is already in a good place. I believe it is time to start getting users . For that we need to prioritise UX and DIDs allow us to offer a better UX than passwords, with increased security and giving people back their data. Let's ship it.","title":"Where's the UX?"},{"location":"rwot8/topics-and-advance-readings/digital-contraception/","text":"Digital Contraception By Mitzi L\u00e1szl\u00f3 email@mitzilaszlo.org \u201cPersonal Data means any information relating to an identified or identifiable natural person (\u2018data subject\u2019); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person;\u201d - General Data Protection Regulation Article 4 (1) The collective of one individual's personal data forms a digital identity (or perhaps digital alter ego is more fitting). A digital identity encompasses all of our personal data shadowing, representing and connected to our physical and ideological self. Data points are linked in meaning. For example, the genetic profile of one person can give insight into the genetic profile of family members. Data points can lead to deductions that result in other data points. When does one digital identity start and another digital identity stop? At what point does data become public as a result of describing a crowd? If one person records their observations on another person who owns those observations? The observer or the observed? What responsibilities do the observer and the observed have in relation to each other? Since the massive scale and systematisation of observation of people and their thoughts as a result of the Web, these questions are increasingly important to address. The question of personal data falls into an unknown territory in between corporate ownership, intellectual property, and slavery. Ownership involves determining rights and duties over property. While the Web is not owned by anyone, corporations have come to collect, store, and control the personal data, creating value making use of data collection, search engines and communication tools. By default, as a side effect to owning the intellectual property making up the online tools, these corporations have been collecting our digital identities as raw material for the services delivered to other companies at a profit. Data can be replicated making it somewhat like intellectual property. Multiple individuals can hold a copy of a digital identity. The replicability of data makes the question of control complex. Ownership implies exclusivity, particularly with abstract concepts like ideas or data points. It is not enough to simply have a copy of your own data. Others should be restricted in their access to what is yours. Knowing what data others keep is a near impossible task. The simpler approach would be to cloak yourself in nonsense. To ensure that corporations or institutions do not have a copy of your data it is possible to send noise to confuse the data that they have. For example, a robot could randomly search terms that you would not be inclined to usually search for making that data obtained by the search engine useless through confusion. Can one person legitimately control the digital identity of another person? Slavery, the ownership of a person, is outlawed in all recognised countries. The institution of marriage originates from the need to grant property rights. The sexual and reproductive identity of the woman was controlled by the father and passed to the husband. Today, our digital identities are controlled by others, a situation of which the legitimacy is being increasingly questioned. The ability for each and every person to self-determine is a fairly novel concept. If we go from the starting point that individuals should be in control of their digital identities, use of that data by another person, company or institution, requires the explicit permission of the owner. Much like when choosing a contraceptive method, deciding our data sharing preferences is not done in isolation, there are other factors to weigh up and we end up making a risk benefit analysis. What do effective contraception techniques teach us about best practices for effective data sharing preferences? As with sexual consent, it is questionably ethical is permission for the data transaction to go ahead is used as a bargaining chip for an unrelated or superfluous issue of consent, for example, improve marketing recommendations while you are trying to ring your mother. While there are services where you need to share data, these transactions should not be exaggerated and should be held within context. For example, an individual needs to share data to receive adequate medical recommendations, however, that medical data does not automatically need to go to a health insurance provider. These are separate data transactions which should be dealt with as such. How far could we push the free services in exchange for control of your data? Imagine if you went to a restaurant and the food was free on the condition that a stranger sat with you and shouted advertisement. Imagine if there was a bar with free drinks on the condition that the bartender could sell your conversations. For consent to be meaningful the full scope and extent of the transaction needs to be explicitly detailed to the individual who has to be given apt opportunity to engage in the process of evaluating whether they would like to engage. Choosing our preferred contraceptive method is often done through prolonged education of the options and the support of professionals who we sometimes see in a face-to-face setting. If we were to make an informed and considered decision about our digital contraception, what would that look like practically? How would you categorise and weigh up the risks and the benefits of data transactions in such a way that there was adequate protection while maintaining pleasure and convenience of online life? How could you make rules of engagement that catered to a variety of perspectives? How could you inform the person what the implications of each of the options were? Perhaps in the same way that we go to a professional to seek advice and dialogue to ensure an informed decision, we would need to seek face to face advice from a data ethics professional who can walk us through the alternatives. Timing is critical i.e. these issues should be dealt with in a calm moment with time to reflect, not in the moment you want to buy a train ticket or are experiencing a medical emergency. If we were to choose our contraception in the same way that we accept terms and conditions online there would likely be an increase in the birth rate as well as the rate of transmission of sexually transmitted diseases. Making decisions in an environment that is not pressurized and allows for careful consideration and deliberation gives room to the individual to consider the options. What would it look like if we made decisions about our data sharing options well in advance to the action? The permission needs to be given in a format which is explicit, not implied. Just because you chose an application to chat with your partner does not mean that this app needs access to your entire list of contacts. The button which you click to give permission should not be designed in such a way that the automatic behaviour is opting in. For example, in binary choices if one button is smaller than the other, or if one button is hidden in the design and the other jumps out at you, or if one button requires multiple clicks whereas the other is a single click. While a person could give consent on a general topic to be continuous, it should always be possible to retract that permission for future transactions. Circumstances and opinions can change. As it is possible to adapt your contraception, it should be possible to adapt your data sharing preferences. However, there are some irreversible consequences of certain routes and these should be clearly explained. Similarly, to consent for sexual activity, retraction of past consent for data transactions is not feasible. For example, it would be possible for an individual to give consent to use their personal data for any cause advancing the treatment of cardiovascular disease until further notice. Until the human changes their mind, these transactions can continue to occur seamlessly without the involvement of the human. Additionally, there needs to be protocols for when there are undesirable side effects to mitigate the damage. Who should decide which digital contraceptive is used by who? Can data, that reveals such intimate details about our personalities and preferences until it is essentially become an extension of ourselves be considered company property? Who is the rightful controller of data? The collector of the data, who has invested resources into the collection and storage of that data? Or the person who that data describes? If a person starts to use a marketing tool when under the age of consent with the understanding that is merely an address book, is this consent valid? Summarising, personal data is an extension of self. Self-determination requires the ability to be able to control one\u2019s digital identity. Data control requires the development of digital contraception, i.e. techniques that classify data sharing options in such a way that an individual can conveniently weigh up the risks and benefits of the transaction.","title":"Digital Contraception"},{"location":"rwot8/topics-and-advance-readings/digital-contraception/#digital-contraception","text":"By Mitzi L\u00e1szl\u00f3 email@mitzilaszlo.org \u201cPersonal Data means any information relating to an identified or identifiable natural person (\u2018data subject\u2019); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person;\u201d - General Data Protection Regulation Article 4 (1) The collective of one individual's personal data forms a digital identity (or perhaps digital alter ego is more fitting). A digital identity encompasses all of our personal data shadowing, representing and connected to our physical and ideological self. Data points are linked in meaning. For example, the genetic profile of one person can give insight into the genetic profile of family members. Data points can lead to deductions that result in other data points. When does one digital identity start and another digital identity stop? At what point does data become public as a result of describing a crowd? If one person records their observations on another person who owns those observations? The observer or the observed? What responsibilities do the observer and the observed have in relation to each other? Since the massive scale and systematisation of observation of people and their thoughts as a result of the Web, these questions are increasingly important to address. The question of personal data falls into an unknown territory in between corporate ownership, intellectual property, and slavery. Ownership involves determining rights and duties over property. While the Web is not owned by anyone, corporations have come to collect, store, and control the personal data, creating value making use of data collection, search engines and communication tools. By default, as a side effect to owning the intellectual property making up the online tools, these corporations have been collecting our digital identities as raw material for the services delivered to other companies at a profit. Data can be replicated making it somewhat like intellectual property. Multiple individuals can hold a copy of a digital identity. The replicability of data makes the question of control complex. Ownership implies exclusivity, particularly with abstract concepts like ideas or data points. It is not enough to simply have a copy of your own data. Others should be restricted in their access to what is yours. Knowing what data others keep is a near impossible task. The simpler approach would be to cloak yourself in nonsense. To ensure that corporations or institutions do not have a copy of your data it is possible to send noise to confuse the data that they have. For example, a robot could randomly search terms that you would not be inclined to usually search for making that data obtained by the search engine useless through confusion. Can one person legitimately control the digital identity of another person? Slavery, the ownership of a person, is outlawed in all recognised countries. The institution of marriage originates from the need to grant property rights. The sexual and reproductive identity of the woman was controlled by the father and passed to the husband. Today, our digital identities are controlled by others, a situation of which the legitimacy is being increasingly questioned. The ability for each and every person to self-determine is a fairly novel concept. If we go from the starting point that individuals should be in control of their digital identities, use of that data by another person, company or institution, requires the explicit permission of the owner. Much like when choosing a contraceptive method, deciding our data sharing preferences is not done in isolation, there are other factors to weigh up and we end up making a risk benefit analysis. What do effective contraception techniques teach us about best practices for effective data sharing preferences? As with sexual consent, it is questionably ethical is permission for the data transaction to go ahead is used as a bargaining chip for an unrelated or superfluous issue of consent, for example, improve marketing recommendations while you are trying to ring your mother. While there are services where you need to share data, these transactions should not be exaggerated and should be held within context. For example, an individual needs to share data to receive adequate medical recommendations, however, that medical data does not automatically need to go to a health insurance provider. These are separate data transactions which should be dealt with as such. How far could we push the free services in exchange for control of your data? Imagine if you went to a restaurant and the food was free on the condition that a stranger sat with you and shouted advertisement. Imagine if there was a bar with free drinks on the condition that the bartender could sell your conversations. For consent to be meaningful the full scope and extent of the transaction needs to be explicitly detailed to the individual who has to be given apt opportunity to engage in the process of evaluating whether they would like to engage. Choosing our preferred contraceptive method is often done through prolonged education of the options and the support of professionals who we sometimes see in a face-to-face setting. If we were to make an informed and considered decision about our digital contraception, what would that look like practically? How would you categorise and weigh up the risks and the benefits of data transactions in such a way that there was adequate protection while maintaining pleasure and convenience of online life? How could you make rules of engagement that catered to a variety of perspectives? How could you inform the person what the implications of each of the options were? Perhaps in the same way that we go to a professional to seek advice and dialogue to ensure an informed decision, we would need to seek face to face advice from a data ethics professional who can walk us through the alternatives. Timing is critical i.e. these issues should be dealt with in a calm moment with time to reflect, not in the moment you want to buy a train ticket or are experiencing a medical emergency. If we were to choose our contraception in the same way that we accept terms and conditions online there would likely be an increase in the birth rate as well as the rate of transmission of sexually transmitted diseases. Making decisions in an environment that is not pressurized and allows for careful consideration and deliberation gives room to the individual to consider the options. What would it look like if we made decisions about our data sharing options well in advance to the action? The permission needs to be given in a format which is explicit, not implied. Just because you chose an application to chat with your partner does not mean that this app needs access to your entire list of contacts. The button which you click to give permission should not be designed in such a way that the automatic behaviour is opting in. For example, in binary choices if one button is smaller than the other, or if one button is hidden in the design and the other jumps out at you, or if one button requires multiple clicks whereas the other is a single click. While a person could give consent on a general topic to be continuous, it should always be possible to retract that permission for future transactions. Circumstances and opinions can change. As it is possible to adapt your contraception, it should be possible to adapt your data sharing preferences. However, there are some irreversible consequences of certain routes and these should be clearly explained. Similarly, to consent for sexual activity, retraction of past consent for data transactions is not feasible. For example, it would be possible for an individual to give consent to use their personal data for any cause advancing the treatment of cardiovascular disease until further notice. Until the human changes their mind, these transactions can continue to occur seamlessly without the involvement of the human. Additionally, there needs to be protocols for when there are undesirable side effects to mitigate the damage. Who should decide which digital contraceptive is used by who? Can data, that reveals such intimate details about our personalities and preferences until it is essentially become an extension of ourselves be considered company property? Who is the rightful controller of data? The collector of the data, who has invested resources into the collection and storage of that data? Or the person who that data describes? If a person starts to use a marketing tool when under the age of consent with the understanding that is merely an address book, is this consent valid? Summarising, personal data is an extension of self. Self-determination requires the ability to be able to control one\u2019s digital identity. Data control requires the development of digital contraception, i.e. techniques that classify data sharing options in such a way that an individual can conveniently weigh up the risks and benefits of the transaction.","title":"Digital Contraception"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/","text":"Assist underrepresented groups when entering the labour market through self sovereign identity Authors Karolin Siebert Abstract One issue within various branches is a lack of diversity, diversity meaning: seeing people irrespective of gender, race, religion, culture or sexual orientation. How can digital decentralised identities help? Some points that will be covered: allowing job applicants to approach new positions by incorporating digital identities in the application and hiring process giving applicants control over the components of their identity that they wanna provide Background Barcelona, Spain, Diversity in Tech Problem Statement As a consequence of increasing migration we have workers educated by a wild mix of educational systems, this should not be limiting them from approaching and receiving positions that they actually would like to work in and would be suitable for. Official diplomas and certificates, proving previous studies and work experience, are not necessarily recognised, available in physical form or easily translatable in a certified manner. In addition to that and estimate of the World Bank states that 1 Billion of people are without a formal identity. Being assigned one would give them better access to the work market since just their physical presence and knowledge is not enough in many cases, but rather we require proof for education and experience. Why Decentralise or Web of Trust? By having more control over their identities underrepresented groups could be perceived more independently of what really matters to work, which is the quality of your work and your experience and education. Using verifiable claims references to degrees completed in universities and institutions all over the world can be presented and are directly verified. Own digitally verified documents also protects against loss of academic records. What existing solutions do we have? SelfKey allows to store documents in a digital wallet which are also certified by and identity owner, ( i.e. a notary ) this also reduces the bureaucratic workload and removes the need to make long trips around the globe to figure out paperwork, once a document is certified digitally it is always available. Having references to certified documents easily available protects against loss of academic records. What can be improved? Make SSIDs available and accepted in the context of the work market and further develop it to make it a valid part of a job application and hiring process. Follow through to find a concept using APIs specifically designed to give access to employers to educational data that has been shared by an applicant, because it is conducive to make a decision about the candidate qualification only based on meaningful information. Final Point A point to be discussed could be that to create more diverse teams we need to change our mindset rather than adjusting information we share, the author of the document indicates that our technologies need to represent the notion of our mindsets to put our beliefs into practise. References: https://www.worldbank.org/en/news/press-release/2017/10/12/11-billion-invisible-people-without-id-are-priority-for-new-high-level-advisory-council-on-identification-for-development https://www.w3.org/TR/verifiable-claims-use-cases/#education [https://migrationdataportal.org/?i=stock_abs_&t=2017] (https://migrationdataportal.org/?i=stock_abs_&t=2017) Contact: karolin@colony.io","title":"Assist underrepresented groups when entering the labour market through self sovereign identity"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#assist-underrepresented-groups-when-entering-the-labour-market-through-self-sovereign-identity","text":"","title":"Assist underrepresented groups when entering the labour market through self sovereign identity"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#authors","text":"Karolin Siebert","title":"Authors"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#abstract","text":"One issue within various branches is a lack of diversity, diversity meaning: seeing people irrespective of gender, race, religion, culture or sexual orientation. How can digital decentralised identities help? Some points that will be covered: allowing job applicants to approach new positions by incorporating digital identities in the application and hiring process giving applicants control over the components of their identity that they wanna provide","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#background","text":"Barcelona, Spain, Diversity in Tech","title":"Background"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#problem-statement","text":"As a consequence of increasing migration we have workers educated by a wild mix of educational systems, this should not be limiting them from approaching and receiving positions that they actually would like to work in and would be suitable for. Official diplomas and certificates, proving previous studies and work experience, are not necessarily recognised, available in physical form or easily translatable in a certified manner. In addition to that and estimate of the World Bank states that 1 Billion of people are without a formal identity. Being assigned one would give them better access to the work market since just their physical presence and knowledge is not enough in many cases, but rather we require proof for education and experience.","title":"Problem Statement"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#why-decentralise-or-web-of-trust","text":"By having more control over their identities underrepresented groups could be perceived more independently of what really matters to work, which is the quality of your work and your experience and education. Using verifiable claims references to degrees completed in universities and institutions all over the world can be presented and are directly verified. Own digitally verified documents also protects against loss of academic records.","title":"Why Decentralise or Web of Trust?"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#what-existing-solutions-do-we-have","text":"SelfKey allows to store documents in a digital wallet which are also certified by and identity owner, ( i.e. a notary ) this also reduces the bureaucratic workload and removes the need to make long trips around the globe to figure out paperwork, once a document is certified digitally it is always available. Having references to certified documents easily available protects against loss of academic records.","title":"What existing solutions do we have?"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#what-can-be-improved","text":"Make SSIDs available and accepted in the context of the work market and further develop it to make it a valid part of a job application and hiring process. Follow through to find a concept using APIs specifically designed to give access to employers to educational data that has been shared by an applicant, because it is conducive to make a decision about the candidate qualification only based on meaningful information.","title":"What can be improved?"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#final-point","text":"A point to be discussed could be that to create more diverse teams we need to change our mindset rather than adjusting information we share, the author of the document indicates that our technologies need to represent the notion of our mindsets to put our beliefs into practise.","title":"Final Point"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#references","text":"https://www.worldbank.org/en/news/press-release/2017/10/12/11-billion-invisible-people-without-id-are-priority-for-new-high-level-advisory-council-on-identification-for-development https://www.w3.org/TR/verifiable-claims-use-cases/#education [https://migrationdataportal.org/?i=stock_abs_&t=2017] (https://migrationdataportal.org/?i=stock_abs_&t=2017)","title":"References:"},{"location":"rwot8/topics-and-advance-readings/diverse-teams/#contact","text":"karolin@colony.io","title":"Contact:"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/","text":"Designing a Recipient-Centric Educational Digital Credential Ecosystem Draft: Rebooting Web of Trust VIII Topic Paper by Kim Hamilton Duffy, J. Philipp Schmidt, Joe Andrieu Abstract This paper originated from an exercise to design a digital credentialing ecosystem for universities (as an original target implementation) from first principles -- without assuming specific technology choices or standards. Consistent with the goals of self-sovereign identity (SSI), we wanted this system to be recipient-centric. This included enhanced privacy measures, minimizing issuer-dependencies in use of the credentials, and ease of use. To do this, we outlined the minimal set of requirements, as well as simplifying assumptions for the target deployment. One goal of this exercise was to understand whether (1) certain emerging SSI standards -- including Verifiable Credentials (VCs), Decentralized Identifiers (DIDs) -- and (2) blockchain anchoring are essential to the simplified digital verifiable credentialing system. If so, we wanted to trace them to specific requirements. An additional goal was defining a threat model, to understand which aspects could be handled by the system, and which must be addressed by other systems. In the end, the design we produced did incorporate VCs and DIDs: VCs because of the lightweight framework and interoperability they enable, and DIDs for providing a flexible auth framework. Details about this decision are included in this paper. This exercise highlighted that one of the threats in our model -- that of detecting a malicious party within the issuer organization -- forced specific implementation choices, some of which could be mitigated through trusted timestamping and blockchain technologies. The threat could also be mitigated/detected outside this minimal system -- for example through a SaaS product providing additional audit logs -- but that system must similarly take measures to prevent or detect malicious actors/agents (even outside the issuing organization). Our concern is that an effective digital credentialing ecosystem would make fraud within an issuing organization more appealing, so our final goal of this paper is to raise awareness among the broader SSI community. Note on terminology: we use the term \"recipient\" below as that's widely used in this domain. It means holder/subject. Requirements The credentialing system has the following minimal set of requirements: An issuance should require the consent of both the issuer and recipient A recipient should be able to prove the credential was issued by the issuer without requiring interaction with the issuer The system should minimize the risk of recipient correlation, for example: Avoid subject (recipient) identifier (DID) reuse Incorporate nonces/recipient signatures (to avoid matching on hashed or signed values) The recipient should be involved in the verification process From Verifiable Credential Data Model: \" Holders are positioned between issuers and verifiers and use verifiable credentials to make verifiable presentations to verifiers .\" A side effect is that the recipient can not be forced to present a revoked credential The issuer should be able to revoke the credential It's TBD wether recipient revocation is a requirement. As described above, the Verifiable Credentials interaction model puts the recipient in the verification process, so the recipient can't be forced to present a credential they disavow Anti-correlation and other privacy measures help ensure they can't be linked to the credential But we are concerned this may assume the existence of recipient agent infrastructure TBD whether encryption is essential, but it will be desired in many cases Once revoked, a credential must not return as \"verified\" by a confirming verification process. However, the system must enable proof that the credential was once valid one for some time period Revoking and changing signing keys (issuer or recipient) should not invalidate previously-issued valid credentials Revocation check should not reveal any personally identifiable information (or any additional information about that credential or batch) Beyond this minimal list of functional requirements, we plan to update this with policy-level considerations -- such as environmental sustainability. We expect to extend/refine this list of requirements, but our guiding heuristic is the solution must be \"better than paper\". Specifically, in the process of improving efficiency, portability, and interoperability, we must not threaten privacy and control of the individual in the process. In fact; ideally privacy measures and control of the individual will strictly improve. This goal requires more elaboration in the list of requirements. Designing the system from first principles Use cases Issuer issues educational credentials (degrees, microcredentials, transcripts) to students. Organizations within the system and outside (most critically) want to be able to verify the recipient's credentials without directly consulting the issuing institution. This is relevant, for example, when the recipient wants to apply for a job, graduate school, etc. Consumers/Relying parties not only want to verify the credential; they want to authenticate the recipient corresponds to the subject of the credential. Simplifying Assumptions Let's make the following simplifying initial assumptions (later we'll address moving to more complex scenarios -- or even whether these assumptions are needed) for the digital credentialing system: Limited set of institutions initially Those institutions are trusted to maintain a registry that contains a mapping of institution names to key material Verifiers/Consumers/Relying parties trust the registry and know to consult it when presented with the credential Derived requirements To enable verification that does not involve the issuer, the registry must be highly available and allow lookup of valid issuer keys for a given time range. Issuers want to permit different auth methods, therefore the registry must tell relying parties how to authenticate recipients/subjects (i.e. confirm they are the recipient of the credential). Why Verifiable Credentials We wanted the credentials to have the privacy/security requirements listed in the first section. In addition, we wanted the content to be flexible and determined by the issuer. We wanted these to have a common interoperable structure that was lightweight. This discussion was brief; we simply found no better alternative to Verifiable Credentials. Why DIDs The primary advantage of DIDs (for issuers and recipients) was to maintain flexibility for issuers' different identity providers and auth mechanisms. Among issuers, there may be a preference for existing system like EDUROAM or Touchpoint, but: These are not guaranteed to be usable by the student after graduation Some may prefer to use X.509 associated with domains Using DIDs enables a variety of solutions, and SSI approaches for users who choose that route. Threat model Must handle The system should handle the following threats: A recipient lies about the content of a credential they received A recipient lies about the existence of a (not existing) credential A recipient tampers with the display of the credential they received Unknown Ideally the system should handle these threats, but this forces certain implementation choices. These will be elaborated on in the \"Analysis\" section: Issuer cannot lie about date of issue Malicious actors inside valid issuer institution issues credentials that should not exist Issuance Date To clarify, the date of issue this may differ from the date of award. The issuing institution must be able to backdate awards recieved in the past. The problem here is whether the issuance date timestamp stated by the issuer in the credential can be trusted. Trusted timestamping can be used to support issuer claims of issuance date (or replace). This can be accomplished by a range of methods including TSAs, blockchain anchoring, and Open Timestamps. At the same time, the current VC threat model does not call out this threat, which is why we raise this here. Fraudulent issuance detection This threat involves a bad actor in the issuing institution, who is able to sign with the issuing address, that is issuing fraudulent credentials to friends/collaborators. Does a verification process that cuts out the issuer increase this risk? One benefit of blockchain-anchoring is that one can alert on issuer addresses to detect suspicious issuances. However, if batch issuance is used, (and the issuer is not involved in verification), it may not be possible to detect fraudulent entries without additional metadata or an audit trail of the batch recipients (e.g. if the issuer records who is expected in the batch, and can deterministically reproduce the batch, they can verify that the merkle root matches what's recorded on chain). Which level of the credential issuance solution should handle this threat? And do VC threat models call proper attention to this risk? While this can be solved on a case-by-case basis, we think this problem needs broader awareness. Out of scope The system does not prevent illegitimate issuing organizations, e.g. \"Fake University of North South Idaho\". Other existing mechanisms exist to police them, e.g. accreditation bodies, federal funding guidelines, etc... The system cannot determine whether a backdated credential is valid (e.g. it must allow generation of proof for an individual that graduated 5 years ago) The system cannot determine exact time of issuance; it can only determine the (approx) time of issuance via \"trusted\" timestamps (blockchain anchoring and open timestamps) In-scope threats, by actor Issuer Malicious issuer may revoke a credential, but observers can see it was once valid Malicious issuer may fraudulently issue a credential, but the issuance should be detectable (auditable) Recipient Malicious recipient may obtain a fake credential, but the issuance should be detectable (auditable) Analysis and Next Steps The biggest concern this exercise revealed is that of detecting a malicious party in the issuing organization. It's straightforward to mitigate/detect this problem as part of a broader system deployment, e.g. audit logs in a SaaS product. But that system must similarly take measures to prevent or detect malicious actors/agents. So this is just a special case of a broader set of fraudulent issuance concerns. Solutions may involve a range of tactics in a deployed system, including: - Audit logs - Address monitoring (if blockchain-anchored) - Improved credential provenance measures (described below) To be clear, many deployed credentialing systems do provide such auditing measures, to at least detect fraud and revoke suspect credentials. But are we addressing this in similar ways? What advice and best practices can we pass on to the VC community? On review of these concerns, Joe Andrieu pointed out that the best (and perhaps only) approach is going to be better provenance of the certificate, which leads to \"turtles all the way down\" but which provides the best route to a superiority of information about the credential. This was not covered in our system model, and deserves a closer look. \\ In this model, the credential would not just say \"created and signed by MIT\", but probably also the human actor that triggered the credential creation and signature, their authority to do so on MIT's behalf, and potentially even the system on which it occurred and the version of software used, etc. There's a delegation happening here, which is an issue similar to why bug reports need to know the user's OS, browser, and versions of both. Unless you characterize the system used to generate the credential, you only have the trust in key management of the issuing entity for evaluating the quality of the credential. \\ \\ It's fairly banal attack for a trojan horse that wraps itself around the signing app to get inside the signing machine and generate false signatures. The user unwittingly uses the trojan horse instead of the real application, including the authentication into the private keys. This is why some of the IoT credential assurance procedures require using a machine that has never been and never will be on the network--only an isolated device meets the standard. \\ \\ The current set of requirements outlined in this paper do not address this kind of provenance, extending to the need to check the status of such delegation relative to the time of issuance. The latter originates from the system requirement is to allow an institution to revoke a person's ability to generate valid credentials without revoking credentials properly issued by that same person. \\ \\ The added provenance also allows verifiers to spot known compromised software packages and request the holder get a fresh credential. \\ We'd like to get community feedback on this topic, so we can share detection/mitigation approaches, and improve our guidance and best practices around credentialing ecosystems. References Verifiable Credentials Use Case and Requirements: https://w3c.github.io/vc-data-model/#use-cases-and-requirements Blockcerts Roadmap: https://github.com/WebOfTrustInfo/rwot7/blob/master/topics-and-advance-readings/blockcerts_roadmap.md VC Data Model: https://w3c.github.io/vc-data-model/ VC Data Model Explainer: https://github.com/w3c/vc-data-model/blob/gh-pages/VCDMExplainer.md DID Primer: https://github.com/WebOfTrustInfo/rwot5-boston/blob/master/topics-and-advance-readings/did-primer.md","title":"Designing a Recipient-Centric Educational Digital Credential Ecosystem"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#designing-a-recipient-centric-educational-digital-credential-ecosystem","text":"","title":"Designing a Recipient-Centric Educational Digital Credential Ecosystem"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#draft-rebooting-web-of-trust-viii-topic-paper","text":"by Kim Hamilton Duffy, J. Philipp Schmidt, Joe Andrieu","title":"Draft: Rebooting Web of Trust VIII Topic Paper"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#abstract","text":"This paper originated from an exercise to design a digital credentialing ecosystem for universities (as an original target implementation) from first principles -- without assuming specific technology choices or standards. Consistent with the goals of self-sovereign identity (SSI), we wanted this system to be recipient-centric. This included enhanced privacy measures, minimizing issuer-dependencies in use of the credentials, and ease of use. To do this, we outlined the minimal set of requirements, as well as simplifying assumptions for the target deployment. One goal of this exercise was to understand whether (1) certain emerging SSI standards -- including Verifiable Credentials (VCs), Decentralized Identifiers (DIDs) -- and (2) blockchain anchoring are essential to the simplified digital verifiable credentialing system. If so, we wanted to trace them to specific requirements. An additional goal was defining a threat model, to understand which aspects could be handled by the system, and which must be addressed by other systems. In the end, the design we produced did incorporate VCs and DIDs: VCs because of the lightweight framework and interoperability they enable, and DIDs for providing a flexible auth framework. Details about this decision are included in this paper. This exercise highlighted that one of the threats in our model -- that of detecting a malicious party within the issuer organization -- forced specific implementation choices, some of which could be mitigated through trusted timestamping and blockchain technologies. The threat could also be mitigated/detected outside this minimal system -- for example through a SaaS product providing additional audit logs -- but that system must similarly take measures to prevent or detect malicious actors/agents (even outside the issuing organization). Our concern is that an effective digital credentialing ecosystem would make fraud within an issuing organization more appealing, so our final goal of this paper is to raise awareness among the broader SSI community. Note on terminology: we use the term \"recipient\" below as that's widely used in this domain. It means holder/subject.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#requirements","text":"The credentialing system has the following minimal set of requirements: An issuance should require the consent of both the issuer and recipient A recipient should be able to prove the credential was issued by the issuer without requiring interaction with the issuer The system should minimize the risk of recipient correlation, for example: Avoid subject (recipient) identifier (DID) reuse Incorporate nonces/recipient signatures (to avoid matching on hashed or signed values) The recipient should be involved in the verification process From Verifiable Credential Data Model: \" Holders are positioned between issuers and verifiers and use verifiable credentials to make verifiable presentations to verifiers .\" A side effect is that the recipient can not be forced to present a revoked credential The issuer should be able to revoke the credential It's TBD wether recipient revocation is a requirement. As described above, the Verifiable Credentials interaction model puts the recipient in the verification process, so the recipient can't be forced to present a credential they disavow Anti-correlation and other privacy measures help ensure they can't be linked to the credential But we are concerned this may assume the existence of recipient agent infrastructure TBD whether encryption is essential, but it will be desired in many cases Once revoked, a credential must not return as \"verified\" by a confirming verification process. However, the system must enable proof that the credential was once valid one for some time period Revoking and changing signing keys (issuer or recipient) should not invalidate previously-issued valid credentials Revocation check should not reveal any personally identifiable information (or any additional information about that credential or batch) Beyond this minimal list of functional requirements, we plan to update this with policy-level considerations -- such as environmental sustainability. We expect to extend/refine this list of requirements, but our guiding heuristic is the solution must be \"better than paper\". Specifically, in the process of improving efficiency, portability, and interoperability, we must not threaten privacy and control of the individual in the process. In fact; ideally privacy measures and control of the individual will strictly improve. This goal requires more elaboration in the list of requirements.","title":"Requirements"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#designing-the-system-from-first-principles","text":"","title":"Designing the system from first principles"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#use-cases","text":"Issuer issues educational credentials (degrees, microcredentials, transcripts) to students. Organizations within the system and outside (most critically) want to be able to verify the recipient's credentials without directly consulting the issuing institution. This is relevant, for example, when the recipient wants to apply for a job, graduate school, etc. Consumers/Relying parties not only want to verify the credential; they want to authenticate the recipient corresponds to the subject of the credential.","title":"Use cases"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#simplifying-assumptions","text":"Let's make the following simplifying initial assumptions (later we'll address moving to more complex scenarios -- or even whether these assumptions are needed) for the digital credentialing system: Limited set of institutions initially Those institutions are trusted to maintain a registry that contains a mapping of institution names to key material Verifiers/Consumers/Relying parties trust the registry and know to consult it when presented with the credential","title":"Simplifying Assumptions"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#derived-requirements","text":"To enable verification that does not involve the issuer, the registry must be highly available and allow lookup of valid issuer keys for a given time range. Issuers want to permit different auth methods, therefore the registry must tell relying parties how to authenticate recipients/subjects (i.e. confirm they are the recipient of the credential).","title":"Derived requirements"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#why-verifiable-credentials","text":"We wanted the credentials to have the privacy/security requirements listed in the first section. In addition, we wanted the content to be flexible and determined by the issuer. We wanted these to have a common interoperable structure that was lightweight. This discussion was brief; we simply found no better alternative to Verifiable Credentials.","title":"Why Verifiable Credentials"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#why-dids","text":"The primary advantage of DIDs (for issuers and recipients) was to maintain flexibility for issuers' different identity providers and auth mechanisms. Among issuers, there may be a preference for existing system like EDUROAM or Touchpoint, but: These are not guaranteed to be usable by the student after graduation Some may prefer to use X.509 associated with domains Using DIDs enables a variety of solutions, and SSI approaches for users who choose that route.","title":"Why DIDs"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#threat-model","text":"","title":"Threat model"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#must-handle","text":"The system should handle the following threats: A recipient lies about the content of a credential they received A recipient lies about the existence of a (not existing) credential A recipient tampers with the display of the credential they received","title":"Must handle"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#unknown","text":"Ideally the system should handle these threats, but this forces certain implementation choices. These will be elaborated on in the \"Analysis\" section: Issuer cannot lie about date of issue Malicious actors inside valid issuer institution issues credentials that should not exist","title":"Unknown"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#issuance-date","text":"To clarify, the date of issue this may differ from the date of award. The issuing institution must be able to backdate awards recieved in the past. The problem here is whether the issuance date timestamp stated by the issuer in the credential can be trusted. Trusted timestamping can be used to support issuer claims of issuance date (or replace). This can be accomplished by a range of methods including TSAs, blockchain anchoring, and Open Timestamps. At the same time, the current VC threat model does not call out this threat, which is why we raise this here.","title":"Issuance Date"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#fraudulent-issuance-detection","text":"This threat involves a bad actor in the issuing institution, who is able to sign with the issuing address, that is issuing fraudulent credentials to friends/collaborators. Does a verification process that cuts out the issuer increase this risk? One benefit of blockchain-anchoring is that one can alert on issuer addresses to detect suspicious issuances. However, if batch issuance is used, (and the issuer is not involved in verification), it may not be possible to detect fraudulent entries without additional metadata or an audit trail of the batch recipients (e.g. if the issuer records who is expected in the batch, and can deterministically reproduce the batch, they can verify that the merkle root matches what's recorded on chain). Which level of the credential issuance solution should handle this threat? And do VC threat models call proper attention to this risk? While this can be solved on a case-by-case basis, we think this problem needs broader awareness.","title":"Fraudulent issuance detection"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#out-of-scope","text":"The system does not prevent illegitimate issuing organizations, e.g. \"Fake University of North South Idaho\". Other existing mechanisms exist to police them, e.g. accreditation bodies, federal funding guidelines, etc... The system cannot determine whether a backdated credential is valid (e.g. it must allow generation of proof for an individual that graduated 5 years ago) The system cannot determine exact time of issuance; it can only determine the (approx) time of issuance via \"trusted\" timestamps (blockchain anchoring and open timestamps)","title":"Out of scope"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#in-scope-threats-by-actor","text":"Issuer Malicious issuer may revoke a credential, but observers can see it was once valid Malicious issuer may fraudulently issue a credential, but the issuance should be detectable (auditable) Recipient Malicious recipient may obtain a fake credential, but the issuance should be detectable (auditable)","title":"In-scope threats, by actor"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#analysis-and-next-steps","text":"The biggest concern this exercise revealed is that of detecting a malicious party in the issuing organization. It's straightforward to mitigate/detect this problem as part of a broader system deployment, e.g. audit logs in a SaaS product. But that system must similarly take measures to prevent or detect malicious actors/agents. So this is just a special case of a broader set of fraudulent issuance concerns. Solutions may involve a range of tactics in a deployed system, including: - Audit logs - Address monitoring (if blockchain-anchored) - Improved credential provenance measures (described below) To be clear, many deployed credentialing systems do provide such auditing measures, to at least detect fraud and revoke suspect credentials. But are we addressing this in similar ways? What advice and best practices can we pass on to the VC community? On review of these concerns, Joe Andrieu pointed out that the best (and perhaps only) approach is going to be better provenance of the certificate, which leads to \"turtles all the way down\" but which provides the best route to a superiority of information about the credential. This was not covered in our system model, and deserves a closer look. \\ In this model, the credential would not just say \"created and signed by MIT\", but probably also the human actor that triggered the credential creation and signature, their authority to do so on MIT's behalf, and potentially even the system on which it occurred and the version of software used, etc. There's a delegation happening here, which is an issue similar to why bug reports need to know the user's OS, browser, and versions of both. Unless you characterize the system used to generate the credential, you only have the trust in key management of the issuing entity for evaluating the quality of the credential. \\ \\ It's fairly banal attack for a trojan horse that wraps itself around the signing app to get inside the signing machine and generate false signatures. The user unwittingly uses the trojan horse instead of the real application, including the authentication into the private keys. This is why some of the IoT credential assurance procedures require using a machine that has never been and never will be on the network--only an isolated device meets the standard. \\ \\ The current set of requirements outlined in this paper do not address this kind of provenance, extending to the need to check the status of such delegation relative to the time of issuance. The latter originates from the system requirement is to allow an institution to revoke a person's ability to generate valid credentials without revoking credentials properly issued by that same person. \\ \\ The added provenance also allows verifiers to spot known compromised software packages and request the holder get a fresh credential. \\ We'd like to get community feedback on this topic, so we can share detection/mitigation approaches, and improve our guidance and best practices around credentialing ecosystems.","title":"Analysis and Next Steps"},{"location":"rwot8/topics-and-advance-readings/educational-credentialing-ecosystem/#references","text":"Verifiable Credentials Use Case and Requirements: https://w3c.github.io/vc-data-model/#use-cases-and-requirements Blockcerts Roadmap: https://github.com/WebOfTrustInfo/rwot7/blob/master/topics-and-advance-readings/blockcerts_roadmap.md VC Data Model: https://w3c.github.io/vc-data-model/ VC Data Model Explainer: https://github.com/w3c/vc-data-model/blob/gh-pages/VCDMExplainer.md DID Primer: https://github.com/WebOfTrustInfo/rwot5-boston/blob/master/topics-and-advance-readings/did-primer.md","title":"References"},{"location":"rwot8/topics-and-advance-readings/exploring_adoption_of_sharing_vcs/","text":"Exploring adoptation of VC sharing to provide value today by Snorre Lothar von Gohren Edwin snorre@diwala.io with help from Adrian Gropper agropper@healthurl.com For the community to get faster adoption of the standards we are working on, it is important to explore the the possibilities that can work out of the box, which is are safe enough to be trustworthy and not leak any sensitive data. With Verifiable Credentials it is important to ask the question what are we expecting the issuer, the holder, and / or the verifier to do? Sharing is what provides the end value for both the verifying and the holding party. If there is no sharing, the issuance of the VC has gone to waste. Hence if we are to provide value for the sharing party today, we have to ask the questions of where is this adopted, how can we make this adoption effortless? Can we put most of the work on the service providers? Is the user or another service expected to install software? If so, install software where? Is that software domain-specific or integrated with some existing software? We have to explore methods of trustworthy sharing with all these aspects in mind and do it without too much intrusion on the verifier side? If so we can showcase the value of VC's in a real live scalable example, very quickly. Situation During their users tests and pilots in East Africa, Diwala experienced that the approach for sharing done by identity applications in the decentralized community is not always feasible in an East African situation. Many of the current sharing suggestions are based on software that has to be installed at every point of the process, from verifier, issuer and user. That the process is more pull based rather than push based. In this case it is expected of the verifier to have some sort of software installed to be able to pull the information from the sharing party, and not that the initial interaction can start from the sharing party, in one way or another. Early during the user tests it was agreed that the issuing party needed some kind of software, because they lacked any type of digital and efficient solution to be able to issue verifications to their students. They instantly saw the value a simple software could provide in the long run, and the value gained regarding efficiency and trustworthiness. During tests for the receiving party it soon became clear that an identity application/wallet, such as uPort or Jolocom, could not provide enough services and information around skill verifications and other metadata issued to the subject. So it was concluded that the subject needed an app to be able better to understand and make sense of the data/VCs issued. It was also concluded that the current sharing solution in the testd identity application/wallet, was not sufficient to give instant value. The extra app had to take care of the sharing based on the data that was available from the identity application/wallet. Yet another software installation. For the last party, the verifying party, we concluded that they could not be dependent on having a software installed if this was going to scale in any way. The only way to solve this was to depend on open web technologies to be able to create a simple site that the verifier could load independently. In East Africa, Uganda to be specific, we found that even installing a couple of apps on a phone was problematic without enough incentive or clarity of what value this would provide the user. That was mainly because of the lack of space on the phone. We also saw that the systems used, are not always as integrated as the western culture is used to, meaning it is quite difficult to figure out where in the pipeline a possible software installation would happen to generate the most value. A solution in experimentation What Diwala tries to do is to provide value for the sharing of VCs, given the current technological situation right now. Trying to figure out how to test out the value of VCs with the least possible friction. Diwala has taken the approach to sharing through simple URLs and JWTs. This part still needs a lot of work, but that is why I am raising the question for the community, to work together to build something that is as frictionless as possible, to be able to generate value here and now. How it works Diwala works with uPort as an implementation for the identity solution, signing and initial issuance to the receivers of the VCs. Diwala builds a lightweight certification solution for schools and course holders, that tries to remove the friction of decentralized identity onboarding, help creating VCs for skills and providing a view for the received VCs. This is done with a simple online web administration user interface for the issuing organization. The receiver has two apps, the uPort app for their identity, and the Diwala app for their view and sharing possibilities of the received VCs. Which we call their skill-identity. As mentioned earlier, uPort did not provide enough contextual services for these skill VCs, therefore this second app was created. The app allows for sharing through a simple URL, with the standard functionality for sharing on a phone. The VC, which is a JWT, coming from the uPort app/\u201dyour wallet\u201d is appended to the URL that is being shared. This URL points to a service currently hosted by Diwala, which starts a verification process of the provided VC or VCs. This service verifies the signatures through a did resolver, loads any data that might be provided through the service URLs of the DIDs, and tries to showcase this to the verifier in a way that is trustworthy. This is all done without any communication to the Diwala database, which means this can be converted to a standalone SDK(Software development kit) provided to other services. Questions concering this solution How do can you confirm the VC is coming from the person that sent it? How can the verifier trust the validity of the data and that the signatures are actually verified decentralized? Discussion There are things to discuss and there might not be that the mentioned solution is viable at all. But it is something Diwala saw as a possibility and are currently testing out the value gain of this approach. But in the end, the discussion is about adoption, how can we introduce this as frictionless as possible, and still keep the core values intact. There are a few problem areas to tackle when sharing Replay attacks This can be mitigated with for example uPort\u2019s challenge/response protocol, but then software is needed on all sides. If that protocol is not used, then you would need a an out-of-band method, e.g., ID card, that associates the VC with the individual Trust that it is coming from the person that shared it. This is also mitigated with the same protocol mentioned above, but with bigger intrusion than needed. But again all this comes down to that the signatures are coming from the person who say they are sharing. And that the sharing party owns the keys signed, are they on the users device? How much software needs to be installed? Who holds the software?","title":"Exploring adoptation of VC sharing to provide value today"},{"location":"rwot8/topics-and-advance-readings/exploring_adoption_of_sharing_vcs/#exploring-adoptation-of-vc-sharing-to-provide-value-today","text":"","title":"Exploring adoptation of VC sharing to provide value today"},{"location":"rwot8/topics-and-advance-readings/exploring_adoption_of_sharing_vcs/#by-snorre-lothar-von-gohren-edwin-snorrediwalaio-with-help-from-adrian-gropper-agropperhealthurlcom","text":"For the community to get faster adoption of the standards we are working on, it is important to explore the the possibilities that can work out of the box, which is are safe enough to be trustworthy and not leak any sensitive data. With Verifiable Credentials it is important to ask the question what are we expecting the issuer, the holder, and / or the verifier to do? Sharing is what provides the end value for both the verifying and the holding party. If there is no sharing, the issuance of the VC has gone to waste. Hence if we are to provide value for the sharing party today, we have to ask the questions of where is this adopted, how can we make this adoption effortless? Can we put most of the work on the service providers? Is the user or another service expected to install software? If so, install software where? Is that software domain-specific or integrated with some existing software? We have to explore methods of trustworthy sharing with all these aspects in mind and do it without too much intrusion on the verifier side? If so we can showcase the value of VC's in a real live scalable example, very quickly.","title":"by Snorre Lothar von Gohren Edwin snorre@diwala.io with help from Adrian Gropper agropper@healthurl.com"},{"location":"rwot8/topics-and-advance-readings/exploring_adoption_of_sharing_vcs/#situation","text":"During their users tests and pilots in East Africa, Diwala experienced that the approach for sharing done by identity applications in the decentralized community is not always feasible in an East African situation. Many of the current sharing suggestions are based on software that has to be installed at every point of the process, from verifier, issuer and user. That the process is more pull based rather than push based. In this case it is expected of the verifier to have some sort of software installed to be able to pull the information from the sharing party, and not that the initial interaction can start from the sharing party, in one way or another. Early during the user tests it was agreed that the issuing party needed some kind of software, because they lacked any type of digital and efficient solution to be able to issue verifications to their students. They instantly saw the value a simple software could provide in the long run, and the value gained regarding efficiency and trustworthiness. During tests for the receiving party it soon became clear that an identity application/wallet, such as uPort or Jolocom, could not provide enough services and information around skill verifications and other metadata issued to the subject. So it was concluded that the subject needed an app to be able better to understand and make sense of the data/VCs issued. It was also concluded that the current sharing solution in the testd identity application/wallet, was not sufficient to give instant value. The extra app had to take care of the sharing based on the data that was available from the identity application/wallet. Yet another software installation. For the last party, the verifying party, we concluded that they could not be dependent on having a software installed if this was going to scale in any way. The only way to solve this was to depend on open web technologies to be able to create a simple site that the verifier could load independently. In East Africa, Uganda to be specific, we found that even installing a couple of apps on a phone was problematic without enough incentive or clarity of what value this would provide the user. That was mainly because of the lack of space on the phone. We also saw that the systems used, are not always as integrated as the western culture is used to, meaning it is quite difficult to figure out where in the pipeline a possible software installation would happen to generate the most value.","title":"Situation"},{"location":"rwot8/topics-and-advance-readings/exploring_adoption_of_sharing_vcs/#a-solution-in-experimentation","text":"What Diwala tries to do is to provide value for the sharing of VCs, given the current technological situation right now. Trying to figure out how to test out the value of VCs with the least possible friction. Diwala has taken the approach to sharing through simple URLs and JWTs. This part still needs a lot of work, but that is why I am raising the question for the community, to work together to build something that is as frictionless as possible, to be able to generate value here and now.","title":"A solution in experimentation"},{"location":"rwot8/topics-and-advance-readings/exploring_adoption_of_sharing_vcs/#how-it-works","text":"Diwala works with uPort as an implementation for the identity solution, signing and initial issuance to the receivers of the VCs. Diwala builds a lightweight certification solution for schools and course holders, that tries to remove the friction of decentralized identity onboarding, help creating VCs for skills and providing a view for the received VCs. This is done with a simple online web administration user interface for the issuing organization. The receiver has two apps, the uPort app for their identity, and the Diwala app for their view and sharing possibilities of the received VCs. Which we call their skill-identity. As mentioned earlier, uPort did not provide enough contextual services for these skill VCs, therefore this second app was created. The app allows for sharing through a simple URL, with the standard functionality for sharing on a phone. The VC, which is a JWT, coming from the uPort app/\u201dyour wallet\u201d is appended to the URL that is being shared. This URL points to a service currently hosted by Diwala, which starts a verification process of the provided VC or VCs. This service verifies the signatures through a did resolver, loads any data that might be provided through the service URLs of the DIDs, and tries to showcase this to the verifier in a way that is trustworthy. This is all done without any communication to the Diwala database, which means this can be converted to a standalone SDK(Software development kit) provided to other services.","title":"How it works"},{"location":"rwot8/topics-and-advance-readings/exploring_adoption_of_sharing_vcs/#questions-concering-this-solution","text":"How do can you confirm the VC is coming from the person that sent it? How can the verifier trust the validity of the data and that the signatures are actually verified decentralized?","title":"Questions concering this solution"},{"location":"rwot8/topics-and-advance-readings/exploring_adoption_of_sharing_vcs/#discussion","text":"There are things to discuss and there might not be that the mentioned solution is viable at all. But it is something Diwala saw as a possibility and are currently testing out the value gain of this approach. But in the end, the discussion is about adoption, how can we introduce this as frictionless as possible, and still keep the core values intact.","title":"Discussion"},{"location":"rwot8/topics-and-advance-readings/exploring_adoption_of_sharing_vcs/#there-are-a-few-problem-areas-to-tackle-when-sharing","text":"Replay attacks This can be mitigated with for example uPort\u2019s challenge/response protocol, but then software is needed on all sides. If that protocol is not used, then you would need a an out-of-band method, e.g., ID card, that associates the VC with the individual Trust that it is coming from the person that shared it. This is also mitigated with the same protocol mentioned above, but with bigger intrusion than needed. But again all this comes down to that the signatures are coming from the person who say they are sharing. And that the sharing party owns the keys signed, are they on the users device? How much software needs to be installed? Who holds the software?","title":"There are a few problem areas to tackle when sharing"},{"location":"rwot8/topics-and-advance-readings/financing-self-sovereign-stack/","text":"Financing a Self-sovereign Technology Stack by Adrian Gropper Technology for Human Autonomy Rebooting the Web of Trust aims toward autonomy for individuals. In his seminal 2017 paper, The Path to Self-Sovereign Identity, https://github.com/ChristopherA/self-sovereign-identity/blob/master/ThePathToSelf-SovereignIdentity.md , Christopher Allen lists ten principles of self-sovereign identity. But to achieve autonomy, the individual must have agency and agency requires an ability to act beyond identity. A self-sovereign agent, able to act autonomously to the extent it\u2019s human owner allows, will have many of the same ten principles of self-sovereign identity. Characteristics such as Free software, standards for interoperability, portability, and censorship-resistance are a must for a self-sovereign agent. These same characteristics, however, limit the ability to finance work on a self-sovereign agent in established ways. Bitcoin, ethereum, and other public blockchains are an example of self-sovereign technology. Finance of public blockchain projects is based on issuance of the equivalent of founders\u2019 stock to the developers and the hope that many of them will maintain the technology in order to preserve and grow the value of their stake. Could such a finance mechanism be applied to develop self-sovereign agent technology? This is, to a large extent, the structure behind Ethereum dapps and ConsenSys. As long as Ethereum\u2019s early adopters could reach critical mass and governance stability, their finance of ConsenSys and similar support organizations could, in theory, be used to construct an autonomous agent for an individual. In practice, however an agent requires secrecy to keep an individual\u2019s policies away from public disclosure in a manner reminiscent of zero-knowledge proofs. Along with scalability and the need to sometimes operate without a connection to the network, Ethereum is not an ideal substrate for a self-sovereign agent and the public blockchain finance strategy is difficult to apply. A Standards-Based Self-Sovereign Technology Stack Starting with the second Rebooting Web of Trust, the HIE of One project has been designed and managed as a reference implementation of a self-sovereign technology stack. The agency concept is instantiated as an authorization server based on the User Managed Access (UMA) standard running as Free software on non-proprietary hardware owned by the individual. Client access to the authorization server is based on self-sovereign identity and related credentials of requesting parties. Autonomy is preserved because the authorization server hides their owner\u2019s policies just as a wallet hides the owner\u2019s private keys. An open source reference implementation of standards is nothing more than an open source project and not financeable in the normal sense of the term. How are we to create a supported, sustainable product for anyone to use? HIE of One, PBC was incorporated in 2014 as a Public Benefits Corporation to act as the governance and finance entity. The corporation registered Trustee as trademark for their version of the software and services. There is also a Trustee hardware option. We then set about to apply for grants, seek strategic and financial investors, and considered a token sale linked to the \u201cfat protocol\u201d that is the combination of UMA, OAuth, Decentralized IDentifier, and Verifiable Credentials standards combined into the self-sovereign technology stack branded Trustee. A technology, be it self-sovereign identity or self-sovereign agency, is not sufficient to get customers and have a business. Trustee must actually compete to provide a service that people want to buy and use in the real world. HIE of One chose health records for seriously ill people as the service. HIE stands for Health Information Exchange. Seriously ill or elderly patients interact with many different institutional systems and need to monitor and connect their interfaces in a practical way. The healthcare industry is in the process of adopting a standard for application programming interfaces (API) called FHIR. FHIR is a RESTful API protected by OAuth and therefore easily adapted to delegation of control to an UMA authorization server such as Trustee. The growing availability of health records via a standard API underlies the value proposition for Trustee as a commercial product. Financing Autonomy Aside from the blockchain pioneers, there\u2019s little experience with financing self-sovereign technology in any market segment and it\u2019s not even clear that the path they took in the first decade can be sustained or replicated. It\u2019s a bit like Zooko\u2019s Triangle: You can have two of: Software Freedom, Substitutability, or Decentralization as long as you finance on the basis of control in one of the three areas. But granting control to others compromises autonomy. Over about five years, as we continue to develop Trustee using the open source, non-commercial collaboration model, HIE of One has tried various approaches to financing the transition to commercial product. We tried: Government grants (both on our own and as subcontractors) Citizen coops (we help establish and participate in the Digital Life Collective) Physician coops (via established medical societies and start-up physician initiative) Global Healthcare NGOs (in US and UK, but Trustee is not \u201cresearch\u201d) Blockchain Tokens (so many issues) Blockchain investors (healthcare is sooo complicated) No luck so far. Bootstrapping Self-Sovereign Technology HIE of One is now trying something completely different. To the extent a Free software product can be developed by volunteers and brought to market without up-front tooling, the customers might finance the scale-up and commercialization themselves. This is similar to some public blockchains but the compensation of the developers and early adopters is not directly linked to the protocol itself. Users subscribe to the service with some degree of pre-payment and a promise of future discounts as the network grows. Can bootstrap financing support self-sovereign autonomy? The software is licensed under the Affero GPL. The service is substitutable to the extent the DID, VC, and UMA standards are adopted. The service and support are mostly decentralized with only governance of the Trustee brand and contracts related to it under the control of HIE of One. Business models for self-sovereign identity and self-sovereign technology are experimental. The standards are just starting to gel. The interest in systems that promote freedom of speech and assembly is growing. Business structures for sustainable decentralized and potentially autonomous systems are still to be invented.","title":"Financing a Self-sovereign Technology Stack"},{"location":"rwot8/topics-and-advance-readings/financing-self-sovereign-stack/#financing-a-self-sovereign-technology-stack","text":"","title":"Financing a Self-sovereign Technology Stack"},{"location":"rwot8/topics-and-advance-readings/financing-self-sovereign-stack/#by-adrian-gropper","text":"Technology for Human Autonomy Rebooting the Web of Trust aims toward autonomy for individuals. In his seminal 2017 paper, The Path to Self-Sovereign Identity, https://github.com/ChristopherA/self-sovereign-identity/blob/master/ThePathToSelf-SovereignIdentity.md , Christopher Allen lists ten principles of self-sovereign identity. But to achieve autonomy, the individual must have agency and agency requires an ability to act beyond identity. A self-sovereign agent, able to act autonomously to the extent it\u2019s human owner allows, will have many of the same ten principles of self-sovereign identity. Characteristics such as Free software, standards for interoperability, portability, and censorship-resistance are a must for a self-sovereign agent. These same characteristics, however, limit the ability to finance work on a self-sovereign agent in established ways. Bitcoin, ethereum, and other public blockchains are an example of self-sovereign technology. Finance of public blockchain projects is based on issuance of the equivalent of founders\u2019 stock to the developers and the hope that many of them will maintain the technology in order to preserve and grow the value of their stake. Could such a finance mechanism be applied to develop self-sovereign agent technology? This is, to a large extent, the structure behind Ethereum dapps and ConsenSys. As long as Ethereum\u2019s early adopters could reach critical mass and governance stability, their finance of ConsenSys and similar support organizations could, in theory, be used to construct an autonomous agent for an individual. In practice, however an agent requires secrecy to keep an individual\u2019s policies away from public disclosure in a manner reminiscent of zero-knowledge proofs. Along with scalability and the need to sometimes operate without a connection to the network, Ethereum is not an ideal substrate for a self-sovereign agent and the public blockchain finance strategy is difficult to apply. A Standards-Based Self-Sovereign Technology Stack Starting with the second Rebooting Web of Trust, the HIE of One project has been designed and managed as a reference implementation of a self-sovereign technology stack. The agency concept is instantiated as an authorization server based on the User Managed Access (UMA) standard running as Free software on non-proprietary hardware owned by the individual. Client access to the authorization server is based on self-sovereign identity and related credentials of requesting parties. Autonomy is preserved because the authorization server hides their owner\u2019s policies just as a wallet hides the owner\u2019s private keys. An open source reference implementation of standards is nothing more than an open source project and not financeable in the normal sense of the term. How are we to create a supported, sustainable product for anyone to use? HIE of One, PBC was incorporated in 2014 as a Public Benefits Corporation to act as the governance and finance entity. The corporation registered Trustee as trademark for their version of the software and services. There is also a Trustee hardware option. We then set about to apply for grants, seek strategic and financial investors, and considered a token sale linked to the \u201cfat protocol\u201d that is the combination of UMA, OAuth, Decentralized IDentifier, and Verifiable Credentials standards combined into the self-sovereign technology stack branded Trustee. A technology, be it self-sovereign identity or self-sovereign agency, is not sufficient to get customers and have a business. Trustee must actually compete to provide a service that people want to buy and use in the real world. HIE of One chose health records for seriously ill people as the service. HIE stands for Health Information Exchange. Seriously ill or elderly patients interact with many different institutional systems and need to monitor and connect their interfaces in a practical way. The healthcare industry is in the process of adopting a standard for application programming interfaces (API) called FHIR. FHIR is a RESTful API protected by OAuth and therefore easily adapted to delegation of control to an UMA authorization server such as Trustee. The growing availability of health records via a standard API underlies the value proposition for Trustee as a commercial product. Financing Autonomy Aside from the blockchain pioneers, there\u2019s little experience with financing self-sovereign technology in any market segment and it\u2019s not even clear that the path they took in the first decade can be sustained or replicated. It\u2019s a bit like Zooko\u2019s Triangle: You can have two of: Software Freedom, Substitutability, or Decentralization as long as you finance on the basis of control in one of the three areas. But granting control to others compromises autonomy. Over about five years, as we continue to develop Trustee using the open source, non-commercial collaboration model, HIE of One has tried various approaches to financing the transition to commercial product. We tried: Government grants (both on our own and as subcontractors) Citizen coops (we help establish and participate in the Digital Life Collective) Physician coops (via established medical societies and start-up physician initiative) Global Healthcare NGOs (in US and UK, but Trustee is not \u201cresearch\u201d) Blockchain Tokens (so many issues) Blockchain investors (healthcare is sooo complicated) No luck so far. Bootstrapping Self-Sovereign Technology HIE of One is now trying something completely different. To the extent a Free software product can be developed by volunteers and brought to market without up-front tooling, the customers might finance the scale-up and commercialization themselves. This is similar to some public blockchains but the compensation of the developers and early adopters is not directly linked to the protocol itself. Users subscribe to the service with some degree of pre-payment and a promise of future discounts as the network grows. Can bootstrap financing support self-sovereign autonomy? The software is licensed under the Affero GPL. The service is substitutable to the extent the DID, VC, and UMA standards are adopted. The service and support are mostly decentralized with only governance of the Trustee brand and contracts related to it under the control of HIE of One. Business models for self-sovereign identity and self-sovereign technology are experimental. The standards are just starting to gel. The interest in systems that promote freedom of speech and assembly is growing. Business structures for sustainable decentralized and potentially autonomous systems are still to be invented.","title":"by Adrian Gropper"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/","text":"A Primer on Functional Identity By Joe Andrieu < joe@legreq.com > There are many ways to approach identity. For the Rebooting the Web of Trust workshop [1], we prefer a functional approach, focusing our conversation on how identity works and how it is used. The varied facets of identity are rich. We inevitably bring our own hot buttons and agendas to any discussion of \u201cwhat identity is\u201d. Some engage from a philosophical perspective, others psychological. Some dive into political or cultural issues, while others dissect the meta-physical and spiritual. These different perspectives are valid views of identity\u2019s impact on our lives. More than valid. Vital . They help answer the question of \u201cWhy?\u201d Why identity matters, why we should care. Unfortunately, they also inflame passions and we sometimes talk past each other to make points that seem irrelevant to others, leaving people frustrated and unheard. As engineers and system designers , we\u2019re concerned with how things work . We want to fix what\u2019s broken and build things that improve the world. To help us do that at Rebooting the Web of Trust , we discuss how things function. With identity, this functional perspective sidesteps seductive and inflammatory rabbit holes, without dismissing them. Functional Identity lets us investigate the HOW without prejudice to WHY , viewing identity systems based on how they work. Then, once we understand how they function, we have a solid foundation for discussion about how these systems affect individuals and society. A Functional Definition Identity is how we recognize, remember, and ultimately respond to specific people and things. That\u2019s it . We meet people and learn their names. We observe them and hear gossip and potentially consume related media. We remember what we learn. Then, we apply that knowledge to future dealings. Others do the same with us. Even our sense of our own identity is shaped by how we recognize, remember, and respond to our own actions and reactions. Identity can be a wonderful thing. The joy of a child saying \u201cMomma\u201d or a lover calling out your name. The pride in your name on a diploma. The simple benefit of seeing another\u2019s name tag at a workshop and making a note to yourself about a fascinating conversation. Identity enables so many benefits because it helps us keep track of people and things. It helps us recognize friends, families, and threats; it enables remembering birthdays, preferences, and histories; it gives us the ability to respond to each individual as their own unique person. The functional approach reaches beyond digital systems to understand how identity works throughout society. Our identity is bigger than our digital selves. Our identities existed before and continue to exist independent of any digital representation. Digital identities are simply tools which help organizations and individuals manage real-world identity. Unfortunately, digital systems can unwittingly compromise real-world identity. Sometimes this occurs because digital identity systems fail to consider external effects. Other times, it happens with systems that didn\u2019t even realize they were dealing with identity information. A functional perspective allows engineers to see beyond static attributes and traditional notions of \u201cPersonally Identifiable Information\u201d to better understand how engineering choices can impact identity, even outside their systems. With a better understanding of how identity functions, we will be able to build systems that enhance privacy and human dignity, while improving identity assurance and security. Identity Systems An identity system is a collection of tools and techniques used to keep track of people and things. As individuals, we do this naturally , in our minds. We name things, then use names and distinguishing features to remember what we learn. We treat people differently based on their identity: treating our friends and family differently from strangers and known threats. Organizations create processes, software, and services to achieve similar ends. These identity systems are best understood in terms of how they function, which is the same as how identity has worked since the dawn of mankind . For some, identity systems are provocative because they help organizations keep track of people. They trigger associations with Big Brother and the surveillance state, inspiring dystopic visions of embedded chips and tattooed serial numbers. Discussing a ubiquitous \u201cidentity\u201d layer for the Internet conjures fears of government and corporations constantly tracking what we do, online and off. It should. Because, in fact, these are legitimate abuses of identity feared by civil libertarians and freedom-minded people everywhere. When we talk about identity systems, we are necessarily talking about how we keep track of people and things. Do it badly and we risk accidentally building our own Panopticon prison [1]. Fortunately, by understanding how identity functions, we can avoid, mitigate, and minimize such abuses. Terminology Identity is an information processing task that maps persons or things we might know to what we actually do know. This mapping may be entirely digital or it may rely on physical devices or sensors to provide input from the \u201creal\u201d world. Functional Identity focuses on the innate information processing that occurs in every identity system whether it is natural or engineered, purely digital or physically grounded. In the domain of information processing, informational assets and processes are the essential nouns and verbs that describe how any system works. This next section presents a concise and complete set of nouns and verbs that fully describe identity systems. When you understand these nouns and verbs for a given system, you will understand how that system creates, uses, and impacts identity, both within its boundaries and in the broader context of individuals lives in society. In the diagrams below, the blue boxes are nouns and the red ovals are verbs. Together they comprise the building blocks for describing identity systems. We start with the simplest identity system, using three nouns and a verb: Subjects are entities\u2014people or things\u2014under consideration. Identifiers are labels which refer to entities. They are used to keep track of what we know about those entities. Attributes are what we know about people and things. They describe the state, appearance, or other qualities of an entity. Correlate means to associate attributes with particular entities, to associate what we know about someone with either an identifier in the system or a subject in question. Identity systems correlate subjects with attributes in two ways. First, attributes are associated with identifiers referring to specific subjects, thus building a body of knowledge. Then, when we recognize a subject, we associate them with one or more identifiers, and in doing so, associate them with everything associated with those identifiers. In digital systems, this set of related attributes is sometimes referred as a digital identity or profile. Input and Effect We learn or acquire identity information over time, then apply what we\u2019ve learned to various interactions, usually elsewhere. Acquire** means to gather identity information for use by the system. Apply means to use identity information to affect change outside the identity system, typically to moderate an interaction of the subject with a related system. Identity information might be acquired by observation or by importing from elsewhere. We may learn about someone by watching them, or we may learn through references, rumors, and reputation. Identity systems acquire new information throughout their operational life, just as we continue to learn about people throughout our lives. Once acquired, identity information must be applied in a specific situation to have impact. If we know something about someone and that information never influences our behavior and is never shared, it doesn\u2019t affect the world. The way that identity information is applied tells us how an identity system affects our lives. For example, a website might apply the email associated with my account to allow me to reset my password or it may send me unwanted advertisements. The U.S. Transportation Security Administration (TSA) applies the information on its no-fly list to prevent those identified as potential threats from flying. Making New Ideas We gain new insights by considering both existing identity information and previously unrelated observations. Identity is more than just what we know about people and apply to our interactions. It\u2019s also how we make judgments based on what we know, gaining insights into character, capabilities, and proclivities. Raw data** are data which may or may not contain information relatable to a person or thing. Context tells you why you can rely on any given identity information and what you may do with it. Reason means to evaluate existing identity information to generate new derived attributes. New attributes are created by reasoning using raw data and known attributes. By applying reasoning to existing observations and related knowledge, we can gain insights that neither the subject nor the original author anticipated. Raw data such as search history, web browsing, and the time & location information captured by our phones, may contain identity-related information, even when that was neither the purpose nor the intention at the time of capture.\\ \\ The contexts associated with identity information inform us about appropriate use, including the evidence needed to understand how trustworthy it is. Context answers questions such as: Where did it come from? How did we get it? When was it created or modified? By Whom? What purposes, privileges, and responsibilities are attached? In short, context allows you to evaluate if a given piece of information is credible. In many real-world identity systems, like that enabled by state-issued driver\u2019s licenses, the context is implicit, spatial, and temporal. Online identity systems lack this physical immediacy and need to use other mechanisms to capture and understand context. We also reason using known attributes to derive new ones. For example, we calculate a person\u2019s age based on the birthdate on their driver\u2019s license to determine if they are old enough to drink legally. Credit companies evaluate recent income, past transactions, and projections of future income to set interest rates and make loan approvals. We remember how people treat us and alter our behavior in future interactions. If someone repeatedly breaks their word, we may stop depending on them. Governing Identity Information We go to great lengths to manage identity information. Govern means to manage the creation and flow of identity information so the right people have access for the right reasons at the right time. Sometimes we keep secrets to prevent information from reaching certain people. We do this with tools like encryption, access control, and minimal disclosure. Legal agreements between people, businesses, institutions, and governments specify appropriate use of certain information while laws, regulations, and the courts allow governments and institutions to oversee, monitor, and intervene in the capture and use of identity information. How identity systems govern who controls certain information defines how they preserve and respect privacy. The right to keep private information private is often referred to as the right of privacy. Many people feel their privacy is threatened because so much information is shared over the Internet, in our workplaces, and through our devices. Information we share in different contexts (business, family, community, etc.) can leak unexpectedly and undesirably into other contexts. It is very difficult, as individuals, to track of all the ways we are publicly or privately tracked. Information is shared on social media, tracked in Internet searches, monitored when using navigation software, and captured as we use our phones. The sheer magnitude and complexity of information sharing means the average person is essentially incapable of making informed decisions to consent to appropriate use. Some people give up, divulging personal information without regard to consequences. Others opt-out, participating as little as possible in our digitally connected world. Governance defines who gets to control this complexity and how we do so. Bridging the Gap The nouns and verbs above are grounded in the world of technology and may be unfamiliar for the average individual. More conversational synonyms are presented in the tables below. Use the most appropriate terms for your audience. People, Places and Things This is the point of identity: those people, places, and things we recognize. | Technologists Laypeople | Common meaning | |-------------------|-----------------|-----------------------------------------------------------------------------------| Subject | Person or Thing | Someone or something under consideration. The focus of interaction or discussion. Identity Information These are the abstract nouns of identity, the informational assets created and used by identity systems. Technologists Laypeople Common meaning Identifiers Names Refers to entities. Used to keep track of people and things. Attributes Knowledge What we know about people and things. How we describe the state, appearance, or other qualities of an entity. Raw data Observations Data which may or may not contain correlatable information. Contexts Situations Information which allows us to evaluate if another piece of information is dependable. Identity Actions These are the verbs of identity. These are the actions taken by identity systems working with identity information. Technologists Laypeople Common meaning Acquire Collect Intake or generate identity information for use by the system. Correlate Relate Associate attributes or observations with particular entities. We associate what we know about someone with either an identifier in the system or with a subject in question. Reason Reason Evaluate existing identity information to generate new beliefs, expressed in attributes, captured in statements. Apply Apply Use identity information in a system, typically to moderate interactions with known entities. Govern Control Manage the creation and flow of identity information to the right people at the right time. For technologists: we assign identifiers to subjects . We collect raw data and correlate attributes to the subjects we track, in specific contexts . We reason over raw data and attributes, to derive new attributes . We then apply this information to current and future interactions with subjects. We govern identity information to preserve privacy and give appropriate controls to the right parties. In more ordinary language: we give names to people . We collect observations and linking those observations to people, remembering knowledge about them. We reason over these observations and knowledge to generate new knowledge. We then apply what we know when dealing with those we recognize. We control identity information to preserve privacy and to protect those we love. This is the vocabulary of Functional Identity, a way to discuss and understand identity in terms of functionality: how it works and what it does for us. This is the language of identity for Rebooting the Web of Trust . Why? Engineers, entrepreneurs, and financiers have asked \u201cWhy are we spending so much time with a definition of identity? Why not just build something and fix it if it is broken?\u201d The vital, simple reason is human dignity . When we build interconnected systems without a core understanding of identity, we risk inadvertently compromising human dignity. We risk accidentally building systems that deny self-expression, place individuals in harm\u2019s way, and unintentionally oppress those most in need of self-determination. There are times when the needs of security outweigh the desire for human dignity. Fine. It\u2019s the job of our political systems\u2014local, national, and international\u2014to minimize abuse and to establish boundaries and practices that respect basic human rights. But when engineers unwittingly compromise the ability of individuals to self-express their identity, when we expose personal information in unexpected ways, when our systems deny basic services because of a flawed understanding of identity, that\u2019s avoidable tragedy . What might seem a minor technicality in one conversation could lead to the loss of privacy, liberty, or even life for an individual whose identity is unintentionally compromised. That\u2019s why it pays to understand identity, so the systems we build intentionally enable human dignity instead of accidentally destroy it. Summary Functional Identity focuses on how identity works . At the Rebooting the Web of Trust, we ground our work in the functional notion of identity and avoid the psychological, cultural, political, and philosophical. These notions are important, but they can also distract us from understanding the technical choices involved in building and using identity in today\u2019s networked world. This functional notion of identity began with a conversation at the Internet Identity Workshop [3] in May of 2016, followed by conversations at ID2020 [4] and the second Rebooting the Web of Trust workshop that summer, resulting in the paper \u201cIdentity Crisis\u201d [5]. It continued in subsequent meetings in all three venues, and in two articles published by the People Centered Internet [6, 7]. This primer represents a current take on that conversation, geared to help Rebooting the Web of Trust participants communicate more clearly and collaborate more effectively. We encourage your feedback and look forward to continuing the conversation. [1] http://weboftrust.info [2] https://en.wikipedia.org/wiki/Panopticon [3] https://iiw.idcommons.net [4] http://id2020summit.org/ [5] https://bitly.com/IdentityCrisisPaper [6] https://peoplecentered.net/2017/06/11/speaking-of-identity/ [7] https://peoplecentered.net/2017/07/26/how-identity-can-enable-a-people-centered-internet/","title":"Functional identity primer"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#a-primer-on-functional-identity","text":"By Joe Andrieu < joe@legreq.com > There are many ways to approach identity. For the Rebooting the Web of Trust workshop [1], we prefer a functional approach, focusing our conversation on how identity works and how it is used. The varied facets of identity are rich. We inevitably bring our own hot buttons and agendas to any discussion of \u201cwhat identity is\u201d. Some engage from a philosophical perspective, others psychological. Some dive into political or cultural issues, while others dissect the meta-physical and spiritual. These different perspectives are valid views of identity\u2019s impact on our lives. More than valid. Vital . They help answer the question of \u201cWhy?\u201d Why identity matters, why we should care. Unfortunately, they also inflame passions and we sometimes talk past each other to make points that seem irrelevant to others, leaving people frustrated and unheard. As engineers and system designers , we\u2019re concerned with how things work . We want to fix what\u2019s broken and build things that improve the world. To help us do that at Rebooting the Web of Trust , we discuss how things function. With identity, this functional perspective sidesteps seductive and inflammatory rabbit holes, without dismissing them. Functional Identity lets us investigate the HOW without prejudice to WHY , viewing identity systems based on how they work. Then, once we understand how they function, we have a solid foundation for discussion about how these systems affect individuals and society.","title":"A Primer on Functional Identity"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#a-functional-definition","text":"Identity is how we recognize, remember, and ultimately respond to specific people and things. That\u2019s it . We meet people and learn their names. We observe them and hear gossip and potentially consume related media. We remember what we learn. Then, we apply that knowledge to future dealings. Others do the same with us. Even our sense of our own identity is shaped by how we recognize, remember, and respond to our own actions and reactions. Identity can be a wonderful thing. The joy of a child saying \u201cMomma\u201d or a lover calling out your name. The pride in your name on a diploma. The simple benefit of seeing another\u2019s name tag at a workshop and making a note to yourself about a fascinating conversation. Identity enables so many benefits because it helps us keep track of people and things. It helps us recognize friends, families, and threats; it enables remembering birthdays, preferences, and histories; it gives us the ability to respond to each individual as their own unique person. The functional approach reaches beyond digital systems to understand how identity works throughout society. Our identity is bigger than our digital selves. Our identities existed before and continue to exist independent of any digital representation. Digital identities are simply tools which help organizations and individuals manage real-world identity. Unfortunately, digital systems can unwittingly compromise real-world identity. Sometimes this occurs because digital identity systems fail to consider external effects. Other times, it happens with systems that didn\u2019t even realize they were dealing with identity information. A functional perspective allows engineers to see beyond static attributes and traditional notions of \u201cPersonally Identifiable Information\u201d to better understand how engineering choices can impact identity, even outside their systems. With a better understanding of how identity functions, we will be able to build systems that enhance privacy and human dignity, while improving identity assurance and security.","title":"A Functional Definition"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#identity-systems","text":"An identity system is a collection of tools and techniques used to keep track of people and things. As individuals, we do this naturally , in our minds. We name things, then use names and distinguishing features to remember what we learn. We treat people differently based on their identity: treating our friends and family differently from strangers and known threats. Organizations create processes, software, and services to achieve similar ends. These identity systems are best understood in terms of how they function, which is the same as how identity has worked since the dawn of mankind . For some, identity systems are provocative because they help organizations keep track of people. They trigger associations with Big Brother and the surveillance state, inspiring dystopic visions of embedded chips and tattooed serial numbers. Discussing a ubiquitous \u201cidentity\u201d layer for the Internet conjures fears of government and corporations constantly tracking what we do, online and off. It should. Because, in fact, these are legitimate abuses of identity feared by civil libertarians and freedom-minded people everywhere. When we talk about identity systems, we are necessarily talking about how we keep track of people and things. Do it badly and we risk accidentally building our own Panopticon prison [1]. Fortunately, by understanding how identity functions, we can avoid, mitigate, and minimize such abuses.","title":"Identity Systems"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#terminology","text":"Identity is an information processing task that maps persons or things we might know to what we actually do know. This mapping may be entirely digital or it may rely on physical devices or sensors to provide input from the \u201creal\u201d world. Functional Identity focuses on the innate information processing that occurs in every identity system whether it is natural or engineered, purely digital or physically grounded. In the domain of information processing, informational assets and processes are the essential nouns and verbs that describe how any system works. This next section presents a concise and complete set of nouns and verbs that fully describe identity systems. When you understand these nouns and verbs for a given system, you will understand how that system creates, uses, and impacts identity, both within its boundaries and in the broader context of individuals lives in society. In the diagrams below, the blue boxes are nouns and the red ovals are verbs. Together they comprise the building blocks for describing identity systems. We start with the simplest identity system, using three nouns and a verb: Subjects are entities\u2014people or things\u2014under consideration. Identifiers are labels which refer to entities. They are used to keep track of what we know about those entities. Attributes are what we know about people and things. They describe the state, appearance, or other qualities of an entity. Correlate means to associate attributes with particular entities, to associate what we know about someone with either an identifier in the system or a subject in question. Identity systems correlate subjects with attributes in two ways. First, attributes are associated with identifiers referring to specific subjects, thus building a body of knowledge. Then, when we recognize a subject, we associate them with one or more identifiers, and in doing so, associate them with everything associated with those identifiers. In digital systems, this set of related attributes is sometimes referred as a digital identity or profile.","title":"Terminology"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#input-and-effect","text":"We learn or acquire identity information over time, then apply what we\u2019ve learned to various interactions, usually elsewhere. Acquire** means to gather identity information for use by the system. Apply means to use identity information to affect change outside the identity system, typically to moderate an interaction of the subject with a related system. Identity information might be acquired by observation or by importing from elsewhere. We may learn about someone by watching them, or we may learn through references, rumors, and reputation. Identity systems acquire new information throughout their operational life, just as we continue to learn about people throughout our lives. Once acquired, identity information must be applied in a specific situation to have impact. If we know something about someone and that information never influences our behavior and is never shared, it doesn\u2019t affect the world. The way that identity information is applied tells us how an identity system affects our lives. For example, a website might apply the email associated with my account to allow me to reset my password or it may send me unwanted advertisements. The U.S. Transportation Security Administration (TSA) applies the information on its no-fly list to prevent those identified as potential threats from flying.","title":"Input and Effect"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#making-new-ideas","text":"We gain new insights by considering both existing identity information and previously unrelated observations. Identity is more than just what we know about people and apply to our interactions. It\u2019s also how we make judgments based on what we know, gaining insights into character, capabilities, and proclivities. Raw data** are data which may or may not contain information relatable to a person or thing. Context tells you why you can rely on any given identity information and what you may do with it. Reason means to evaluate existing identity information to generate new derived attributes. New attributes are created by reasoning using raw data and known attributes. By applying reasoning to existing observations and related knowledge, we can gain insights that neither the subject nor the original author anticipated. Raw data such as search history, web browsing, and the time & location information captured by our phones, may contain identity-related information, even when that was neither the purpose nor the intention at the time of capture.\\ \\ The contexts associated with identity information inform us about appropriate use, including the evidence needed to understand how trustworthy it is. Context answers questions such as: Where did it come from? How did we get it? When was it created or modified? By Whom? What purposes, privileges, and responsibilities are attached? In short, context allows you to evaluate if a given piece of information is credible. In many real-world identity systems, like that enabled by state-issued driver\u2019s licenses, the context is implicit, spatial, and temporal. Online identity systems lack this physical immediacy and need to use other mechanisms to capture and understand context. We also reason using known attributes to derive new ones. For example, we calculate a person\u2019s age based on the birthdate on their driver\u2019s license to determine if they are old enough to drink legally. Credit companies evaluate recent income, past transactions, and projections of future income to set interest rates and make loan approvals. We remember how people treat us and alter our behavior in future interactions. If someone repeatedly breaks their word, we may stop depending on them.","title":"Making New Ideas"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#governing-identity-information","text":"We go to great lengths to manage identity information. Govern means to manage the creation and flow of identity information so the right people have access for the right reasons at the right time. Sometimes we keep secrets to prevent information from reaching certain people. We do this with tools like encryption, access control, and minimal disclosure. Legal agreements between people, businesses, institutions, and governments specify appropriate use of certain information while laws, regulations, and the courts allow governments and institutions to oversee, monitor, and intervene in the capture and use of identity information. How identity systems govern who controls certain information defines how they preserve and respect privacy. The right to keep private information private is often referred to as the right of privacy. Many people feel their privacy is threatened because so much information is shared over the Internet, in our workplaces, and through our devices. Information we share in different contexts (business, family, community, etc.) can leak unexpectedly and undesirably into other contexts. It is very difficult, as individuals, to track of all the ways we are publicly or privately tracked. Information is shared on social media, tracked in Internet searches, monitored when using navigation software, and captured as we use our phones. The sheer magnitude and complexity of information sharing means the average person is essentially incapable of making informed decisions to consent to appropriate use. Some people give up, divulging personal information without regard to consequences. Others opt-out, participating as little as possible in our digitally connected world. Governance defines who gets to control this complexity and how we do so.","title":"Governing Identity Information"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#bridging-the-gap","text":"The nouns and verbs above are grounded in the world of technology and may be unfamiliar for the average individual. More conversational synonyms are presented in the tables below. Use the most appropriate terms for your audience.","title":"Bridging the Gap"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#people-places-and-things","text":"This is the point of identity: those people, places, and things we recognize. | Technologists Laypeople | Common meaning | |-------------------|-----------------|-----------------------------------------------------------------------------------| Subject | Person or Thing | Someone or something under consideration. The focus of interaction or discussion.","title":"People, Places and Things"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#identity-information","text":"These are the abstract nouns of identity, the informational assets created and used by identity systems. Technologists Laypeople Common meaning Identifiers Names Refers to entities. Used to keep track of people and things. Attributes Knowledge What we know about people and things. How we describe the state, appearance, or other qualities of an entity. Raw data Observations Data which may or may not contain correlatable information. Contexts Situations Information which allows us to evaluate if another piece of information is dependable.","title":"Identity Information"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#identity-actions","text":"These are the verbs of identity. These are the actions taken by identity systems working with identity information. Technologists Laypeople Common meaning Acquire Collect Intake or generate identity information for use by the system. Correlate Relate Associate attributes or observations with particular entities. We associate what we know about someone with either an identifier in the system or with a subject in question. Reason Reason Evaluate existing identity information to generate new beliefs, expressed in attributes, captured in statements. Apply Apply Use identity information in a system, typically to moderate interactions with known entities. Govern Control Manage the creation and flow of identity information to the right people at the right time. For technologists: we assign identifiers to subjects . We collect raw data and correlate attributes to the subjects we track, in specific contexts . We reason over raw data and attributes, to derive new attributes . We then apply this information to current and future interactions with subjects. We govern identity information to preserve privacy and give appropriate controls to the right parties. In more ordinary language: we give names to people . We collect observations and linking those observations to people, remembering knowledge about them. We reason over these observations and knowledge to generate new knowledge. We then apply what we know when dealing with those we recognize. We control identity information to preserve privacy and to protect those we love. This is the vocabulary of Functional Identity, a way to discuss and understand identity in terms of functionality: how it works and what it does for us. This is the language of identity for Rebooting the Web of Trust .","title":"Identity Actions"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#why","text":"Engineers, entrepreneurs, and financiers have asked \u201cWhy are we spending so much time with a definition of identity? Why not just build something and fix it if it is broken?\u201d The vital, simple reason is human dignity . When we build interconnected systems without a core understanding of identity, we risk inadvertently compromising human dignity. We risk accidentally building systems that deny self-expression, place individuals in harm\u2019s way, and unintentionally oppress those most in need of self-determination. There are times when the needs of security outweigh the desire for human dignity. Fine. It\u2019s the job of our political systems\u2014local, national, and international\u2014to minimize abuse and to establish boundaries and practices that respect basic human rights. But when engineers unwittingly compromise the ability of individuals to self-express their identity, when we expose personal information in unexpected ways, when our systems deny basic services because of a flawed understanding of identity, that\u2019s avoidable tragedy . What might seem a minor technicality in one conversation could lead to the loss of privacy, liberty, or even life for an individual whose identity is unintentionally compromised. That\u2019s why it pays to understand identity, so the systems we build intentionally enable human dignity instead of accidentally destroy it.","title":"Why?"},{"location":"rwot8/topics-and-advance-readings/functional-identity-primer/#summary","text":"Functional Identity focuses on how identity works . At the Rebooting the Web of Trust, we ground our work in the functional notion of identity and avoid the psychological, cultural, political, and philosophical. These notions are important, but they can also distract us from understanding the technical choices involved in building and using identity in today\u2019s networked world. This functional notion of identity began with a conversation at the Internet Identity Workshop [3] in May of 2016, followed by conversations at ID2020 [4] and the second Rebooting the Web of Trust workshop that summer, resulting in the paper \u201cIdentity Crisis\u201d [5]. It continued in subsequent meetings in all three venues, and in two articles published by the People Centered Internet [6, 7]. This primer represents a current take on that conversation, geared to help Rebooting the Web of Trust participants communicate more clearly and collaborate more effectively. We encourage your feedback and look forward to continuing the conversation. [1] http://weboftrust.info [2] https://en.wikipedia.org/wiki/Panopticon [3] https://iiw.idcommons.net [4] http://id2020summit.org/ [5] https://bitly.com/IdentityCrisisPaper [6] https://peoplecentered.net/2017/06/11/speaking-of-identity/ [7] https://peoplecentered.net/2017/07/26/how-identity-can-enable-a-people-centered-internet/","title":"Summary"},{"location":"rwot8/topics-and-advance-readings/glossary-primer/","text":"A Glossary of Terms for Rebooting the Web of Trust by Shannon Appelcline ACL: Access Control List. An authorization methodology where permissions are attached to an object as a list of who can access that object and in what way. Agent: A service or application that acts as a mediator between credential issuers, credential holders, and credential verifiers. Attestation: A statement of some fact (or some opinion) about some entity. Claim and Credential may be preferred synonyms due to W3C's work on Verifiable Credentials . Attribute: Information about a digital identity. Authentication: The act of verifying an identity. Authorization: The act of verifying access permissions. Bitcoin: The cryptocurrency that invented blockchain technology and still the digital currency with the largest market cap. Focused on the purchase of goods and services. Blockcert: A blockchain-anchored credential that is decentralized and trustless. Blockchain: An immutable decentralized ledger maintained by consensus rules. In other words, a sort of online database that everyone can write to if they follow set guidelines. CA: Certificate Authority. An entity that issues certificates. Typically, a centralized authority/. Capability: An authorization methodology where permissions are attached to an entity as a list of what objects that entity can access and in what way. Centralized Authority: A singular entity who has control over some system or process. Certificate: An identity credential. Typically, a public key bound to a name that is signed by some authority. Claim: An attestation about some entity. Credential: A set of one or more claims about the same entity, which might also include other information such as identifiers, proofs, or other metadata. Cryptocurrency: Digital currency protected by cryptographic algorithms. Cryptography: Mathematical processes based on one-way functions that convert a message from a plain-text form to a coded form (and vice-versa). DAD: Decentralized Autonomic Data. Self-regulating or self-managing data that does not reside with a single party, supporting the identification, certification, and securing of streaming data that is processed in a distributed manner. See also \"Decentralized Autonomic Data (DAD) and the Three R's of Key Management\" . Data Minimization: The act of limiting shared data to the minimum necessary. DID: Decentralized Identifier. A portable, globally unique identifier associated with some entity that does not require a centralized authority for registration. See also \"A Short Primer on Decentralized Identifiers\" . DID Document: A document that contains information related to a specific DID. DID Method: A regularized methodology for creating, reading, updating, and revoking a DID. BTCR, IPID, Sovrin, uPort, and Veres Ones are just a few DID methods. Decentralized: Distributed and not dependent upon any central authority. Digital Rights: The codification of authorization to use digital media. ECC: Elliptic Curve Cryptography. A method of public-key cryptography that depends on elliptic curves (y^2 = x^3 + ax + b) and how they behave in finite fields. They improve over classic RSA cryptography with their smaller key size. Entity: A person, organization, concept, or device. Ether: Currently, the third-most valuable cryptocurrency, the \"fuel\" for the ethereum platform. Ethereum: A distributed computer blockchain focused on smart contracts that uses Ether. FIDO: An authentication method for secure two-factor authentication, managed by a hardware key. GDPR: General Data Protection Regulation. European laws that protect the data and privacy of individuals and that place restrictions on how others can use that data. Holder: Someone who possesses credentials, usually (but not always) the subject of the credentials. Hub: A datastore where objects are signed by a digital identity and accessible through unique global identifiers. See also \"Hubs\" and \"Identity Hub Attestation Flows and Components\" . Identifier: A proxy for identity that's used as a label to refer to the entity. For example, a name or UID. Identity: A somewhat nebulous term, defined in different ways by different people. Broadly, it's who or what an entity is. Identity, Digital: A digital representation of identity. Identity, Functional: A model for identity that says, \"Identity is how we keep track of people and things and, in turn, how they keep track of us.\" See also \"A Primer on Functional Identity\" . Issuer: Someone who asserts claims and issues them in credentials. Key: A cryptographic secret used to encrypt or decrypt data. In traditional symmetric cryptography, the same key was used for encrypting and decrypting. In public-key cryptography, a private key is used for encrypting (and signing) while a mathematically related public key is used for decrypting (and verifying). PGP: Pretty Good Privacy. A classic program used for encrypting, decrypting, and signing data. The origin of the Web of Trust, which was designed as a method for determining the trust of public keys, as an alternative to a centralized public-key infrastructure. PKI: Public-Key Infrastructure. A methodology to ensure the creation, storage, distribution, and revocation of public keys. Private Key: Half of the keypair in public-key cryptography. A secret that's used to encrypt and to sign. Public Key: Half of the keypair in public-key cryptography. A publicly distributed key that's used to decrypt and to verify signatures. Public-key Cryptography: A cryptographic process that uses two mathematically related keys, one to encrypt a message and one to decrypt a message; one key (the public key) can be derived from the other key (the private key), but not vice-versa. Repository: A wallet (or other storage area) used to store personal credentials. Reputation: A system for measuring the behavior of entities. Revocation: The act of cancelling digital identity data such as a DID or private key. Ripple: A real-time payment and settlement system designed to bridge transfers between different sorts of money using the XRP cryptocurrency. Although it uses a distributed consensus ledger, it is not a blockchain. Selective Disclosure: A method of sharing information at a granular level, such as revealing some claims but not an entire credential. See also \"Engineering Privacy for Verified Credentials\" . Signature: A means for verifying the authenticity of a message or transaction by signing it with a private key; the signature can then be verified with a public key. Smart Contract: A digital program, often associated with the transaction of cryptocurrency funds. Neither smart nor a contract. SSI: Self-sovereign identity. A decentralized, portable digital identity that does not depend on any centralized authority. See also \"The Path to Self-Sovereign Identity\" . Subject: Someone who is the subject of claims. Trustless: Requiring no trust. A process that is designed such that its rules ensure that all of its participants must act fairly. Usually part of a decentralized design. Verifiable Claims: The original name for Verifiable Credentials. Verifiable Credentials: A tamper-evident credential, per the W3C Data Model . See also \"A Verifiable Credentials Primer\" . Verification: The act of proving the accuracy of something, often verifying a digital signature. Verifier: Someone who verifies credentials. Wallet: A digital means to store private keys and their associated public keys. The term comes from cryptocurrency wallets, which store the keys associated with cryptocurrency transactions, but there are also identity wallets, which store keys related to digital identities. Wallet, Hardware: A hardware gadget that acts as a wallet. Often, a Ledger or a Trezor. Web of Trust: A method for assessing trust based on peer-to-peer processes. More broadly, an area of digital development that focuses on decentralized identity. XRP: Currently, the second-most valuable cryptocurrency, owned by Ripple. Created as a high-speed bridge currency that eliminates exchange fees. Zero-knowledge Proof: A cryptographic method where someone can prove that they know some information without revealing the information. Some inspirations for terms and for the words used to describe them drawn from Verifiable Claims Terminology . If you have any disagreements on definitions on which to include additional terms, please enter a PR.","title":"A Glossary of Terms for Rebooting the Web of Trust"},{"location":"rwot8/topics-and-advance-readings/glossary-primer/#a-glossary-of-terms-for-rebooting-the-web-of-trust","text":"","title":"A Glossary of Terms for Rebooting the Web of Trust"},{"location":"rwot8/topics-and-advance-readings/glossary-primer/#by-shannon-appelcline","text":"ACL: Access Control List. An authorization methodology where permissions are attached to an object as a list of who can access that object and in what way. Agent: A service or application that acts as a mediator between credential issuers, credential holders, and credential verifiers. Attestation: A statement of some fact (or some opinion) about some entity. Claim and Credential may be preferred synonyms due to W3C's work on Verifiable Credentials . Attribute: Information about a digital identity. Authentication: The act of verifying an identity. Authorization: The act of verifying access permissions. Bitcoin: The cryptocurrency that invented blockchain technology and still the digital currency with the largest market cap. Focused on the purchase of goods and services. Blockcert: A blockchain-anchored credential that is decentralized and trustless. Blockchain: An immutable decentralized ledger maintained by consensus rules. In other words, a sort of online database that everyone can write to if they follow set guidelines. CA: Certificate Authority. An entity that issues certificates. Typically, a centralized authority/. Capability: An authorization methodology where permissions are attached to an entity as a list of what objects that entity can access and in what way. Centralized Authority: A singular entity who has control over some system or process. Certificate: An identity credential. Typically, a public key bound to a name that is signed by some authority. Claim: An attestation about some entity. Credential: A set of one or more claims about the same entity, which might also include other information such as identifiers, proofs, or other metadata. Cryptocurrency: Digital currency protected by cryptographic algorithms. Cryptography: Mathematical processes based on one-way functions that convert a message from a plain-text form to a coded form (and vice-versa). DAD: Decentralized Autonomic Data. Self-regulating or self-managing data that does not reside with a single party, supporting the identification, certification, and securing of streaming data that is processed in a distributed manner. See also \"Decentralized Autonomic Data (DAD) and the Three R's of Key Management\" . Data Minimization: The act of limiting shared data to the minimum necessary. DID: Decentralized Identifier. A portable, globally unique identifier associated with some entity that does not require a centralized authority for registration. See also \"A Short Primer on Decentralized Identifiers\" . DID Document: A document that contains information related to a specific DID. DID Method: A regularized methodology for creating, reading, updating, and revoking a DID. BTCR, IPID, Sovrin, uPort, and Veres Ones are just a few DID methods. Decentralized: Distributed and not dependent upon any central authority. Digital Rights: The codification of authorization to use digital media. ECC: Elliptic Curve Cryptography. A method of public-key cryptography that depends on elliptic curves (y^2 = x^3 + ax + b) and how they behave in finite fields. They improve over classic RSA cryptography with their smaller key size. Entity: A person, organization, concept, or device. Ether: Currently, the third-most valuable cryptocurrency, the \"fuel\" for the ethereum platform. Ethereum: A distributed computer blockchain focused on smart contracts that uses Ether. FIDO: An authentication method for secure two-factor authentication, managed by a hardware key. GDPR: General Data Protection Regulation. European laws that protect the data and privacy of individuals and that place restrictions on how others can use that data. Holder: Someone who possesses credentials, usually (but not always) the subject of the credentials. Hub: A datastore where objects are signed by a digital identity and accessible through unique global identifiers. See also \"Hubs\" and \"Identity Hub Attestation Flows and Components\" . Identifier: A proxy for identity that's used as a label to refer to the entity. For example, a name or UID. Identity: A somewhat nebulous term, defined in different ways by different people. Broadly, it's who or what an entity is. Identity, Digital: A digital representation of identity. Identity, Functional: A model for identity that says, \"Identity is how we keep track of people and things and, in turn, how they keep track of us.\" See also \"A Primer on Functional Identity\" . Issuer: Someone who asserts claims and issues them in credentials. Key: A cryptographic secret used to encrypt or decrypt data. In traditional symmetric cryptography, the same key was used for encrypting and decrypting. In public-key cryptography, a private key is used for encrypting (and signing) while a mathematically related public key is used for decrypting (and verifying). PGP: Pretty Good Privacy. A classic program used for encrypting, decrypting, and signing data. The origin of the Web of Trust, which was designed as a method for determining the trust of public keys, as an alternative to a centralized public-key infrastructure. PKI: Public-Key Infrastructure. A methodology to ensure the creation, storage, distribution, and revocation of public keys. Private Key: Half of the keypair in public-key cryptography. A secret that's used to encrypt and to sign. Public Key: Half of the keypair in public-key cryptography. A publicly distributed key that's used to decrypt and to verify signatures. Public-key Cryptography: A cryptographic process that uses two mathematically related keys, one to encrypt a message and one to decrypt a message; one key (the public key) can be derived from the other key (the private key), but not vice-versa. Repository: A wallet (or other storage area) used to store personal credentials. Reputation: A system for measuring the behavior of entities. Revocation: The act of cancelling digital identity data such as a DID or private key. Ripple: A real-time payment and settlement system designed to bridge transfers between different sorts of money using the XRP cryptocurrency. Although it uses a distributed consensus ledger, it is not a blockchain. Selective Disclosure: A method of sharing information at a granular level, such as revealing some claims but not an entire credential. See also \"Engineering Privacy for Verified Credentials\" . Signature: A means for verifying the authenticity of a message or transaction by signing it with a private key; the signature can then be verified with a public key. Smart Contract: A digital program, often associated with the transaction of cryptocurrency funds. Neither smart nor a contract. SSI: Self-sovereign identity. A decentralized, portable digital identity that does not depend on any centralized authority. See also \"The Path to Self-Sovereign Identity\" . Subject: Someone who is the subject of claims. Trustless: Requiring no trust. A process that is designed such that its rules ensure that all of its participants must act fairly. Usually part of a decentralized design. Verifiable Claims: The original name for Verifiable Credentials. Verifiable Credentials: A tamper-evident credential, per the W3C Data Model . See also \"A Verifiable Credentials Primer\" . Verification: The act of proving the accuracy of something, often verifying a digital signature. Verifier: Someone who verifies credentials. Wallet: A digital means to store private keys and their associated public keys. The term comes from cryptocurrency wallets, which store the keys associated with cryptocurrency transactions, but there are also identity wallets, which store keys related to digital identities. Wallet, Hardware: A hardware gadget that acts as a wallet. Often, a Ledger or a Trezor. Web of Trust: A method for assessing trust based on peer-to-peer processes. More broadly, an area of digital development that focuses on decentralized identity. XRP: Currently, the second-most valuable cryptocurrency, owned by Ripple. Created as a high-speed bridge currency that eliminates exchange fees. Zero-knowledge Proof: A cryptographic method where someone can prove that they know some information without revealing the information. Some inspirations for terms and for the words used to describe them drawn from Verifiable Claims Terminology . If you have any disagreements on definitions on which to include additional terms, please enter a PR.","title":"by Shannon Appelcline"},{"location":"rwot8/topics-and-advance-readings/graphs-everywhere/","text":"I see graphs everywhere. On the importance of graphs and graph analysis way of thinking. Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: short_topic_paper_to_open_discussion, graphs, graph analysis I have a confession to make - I see graphs everywhere. Even in the places where other people do not see them. I do not remember when exactly I started to see them. Probably around the time when I begin to think that Identity is a Graph of Claims , or maybe when I saw this image at put it at the home page of Validbook / The United Humans services POC as a symbol of humanity. Now I cannot get rid of seeing everything as graphs - our universe, our societies, families, our bodies and most obviously our brains made of billions of neurons. If you think about this, even human consciousness, thoughts and the perception of beauty are undeniably just dynamic changes in the graph of neurons. Graph analysis is going to help us understand the biggest scientific secrets out there (brain, economies, universe). I wanted to share this insight of seeing everything as graphs with RWOT community, as this way of looking at things makes it easier to understand DIDs/VCs/WOTs systems. Identity, verifiable claims, WOT/PKIs, etc are nothing but graphs of claims and in order for them to become meaningful/verified/secured they have to be traversed. Thus the task of DIDs/VCs/WOTs designer/architect comes down to creating the most effective/optimal graphs and respective graph traversal tools. Also, I'd love to find people, who share this passion for graphs, to explore applicability of different graph analysis methods and tools to various RWOT-related interests.","title":"I see graphs everywhere. On the importance of graphs and graph analysis way of thinking."},{"location":"rwot8/topics-and-advance-readings/graphs-everywhere/#i-see-graphs-everywhere-on-the-importance-of-graphs-and-graph-analysis-way-of-thinking","text":"Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: short_topic_paper_to_open_discussion, graphs, graph analysis I have a confession to make - I see graphs everywhere. Even in the places where other people do not see them. I do not remember when exactly I started to see them. Probably around the time when I begin to think that Identity is a Graph of Claims , or maybe when I saw this image at put it at the home page of Validbook / The United Humans services POC as a symbol of humanity. Now I cannot get rid of seeing everything as graphs - our universe, our societies, families, our bodies and most obviously our brains made of billions of neurons. If you think about this, even human consciousness, thoughts and the perception of beauty are undeniably just dynamic changes in the graph of neurons. Graph analysis is going to help us understand the biggest scientific secrets out there (brain, economies, universe). I wanted to share this insight of seeing everything as graphs with RWOT community, as this way of looking at things makes it easier to understand DIDs/VCs/WOTs systems. Identity, verifiable claims, WOT/PKIs, etc are nothing but graphs of claims and in order for them to become meaningful/verified/secured they have to be traversed. Thus the task of DIDs/VCs/WOTs designer/architect comes down to creating the most effective/optimal graphs and respective graph traversal tools. Also, I'd love to find people, who share this passion for graphs, to explore applicability of different graph analysis methods and tools to various RWOT-related interests.","title":"I see graphs everywhere. On the importance of graphs and graph analysis way of thinking."},{"location":"rwot8/topics-and-advance-readings/handshake/","text":"Handshake by Boyma Fahnbulleh \\< boymanjor@protonmail.com > Introduction Handshake is a decentralized, permission-less naming protocol compatible with DNS. We seek to solve Zooko's triangle through the use of a utxo-based blockchain, which manages the registration, renewal, and transfer of DNS top-level domains (TLDs). The initial goal is not to replace the DNS protocol but to replace the root zone file and the root servers with a decentralized, public commons. By tying name ownership to a utxo, and embedding DNS records into its metadata, a chain of trust can be created by a digital signature and verified by querying blockchain data. A decentralized network of validating peers anchors this chain of trust. The ultimate goal is to provide an alternative to existing Certificate Authorities. Our naming protocol differs from its predecessors in that it has no concept of subdomains at the consensus layer. The protocol also provides secure name resolutions via light client resolvers by committing to name data in the block headers and using compact, merklelized proofs of inclusion and non-inclusion. We believe this project could be used to add human readability to DIDs by committing to them in the DNS records of names registered on the network. Or the names themselves could exist as DIDs and DID documents could be committed to in the DNS records. Name Auctions Second-price, blind bid (Vickrey) auctions manage the issuance of names on-chain. A consensus-level covenant system facilitates these auctions by encumbering utxos with spending restrictions. A collection of these covenants model an auction state machine. For example, to bid on a name, one must spend a utxo in a transaction which creates an output which carries a 'BID' covenant. This output will have a value which is equal to (or higher) than the amount the user intends to bid on the name. It will also include the name up for auction and a commitment to the actual bid amount. The 'BID' covenant will restrict this output from being spent in any transaction that does not create an output with a 'REVEAL' covenant. The 'REVEAL' covenant essentially reveals the actual bid amount and allows the excess value in the 'BID' covenant to be spent freely. The rest of the auction process, as well as DNS record updates and name transfers, are modeled in this covenant system. Network Bootstrapping Consensus rules reserve all entries in ICANN's existing root zone file. Names in the list of Alexa top 100,000 domains are also reserved. The latter names are converted to TLDs by selecting their first domain name label. Name owners can bypass the auction system and claim names through the use of DNSSEC ownership proofs. We also have a sunrise period to allow trademark holders without domains in the Alexa top 100,000 to reserve names. The DNSSEC ownership proofs mentioned above are a stricter subset of DNSSEC proofs . They do not allow for CNAME glue or wildcards, and every label must be separated by a zone cut using a typical DS-to-DNSKEY setup for referrals. All zone referrals are retrieved and combined to produce the final proof. These proofs must stem from ICANN's key-signing keys (KSKs) to the final ZSK in the target zone. The ultimate zone-signing key (ZSK) must sign a TXT record which commits to the name's desired address on the blockchain. The proof is broadcast to the peer-to-peer network and included by miners in the coinbase transaction of a block. The consensus rules dictate that the miner must create an output for the associated proof, granting the name to the committed address. Software hsd is a full node, Javascript implementation of the protocol and an authoritative name resolver for the root zone. hnsd is an SPV name resolver written in C. It acts as a light client to the blockchain, as well as a recursive name server. It can serve provable resource records without having the resource requirements of a full node. Project Paper This document is a high-level overview of the Handshake protocol. For a more detailed description of the protocol, please read the project paper .","title":"Handshake"},{"location":"rwot8/topics-and-advance-readings/handshake/#handshake","text":"by Boyma Fahnbulleh \\< boymanjor@protonmail.com >","title":"Handshake"},{"location":"rwot8/topics-and-advance-readings/handshake/#introduction","text":"Handshake is a decentralized, permission-less naming protocol compatible with DNS. We seek to solve Zooko's triangle through the use of a utxo-based blockchain, which manages the registration, renewal, and transfer of DNS top-level domains (TLDs). The initial goal is not to replace the DNS protocol but to replace the root zone file and the root servers with a decentralized, public commons. By tying name ownership to a utxo, and embedding DNS records into its metadata, a chain of trust can be created by a digital signature and verified by querying blockchain data. A decentralized network of validating peers anchors this chain of trust. The ultimate goal is to provide an alternative to existing Certificate Authorities. Our naming protocol differs from its predecessors in that it has no concept of subdomains at the consensus layer. The protocol also provides secure name resolutions via light client resolvers by committing to name data in the block headers and using compact, merklelized proofs of inclusion and non-inclusion. We believe this project could be used to add human readability to DIDs by committing to them in the DNS records of names registered on the network. Or the names themselves could exist as DIDs and DID documents could be committed to in the DNS records.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/handshake/#name-auctions","text":"Second-price, blind bid (Vickrey) auctions manage the issuance of names on-chain. A consensus-level covenant system facilitates these auctions by encumbering utxos with spending restrictions. A collection of these covenants model an auction state machine. For example, to bid on a name, one must spend a utxo in a transaction which creates an output which carries a 'BID' covenant. This output will have a value which is equal to (or higher) than the amount the user intends to bid on the name. It will also include the name up for auction and a commitment to the actual bid amount. The 'BID' covenant will restrict this output from being spent in any transaction that does not create an output with a 'REVEAL' covenant. The 'REVEAL' covenant essentially reveals the actual bid amount and allows the excess value in the 'BID' covenant to be spent freely. The rest of the auction process, as well as DNS record updates and name transfers, are modeled in this covenant system.","title":"Name Auctions"},{"location":"rwot8/topics-and-advance-readings/handshake/#network-bootstrapping","text":"Consensus rules reserve all entries in ICANN's existing root zone file. Names in the list of Alexa top 100,000 domains are also reserved. The latter names are converted to TLDs by selecting their first domain name label. Name owners can bypass the auction system and claim names through the use of DNSSEC ownership proofs. We also have a sunrise period to allow trademark holders without domains in the Alexa top 100,000 to reserve names. The DNSSEC ownership proofs mentioned above are a stricter subset of DNSSEC proofs . They do not allow for CNAME glue or wildcards, and every label must be separated by a zone cut using a typical DS-to-DNSKEY setup for referrals. All zone referrals are retrieved and combined to produce the final proof. These proofs must stem from ICANN's key-signing keys (KSKs) to the final ZSK in the target zone. The ultimate zone-signing key (ZSK) must sign a TXT record which commits to the name's desired address on the blockchain. The proof is broadcast to the peer-to-peer network and included by miners in the coinbase transaction of a block. The consensus rules dictate that the miner must create an output for the associated proof, granting the name to the committed address.","title":"Network Bootstrapping"},{"location":"rwot8/topics-and-advance-readings/handshake/#software","text":"hsd is a full node, Javascript implementation of the protocol and an authoritative name resolver for the root zone. hnsd is an SPV name resolver written in C. It acts as a light client to the blockchain, as well as a recursive name server. It can serve provable resource records without having the resource requirements of a full node.","title":"Software"},{"location":"rwot8/topics-and-advance-readings/handshake/#project-paper","text":"This document is a high-level overview of the Handshake protocol. For a more detailed description of the protocol, please read the project paper .","title":"Project Paper"},{"location":"rwot8/topics-and-advance-readings/humanity-API/","text":"Humanity's API - solving Fermi paradox. Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: Fermi paradox, hypothesis, graph analysis, humanity There is a well known Fermi paradox [1]. It goes like this. There are millions or even billions Earth-like planets in the Galaxy, many of them are much older than the Earth, by millions, billions of years. If the life on Earth is typical, then some of these planets may have developed intelligent life who have developed interstellar travel. Even at the slow pace of currently envisioned interstellar travel, the Milky Way galaxy could be completely traversed in a few million years. According to this line of reasoning, the Earth should have already been visited by extraterrestrial aliens. So, where are the aliens? Or, at least, why do they not contact us? There are a lot of hypothesis that try to explain this paradox. Some of them blame the \"no-contact situation\" on the absence of \"hard\" technologies that enable interstellar travel or communication. They say that interstellar travel/communication is too difficult, it requires too much energy or precision. We have not figured out how to do it yet. Because of this we cannot find others and they cannot find us or they do not want to contact us until we have such technology. As we now begin to understand, the exoplanet detection and the interstellar travel is not that hard even for current human technologies [2,3]. Then assuming we are not that unique and the intelligent life is common, why we have not been contacted already. Surely much older and advanced civilizations, noticed our planet brimming with life millions of years ago. Would it be possible, that the reason we have not been contacted yet, does not lie in the area of \"hard\" technologies of interstellar communication, but in the area of \"soft\" technologies - in our inability to understand ourselves, to understand how humans work on individual, group and especially on global level. On the individual level, we do not really know how brain biology and human psychology works. On the group level, we do not really know how groups work - we do not know how to effectively solve tragedy of commons, how to regulate local and national economies. On the global level - we have the same unknowns as on the individual and the group levels just taken to another level of complexity because of the scale and additional inter-group problems (international relations). I propose the hypothethis, that the correct (scientifically proven) understanding of how humans work (as individuals and as small, large and global groups) and consequently effective local and global human cooperation is the prerequisite for us to be abele to communicate with the interstellar civilizations, and probably to be contacted by them. This is especially related to our understanding of the humanity. We cannot expect to be contacted by the advanced interstellar civilizations, if we do not have clear view and understanding on how humanity works and what humanity means/does/how it regulates as a whole social organism. Before anyone (including ourselves) can talk with humanity we need to have good answers to the questions - \"What it means to be a humanity? What it means to talk to humanity?\". From the software engineer perspective, in order to be able to talk to humanity, we need to understand humanity's architecture and build \"API\" endpoints to it. First of all, we would need this \"Humanity's API\" not just to communicate with aliens, but for our own sake to be able to effectively cooperate on the global scale. Humanity's API should be: * inclusive - provide access to all people who wants to connect * representative - have most of the people or at least representative part of all people connected * real-time - any human who is connected and willing to communicate should be able to do it in real time * robust/antifragile - be decentralized, not have \"one point of failure\" * transparent/visible - everyone should be able to see who, how, why is connected / have access to it * easy/convenient/understandable - any literate person should be able to use it and learn how it works * censorless - it should not be possible to shut down Humanity's API or censor access to it * two-way - API should allow post and receive information from humanity. In other words you should be able to talk to humanity, and humanity should be able to talk to you. In the real-life, from human perspective, Humanity API will probably look very similar to the current Internet forums (Twitter, Reddit, StackExchange) and search services. You will be able to post/receive question and answer, and participate in the debates with it. There will be 2 big differences in how Humanity API differ from the current Internet forums: * assurance of humannes - you will be sure that you talk with humans and you will know how they are connected to other humans, who they represent * autorouting/aggregation/representativeness - there will be a trusted reliable algorithm that will be able to route requests to correct humans or groups of humans and when necessary correctly aggregate responses from many humans in the correct/representative and meaningful for the recipient way. For Humanity's API to be able to do this, it will have to be based on a backend able to do high performance near real time analysis of large graphs.* In some cases such graph analysis is already possible with current technologies (e.g. Google search), but the near real time advanced analysis of large graphs will probably require quantum computers. The recent practical developments of quantum computing give realistic hope that this will be possible in the near future[4]. When we build \"Humanity's API\" and get accustomed to using it for our own purposes, then one day we can expect to receive a message from aliens to one of API endpoints (from human user perspective it will probably appear in the Humanity's main discussion room). The message will say something like this - \"Hello humanity! Glad you figured out this contactability stuff. Welcome to the intergalactic community of contactable intelligent life. Join the conversation, it's happening out here! Your closest galactic neighbor.\"** References [1] Fermi paradox - https://en.wikipedia.org/wiki/Fermi_paradox [2] Kepler project- https://en.wikipedia.org/wiki/Kepler_(spacecraft) [3] Breakthrough Starshot - https://en.wikipedia.org/wiki/Breakthrough_Starshot [4] Quantum computing news - https://www.google.com/search?q=quantum+computing+news+optimization+problems Notes * Advanced graph analysis is probably also required to understand how brain works. Of course the limit on speed of light will still be there - but hopefully aliens sent us the rich snapshot of the intergalactic internet, waiting around to connect to our \"API\" . I'd be really surprised if there is no intergalactic community of contactable intelligent life out there. We cannot expect to be the first or the smartest, can we? Even if we are, we will have to create such community or if there is no one out there become one.","title":"Humanity's API - solving Fermi paradox."},{"location":"rwot8/topics-and-advance-readings/humanity-API/#humanitys-api-solving-fermi-paradox","text":"Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: Fermi paradox, hypothesis, graph analysis, humanity There is a well known Fermi paradox [1]. It goes like this. There are millions or even billions Earth-like planets in the Galaxy, many of them are much older than the Earth, by millions, billions of years. If the life on Earth is typical, then some of these planets may have developed intelligent life who have developed interstellar travel. Even at the slow pace of currently envisioned interstellar travel, the Milky Way galaxy could be completely traversed in a few million years. According to this line of reasoning, the Earth should have already been visited by extraterrestrial aliens. So, where are the aliens? Or, at least, why do they not contact us? There are a lot of hypothesis that try to explain this paradox. Some of them blame the \"no-contact situation\" on the absence of \"hard\" technologies that enable interstellar travel or communication. They say that interstellar travel/communication is too difficult, it requires too much energy or precision. We have not figured out how to do it yet. Because of this we cannot find others and they cannot find us or they do not want to contact us until we have such technology. As we now begin to understand, the exoplanet detection and the interstellar travel is not that hard even for current human technologies [2,3]. Then assuming we are not that unique and the intelligent life is common, why we have not been contacted already. Surely much older and advanced civilizations, noticed our planet brimming with life millions of years ago. Would it be possible, that the reason we have not been contacted yet, does not lie in the area of \"hard\" technologies of interstellar communication, but in the area of \"soft\" technologies - in our inability to understand ourselves, to understand how humans work on individual, group and especially on global level. On the individual level, we do not really know how brain biology and human psychology works. On the group level, we do not really know how groups work - we do not know how to effectively solve tragedy of commons, how to regulate local and national economies. On the global level - we have the same unknowns as on the individual and the group levels just taken to another level of complexity because of the scale and additional inter-group problems (international relations). I propose the hypothethis, that the correct (scientifically proven) understanding of how humans work (as individuals and as small, large and global groups) and consequently effective local and global human cooperation is the prerequisite for us to be abele to communicate with the interstellar civilizations, and probably to be contacted by them. This is especially related to our understanding of the humanity. We cannot expect to be contacted by the advanced interstellar civilizations, if we do not have clear view and understanding on how humanity works and what humanity means/does/how it regulates as a whole social organism. Before anyone (including ourselves) can talk with humanity we need to have good answers to the questions - \"What it means to be a humanity? What it means to talk to humanity?\". From the software engineer perspective, in order to be able to talk to humanity, we need to understand humanity's architecture and build \"API\" endpoints to it. First of all, we would need this \"Humanity's API\" not just to communicate with aliens, but for our own sake to be able to effectively cooperate on the global scale. Humanity's API should be: * inclusive - provide access to all people who wants to connect * representative - have most of the people or at least representative part of all people connected * real-time - any human who is connected and willing to communicate should be able to do it in real time * robust/antifragile - be decentralized, not have \"one point of failure\" * transparent/visible - everyone should be able to see who, how, why is connected / have access to it * easy/convenient/understandable - any literate person should be able to use it and learn how it works * censorless - it should not be possible to shut down Humanity's API or censor access to it * two-way - API should allow post and receive information from humanity. In other words you should be able to talk to humanity, and humanity should be able to talk to you. In the real-life, from human perspective, Humanity API will probably look very similar to the current Internet forums (Twitter, Reddit, StackExchange) and search services. You will be able to post/receive question and answer, and participate in the debates with it. There will be 2 big differences in how Humanity API differ from the current Internet forums: * assurance of humannes - you will be sure that you talk with humans and you will know how they are connected to other humans, who they represent * autorouting/aggregation/representativeness - there will be a trusted reliable algorithm that will be able to route requests to correct humans or groups of humans and when necessary correctly aggregate responses from many humans in the correct/representative and meaningful for the recipient way. For Humanity's API to be able to do this, it will have to be based on a backend able to do high performance near real time analysis of large graphs.* In some cases such graph analysis is already possible with current technologies (e.g. Google search), but the near real time advanced analysis of large graphs will probably require quantum computers. The recent practical developments of quantum computing give realistic hope that this will be possible in the near future[4]. When we build \"Humanity's API\" and get accustomed to using it for our own purposes, then one day we can expect to receive a message from aliens to one of API endpoints (from human user perspective it will probably appear in the Humanity's main discussion room). The message will say something like this - \"Hello humanity! Glad you figured out this contactability stuff. Welcome to the intergalactic community of contactable intelligent life. Join the conversation, it's happening out here! Your closest galactic neighbor.\"**","title":"Humanity's API - solving Fermi paradox."},{"location":"rwot8/topics-and-advance-readings/humanity-API/#references","text":"[1] Fermi paradox - https://en.wikipedia.org/wiki/Fermi_paradox [2] Kepler project- https://en.wikipedia.org/wiki/Kepler_(spacecraft) [3] Breakthrough Starshot - https://en.wikipedia.org/wiki/Breakthrough_Starshot [4] Quantum computing news - https://www.google.com/search?q=quantum+computing+news+optimization+problems","title":"References"},{"location":"rwot8/topics-and-advance-readings/humanity-API/#notes","text":"* Advanced graph analysis is probably also required to understand how brain works. Of course the limit on speed of light will still be there - but hopefully aliens sent us the rich snapshot of the intergalactic internet, waiting around to connect to our \"API\" . I'd be really surprised if there is no intergalactic community of contactable intelligent life out there. We cannot expect to be the first or the smartest, can we? Even if we are, we will have to create such community or if there is no one out there become one.","title":"Notes"},{"location":"rwot8/topics-and-advance-readings/identity-containers/","text":"Sikaris - Decentralized Identity with Identity Containers (SSI) Let's make DIDs and ERC725 work together. Full proposal : Sikaris Description An implementation proposal for a Self Sovereign Identity (SSI) network using Blockchain as a DKPI Long story short, We suggest to store just a Merkle Root of the keys and claims being holded by an entity (user, organization..,) and set a list standard endpoints to query and interact between entities. This way most of the information is transferred off-chain and the whole system should be GDPR compliant. Digital ID - SSI Nodes Schema : A Digital ID is composed by : DID . A unique Decentralized Identifier. This identifier includes information about the plaform (network) the identity is deployed to. DID Document . For every DID We will have a DID Document containinng the basic information the entity needs to A&A. ID Container . Every entity will have a container holding all its data. Smart Contract . This SC will become our Digital Twin in the Blockchain Platform. Once deployed, the resulting address of the SC will be part of the DID. Keys . List of Keys owned by the entity. Verifiable Claims . List of Verifiable Claims. So far nothing new :) 1. Architecture Every entity will be linked to a SSI Node where her/his/its ID will reside inside one ID Container. It can be in a single user node, a pool of Ids (many ID containers in a node) or maybe a professional SSI Node with extra services (recovery, safe storage...). This means that some nodes in the Blockchain network will be acting as Identity SSI Nodes, will store information and will be able to do some basic actions for the entity/ies, meaning these nodes will have an API to resolve some basic ID endpoints. All these SSI Nodes will be connected to each other. 1.1. ID Container Each Entity (one or many) in the Identity node will have an encrypted (and portable) sqlite file, the name of the database will be the DID itself and inside we are going to store (encrypted with the Public Encryption Key of the user) : Wallet : Management Key for the Entity to update the Smart Contract holding Identity information. DID Document : Modified version of the DID Doc. Keys : Public keys owned by the entity Claims : Verifiable Claims. Documents : docs sent by other entities and accepted by the user. Dapps Info : extra information regarding other dapps. ID Containers inside SSI Nodes become the Inbox for contacts, documents, keys and claims for the entity 1.2. Portability ID Containers will be easy to backup and/or move between SSI Nodes. Also with light nodes, Container IDs could even live in a Mobile Device. Any user could run a SSI Node with their container inside and at any tinme move it to a professional service. 1.3. Version 0.1 : Storing a JSON File as a Merkle Tree Instead of having a list of Keys in one Smart Contract in the Blockchain (ERC725) or a list of claims (ERC735) I propose to store only the Merkle Root of the lists, and provide with the necessary endpoints to show keys and verify claims. Most of the actions will happen off-chain . This way when asked about a particular leaf of the tree we can show specific information and prove its validity without compromising the hash of any other field. 1.4. Version 0.2. In a next stage, instead of storing Merkle Trees, we will use Zero Knowlege Proofs (zk-Snarks): The basic idea is to assign a Prime number to each Hash and then create a ZKP circiut with the multiplication of all these numbers (See more info here). In both versions, v0.1 and v0.2 we will refer to the value resulting as Root : Keys Root and Claims Root. 1.5. Identity Smart Contract (IDSC) For each ID Container (entity) we will deploy one ERC725 Smart Contract and one DID Document on IPFS. We are following the new ERC725 alliance specs where: 1. ERC725 is a Proxy contract holding d arbitrary data through a generic key/value store. Owner is stored at at key 0x000... 2. ERC734 is a list of Keys. 3. ERC735 is a list of Claims. For our Identity we need: Identity Management. How to update the SCID, who can do it. There's a Management Key that could be an account or another Smart Contract, so we could easily change to a more sophisticated Base Contract (RBAC) with roles and permissions (we could separate DIDs Management and Claims Management). We will store the owner at key 0x0000. DIDs management. IPFS address of the Did Doc where the Public-Public Keys are. We will use hash('did') as key. Private-Public Keys. a Merkle Root or a zk-Snark contract. No Private information is ever shared in the Blockchain. We are not sharing any information about our keys but we can prove at any time that we own them. This way we can also group Keys by using a diferent key to store it :hash('keys1'), hash('login-keys')... Claims Management. List of Claims isued by the entity. For the same reasons, instead of storing a list of claims (like in the ERC735 proposal), we will save in the contract the Claims Merkle Root (or kz-Snark). This way we can also group Keys by using a diferent key to store it :hash('claims1'), hash('personal-claims')... 2. Personal Information DIDs, Keys, Claims, Contacts, Documents and Dapps. On this first version we will allow Identity Nodes to act as APIs, so they will be able to answer questions about the identity (claims, docs...). But then I need to link an IP address to a particular ID (less privacy). We could will build a P2P network based on Libp2p to be able to send and receive messages to a particular DID without knowing where the DID is located (its IP address), but that would flood the netork with too many messages. Also Whisper looked like a good idea (We still think it is) using a topic just for Identity. All communications with another DID will be sent with its ID container and encrypted with the public key of the destiny entity. Before any information oculd be shared a Contact Handshake must be done. During this process, both entities agree on stablishing a Secure Id Channel between both. For each channel (entity we are connected to), the Container ID will store the permission allowed for each claim, document and key. 2.1 DID Document. We will start with a basic implementation of DIDs. { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:ala:123456789abcdefghi\", \"publicKey\": [{ \"id\": \"did:example:123456789abcdefghi#login\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:example:123456789abcdefghi#encryption\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }], \"authentication\": [{ \"publicKey\": \"did:ala:123456789abcdefghi#login\" }], \"service\": [{ \"id\": \"did:ala:123456789abcdefghi;did\", \"type\": \"IdContainerService\", \"serviceEndpoint\": \"/ip4/caelumlabs.com/user/1223\" }], \"created\": \"2018-10-10T17:00:00Z\", \"updated\": \"2018-10-12T17:00:00Z\" } There are a few differences with the Original DIDs specs. The idea came after speaking with Victor Bjelkholm about the LibP2P (IPFS) proposal for multiformats . Multiaddr is a format for encoding addresses from various well-established network protocols. It is useful to write applications that future-proof their use of addresses, and allow multiple transport protocols and addresses to coexist. This file will be stored on IPFS. The address will be stored on the Identity Contract (ERC725) 2.2. Private Public Keys At the Identity Node we will store a list of all the keys belonging to the entity (as a Merkle Tree or as a zk-Snark). Keys Root . At the Container ID, these keys will be stored (and shared when necessary) using this format. It can be used for Wallets where we don't want anyone to relate the balance in an ERC20 contract and our DID. \"publicKey\": { \"type\": \"Ed25519VerificationKey2018\", \"id\": \"did:ala:123456789abcdefghi#keys-1\", \"owner\": \"did:ala:123456789abcdefghi\", \"publicKeyBase58\": \"H3C2AVvLMv6gmMNam3uVAjZpfkcJCwDwnZn6z3wXmqPV\" } 2.3. Verifiable Claims We'll use the W3C Verifiable Claims Data Model and Representations Spec - entity : A thing with distinct and independent existence such as a person, organization, concept, or device. - subject : An entity about which claims may be made. - claim : A statement made by an entity about a subject. A verifiable claim is a claim that is effectively tamper-proof and whose authorship can be cryptographically verified. Multiple claims may be bundled together into a set of claims. We will start with self signed Verified Claims. { \"@context\": \"https://w3id.org/security/v1\", \"id\": \"did:ala:1234...#claim-1\", \"type\": [\"Credential\", \"ProofOfAgeCredential\"], \"issuer\": \"did:ala:8881111...\", \"issued\": \"2018-10-01\", \"claim\": { \"id\": \"did:example:1234...\", \"ageOver\": 21 }, \"signature\": { \"type\": \"LinkedDataSignature2015\", \"created\": \"2016-06-18T21:19:10Z\", \"creator\": \"did:ala:8881111...#key-1\", \"domain\": \"json-ld.org\", \"nonce\": \"598c63d6\", \"signatureValue\": \"BavEll0/I1zpYw8XNi1bgVg/sCneO4Jugez8RwDg/+ MCRVpjOboDoe4SxxKjkCOvKiCHGDvc4krqi6Z1n0UfqzxGfmatCuFibcC1wps PRdW+gGsutPTLzvueMWmFhwYmfIFpbBu95t501+rSLHIEuujM/+PXr9Cky6Ed +W3JT24=\" } } Here an issuer (did:ala:8881111...) is verifying with a public key owned by the entity (did:ala:8881111...#key-1) available at the DDOC, that one persona (did:ala:1234...) is over 21. The Id of the claim is did:ala:1234...#claim-1 All the claims will be used to build the Claims Merkle Root or the zk-snark : Claims Root In this proposal Claims work in one direction only. Issuer is the responsible to create a Claim, send the claim to the entity he's claiming something about and then updating the Claims Root in its own Smart Contract. Revocation is as easy as deleting the hash of the claim from its own Claim Root. 2.4. Contacts Before asking for any information about another entity we need to stablish a Secure Id Channel netween both entites. Any communication in this channel will be encrypted using the Encryption Key avaliable at the DID Document. To stablish a contact both entites must verify their own identities first and agree on the creation of the channel. Once the Channel is ready I can ask for a Claim, a Document and/or a Key : Shares. Information will be shared when: 1. Entity (verified) asks for a Key, a Claim or a Document. 2. Which Information is required : Name, Proof, Document.... 3. How the data is going to be used (how and what for) 4. For how long The contacts will be stored on each Id Container. 2.5. Documents The Id Container will act as an Inbox for certified documents. First we need to stablish a Secure Id Channel, and after that I can request to put or get a document. All the documents will be stored in the Id Container. Example : I make a payment to an ERC725, and the entity sends me an Invoice for that payment to my Id Inbox for documents. 2.6. Dapps Any Dap can use the Id Container to store personal information of the user. 3. Recommended readings SSI by Christpher Allen Decentralized Identifiers (DIDs) Verifiable Claims DID Auth IPFS & LibP2P Multiformats ERC proposals : ERC725 , ERC735 , ERC780 , ERC1056 , ERC1484","title":"Sikaris - Decentralized Identity with Identity Containers (SSI)"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#sikaris-decentralized-identity-with-identity-containers-ssi","text":"Let's make DIDs and ERC725 work together. Full proposal : Sikaris","title":"Sikaris - Decentralized Identity with Identity Containers (SSI)"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#description","text":"An implementation proposal for a Self Sovereign Identity (SSI) network using Blockchain as a DKPI Long story short, We suggest to store just a Merkle Root of the keys and claims being holded by an entity (user, organization..,) and set a list standard endpoints to query and interact between entities. This way most of the information is transferred off-chain and the whole system should be GDPR compliant.","title":"Description"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#digital-id-ssi-nodes","text":"Schema : A Digital ID is composed by : DID . A unique Decentralized Identifier. This identifier includes information about the plaform (network) the identity is deployed to. DID Document . For every DID We will have a DID Document containinng the basic information the entity needs to A&A. ID Container . Every entity will have a container holding all its data. Smart Contract . This SC will become our Digital Twin in the Blockchain Platform. Once deployed, the resulting address of the SC will be part of the DID. Keys . List of Keys owned by the entity. Verifiable Claims . List of Verifiable Claims. So far nothing new :)","title":"Digital ID - SSI Nodes"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#1-architecture","text":"Every entity will be linked to a SSI Node where her/his/its ID will reside inside one ID Container. It can be in a single user node, a pool of Ids (many ID containers in a node) or maybe a professional SSI Node with extra services (recovery, safe storage...). This means that some nodes in the Blockchain network will be acting as Identity SSI Nodes, will store information and will be able to do some basic actions for the entity/ies, meaning these nodes will have an API to resolve some basic ID endpoints. All these SSI Nodes will be connected to each other.","title":"1. Architecture"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#11-id-container","text":"Each Entity (one or many) in the Identity node will have an encrypted (and portable) sqlite file, the name of the database will be the DID itself and inside we are going to store (encrypted with the Public Encryption Key of the user) : Wallet : Management Key for the Entity to update the Smart Contract holding Identity information. DID Document : Modified version of the DID Doc. Keys : Public keys owned by the entity Claims : Verifiable Claims. Documents : docs sent by other entities and accepted by the user. Dapps Info : extra information regarding other dapps. ID Containers inside SSI Nodes become the Inbox for contacts, documents, keys and claims for the entity","title":"1.1. ID Container"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#12-portability","text":"ID Containers will be easy to backup and/or move between SSI Nodes. Also with light nodes, Container IDs could even live in a Mobile Device. Any user could run a SSI Node with their container inside and at any tinme move it to a professional service.","title":"1.2. Portability"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#13-version-01-storing-a-json-file-as-a-merkle-tree","text":"Instead of having a list of Keys in one Smart Contract in the Blockchain (ERC725) or a list of claims (ERC735) I propose to store only the Merkle Root of the lists, and provide with the necessary endpoints to show keys and verify claims. Most of the actions will happen off-chain . This way when asked about a particular leaf of the tree we can show specific information and prove its validity without compromising the hash of any other field.","title":"1.3. Version 0.1 : Storing a JSON File as a Merkle Tree"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#14-version-02","text":"In a next stage, instead of storing Merkle Trees, we will use Zero Knowlege Proofs (zk-Snarks): The basic idea is to assign a Prime number to each Hash and then create a ZKP circiut with the multiplication of all these numbers (See more info here). In both versions, v0.1 and v0.2 we will refer to the value resulting as Root : Keys Root and Claims Root.","title":"1.4. Version 0.2."},{"location":"rwot8/topics-and-advance-readings/identity-containers/#15-identity-smart-contract-idsc","text":"For each ID Container (entity) we will deploy one ERC725 Smart Contract and one DID Document on IPFS. We are following the new ERC725 alliance specs where: 1. ERC725 is a Proxy contract holding d arbitrary data through a generic key/value store. Owner is stored at at key 0x000... 2. ERC734 is a list of Keys. 3. ERC735 is a list of Claims. For our Identity we need: Identity Management. How to update the SCID, who can do it. There's a Management Key that could be an account or another Smart Contract, so we could easily change to a more sophisticated Base Contract (RBAC) with roles and permissions (we could separate DIDs Management and Claims Management). We will store the owner at key 0x0000. DIDs management. IPFS address of the Did Doc where the Public-Public Keys are. We will use hash('did') as key. Private-Public Keys. a Merkle Root or a zk-Snark contract. No Private information is ever shared in the Blockchain. We are not sharing any information about our keys but we can prove at any time that we own them. This way we can also group Keys by using a diferent key to store it :hash('keys1'), hash('login-keys')... Claims Management. List of Claims isued by the entity. For the same reasons, instead of storing a list of claims (like in the ERC735 proposal), we will save in the contract the Claims Merkle Root (or kz-Snark). This way we can also group Keys by using a diferent key to store it :hash('claims1'), hash('personal-claims')...","title":"1.5. Identity Smart Contract (IDSC)"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#2-personal-information","text":"","title":"2. Personal Information"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#dids-keys-claims-contacts-documents-and-dapps","text":"On this first version we will allow Identity Nodes to act as APIs, so they will be able to answer questions about the identity (claims, docs...). But then I need to link an IP address to a particular ID (less privacy). We could will build a P2P network based on Libp2p to be able to send and receive messages to a particular DID without knowing where the DID is located (its IP address), but that would flood the netork with too many messages. Also Whisper looked like a good idea (We still think it is) using a topic just for Identity. All communications with another DID will be sent with its ID container and encrypted with the public key of the destiny entity. Before any information oculd be shared a Contact Handshake must be done. During this process, both entities agree on stablishing a Secure Id Channel between both. For each channel (entity we are connected to), the Container ID will store the permission allowed for each claim, document and key.","title":"DIDs, Keys, Claims, Contacts, Documents and Dapps."},{"location":"rwot8/topics-and-advance-readings/identity-containers/#21-did-document","text":"We will start with a basic implementation of DIDs. { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:ala:123456789abcdefghi\", \"publicKey\": [{ \"id\": \"did:example:123456789abcdefghi#login\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }, { \"id\": \"did:example:123456789abcdefghi#encryption\", \"type\": \"RsaVerificationKey2018\", \"owner\": \"did:example:123456789abcdefghi\", \"publicKeyPem\": \"-----BEGIN PUBLIC KEY...END PUBLIC KEY-----\\r\\n\" }], \"authentication\": [{ \"publicKey\": \"did:ala:123456789abcdefghi#login\" }], \"service\": [{ \"id\": \"did:ala:123456789abcdefghi;did\", \"type\": \"IdContainerService\", \"serviceEndpoint\": \"/ip4/caelumlabs.com/user/1223\" }], \"created\": \"2018-10-10T17:00:00Z\", \"updated\": \"2018-10-12T17:00:00Z\" } There are a few differences with the Original DIDs specs. The idea came after speaking with Victor Bjelkholm about the LibP2P (IPFS) proposal for multiformats . Multiaddr is a format for encoding addresses from various well-established network protocols. It is useful to write applications that future-proof their use of addresses, and allow multiple transport protocols and addresses to coexist. This file will be stored on IPFS. The address will be stored on the Identity Contract (ERC725)","title":"2.1 DID Document."},{"location":"rwot8/topics-and-advance-readings/identity-containers/#22-private-public-keys","text":"At the Identity Node we will store a list of all the keys belonging to the entity (as a Merkle Tree or as a zk-Snark). Keys Root . At the Container ID, these keys will be stored (and shared when necessary) using this format. It can be used for Wallets where we don't want anyone to relate the balance in an ERC20 contract and our DID. \"publicKey\": { \"type\": \"Ed25519VerificationKey2018\", \"id\": \"did:ala:123456789abcdefghi#keys-1\", \"owner\": \"did:ala:123456789abcdefghi\", \"publicKeyBase58\": \"H3C2AVvLMv6gmMNam3uVAjZpfkcJCwDwnZn6z3wXmqPV\" }","title":"2.2. Private Public Keys"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#23-verifiable-claims","text":"We'll use the W3C Verifiable Claims Data Model and Representations Spec - entity : A thing with distinct and independent existence such as a person, organization, concept, or device. - subject : An entity about which claims may be made. - claim : A statement made by an entity about a subject. A verifiable claim is a claim that is effectively tamper-proof and whose authorship can be cryptographically verified. Multiple claims may be bundled together into a set of claims. We will start with self signed Verified Claims. { \"@context\": \"https://w3id.org/security/v1\", \"id\": \"did:ala:1234...#claim-1\", \"type\": [\"Credential\", \"ProofOfAgeCredential\"], \"issuer\": \"did:ala:8881111...\", \"issued\": \"2018-10-01\", \"claim\": { \"id\": \"did:example:1234...\", \"ageOver\": 21 }, \"signature\": { \"type\": \"LinkedDataSignature2015\", \"created\": \"2016-06-18T21:19:10Z\", \"creator\": \"did:ala:8881111...#key-1\", \"domain\": \"json-ld.org\", \"nonce\": \"598c63d6\", \"signatureValue\": \"BavEll0/I1zpYw8XNi1bgVg/sCneO4Jugez8RwDg/+ MCRVpjOboDoe4SxxKjkCOvKiCHGDvc4krqi6Z1n0UfqzxGfmatCuFibcC1wps PRdW+gGsutPTLzvueMWmFhwYmfIFpbBu95t501+rSLHIEuujM/+PXr9Cky6Ed +W3JT24=\" } } Here an issuer (did:ala:8881111...) is verifying with a public key owned by the entity (did:ala:8881111...#key-1) available at the DDOC, that one persona (did:ala:1234...) is over 21. The Id of the claim is did:ala:1234...#claim-1 All the claims will be used to build the Claims Merkle Root or the zk-snark : Claims Root In this proposal Claims work in one direction only. Issuer is the responsible to create a Claim, send the claim to the entity he's claiming something about and then updating the Claims Root in its own Smart Contract. Revocation is as easy as deleting the hash of the claim from its own Claim Root.","title":"2.3.  Verifiable Claims"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#24-contacts","text":"Before asking for any information about another entity we need to stablish a Secure Id Channel netween both entites. Any communication in this channel will be encrypted using the Encryption Key avaliable at the DID Document. To stablish a contact both entites must verify their own identities first and agree on the creation of the channel. Once the Channel is ready I can ask for a Claim, a Document and/or a Key : Shares. Information will be shared when: 1. Entity (verified) asks for a Key, a Claim or a Document. 2. Which Information is required : Name, Proof, Document.... 3. How the data is going to be used (how and what for) 4. For how long The contacts will be stored on each Id Container.","title":"2.4. Contacts"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#25-documents","text":"The Id Container will act as an Inbox for certified documents. First we need to stablish a Secure Id Channel, and after that I can request to put or get a document. All the documents will be stored in the Id Container. Example : I make a payment to an ERC725, and the entity sends me an Invoice for that payment to my Id Inbox for documents.","title":"2.5. Documents"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#26-dapps","text":"Any Dap can use the Id Container to store personal information of the user.","title":"2.6. Dapps"},{"location":"rwot8/topics-and-advance-readings/identity-containers/#3-recommended-readings","text":"SSI by Christpher Allen Decentralized Identifiers (DIDs) Verifiable Claims DID Auth IPFS & LibP2P Multiformats ERC proposals : ERC725 , ERC735 , ERC780 , ERC1056 , ERC1484","title":"3. Recommended readings"},{"location":"rwot8/topics-and-advance-readings/identity-for-people-places/","text":"Generating Actionable Intelligence with Wallets and Agents for People & Places Venn.Agency Sam Mathews Chase samantha@venn.agency Joni McKervey joni@venn.agency The purpose of this paper is to discuss building in the principals of sovereign identity systems for places as well as people to better manage multiple stakeholders and layers of access, while preserving both the saliency of digital information and the individual privacy and protections of the physical world. We posit that, by establishing place identity (\u200bspatial coordinate\u200b tokens) and instituting decentralized architecture for data exchange (wallets, verifiable claims, non-surveillant networks), we can build fully functioning sovereign identity systems and inspire their adoption at scale by organizations across multiple industries. We will explore the hypothesis that generating actionable intelligence without surveillance will: - Alleviate operational burden & risk of mass data collection for the organization - Protect the rights and sensitive personal information of employees - Create measurable improvements in safety, efficiency, trust and communication Acknowledging that there are many theoretic use cases and explorations of related solutions and yet few projects grounded in real-world testing, we will focus on the construction of such a system within the framework of a pilot currently under development. \u200bThe pilot proposes to build a local, non-surveillant system for workplace accountability, information management, access architecture, safety auditing and training for a large-scale industrial operation. This paper will explore the design of this system through two use cases (outlined below). Areas of focus: safety training, spatial awareness, proof of knowledge, and incentivizing safety practices that require lifestyle change like sleep and exercise. Use Case #1 \u2014 Establishing Spatial Identity with Coordinate Tokens We combine a photorealistic 3D scan of a building with its blueprints to create a digital model of the space. This model is overlain with a grid to which coordinates are assigned. Proposed actions for this model: - A tagging system that attaches information to coordinates on the grid - Tags may include: task assignments, reminders, warnings, permission requests, timers, security gates, log-ins and -outs, - Workstation or location-specific training tools and games - An immutable ledger of interactions in the space Questions to explore: - Who are the stakeholders (eg, employee, organization, insurance brokers, government, regulatory bodies) and how do they interact? - How is this model deployed so that anyone who needs to can access it with the level/layer of information relevant to them? - How are the layers of access and control managed? - How does a person attach a tag to the appropriate coordinates? - How does an employee\u2019s wallet interact with the information tagged to the grid? - How is this model and its information backed up? - What protocols can we create to establish Place Identity? - When does a Place ID get called? In what order? - How is a coordinate token issued? - How does a place use a wallet? Can that wallet represent the place autonomously? Use Case #2 \u2014 Safety through Incentivized Wellness It's unacceptable to give raw health data to employers or any organization that isn't explicitly caring for you medically. How can we design an identity manager that lets each user hold onto their own health data? I'm imagining that an insurer can acknowledge 3 different sleep app data, and 6 different heart rate monitors, these can be linked to the employee wallet, an employer incentivizes 5 hours or more of sleep a night and cardio exercise twice a week. The employer sends a schema overlay request to the employee wallet, the employee wallet runs the request against the data and returns either a yes or no token. Questions to explore: - What data should be shared equally between employer and employee? - What data should remain the employees\u2019 only? Why? - What data should remain the employers\u2019 only? Why? - What data should be shared organization-wide? Why?","title":"Identity for people places"},{"location":"rwot8/topics-and-advance-readings/idm-concept/","text":"Identity Manager Concept Authors : Andr\u00e9 Cruz, Jo\u00e3o Santos, Pedro Teixeira Introduction Research: standards & foundations Decentralized Identifiers Verifiable Credentials Using DID-Auth to prove control of the DID IDM - Identity Manager Preface IDM Wallet IDM Wallet UI IDM Client IDM Bridge Introduction The benefits of using P2P technologies, which enable truly decentralized applications to exist, are obvious to those well versed in the topic. However, in order to attain massive user adoption, these decentralized applications need to be better than the incumbents in almost every aspect -- usability being the one where they usually lack the most. Most of the cryptographic identity solutions out there rely on a single key pair where the public key identifies a single identity. While simple, this solution has many drawbacks, the most important one being that you can't recover your identity if the private key gets compromised. To explore potential solutions, the IPFS DDC WG decided to invest some time researching what decentralized identity solutions may be used. This paper is a follow-up of RFC Peer-Star-Identity . Research: standards & foundations Decentralized Identifiers Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, \"self-sovereign\" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. A DID always identifies a person, organization or thing. They have a specific syntax: <scheme>:<method>:<method-identifier> (e.g. did:ipid:a1B2c3d4E5 ). DIDs resolve to DID-Documents \u2014 simple documents that describe how to use that specific DID. Each DID-Document contains at least three things: Cryptographic Material, Authentication Suites and Service Endpoints. Cryptographic Material combined with Authentication Suites provide a set of mechanisms to authenticate as the DID subject (e.g. public keys, pseudonymous biometric protocols, etc.). Service Endpoints enable trusted interactions with the DID subject. In the real world, people may use different devices to interact with others. Each of those should have their public keys listed in the DID-Document, making it possible to add and remove them as needed while keeping the same DID. Granular permissions & authentication As previously stated, people use different devices in their daily lives. Each of those devices have different security guarantees. As an example, a device in your home is less likely to be lost or stolen when compared to a mobile phone. On the other hand, a mobile phone may have its storage encrypted which is more secure than an unencrypted one. Having that said, the level of permissions should be considered when adding a new device. Moreover, if a device is going to be used for authentication, its public key should be listed in the authentication field of the DID-Document. This is necessary for DID-Auth or other authentication mechanisms to assert that the device is valid for authentication. Verifiable Credentials Verifiable Credentials is a format for interoperable, cryptographically-verifiable digital credentials. DIDs begin by being \"trustless\" in the sense that they don't directly provide meaningful identity attributes. But trust between DID-identified peers can be built up through the exchange of verifiable credentials - credentials about identity attributes that include cryptographic proof. These proofs can be verified by reference to the issuer's DID and DID-Document. Because the ecosystem is still in its infancy and there's a lack of trusted issuers, identities may self-issue credentials. More specifically, they may issue credentials that define personal attributes about themselves, like their name and birthdate, and credentials that prove they own certain profiles on social networks, similar to how Keybase does. As of today, many people trust the mainstream social networks, such as Facebook and Twitter, and identities may use them to post cryptographic proofs that link their profiles to a hash of their DID. As time passes by and the ecosystem gets mature, identities will hold credentials issued by others that may be used in a variety of scenarios and purposes. Using DID-Auth to prove control of the DID DID-Auth is an handshake ceremony where an entity proves control over an identity to a relying party. In an authentication scenario between an user and an application, the application may want to ensure that the entity is in control of the DID it presented. Similarly, the user being authenticated may also check if the application's DID is in the application's control. Another typical scenario is when an entity requests access to some private data and the holder of the data needs check if that entity is in control of the identity's DID before handing the key to decrypt that data. In both scenarios, verifiable credentials may be exchanged so that the credibility of those identities may be evaluated during the handshake. IDM - Identity Manager Preface While DID's, Verifiable Credentials and DID-Auth provide interoperable models for common use-cases, the reality is that current identity wallets are closed in their own DID-method ecosystems. On one hand, applications wanting to embrace identities using different DID-methods have to integrate with different identity wallets, such as uPort and blockstack . These wallets have SDKs with different APIs, increasing integration complexity and crippling adoption. On the other hand, users are asked to authenticate using specific DID-methods because the application they are interacting with is limited by the DID-methods they support. Moreover, and often as a consequence, users are required to use multiple wallets to manage different identities they own. As an example, an user that owns its persona's DID and its company's DID must use different wallets in case they were created using different DID-methods. The Identity Manager is a unified identity wallet that aims to support multiple DIDs and multiple DID-methods, where: Users create and import identities from different DID-methods Users manage the verifiable credentials of the identities they have, with an initial focus on self-issued credentials Users manage the list of devices for each identity, providing the ability to revoke a compromised device Users manage the list of applications they have interacted with, providing the ability to revoke an application All the identities' data, such as its verifiable credentials, devices list and applications list, is stored encrypted The same identities' data is replicated across wallets to be kept in sync Interactions between identities and applications, such as authenticating, signing and verification of signatures, are mediated by the wallet Components, interfaces and protocols are detailed and explained in an open specification, allowing anyone to implement their own IDM From a higher-level perspective, the Identity Manager - or IDM for short - is composed of four components: IDM Wallet, IDM Wallet UI, IDM Client and IDM Bridge: IDM Wallet The IDM Wallet is similar to the physical wallet you carry everyday. It contains all the digital identities of its holder and all the information attached to them in a secure and encrypted way. Note that the IDM Wallet is an headless component, meaning it has no graphical user interface. Anyone wanting to build such GUIs, may choose from a variety of SDKs written in different programming languages, all based on the IDM Wallet spec . More information about the IDM Wallet UI can be found later on this document. There will be a reference IDM Wallet written in JavaScript, suitable to use inside a browser. Locker The IDM Wallet will have a locking mechanism to protect its access. A variety of locking types may be used to unlock the wallet, such as a passphrase, pattern, typeid, fingerprint, faceid, and other biometric validators. The locker holds a single secret that will be encrypted for each locking type. That single secret will be used to encrypt and decrypt all the data that lives inside the wallet. Managing identities Users will be able to create identities using their preferred DID-method or import existing ones. They will be guided throughout the process in the UI according to the chosen DID-method. The outcome of the creation of an identity will be a new DID, a Master Key Pair and a Device Key Pair where their correspondent public keys are listed in the DID-Document. The Master Private Key is in complete control of the DID-Document and should be used as little as possible. For that reason, it should be stored outside the IDM Wallet in a secure and recoverable way. One of those ways is via a Paper Key, where the Master Private Key is printed using machine readable representations, such as a QR code. Shamir's Secret Sharing may also be used to split the Master Private Key into different secrets that can be shared with trustees, like family and close friends. Even if the Paper Key is lost, users may recover their Master Private Key by collecting those secrets from them. For the import case, a Device Key Pair will be created and its public key added to the DID-Document as well. Depending on the DID-method, the Master Private Key might be needed to update the DID-Document. Some DID-methods might support granular permissions. This enables users to grant a specific set of permissions to different devices based on how secure the device is and how likely it is to be lost or robbed. Managing verifiable credentials Users will be able to view, add and remove Verifiable Credentials to and from their wallet. As we previously explained in the Verifiable Credentials section, the IDM Wallet will initially focus on self-issued credentials and expand to credentials issued by others when the ecosystem gets mature. Depending on the identities' DID-methods and their capabilities, where and how the credentials are stored might be pre-defined by the DID-method and may be either off-chain or on-chain. As an example, EIP780 proposes an on-chain registry for Ethereum based identities to store credentials, enabling persons, smart contracts, and machines to issue credentials about each other. The IDM Wallet will employ best-practices for each DID-method when storing credentials. Authentication on applications Most applications need to know the identity of the user so that they can provide a customized experience. Applications may start an authentication process, issuing an authentication request to an IDM Wallet, containing the application DID and Verifiable Credentials with its details, such as the application name, and the credentials it wants to receive from an identity. The IDM Wallet will perform DID-Auth to ensure that the application is in control of the DID it presented. If successful, the user is then prompted to select an identity and to accept what the application is asking for. If the user accepted the prompt, a new session is created and will be used for any further interactions between that application and that IDM Wallet. The session is composed of a key-pair where the public key is known by both, serving as the session identifier, while the private key is only known by that IDM Wallet. Additionally, all sessions have a max-age, meaning they expire as time passes. Signing artifacts on applications Applications may want to sign artifacts, such as regular data or ephemeral keys. This will be possible in two different ways: Sign with the Session Private Key Sign with the Device Private Key The first method is less intrusive and happens transparently to the user, but is also less secure. More specifically, a robber who steals an IDM Wallet may still sign artifacts with the Session Private Key as long as a session is valid because that key is stored unencrypted. Verifiers will still see those signatures as valid until the DID owner revokes the stolen device. The second method is more secure as the user is prompted to unlock IDM to decrypt the Device Private Key, but it is more intrusive. The interaction is similar to the authentication process, where the user is prompted inside the IDM Wallet to sign. Because the Device Private Key is encrypted, this gives the DID owner plenty of time to revoke that key in another IDM Wallet. Ultimately, the application may choose between both methods for different situations depending on the security degree they want to have. Verifying signatures Any party might verify the authenticity and authorship of artifacts signed by others. The verification of an artifact signed by the Session Public Key is performed by: Verifying the signature of the artifact against the Session Public Key of the author. Verifying the signature of the Session Public Key against the Device Public Key of the author. This proves that the Session Public Key was authorized by the Device Private Key. Verifying if the Device Public Key of the author is present in the DID-Document associated with the DID. This proves that the Device Public Key was authorized by the author's identity. If Device Public Key is flagged as revoked, compare its revocation effective date with the signature date The verification of an artifact signed by the Device Public Key is performed by: Verifying the signature of the artifact against the Device Public Key of the author. Verifying if the Device Public Key of the author is present in the DID-Document associated with the DID. This proves that the Device Public Key was authorized by the Identity (author). If Device Public Key is flagged as revoked, compare its revocation effective date with the signature date Managing application sessions Users will be able to revoke one or all application sessions from within the identity's applications list. After revoking a session, the application will no longer be able to use that session, effectively stopping it from being able to perform operations that require the Session and Device Private Keys. Revoking a device Users must be able to revoke a device associated with an identity. To do so, the DID-Document needs to be updated in order to signal that the Device Public Key was revoked, including the effective date which is important when verifying signatures. Depending on the DID-method and the permissions of the Device Private Key, higher level keys or other processes might be necessary to update the DID-Document, such as using the Master Private Key. The way a public key is declared as revoked is yet to be standardized in the DID spec. Nevertheless, a DID-Document may be augmented via the @context attribute to support having that information. Replication and synchronization The same identity might be held inside different IDM Wallets. For that reason, the identities' data, such as its credentials and applications, will be seamlessly replicated to other IDM Wallets using P2P technologies powered by CRDTs . Any data stored off-chain or on-chain as per the DID-method best-practices will be taken into consideration, meaning they will be synchronized accordingly. IDM Wallet UI End users need a graphical user-interface to interact with their IDM Wallet. An IDM Wallet UI is an application that will make use of the most suitable IDM Wallet SDK for the programming language they choose. Even though anyone can implement its own IDM Wallet UI application, there is a need to define some standards in regards to user journeys, user experience and interactions. For that reason, there will be a reference IDM Wallet UI based on web-technologies to be used inside a browser, that can be packed as an Electron in the future. Reference UI diagrams User-journeys diagram: Information Architecture diagram: Reference UI concept The goal of the reference UI is to have a really polished interface with a premium feel. It must be easy to use, distinct, reliable, accessible and provide a feel of credibility to the users. IDM Client Applications need to interact with the IDM Wallet. This process will be facilitated by the IDM Client, which will be available as multiple SDKs for different programming languages. Each SDK will provide a simple and intuitive interface based on the IDM Client Spec and contain means to authenticate, unauthenticate, sign, verify signatures, amongst others. There will be a reference IDM Client written in JavaScript, suitable to use inside a browser. IDM Bridge While applications use the IDM Client to interact with an IDM Wallet, the way they reach each other and communicate is handled by the IDM Bridge. Applications run on a variety of contexts, from within browsers to native applications. An IDM Wallet might coexist in the same context as these applications or, more often, in different contexts and even equipments. Below there's a list of possible scenarios and respective solutions: Both an Application and an IDM Wallet running on the same browser, on the same equipment: Solution: Use an iframe and the postMessage API An Application running on a browser and an IDM Wallet running as a native OS application, on the same equipment Solution: The IDM Wallet exposes a local WebSocket server on a pre-defined port known by the IDM Client Both the Application and IDM Wallet are native OS applications, running on the same equipment Solution: Use inter-process communication, preferring mechanisms offered by the OS An Application running on a laptop and an IDM Wallet running on a smartphone Solution: Use IPFS's pubsub to find and talk to each other Conceptually, the IDM Bridge is composed by two parts: the provider-side and the consumer-side. The provider-side is embedded in the IDM Wallet while the consumer-side is embedded in the IDM Client. Each one of the sides have a set of transports that they support to communicate. Moreover, the IDM Client has a discovery mechanism that finds the most appropriate IDM Wallet to talk to, starting by locating one closer to its own context that also supports one of its transports. For reference, there are various degrees of closeness, from closest to the furthest: exactly the same context (e.g.: within same browser), same machine, same network or different network. The discovery is likely to be more transparent and automatic if both sides are close to each other. For example, in scenario 2 , the IDM Client can try to initiate a WebSocket connection to localhost:<predefined-port> to check if there's a IDM Wallet running there. On the contrary, the further both sides are, the less automatic the process is and might require users to mediate the process by scanning QR-codes, inputting numbers, or other mechanisms. Messages are exchanged from the IDM Client to the IDM Wallet and vice-versa through the IDM Bridge. Those messages will be defined as part of the IDM Bridge spec to ensure the interoperability between different implementations. There will be a reference IDM Bridge written in JavaScript with the goal to solve scenario 1 by leveraging the postMessage API to create a communication channel between IDM Clients and a IDM Wallet running on a pre-defined domain.","title":"Identity Manager Concept"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#identity-manager-concept","text":"Authors : Andr\u00e9 Cruz, Jo\u00e3o Santos, Pedro Teixeira Introduction Research: standards & foundations Decentralized Identifiers Verifiable Credentials Using DID-Auth to prove control of the DID IDM - Identity Manager Preface IDM Wallet IDM Wallet UI IDM Client IDM Bridge","title":"Identity Manager Concept"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#introduction","text":"The benefits of using P2P technologies, which enable truly decentralized applications to exist, are obvious to those well versed in the topic. However, in order to attain massive user adoption, these decentralized applications need to be better than the incumbents in almost every aspect -- usability being the one where they usually lack the most. Most of the cryptographic identity solutions out there rely on a single key pair where the public key identifies a single identity. While simple, this solution has many drawbacks, the most important one being that you can't recover your identity if the private key gets compromised. To explore potential solutions, the IPFS DDC WG decided to invest some time researching what decentralized identity solutions may be used. This paper is a follow-up of RFC Peer-Star-Identity .","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#research-standards-foundations","text":"","title":"Research: standards &amp; foundations"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#decentralized-identifiers","text":"Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, \"self-sovereign\" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. A DID always identifies a person, organization or thing. They have a specific syntax: <scheme>:<method>:<method-identifier> (e.g. did:ipid:a1B2c3d4E5 ). DIDs resolve to DID-Documents \u2014 simple documents that describe how to use that specific DID. Each DID-Document contains at least three things: Cryptographic Material, Authentication Suites and Service Endpoints. Cryptographic Material combined with Authentication Suites provide a set of mechanisms to authenticate as the DID subject (e.g. public keys, pseudonymous biometric protocols, etc.). Service Endpoints enable trusted interactions with the DID subject. In the real world, people may use different devices to interact with others. Each of those should have their public keys listed in the DID-Document, making it possible to add and remove them as needed while keeping the same DID.","title":"Decentralized Identifiers"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#granular-permissions-authentication","text":"As previously stated, people use different devices in their daily lives. Each of those devices have different security guarantees. As an example, a device in your home is less likely to be lost or stolen when compared to a mobile phone. On the other hand, a mobile phone may have its storage encrypted which is more secure than an unencrypted one. Having that said, the level of permissions should be considered when adding a new device. Moreover, if a device is going to be used for authentication, its public key should be listed in the authentication field of the DID-Document. This is necessary for DID-Auth or other authentication mechanisms to assert that the device is valid for authentication.","title":"Granular permissions &amp; authentication"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#verifiable-credentials","text":"Verifiable Credentials is a format for interoperable, cryptographically-verifiable digital credentials. DIDs begin by being \"trustless\" in the sense that they don't directly provide meaningful identity attributes. But trust between DID-identified peers can be built up through the exchange of verifiable credentials - credentials about identity attributes that include cryptographic proof. These proofs can be verified by reference to the issuer's DID and DID-Document. Because the ecosystem is still in its infancy and there's a lack of trusted issuers, identities may self-issue credentials. More specifically, they may issue credentials that define personal attributes about themselves, like their name and birthdate, and credentials that prove they own certain profiles on social networks, similar to how Keybase does. As of today, many people trust the mainstream social networks, such as Facebook and Twitter, and identities may use them to post cryptographic proofs that link their profiles to a hash of their DID. As time passes by and the ecosystem gets mature, identities will hold credentials issued by others that may be used in a variety of scenarios and purposes.","title":"Verifiable Credentials"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#using-did-auth-to-prove-control-of-the-did","text":"DID-Auth is an handshake ceremony where an entity proves control over an identity to a relying party. In an authentication scenario between an user and an application, the application may want to ensure that the entity is in control of the DID it presented. Similarly, the user being authenticated may also check if the application's DID is in the application's control. Another typical scenario is when an entity requests access to some private data and the holder of the data needs check if that entity is in control of the identity's DID before handing the key to decrypt that data. In both scenarios, verifiable credentials may be exchanged so that the credibility of those identities may be evaluated during the handshake.","title":"Using DID-Auth to prove control of the DID"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#idm-identity-manager","text":"","title":"IDM - Identity Manager"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#preface","text":"While DID's, Verifiable Credentials and DID-Auth provide interoperable models for common use-cases, the reality is that current identity wallets are closed in their own DID-method ecosystems. On one hand, applications wanting to embrace identities using different DID-methods have to integrate with different identity wallets, such as uPort and blockstack . These wallets have SDKs with different APIs, increasing integration complexity and crippling adoption. On the other hand, users are asked to authenticate using specific DID-methods because the application they are interacting with is limited by the DID-methods they support. Moreover, and often as a consequence, users are required to use multiple wallets to manage different identities they own. As an example, an user that owns its persona's DID and its company's DID must use different wallets in case they were created using different DID-methods. The Identity Manager is a unified identity wallet that aims to support multiple DIDs and multiple DID-methods, where: Users create and import identities from different DID-methods Users manage the verifiable credentials of the identities they have, with an initial focus on self-issued credentials Users manage the list of devices for each identity, providing the ability to revoke a compromised device Users manage the list of applications they have interacted with, providing the ability to revoke an application All the identities' data, such as its verifiable credentials, devices list and applications list, is stored encrypted The same identities' data is replicated across wallets to be kept in sync Interactions between identities and applications, such as authenticating, signing and verification of signatures, are mediated by the wallet Components, interfaces and protocols are detailed and explained in an open specification, allowing anyone to implement their own IDM From a higher-level perspective, the Identity Manager - or IDM for short - is composed of four components: IDM Wallet, IDM Wallet UI, IDM Client and IDM Bridge:","title":"Preface"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#idm-wallet","text":"The IDM Wallet is similar to the physical wallet you carry everyday. It contains all the digital identities of its holder and all the information attached to them in a secure and encrypted way. Note that the IDM Wallet is an headless component, meaning it has no graphical user interface. Anyone wanting to build such GUIs, may choose from a variety of SDKs written in different programming languages, all based on the IDM Wallet spec . More information about the IDM Wallet UI can be found later on this document. There will be a reference IDM Wallet written in JavaScript, suitable to use inside a browser.","title":"IDM Wallet"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#locker","text":"The IDM Wallet will have a locking mechanism to protect its access. A variety of locking types may be used to unlock the wallet, such as a passphrase, pattern, typeid, fingerprint, faceid, and other biometric validators. The locker holds a single secret that will be encrypted for each locking type. That single secret will be used to encrypt and decrypt all the data that lives inside the wallet.","title":"Locker"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#managing-identities","text":"Users will be able to create identities using their preferred DID-method or import existing ones. They will be guided throughout the process in the UI according to the chosen DID-method. The outcome of the creation of an identity will be a new DID, a Master Key Pair and a Device Key Pair where their correspondent public keys are listed in the DID-Document. The Master Private Key is in complete control of the DID-Document and should be used as little as possible. For that reason, it should be stored outside the IDM Wallet in a secure and recoverable way. One of those ways is via a Paper Key, where the Master Private Key is printed using machine readable representations, such as a QR code. Shamir's Secret Sharing may also be used to split the Master Private Key into different secrets that can be shared with trustees, like family and close friends. Even if the Paper Key is lost, users may recover their Master Private Key by collecting those secrets from them. For the import case, a Device Key Pair will be created and its public key added to the DID-Document as well. Depending on the DID-method, the Master Private Key might be needed to update the DID-Document. Some DID-methods might support granular permissions. This enables users to grant a specific set of permissions to different devices based on how secure the device is and how likely it is to be lost or robbed.","title":"Managing identities"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#managing-verifiable-credentials","text":"Users will be able to view, add and remove Verifiable Credentials to and from their wallet. As we previously explained in the Verifiable Credentials section, the IDM Wallet will initially focus on self-issued credentials and expand to credentials issued by others when the ecosystem gets mature. Depending on the identities' DID-methods and their capabilities, where and how the credentials are stored might be pre-defined by the DID-method and may be either off-chain or on-chain. As an example, EIP780 proposes an on-chain registry for Ethereum based identities to store credentials, enabling persons, smart contracts, and machines to issue credentials about each other. The IDM Wallet will employ best-practices for each DID-method when storing credentials.","title":"Managing verifiable credentials"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#authentication-on-applications","text":"Most applications need to know the identity of the user so that they can provide a customized experience. Applications may start an authentication process, issuing an authentication request to an IDM Wallet, containing the application DID and Verifiable Credentials with its details, such as the application name, and the credentials it wants to receive from an identity. The IDM Wallet will perform DID-Auth to ensure that the application is in control of the DID it presented. If successful, the user is then prompted to select an identity and to accept what the application is asking for. If the user accepted the prompt, a new session is created and will be used for any further interactions between that application and that IDM Wallet. The session is composed of a key-pair where the public key is known by both, serving as the session identifier, while the private key is only known by that IDM Wallet. Additionally, all sessions have a max-age, meaning they expire as time passes.","title":"Authentication on applications"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#signing-artifacts-on-applications","text":"Applications may want to sign artifacts, such as regular data or ephemeral keys. This will be possible in two different ways: Sign with the Session Private Key Sign with the Device Private Key The first method is less intrusive and happens transparently to the user, but is also less secure. More specifically, a robber who steals an IDM Wallet may still sign artifacts with the Session Private Key as long as a session is valid because that key is stored unencrypted. Verifiers will still see those signatures as valid until the DID owner revokes the stolen device. The second method is more secure as the user is prompted to unlock IDM to decrypt the Device Private Key, but it is more intrusive. The interaction is similar to the authentication process, where the user is prompted inside the IDM Wallet to sign. Because the Device Private Key is encrypted, this gives the DID owner plenty of time to revoke that key in another IDM Wallet. Ultimately, the application may choose between both methods for different situations depending on the security degree they want to have.","title":"Signing artifacts on applications"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#verifying-signatures","text":"Any party might verify the authenticity and authorship of artifacts signed by others. The verification of an artifact signed by the Session Public Key is performed by: Verifying the signature of the artifact against the Session Public Key of the author. Verifying the signature of the Session Public Key against the Device Public Key of the author. This proves that the Session Public Key was authorized by the Device Private Key. Verifying if the Device Public Key of the author is present in the DID-Document associated with the DID. This proves that the Device Public Key was authorized by the author's identity. If Device Public Key is flagged as revoked, compare its revocation effective date with the signature date The verification of an artifact signed by the Device Public Key is performed by: Verifying the signature of the artifact against the Device Public Key of the author. Verifying if the Device Public Key of the author is present in the DID-Document associated with the DID. This proves that the Device Public Key was authorized by the Identity (author). If Device Public Key is flagged as revoked, compare its revocation effective date with the signature date","title":"Verifying signatures"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#managing-application-sessions","text":"Users will be able to revoke one or all application sessions from within the identity's applications list. After revoking a session, the application will no longer be able to use that session, effectively stopping it from being able to perform operations that require the Session and Device Private Keys.","title":"Managing application sessions"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#revoking-a-device","text":"Users must be able to revoke a device associated with an identity. To do so, the DID-Document needs to be updated in order to signal that the Device Public Key was revoked, including the effective date which is important when verifying signatures. Depending on the DID-method and the permissions of the Device Private Key, higher level keys or other processes might be necessary to update the DID-Document, such as using the Master Private Key. The way a public key is declared as revoked is yet to be standardized in the DID spec. Nevertheless, a DID-Document may be augmented via the @context attribute to support having that information.","title":"Revoking a device"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#replication-and-synchronization","text":"The same identity might be held inside different IDM Wallets. For that reason, the identities' data, such as its credentials and applications, will be seamlessly replicated to other IDM Wallets using P2P technologies powered by CRDTs . Any data stored off-chain or on-chain as per the DID-method best-practices will be taken into consideration, meaning they will be synchronized accordingly.","title":"Replication and synchronization"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#idm-wallet-ui","text":"End users need a graphical user-interface to interact with their IDM Wallet. An IDM Wallet UI is an application that will make use of the most suitable IDM Wallet SDK for the programming language they choose. Even though anyone can implement its own IDM Wallet UI application, there is a need to define some standards in regards to user journeys, user experience and interactions. For that reason, there will be a reference IDM Wallet UI based on web-technologies to be used inside a browser, that can be packed as an Electron in the future.","title":"IDM Wallet UI"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#reference-ui-diagrams","text":"User-journeys diagram: Information Architecture diagram:","title":"Reference UI diagrams"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#reference-ui-concept","text":"The goal of the reference UI is to have a really polished interface with a premium feel. It must be easy to use, distinct, reliable, accessible and provide a feel of credibility to the users.","title":"Reference UI concept"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#idm-client","text":"Applications need to interact with the IDM Wallet. This process will be facilitated by the IDM Client, which will be available as multiple SDKs for different programming languages. Each SDK will provide a simple and intuitive interface based on the IDM Client Spec and contain means to authenticate, unauthenticate, sign, verify signatures, amongst others. There will be a reference IDM Client written in JavaScript, suitable to use inside a browser.","title":"IDM Client"},{"location":"rwot8/topics-and-advance-readings/idm-concept/#idm-bridge","text":"While applications use the IDM Client to interact with an IDM Wallet, the way they reach each other and communicate is handled by the IDM Bridge. Applications run on a variety of contexts, from within browsers to native applications. An IDM Wallet might coexist in the same context as these applications or, more often, in different contexts and even equipments. Below there's a list of possible scenarios and respective solutions: Both an Application and an IDM Wallet running on the same browser, on the same equipment: Solution: Use an iframe and the postMessage API An Application running on a browser and an IDM Wallet running as a native OS application, on the same equipment Solution: The IDM Wallet exposes a local WebSocket server on a pre-defined port known by the IDM Client Both the Application and IDM Wallet are native OS applications, running on the same equipment Solution: Use inter-process communication, preferring mechanisms offered by the OS An Application running on a laptop and an IDM Wallet running on a smartphone Solution: Use IPFS's pubsub to find and talk to each other Conceptually, the IDM Bridge is composed by two parts: the provider-side and the consumer-side. The provider-side is embedded in the IDM Wallet while the consumer-side is embedded in the IDM Client. Each one of the sides have a set of transports that they support to communicate. Moreover, the IDM Client has a discovery mechanism that finds the most appropriate IDM Wallet to talk to, starting by locating one closer to its own context that also supports one of its transports. For reference, there are various degrees of closeness, from closest to the furthest: exactly the same context (e.g.: within same browser), same machine, same network or different network. The discovery is likely to be more transparent and automatic if both sides are close to each other. For example, in scenario 2 , the IDM Client can try to initiate a WebSocket connection to localhost:<predefined-port> to check if there's a IDM Wallet running there. On the contrary, the further both sides are, the less automatic the process is and might require users to mediate the process by scanning QR-codes, inputting numbers, or other mechanisms. Messages are exchanged from the IDM Client to the IDM Wallet and vice-versa through the IDM Bridge. Those messages will be defined as part of the IDM Bridge spec to ensure the interoperability between different implementations. There will be a reference IDM Bridge written in JavaScript with the goal to solve scenario 1 by leveraging the postMessage API to create a communication channel between IDM Clients and a IDM Wallet running on a pre-defined domain.","title":"IDM Bridge"},{"location":"rwot8/topics-and-advance-readings/idm-spec/","text":"Identity Manager Specification NOTE: This is a draft, we will be updating the document during the next days By Adin Schmahmann <adin@protocol.ai>, Protocol Labs; Andr\u00e9 Cruz <andre@moxy.studio>, MOXY; and Pedro Teixeira, Protocol Labs <pedro@protocol.ai>, Protocol Labs. The IDM (Identity Manager) project itself is open-source, meaning that anyone should be able to look at the specification and implement it in their language of choice. This also enables implementing different user-interfaces. There will be a reference implementation of this spec written in JavaScript as well as a web based user-interface. The goals of the workshop are to present the spec, gather feedback and discuss some unresolved topics. Index: IDM Client IDM Wallet IDM Bridge Data Types To Discuss IDM Client // authenticate, unauthenticate & obtain current session .authenticate({ Array<CredentialScope> scopes } options): Promise<Session> .unauthenticate(): Promise .getSession(): Promise<Session> .onSessionChange((Session session) => {}): Function (to remove the listener) // signing and verifying .sign(ArrayBuffer data, { IdentifiedSignatureKeyType keyType = session, String previewUrl } options): Promise<IdentifiedSignature> .verifySignature(ArrayBuffer data, IdentifiedSignature signature): Promise<Boolean> IDM Wallet Main scopes: .locker .storage .did .identities .session .locker .locker.unlock(LockerType type, (Any challenge) => Any solution): Promise .locker.lock() .locker.isLocked(): Boolean .locker.onLockedChange((Boolean locked) => {}): Function (to remove the listener) .locker.getPublicKey(): PublicKey .locker.getPrivateKey(): PrivateKey .locker.locks.list(): Promise<Array<LockerType>> .locker.locks.set(LockerType type, Any solutions) .locker.locks.unset(LockerType type) TODO: - refreshLock to keep unlocked because we did a op - where the unlocking time is defined .storage .storage.get(String<String>|String key, { PrivateKey decryptKey } options): Promise<Object<String,Any>> .storage.set(Object<String, Any>, { Number maxAge, PublicKey encryptKey } options): Promise .storage.remove(String<String>|String key): Promise .storage.clear(): Promise .storage.getBytesInUse([String<String>|String key]): Promise<Number> .did .did.resolve(String did): Pending<DidDocument> .did.create(String didMethod, Object parameters): Promise<{ String did, KeyPair deviceKeyPair, [KeyPair masterKeyPair] }> .did.import(String did, Object parameters): Promise<{ String did, KeyPair deviceKeyPair> }> .did.verifySignature(IdentifiedSignature signature): Pending<Boolean> .did.methods.list(DIDMethodPurpose purpose) .did.methods.isSupported(String didMethod, DIDMethodPurpose purpose) TODO: add did auth .identities .identities.list(): Promise<Array<Identity>> .identities.add(String did, KeyPair deviceKeyPair, [KeyPair masterKeyPair]): Promise<Identity> .identities.get(String did): Identity .identities.remove(String did): Promise Note: Identity is actually a instance of a \"class\" and not a data-structure identity identity.credentials identity.credentials.list(): Array<Credential> identity.credentials.listByScope(CredentialScope scope): Array<Credential> identity.credentials.add(Credential credential, [CredentialScope scope]) identity.credentials.remove(String credentialId) identity.credentials.onChange((Array<Credentials> credentials) => {}) : Function (to remove the listener) identity.devices identity.devices.list(): Array<Device> identity.devices.add(Device device, PrivateKey masterKey) identity.devices.revoke(PublicKey deviceKey, PrivateKey masterKey) identity.devices.update(PublicKey deviceKey, Device device) identity.devices.onChange((Array<Device> devices) => {}) : Function (to remove the listener) identity.applications identity.apps.list(): Array<App> identity.apps.add(App app) identity.apps.revoke(String appId) identity.apps.getUsage(String appId): AppUsage identity.apps.onChange((Array<App>) => {}) : Function (to remove the listener) identity.replication identity.replication.getStatus(): ReplicationStatus identity.replication.start(): Promise identity.replication.stop(): Promise The IDM reference implementation will use peer-base . The replication protocol will later be defined as part of the peer-base project itself. .session .session.create(App app, String did, { Array<CredentialScope> scopes, Number maxAge } options): Promise<Session> .session.destroy(String sessionId): Promise .session.isValid(String sessionId): Session .session.getById(String sessionId): Session .session.sign(sessionId, ArrayBuffer data, { SignatureKeyType keyType = 'session', String previewUrl } options): Promise<IdentifiedSignature> IDM Bridge The way the IDM Client and the IDM Wallet communicate depends on the environment they are running. Also, they should be interoperable even if they are implemented in different languages. The IDM Bridge ensures these two requirements. Here's a few environment scenarios: The DApp and the IDM Wallet are running in the same browser The DApp is a web app running on Chrome and IDM Wallet is an electron app on the same computer The DApp is a mobile app and IDM wallet is an electron app on a computer TODO: This is complex, discuss this Data Types App { String id, String name, String homepageUrl, Array<Icon> icons } AppUsage { Number interactionsCount, String addedAt, String lastUsedAt } ChainedKey { PublicKey key, ChainedKey parent, String (multisig?) parentSignature } Credential (https://github.com/w3c/vc-data-model) CredentialScope enum(details, social) DIDMethodPurpose enum(resolve, create, import) DIDDocument (https://w3c-ccg.github.io/did-spec/#simple-examples) DIDMethodInfo { String name, String homepageUrl, String description, Array<Icon> icons } Device { PublicKey key, String name, DeviceType type } DeviceType enum(laptop, desktop, phone, tablet) KeyPair { PublicKey publicKey, PrivateKey privateKey } Icon { Number width, Number height, String type, String url } IdentifiedSignature { String did, String date, ChainedKey chainedKey, String (multisig?) signature } IdentifiedSignatureKeyType enum(device, session) Identity { String did, IdentityType type, Schema.org details } IdentityType enum(person, organization, other) LockerType enum(passphrase, fingerprint, faceid) PrivateKey String (multikey?) PublicKey String (multikey?) ReplicationConsistency { incoming: Array<String>, outgoing: Array<String> } ReplicationStage enum(inactive, starting, active, stopping) ReplicationStatus { ReplicationStage stage, ReplicationConsistency consistency } Session { String id, String appId, String createdAt, String expiresAt, Identity identity, Object<CredentialScope, Array<Credential>> credentials } To Discuss IDM Bridge multikey multisignature","title":"Identity Manager Specification"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#identity-manager-specification","text":"NOTE: This is a draft, we will be updating the document during the next days By Adin Schmahmann <adin@protocol.ai>, Protocol Labs; Andr\u00e9 Cruz <andre@moxy.studio>, MOXY; and Pedro Teixeira, Protocol Labs <pedro@protocol.ai>, Protocol Labs. The IDM (Identity Manager) project itself is open-source, meaning that anyone should be able to look at the specification and implement it in their language of choice. This also enables implementing different user-interfaces. There will be a reference implementation of this spec written in JavaScript as well as a web based user-interface. The goals of the workshop are to present the spec, gather feedback and discuss some unresolved topics. Index: IDM Client IDM Wallet IDM Bridge Data Types To Discuss","title":"Identity Manager Specification"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#idm-client","text":"// authenticate, unauthenticate & obtain current session .authenticate({ Array<CredentialScope> scopes } options): Promise<Session> .unauthenticate(): Promise .getSession(): Promise<Session> .onSessionChange((Session session) => {}): Function (to remove the listener) // signing and verifying .sign(ArrayBuffer data, { IdentifiedSignatureKeyType keyType = session, String previewUrl } options): Promise<IdentifiedSignature> .verifySignature(ArrayBuffer data, IdentifiedSignature signature): Promise<Boolean>","title":"IDM Client"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#idm-wallet","text":"Main scopes: .locker .storage .did .identities .session","title":"IDM Wallet"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#locker","text":".locker.unlock(LockerType type, (Any challenge) => Any solution): Promise .locker.lock() .locker.isLocked(): Boolean .locker.onLockedChange((Boolean locked) => {}): Function (to remove the listener) .locker.getPublicKey(): PublicKey .locker.getPrivateKey(): PrivateKey .locker.locks.list(): Promise<Array<LockerType>> .locker.locks.set(LockerType type, Any solutions) .locker.locks.unset(LockerType type) TODO: - refreshLock to keep unlocked because we did a op - where the unlocking time is defined","title":".locker"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#storage","text":".storage.get(String<String>|String key, { PrivateKey decryptKey } options): Promise<Object<String,Any>> .storage.set(Object<String, Any>, { Number maxAge, PublicKey encryptKey } options): Promise .storage.remove(String<String>|String key): Promise .storage.clear(): Promise .storage.getBytesInUse([String<String>|String key]): Promise<Number>","title":".storage"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#did","text":".did.resolve(String did): Pending<DidDocument> .did.create(String didMethod, Object parameters): Promise<{ String did, KeyPair deviceKeyPair, [KeyPair masterKeyPair] }> .did.import(String did, Object parameters): Promise<{ String did, KeyPair deviceKeyPair> }> .did.verifySignature(IdentifiedSignature signature): Pending<Boolean> .did.methods.list(DIDMethodPurpose purpose) .did.methods.isSupported(String didMethod, DIDMethodPurpose purpose) TODO: add did auth","title":".did"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#identities","text":".identities.list(): Promise<Array<Identity>> .identities.add(String did, KeyPair deviceKeyPair, [KeyPair masterKeyPair]): Promise<Identity> .identities.get(String did): Identity .identities.remove(String did): Promise Note: Identity is actually a instance of a \"class\" and not a data-structure","title":".identities"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#identity","text":"","title":"identity"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#identitycredentials","text":"identity.credentials.list(): Array<Credential> identity.credentials.listByScope(CredentialScope scope): Array<Credential> identity.credentials.add(Credential credential, [CredentialScope scope]) identity.credentials.remove(String credentialId) identity.credentials.onChange((Array<Credentials> credentials) => {}) : Function (to remove the listener)","title":"identity.credentials"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#identitydevices","text":"identity.devices.list(): Array<Device> identity.devices.add(Device device, PrivateKey masterKey) identity.devices.revoke(PublicKey deviceKey, PrivateKey masterKey) identity.devices.update(PublicKey deviceKey, Device device) identity.devices.onChange((Array<Device> devices) => {}) : Function (to remove the listener)","title":"identity.devices"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#identityapplications","text":"identity.apps.list(): Array<App> identity.apps.add(App app) identity.apps.revoke(String appId) identity.apps.getUsage(String appId): AppUsage identity.apps.onChange((Array<App>) => {}) : Function (to remove the listener)","title":"identity.applications"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#identityreplication","text":"identity.replication.getStatus(): ReplicationStatus identity.replication.start(): Promise identity.replication.stop(): Promise The IDM reference implementation will use peer-base . The replication protocol will later be defined as part of the peer-base project itself.","title":"identity.replication"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#session","text":".session.create(App app, String did, { Array<CredentialScope> scopes, Number maxAge } options): Promise<Session> .session.destroy(String sessionId): Promise .session.isValid(String sessionId): Session .session.getById(String sessionId): Session .session.sign(sessionId, ArrayBuffer data, { SignatureKeyType keyType = 'session', String previewUrl } options): Promise<IdentifiedSignature>","title":".session"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#idm-bridge","text":"The way the IDM Client and the IDM Wallet communicate depends on the environment they are running. Also, they should be interoperable even if they are implemented in different languages. The IDM Bridge ensures these two requirements. Here's a few environment scenarios: The DApp and the IDM Wallet are running in the same browser The DApp is a web app running on Chrome and IDM Wallet is an electron app on the same computer The DApp is a mobile app and IDM wallet is an electron app on a computer TODO: This is complex, discuss this","title":"IDM Bridge"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#data-types","text":"App { String id, String name, String homepageUrl, Array<Icon> icons } AppUsage { Number interactionsCount, String addedAt, String lastUsedAt } ChainedKey { PublicKey key, ChainedKey parent, String (multisig?) parentSignature } Credential (https://github.com/w3c/vc-data-model) CredentialScope enum(details, social) DIDMethodPurpose enum(resolve, create, import) DIDDocument (https://w3c-ccg.github.io/did-spec/#simple-examples) DIDMethodInfo { String name, String homepageUrl, String description, Array<Icon> icons } Device { PublicKey key, String name, DeviceType type } DeviceType enum(laptop, desktop, phone, tablet) KeyPair { PublicKey publicKey, PrivateKey privateKey } Icon { Number width, Number height, String type, String url } IdentifiedSignature { String did, String date, ChainedKey chainedKey, String (multisig?) signature } IdentifiedSignatureKeyType enum(device, session) Identity { String did, IdentityType type, Schema.org details } IdentityType enum(person, organization, other) LockerType enum(passphrase, fingerprint, faceid) PrivateKey String (multikey?) PublicKey String (multikey?) ReplicationConsistency { incoming: Array<String>, outgoing: Array<String> } ReplicationStage enum(inactive, starting, active, stopping) ReplicationStatus { ReplicationStage stage, ReplicationConsistency consistency } Session { String id, String appId, String createdAt, String expiresAt, Identity identity, Object<CredentialScope, Array<Credential>> credentials }","title":"Data Types"},{"location":"rwot8/topics-and-advance-readings/idm-spec/#to-discuss","text":"IDM Bridge multikey multisignature","title":"To Discuss"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/","text":"Implementing threshold schemes Author Daan Sprenkels <hello@dsprenkels.com> Abstract \"Shamir secret sharing is a method to split secrets into shares, and to later recombine them. However, it does not feature integrity protection of the secret. This article elaborates on Feldman VSS and Pederson VSS, which do protect the message integrity. Furthermore, we show how hashing the shares also protects the message integrity, but is vulnerable to a cheating dealer. Markdown does not really support footnotes. In this document, I use blockquotes for this purpose. A couple of weeks ago, Christopher Allen requested me to \"summarize [my] post https://dsprenkels.com/sss-34c3.html\" and submit it to the RWOT8 repo . 1. Introduction Threshold schemes are cryptographic schemes that allow a group of people to split a secret message $m$ up in a set of different shares $s_i$. These shares can later be used to recombine the shares into the original message. A prominent detail of threshold schemes is that the dealer can specify the threshold of the amount shares that needs to be reached for the message to be recombined. For instance, they can split a message into 5 shares, but allow the message to be recovered if only 4 shares are recombined. Should one share be lost by any of the participants, then the message can still be recovered. In this article, I will try to set forth a general description on Shamir's secret sharing scheme in Section 2, and the two verifiable secret sharing schemes from Feldman and Pederson in Section 2. In Section 4 I will introduce a patched version of Shamir's scheme, using hash functions and analyze its security properties. In Section 5, I will briefly argue about some details that are important when choosing and implementing these kinds of schemes. 2. Shamir secret sharing The first and still most relevant scheme is Shamir's secret sharing scheme (SSS). The idea is quite simple. Consider the dealer has a message $m$, which they want to split into $n=5$ shares, with a recombination threshold of $t=4$. Distribution. The dealer: generates a random polynomial $f(x)$ of order $t$, i.e. $f(x) = a_{t-1}x^{t-1} + \\ldots + a_{0}$; sets the lowest order coefficient $a_0$ to $m$, s.t. $f(0) = m$; evaluates $n$ points on the polynomial: $s_{1} = (1, f(1)), \\ldots, s_{n} = (n, f(n))$, and distributes them among the participants. Reconstruction. The participants: collect at least $t$ shares; fit a polynomial $f'(x)$ between the points $s_{1}, \\ldots, s_{t}$, which can be done using Lagrange interpolation; and they recover $m = f'(0)$. Essentially, in Shamir's scheme the dealer creates a system of $t$ equations that the participants have to solve to get the message back. Indeed, the recombination could also be implemented using another kind of solving algorithm, like Gaussian Elimination . Figure 1 shows an example of this idea. Figure 1: In this case the dealer has their secret $m = 42$. They choose the threshold $t = 3$ and will be splitting $m$ into $n = 4$ shares. They generate the polynomial $f(x) = 4x^2 - 25x + 42$. If we plot the shares for $x = 1$ up to $x = 4$, we can see that we could fit this polynomial back from the points. To do this, we would need at least 3 points. Shamir's scheme is---what we call in crypto--- information theoretically secure for confidentiality. In this case, this means that the $y$-coordinates of the shares cannot be distinguished from numbers that are purely random. The consequence is that without a sufficient number of shares, the secret can never be recovered. No computer in the world---not even a quantum computer---has enough power to \"crack\" the secret. In the previous paragraph, I explicitly mention that SSS is secure for confidentiality . That is because SSS is very much insecure for integrity . If one of the participants knows something about the actual value of the secret, they can manipulate their share to force $m$ into some value that they prefer. Therefore, one should not share any value that has to be integrity-protected in some kind of way, using only schoolbook SSS. Conversely, any participant can alter their share such that the original secret will be garbled. This will generally not be detectable by the others. Basically, when you are using schoolbook SSS, you must only share keys that are generated randomly. If you are interested in the details of forging shares, I refer you to an explanation I wrote earlier on the Crypto StackExchange. Of schoolbook SSS, we can write down some benefits and downsides. Benefits Information-theoretical security for confidentiality. Needs no mathematical assumptions. Downsides No integrity protection. Thus, the dealer can cheat. 3. Background: verifiable secret sharing We could quite easily patch SSS to secure the integrity of the secret, I will get back to this further in this article. However, before rolling our own crypto, I would like us to first take a brief look at the two most popular verifiable threshold schemes . Maybe we can take some inspiration from those, before trying to solve the integrity problem ourselves. Note, these descriptions come from B. Schoenmakers' lecture notes on Cryptographic Protocols (Chapter 6). These notes contain more in depth security analyses on these threshold schemes. 3.1. Feldman VSS In, Feldman VSS , let $\\langle g \\rangle$ be a group wherein the Diffie-Hellman assumption holds. Distribution. The dealer: runs Shamir's scheme on $m$ and gets $f(x), s_{1}, \\ldots, s_{n}$; and computes and broadcasts commitments $B_{j} = g^{a_j}$ for $0 \\le j < t$. Reconstruction. The participants: each verify all the shares by checking whether $g^{s_i} = \\prod_{j=0}^{t-1} B_j^{i^j}$; and run Shamir's reconstruction protocol to recover $m$. Benefits: DL-based security for confidentiality. Information-theoretical security for integrity. Dealer cannot cheat. Downsides: Assumes DL assumption. Can only share uniformly random messages. 3.2. Pederson VSS In Pederson VSS , let $\\langle g \\rangle$ be a group wherein the Diffie-Hellman assumption holds. Also, let $h$ be a random element from $\\langle g \\rangle$, s.t. the discrete logarithm $\\log_g h$ is not known to anybody. Distribution. The dealer: generates two random polynomials $f_a(x),f_b(x)$ of order $t$, i.e. $f_a(x) = a_{t-1}x^{t-1} + \\ldots + a_{0}$ and $f_b(x) = b_{t-1}x^{t-1} + \\ldots + b_{0}$; sets the lowest order coefficients $a_0,b_0$ to $m$, s.t. $f_a(0) = f_b(0) = m$; evaluates $n$ points on the polynomial $s_{1} = (1, f_a(1), f_b(1)), \\ldots, s_{n} = (n, f_a(n), f_b(n))$, distributes them among the participants; and computes and broadcasts the commitments $C_j = g^{a_j}h^{b_j}$ for = $0 \\le j < t$. Reconstruction. The participants: collect at least $t$ shares; fit a polynomial $f'(x)$ between the points $s_{1}, \\ldots, s_{t}$, which can be done using Lagrange interpolation; and they recover $m = f'(0)$. Benefits Information theoretical security for confidentiality. DL-based security for integrity. Dealer cannot cheat. Downsides Assumes DL assumption. 3.3. Observation We see that both schemes are based on broadcasting commitment values. These commitments force the shares to be some kind of value, without revealing that actual value. A participant can use these commitments to verify if their share is actually correct, but from these commitments, one cannot reproduce the shares. 4. SSS with hashed shares From my experiences implementing crypto, I find that correctly implementing discrete logarithm-based schemes is very difficult. We do however know another building block which we can use to build a commitment scheme, that is the hash function . For RWOT8, this was first proposed by Peg in his article : He says: \"This is also something we considered, but feel that it gives custodians more unnecessary extra information and less accountability compared to other methods.\" So, instead of committing to the polynomial coefficients, we commit to the values of the shares. I.e. we propose the following scheme: Distribution. The dealer: runs Shamir's scheme on $m$ and gets $f(x), s_{1}, \\ldots, s_{n}$; and computes and broadcasts commitments $D_i = H(s_i)$ for $1 \\le i \\le n$, where $H$ is a preimage resistant hash function. Reconstruction. The participants: each verify all the shares by checking whether $H(s_i) = D_i$; and run Shamir's reconstruction protocol to recover $m$. Broadcasting the hashes of the shares makes sure that no participant can manipulate with the shares, because their share's hash would not verify in the reconstruction phase. Futhermore, the other participants will know if a specific share is invalid, instead of ending up with a garbled or different reconstructed message. This property is shown by the following proof: Claim. Assume $H$ is preimage resistant, an honest dealer, a single malicious participant cannot force $m' \\ne m$ to be reconstructed. Proof. Assume a participant is able to force the reconstruction of $m'$, given $s_1, \\ldots, s_t, D_1, \\ldots, D_t$. Because $m' \\ne m$, $f'(x) \\ne f(x)$. Because $f(x)$ is of order $t$, at least one of the shares must be different from its original. W.l.o.g., assume only one share $s_\\mathcal{A}'$ is different from its original $s_\\mathcal{A}$. However, because the reconstruction succeeded, we know that $H(s_\\mathcal{A}') = D_A$, but $s_\\mathcal{A}' \\ne s_\\mathcal{A}$, so this contradicts preimage resistance. $\\square$ So if this option was always, possible, why is it not very commonly described by cryptographers as a verifiable secret sharing scheme? Well, the reason is that in this hash-based scheme, the dealer can cheat. In particular, the dealer can act as follows: They split a message into shares and distribute them to the participants. They compute the commitments $D_1, \\ldots, D_n$, but secretly changes one of them to $D_\\text{bad}$ before publishing. During the reconstruction phase, the participants will verify the shares and see that some share does not verify correctly. ] The participants will not know whether this participant was malicious, or if the dealer was malicious. In short, if the dealer is not completely trusted, we should not use this scheme. Instead we should probably use one of Section 3. In conclusion, here is a listing of benefits and downsides, similar to the listings that were provided in the previous sections. Benefits Information theoretical security for confidentiality. Hash-based security for integrity. Downsides Dealer can cheat. Assumes hash function preimage resistance. 5. Implementing a secret sharing scheme This section will be different from the previous ones. Where the previous sections are mainly about the theory, this will mainly be based on my personal experiences while implementing my personal secret sharing library and other cryptographic software. As we all probably know, writing cryptographic software is hard. The main nemesis of a piece of cryptographic code is the side-channel attack . On regular PCs, the main attack that must be protected against is the (cache) timing attack. These attacks exploit the time that an algorithm takes to execute, which---on a computer---can be measured with a really precise resolution. A proposed solution to defend against this is to only run code on your computer that is trusted. However, we all know that this is nigh-on impossible to achieve, especially when we realize that web pages run tons of code that is not audited by the user. Instead, we have to write all the software in a deterministic manner. That means we cannot use if-statements or table-lookups that depend on secret data. Another consideration is the complexity of our cryptosystem. As a rule of thumb: Symmetric crypto and hash functions are relatively easy; Asymmetric crypto is generally very hard. Therefore, I would personally stay away from any discrete log based system, unless the requirements really ask for it. In my own library , I have opted to use the off-the-shelf AEAD \"Salsa20/Poly1305\" for integrity protection, and I have implemented all the arithmetic related to SSS in $GF(256)$ using bitslicing . An interesting case where we saw that the implementation of asymmetric crypto is hard where cryptonote-based currencies forgot to protect against small-subgroup attacks in Curve25519 in 2017.","title":"Implementing threshold schemes"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#implementing-threshold-schemes","text":"","title":"Implementing threshold schemes"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#author","text":"Daan Sprenkels <hello@dsprenkels.com>","title":"Author"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#abstract","text":"\"Shamir secret sharing is a method to split secrets into shares, and to later recombine them. However, it does not feature integrity protection of the secret. This article elaborates on Feldman VSS and Pederson VSS, which do protect the message integrity. Furthermore, we show how hashing the shares also protects the message integrity, but is vulnerable to a cheating dealer. Markdown does not really support footnotes. In this document, I use blockquotes for this purpose. A couple of weeks ago, Christopher Allen requested me to \"summarize [my] post https://dsprenkels.com/sss-34c3.html\" and submit it to the RWOT8 repo .","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#1-introduction","text":"Threshold schemes are cryptographic schemes that allow a group of people to split a secret message $m$ up in a set of different shares $s_i$. These shares can later be used to recombine the shares into the original message. A prominent detail of threshold schemes is that the dealer can specify the threshold of the amount shares that needs to be reached for the message to be recombined. For instance, they can split a message into 5 shares, but allow the message to be recovered if only 4 shares are recombined. Should one share be lost by any of the participants, then the message can still be recovered. In this article, I will try to set forth a general description on Shamir's secret sharing scheme in Section 2, and the two verifiable secret sharing schemes from Feldman and Pederson in Section 2. In Section 4 I will introduce a patched version of Shamir's scheme, using hash functions and analyze its security properties. In Section 5, I will briefly argue about some details that are important when choosing and implementing these kinds of schemes.","title":"1. Introduction"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#2-shamir-secret-sharing","text":"The first and still most relevant scheme is Shamir's secret sharing scheme (SSS). The idea is quite simple. Consider the dealer has a message $m$, which they want to split into $n=5$ shares, with a recombination threshold of $t=4$. Distribution. The dealer: generates a random polynomial $f(x)$ of order $t$, i.e. $f(x) = a_{t-1}x^{t-1} + \\ldots + a_{0}$; sets the lowest order coefficient $a_0$ to $m$, s.t. $f(0) = m$; evaluates $n$ points on the polynomial: $s_{1} = (1, f(1)), \\ldots, s_{n} = (n, f(n))$, and distributes them among the participants. Reconstruction. The participants: collect at least $t$ shares; fit a polynomial $f'(x)$ between the points $s_{1}, \\ldots, s_{t}$, which can be done using Lagrange interpolation; and they recover $m = f'(0)$. Essentially, in Shamir's scheme the dealer creates a system of $t$ equations that the participants have to solve to get the message back. Indeed, the recombination could also be implemented using another kind of solving algorithm, like Gaussian Elimination . Figure 1 shows an example of this idea. Figure 1: In this case the dealer has their secret $m = 42$. They choose the threshold $t = 3$ and will be splitting $m$ into $n = 4$ shares. They generate the polynomial $f(x) = 4x^2 - 25x + 42$. If we plot the shares for $x = 1$ up to $x = 4$, we can see that we could fit this polynomial back from the points. To do this, we would need at least 3 points. Shamir's scheme is---what we call in crypto--- information theoretically secure for confidentiality. In this case, this means that the $y$-coordinates of the shares cannot be distinguished from numbers that are purely random. The consequence is that without a sufficient number of shares, the secret can never be recovered. No computer in the world---not even a quantum computer---has enough power to \"crack\" the secret. In the previous paragraph, I explicitly mention that SSS is secure for confidentiality . That is because SSS is very much insecure for integrity . If one of the participants knows something about the actual value of the secret, they can manipulate their share to force $m$ into some value that they prefer. Therefore, one should not share any value that has to be integrity-protected in some kind of way, using only schoolbook SSS. Conversely, any participant can alter their share such that the original secret will be garbled. This will generally not be detectable by the others. Basically, when you are using schoolbook SSS, you must only share keys that are generated randomly. If you are interested in the details of forging shares, I refer you to an explanation I wrote earlier on the Crypto StackExchange. Of schoolbook SSS, we can write down some benefits and downsides.","title":"2. Shamir secret sharing"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#benefits","text":"Information-theoretical security for confidentiality. Needs no mathematical assumptions.","title":"Benefits"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#downsides","text":"No integrity protection. Thus, the dealer can cheat.","title":"Downsides"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#3-background-verifiable-secret-sharing","text":"We could quite easily patch SSS to secure the integrity of the secret, I will get back to this further in this article. However, before rolling our own crypto, I would like us to first take a brief look at the two most popular verifiable threshold schemes . Maybe we can take some inspiration from those, before trying to solve the integrity problem ourselves. Note, these descriptions come from B. Schoenmakers' lecture notes on Cryptographic Protocols (Chapter 6). These notes contain more in depth security analyses on these threshold schemes.","title":"3. Background: verifiable secret sharing"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#31-feldman-vss","text":"In, Feldman VSS , let $\\langle g \\rangle$ be a group wherein the Diffie-Hellman assumption holds. Distribution. The dealer: runs Shamir's scheme on $m$ and gets $f(x), s_{1}, \\ldots, s_{n}$; and computes and broadcasts commitments $B_{j} = g^{a_j}$ for $0 \\le j < t$. Reconstruction. The participants: each verify all the shares by checking whether $g^{s_i} = \\prod_{j=0}^{t-1} B_j^{i^j}$; and run Shamir's reconstruction protocol to recover $m$. Benefits: DL-based security for confidentiality. Information-theoretical security for integrity. Dealer cannot cheat. Downsides: Assumes DL assumption. Can only share uniformly random messages.","title":"3.1. Feldman VSS"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#32-pederson-vss","text":"In Pederson VSS , let $\\langle g \\rangle$ be a group wherein the Diffie-Hellman assumption holds. Also, let $h$ be a random element from $\\langle g \\rangle$, s.t. the discrete logarithm $\\log_g h$ is not known to anybody. Distribution. The dealer: generates two random polynomials $f_a(x),f_b(x)$ of order $t$, i.e. $f_a(x) = a_{t-1}x^{t-1} + \\ldots + a_{0}$ and $f_b(x) = b_{t-1}x^{t-1} + \\ldots + b_{0}$; sets the lowest order coefficients $a_0,b_0$ to $m$, s.t. $f_a(0) = f_b(0) = m$; evaluates $n$ points on the polynomial $s_{1} = (1, f_a(1), f_b(1)), \\ldots, s_{n} = (n, f_a(n), f_b(n))$, distributes them among the participants; and computes and broadcasts the commitments $C_j = g^{a_j}h^{b_j}$ for = $0 \\le j < t$. Reconstruction. The participants: collect at least $t$ shares; fit a polynomial $f'(x)$ between the points $s_{1}, \\ldots, s_{t}$, which can be done using Lagrange interpolation; and they recover $m = f'(0)$.","title":"3.2. Pederson VSS"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#benefits_1","text":"Information theoretical security for confidentiality. DL-based security for integrity. Dealer cannot cheat.","title":"Benefits"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#downsides_1","text":"Assumes DL assumption.","title":"Downsides"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#33-observation","text":"We see that both schemes are based on broadcasting commitment values. These commitments force the shares to be some kind of value, without revealing that actual value. A participant can use these commitments to verify if their share is actually correct, but from these commitments, one cannot reproduce the shares.","title":"3.3. Observation"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#4-sss-with-hashed-shares","text":"From my experiences implementing crypto, I find that correctly implementing discrete logarithm-based schemes is very difficult. We do however know another building block which we can use to build a commitment scheme, that is the hash function . For RWOT8, this was first proposed by Peg in his article : He says: \"This is also something we considered, but feel that it gives custodians more unnecessary extra information and less accountability compared to other methods.\" So, instead of committing to the polynomial coefficients, we commit to the values of the shares. I.e. we propose the following scheme: Distribution. The dealer: runs Shamir's scheme on $m$ and gets $f(x), s_{1}, \\ldots, s_{n}$; and computes and broadcasts commitments $D_i = H(s_i)$ for $1 \\le i \\le n$, where $H$ is a preimage resistant hash function. Reconstruction. The participants: each verify all the shares by checking whether $H(s_i) = D_i$; and run Shamir's reconstruction protocol to recover $m$. Broadcasting the hashes of the shares makes sure that no participant can manipulate with the shares, because their share's hash would not verify in the reconstruction phase. Futhermore, the other participants will know if a specific share is invalid, instead of ending up with a garbled or different reconstructed message. This property is shown by the following proof: Claim. Assume $H$ is preimage resistant, an honest dealer, a single malicious participant cannot force $m' \\ne m$ to be reconstructed. Proof. Assume a participant is able to force the reconstruction of $m'$, given $s_1, \\ldots, s_t, D_1, \\ldots, D_t$. Because $m' \\ne m$, $f'(x) \\ne f(x)$. Because $f(x)$ is of order $t$, at least one of the shares must be different from its original. W.l.o.g., assume only one share $s_\\mathcal{A}'$ is different from its original $s_\\mathcal{A}$. However, because the reconstruction succeeded, we know that $H(s_\\mathcal{A}') = D_A$, but $s_\\mathcal{A}' \\ne s_\\mathcal{A}$, so this contradicts preimage resistance. $\\square$ So if this option was always, possible, why is it not very commonly described by cryptographers as a verifiable secret sharing scheme? Well, the reason is that in this hash-based scheme, the dealer can cheat. In particular, the dealer can act as follows: They split a message into shares and distribute them to the participants. They compute the commitments $D_1, \\ldots, D_n$, but secretly changes one of them to $D_\\text{bad}$ before publishing. During the reconstruction phase, the participants will verify the shares and see that some share does not verify correctly. ] The participants will not know whether this participant was malicious, or if the dealer was malicious. In short, if the dealer is not completely trusted, we should not use this scheme. Instead we should probably use one of Section 3. In conclusion, here is a listing of benefits and downsides, similar to the listings that were provided in the previous sections.","title":"4. SSS with hashed shares"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#benefits_2","text":"Information theoretical security for confidentiality. Hash-based security for integrity.","title":"Benefits"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#downsides_2","text":"Dealer can cheat. Assumes hash function preimage resistance.","title":"Downsides"},{"location":"rwot8/topics-and-advance-readings/implementing-threshold-schemes/#5-implementing-a-secret-sharing-scheme","text":"This section will be different from the previous ones. Where the previous sections are mainly about the theory, this will mainly be based on my personal experiences while implementing my personal secret sharing library and other cryptographic software. As we all probably know, writing cryptographic software is hard. The main nemesis of a piece of cryptographic code is the side-channel attack . On regular PCs, the main attack that must be protected against is the (cache) timing attack. These attacks exploit the time that an algorithm takes to execute, which---on a computer---can be measured with a really precise resolution. A proposed solution to defend against this is to only run code on your computer that is trusted. However, we all know that this is nigh-on impossible to achieve, especially when we realize that web pages run tons of code that is not audited by the user. Instead, we have to write all the software in a deterministic manner. That means we cannot use if-statements or table-lookups that depend on secret data. Another consideration is the complexity of our cryptosystem. As a rule of thumb: Symmetric crypto and hash functions are relatively easy; Asymmetric crypto is generally very hard. Therefore, I would personally stay away from any discrete log based system, unless the requirements really ask for it. In my own library , I have opted to use the off-the-shelf AEAD \"Salsa20/Poly1305\" for integrity protection, and I have implemented all the arithmetic related to SSS in $GF(256)$ using bitslicing . An interesting case where we saw that the implementation of asymmetric crypto is hard where cryptonote-based currencies forgot to protect against small-subgroup attacks in Curve25519 in 2017.","title":"5. Implementing a secret sharing scheme"},{"location":"rwot8/topics-and-advance-readings/introductory-capability-dht-concept/","text":"Introductory Capability DHT Submission to: Rebooting the Web of Trust #8 (March 2019, Barcelona) Author: James Foley < james.foley@protonmail.com > The Object Capability software design paradigm is a powerful philosophy for the programming of decentralized applications particularly in the realms of security and rights management. There is great potential natural interoperability between programs through the interfaces of capabilities, easing efforts of program compositionality and enabling social interaction much greater than the sum of its parts. A wide variety of applications are possible and one can imagine fabric allowing applications to travel amongst hosts, remaining virtually accessible regardless of their physical substrate, not to mention the normal model of deploying an application to any server available. Any social application could be built in such a paradigm, be it a personal agent, or an application that is a larger composition of agents in some configuration, for instance a user\u2019s personal application representing their own selves introducing itself to a private social network of peers, a refugee organization aiding their search for sanctuary, or what have you. A key detail of such a system would be addressability for purposes of introduction between agents using durable names to connecting different nodes to create new networks, in a similar way that IP is employed today. This may demand a solution that works as an introductory network to a wide array of capability based systems, even ones running on different protocols and contexts, or perhaps even systems that are not compatible. It could work as a public utility providing resilient connectability to any application. One implementation solution could be a distributed capability routing network comprised of a DHT such that the keys would be resilient permanent addresses at which to reach up-to-date routing information with which to connect to an application at the prevailing moment. The advantages of such an approach would be resilience against centralized authorities with regard to addressing information. A node could migrate hosting infrastructure and its location on the internet all while maintaining a reliable address in the form of a capability registered on such a DHT and visible and consumable to all. A user running a node in this capability routing network would participate in the routing requests to their destination addresses. It would also be able to provide routing information at its own address that would be used to render a connection to the capability based application it is concerned with. Such information could simply be IP address and authentication information such as a capability address to invoke at that destination, or the node could be running a program to determine how to most correctly connect the incoming request to its larger application at that prevailing moment. A Kademlia style DHT would be a candidate to deploy such a network, mainly with regard to its addressing abilities with its built in notion of distance in address space. It would provide efficient routing towards an introductory capability. Storage optimizations such as caching data throughout the network would likely be meaningless for reasons of security possibly liveness. For security, caching throughout different nodes on the netowkr may invalidate a key affordance of such a network, the network privacy to each application utilizing it. For liveness, it may be potentially difficult to continually update the cached values of a key, which could cause problems for an application of scale trying to introduce new agents. At the same time, incoming requests would be very light weight and therefore throughput would not necessarily suffer. Should that become a bottleneck, then there is nothing stopping one application from occupying multiple keys on the DHT with introductory capabilities. This system stands apart from DNS and other decentralized solutions for addressing in the way that it would be a pure capability network, where each capability is granularly published, as opposed to different subdomains holding a single capability, depending on the hosting of that larger domain. It would lend itself to a pure capability workflow and all the potential tools afforded by that such as formal verification and clear legibility of roles in a distributed context.","title":"Introductory Capability DHT"},{"location":"rwot8/topics-and-advance-readings/introductory-capability-dht-concept/#introductory-capability-dht","text":"Submission to: Rebooting the Web of Trust #8 (March 2019, Barcelona) Author: James Foley < james.foley@protonmail.com > The Object Capability software design paradigm is a powerful philosophy for the programming of decentralized applications particularly in the realms of security and rights management. There is great potential natural interoperability between programs through the interfaces of capabilities, easing efforts of program compositionality and enabling social interaction much greater than the sum of its parts. A wide variety of applications are possible and one can imagine fabric allowing applications to travel amongst hosts, remaining virtually accessible regardless of their physical substrate, not to mention the normal model of deploying an application to any server available. Any social application could be built in such a paradigm, be it a personal agent, or an application that is a larger composition of agents in some configuration, for instance a user\u2019s personal application representing their own selves introducing itself to a private social network of peers, a refugee organization aiding their search for sanctuary, or what have you. A key detail of such a system would be addressability for purposes of introduction between agents using durable names to connecting different nodes to create new networks, in a similar way that IP is employed today. This may demand a solution that works as an introductory network to a wide array of capability based systems, even ones running on different protocols and contexts, or perhaps even systems that are not compatible. It could work as a public utility providing resilient connectability to any application. One implementation solution could be a distributed capability routing network comprised of a DHT such that the keys would be resilient permanent addresses at which to reach up-to-date routing information with which to connect to an application at the prevailing moment. The advantages of such an approach would be resilience against centralized authorities with regard to addressing information. A node could migrate hosting infrastructure and its location on the internet all while maintaining a reliable address in the form of a capability registered on such a DHT and visible and consumable to all. A user running a node in this capability routing network would participate in the routing requests to their destination addresses. It would also be able to provide routing information at its own address that would be used to render a connection to the capability based application it is concerned with. Such information could simply be IP address and authentication information such as a capability address to invoke at that destination, or the node could be running a program to determine how to most correctly connect the incoming request to its larger application at that prevailing moment. A Kademlia style DHT would be a candidate to deploy such a network, mainly with regard to its addressing abilities with its built in notion of distance in address space. It would provide efficient routing towards an introductory capability. Storage optimizations such as caching data throughout the network would likely be meaningless for reasons of security possibly liveness. For security, caching throughout different nodes on the netowkr may invalidate a key affordance of such a network, the network privacy to each application utilizing it. For liveness, it may be potentially difficult to continually update the cached values of a key, which could cause problems for an application of scale trying to introduce new agents. At the same time, incoming requests would be very light weight and therefore throughput would not necessarily suffer. Should that become a bottleneck, then there is nothing stopping one application from occupying multiple keys on the DHT with introductory capabilities. This system stands apart from DNS and other decentralized solutions for addressing in the way that it would be a pure capability network, where each capability is granularly published, as opposed to different subdomains holding a single capability, depending on the hosting of that larger domain. It would lend itself to a pure capability workflow and all the potential tools afforded by that such as formal verification and clear legibility of roles in a distributed context.","title":"Introductory Capability DHT"},{"location":"rwot8/topics-and-advance-readings/journalistic-use-cases/","text":"Journalistic use-cases for SSI: signatures, verified claims, and canonical-text registries DrafT: RWoT 8 Topic Paper by Juan Caballero & Jefferson Sofarelli (Berlin, DE) Abstract The goal of this paper is to compare notes with the SSI technical community and hopefully to model some possible solutions for middleware systems and/or SSI-powered widgets for CMS systems. Some of these could, we hope, be built soon on current platforms, to start benefitting journalists, factcheckers, and publics as soon as possible. Others, at industry-wide, national-scale, or semi-global scale, could not be built as quickly, but planning them in dialogue with the broader community is also a priority for us. Background This paper originates from a few separate working-groups with journalists to identify pain points and to imagine possible use-cases for SSI, drawing in particular from the experiences of developing-world journalists and editors battling misinformation, misattribution, and other identity issues in the press. The \"first world bias\" of tech platforms and social media has been discussed at length, so we will take it for granted, but would add that the journalistic pain points around misattribution, misinformation, and privacy are felt much more acutely in developing nations with potential regime-changes imminent and more fraught relations between journalists and government, and in many case the journalists working in these contexts prove themselves to be early-adopters of privacy tech and publishing tools compared to their developed-world analogues. One basic problem exacerbated by social-media manipulation and other misinformation campaigns is misattribution-- from doctored twitter screengrabs, to altered soundbytes to citations invented wholesale, this is a growing nuisance in the election cycles of the developing world. Many journalists have expressed a desire to extend the \"verification\" scheme of a closed platform (Twitter being the most common example) to something more cross-platform, such that with some degree of certainty whole articles could be cryptographically signed by a keyholder who has also signed public profiles like a twitter page, a facebook page, etc. Given that many news platforms are currently upgrading and expanding the technical capabilities of their CMS systems and backends, especially in countries battling misinformation proactively like Brasil and India, the timing would seem to be urgent. Our long-term proposal is to design an SSI-platform-agnostic DID widget or middle-ware system for CMSs such that at the time of committing a canonical version of a published piece on a given publication's CMS, that canonical version could be hashed and signed, with signature and hash being stored in an immutable, external record against which the signature could later be checked (i.e., making a verifiable claim of authorship linking the article's original published form to a DID controlled by its author). We have broken up the building blocks to be discussed at RWoT as: 1.) SSI capabilities for binding DIDs to articles in journalistic CMSs A simple first step would simply be a way of attaching a DID signature both to each author's user profile in a CMS (assuming industry-standard, i.e. not very complex CMS access control model), and also to their individual signed works. We were assuming we would work first with a a few specific proprietary CMSs, and eventually consider an exportable/agnostic widget across multiple CMS systems. A) For the sake of prototyping and division of labor/code control, would be to have an extra field in the CMS system in the profile for the author to manually add their DID as a string. This manual-DID capacity, perhaps obsoleted later, would circumvent the need to 'Login with DID button' in a working prototype-- what's more, only a CMS admin, not each individual author, would need this direct access, and could be done once per account. This presents its own issues (particularly if the DID needs to be alterable/updatable in the case of rotation/recovery), but for the sake of prototyping and early adoption, implementors would likely prefer they not have to execute ANY external code in the backend. This would thus allow authors to confirm/sign individual pieces/articles afterwards against the DID thus saved in the system. In addition, the hash / finterprint of that article could still be stored externally (see below, component 3), so a later verfication is still possible. B) From a UX point of view, it's worth explicitly stating that regardless of whether (C) DID is presented at login and/or (A) DID is stored manually or programmatically alongside other account credentials in the access control model of the CMS, this session-level and account-level signature is NOT enough for the journalistic use cases we've studied-- there needs to be a unique signature for EACH ARTICLE in the CMS. (Perhaps even for each version of the article depending on the costs and structure of the CMS and of component #3!). The vulnerabilities of an open session (cookie hijacks, or even just physical control of the computer while the session is open) make a unique DID authentication at time of article commitment essential. By analogy to amazon (which requires password confirmation/re-entry before purchase), DID-pegged publishing should reaffirm/authenticate at time of finalizing any publication. Cell-phone-based SSI authentication (say, by QRCode) is our working draft thesis, although we want all the tires kicked and all the alternatives discussed if there's time! C) One SSI capability could be a \"Sign in with SSI button\" alongside other at point of CMS author login. We would like to hear from the SSI community if there are interoperable standards emerging of the kind proposed publically by uPort for various major CMS systems commonly used in journalism, such that a DID holder could verify their identity and account across publications. This is more important to UX and uptake of a finished solution than to the actual verification needs the broader system could meet, but there are also use-cases where a publication might want to attest that an author uploaded their own work directly/themselves, or when. For these reasons, 2.) SSI for casual journalists and occasional contributors Similarly, many controversial and influential pieces of journalism are not written by professional journalists, but by experts and technicians from various other fields. For these kinds of authors only occasionally publishing open letters, \"op-ed\"s, and other public statements, it is sometimes necessary to be able to make verifiable claims about education, expertise, and employment, rather than linking back to a short trail of publications. For this reason, our group would like to check in with the Verified Claims for Education working group and the representatives of government on the status of various registries that soon be online for these purposes. Anything we design and pilot would need to be maximally interoperable with emerging Verified Claims (#VC) standards to be useful in this corner case. The idea is to imagine a minimal meta-ID system or middleware, along the model of \"Twitter verification,\" whether that be powered by DID #VCs or otherwise. 3.) An SSI-powered immutable publication registry Immutable registries of articles (technically, of canonical versions of article, with later corrections stored elsewhere) are increasingly becoming popular projects among journalists for many reasons. The Ethereum-based startup Civic (and its subsidiary/partner, Popula) has been piloting a program to actually encode all of its published pieces immutably in the ethereum chain itself, while other publications have discussed other immutable and/or decentralized alternatives to traditional, hosted RSS or XML. An assessment of technical options and costs at different scales will probably be beyond the scale of a single RWoT meeting, but sketching out the parameters for such an assessment in consultation with the community might be a reasonable goal for this RWoT. Other ramifications We are not yet at the stage of actively researching GDPR or eIDAS ramifications, but we are hoping to learn more about those from other, further along projects and welcome any input.","title":"Journalistic use-cases for SSI:  signatures, verified claims, and canonical-text registries"},{"location":"rwot8/topics-and-advance-readings/journalistic-use-cases/#journalistic-use-cases-for-ssi-signatures-verified-claims-and-canonical-text-registries","text":"","title":"Journalistic use-cases for SSI:  signatures, verified claims, and canonical-text registries"},{"location":"rwot8/topics-and-advance-readings/journalistic-use-cases/#draft-rwot-8-topic-paper","text":"by Juan Caballero & Jefferson Sofarelli (Berlin, DE)","title":"DrafT: RWoT 8 Topic Paper"},{"location":"rwot8/topics-and-advance-readings/journalistic-use-cases/#abstract","text":"The goal of this paper is to compare notes with the SSI technical community and hopefully to model some possible solutions for middleware systems and/or SSI-powered widgets for CMS systems. Some of these could, we hope, be built soon on current platforms, to start benefitting journalists, factcheckers, and publics as soon as possible. Others, at industry-wide, national-scale, or semi-global scale, could not be built as quickly, but planning them in dialogue with the broader community is also a priority for us.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/journalistic-use-cases/#background","text":"This paper originates from a few separate working-groups with journalists to identify pain points and to imagine possible use-cases for SSI, drawing in particular from the experiences of developing-world journalists and editors battling misinformation, misattribution, and other identity issues in the press. The \"first world bias\" of tech platforms and social media has been discussed at length, so we will take it for granted, but would add that the journalistic pain points around misattribution, misinformation, and privacy are felt much more acutely in developing nations with potential regime-changes imminent and more fraught relations between journalists and government, and in many case the journalists working in these contexts prove themselves to be early-adopters of privacy tech and publishing tools compared to their developed-world analogues. One basic problem exacerbated by social-media manipulation and other misinformation campaigns is misattribution-- from doctored twitter screengrabs, to altered soundbytes to citations invented wholesale, this is a growing nuisance in the election cycles of the developing world. Many journalists have expressed a desire to extend the \"verification\" scheme of a closed platform (Twitter being the most common example) to something more cross-platform, such that with some degree of certainty whole articles could be cryptographically signed by a keyholder who has also signed public profiles like a twitter page, a facebook page, etc. Given that many news platforms are currently upgrading and expanding the technical capabilities of their CMS systems and backends, especially in countries battling misinformation proactively like Brasil and India, the timing would seem to be urgent. Our long-term proposal is to design an SSI-platform-agnostic DID widget or middle-ware system for CMSs such that at the time of committing a canonical version of a published piece on a given publication's CMS, that canonical version could be hashed and signed, with signature and hash being stored in an immutable, external record against which the signature could later be checked (i.e., making a verifiable claim of authorship linking the article's original published form to a DID controlled by its author). We have broken up the building blocks to be discussed at RWoT as:","title":"Background"},{"location":"rwot8/topics-and-advance-readings/journalistic-use-cases/#1-ssi-capabilities-for-binding-dids-to-articles-in-journalistic-cmss","text":"A simple first step would simply be a way of attaching a DID signature both to each author's user profile in a CMS (assuming industry-standard, i.e. not very complex CMS access control model), and also to their individual signed works. We were assuming we would work first with a a few specific proprietary CMSs, and eventually consider an exportable/agnostic widget across multiple CMS systems. A) For the sake of prototyping and division of labor/code control, would be to have an extra field in the CMS system in the profile for the author to manually add their DID as a string. This manual-DID capacity, perhaps obsoleted later, would circumvent the need to 'Login with DID button' in a working prototype-- what's more, only a CMS admin, not each individual author, would need this direct access, and could be done once per account. This presents its own issues (particularly if the DID needs to be alterable/updatable in the case of rotation/recovery), but for the sake of prototyping and early adoption, implementors would likely prefer they not have to execute ANY external code in the backend. This would thus allow authors to confirm/sign individual pieces/articles afterwards against the DID thus saved in the system. In addition, the hash / finterprint of that article could still be stored externally (see below, component 3), so a later verfication is still possible. B) From a UX point of view, it's worth explicitly stating that regardless of whether (C) DID is presented at login and/or (A) DID is stored manually or programmatically alongside other account credentials in the access control model of the CMS, this session-level and account-level signature is NOT enough for the journalistic use cases we've studied-- there needs to be a unique signature for EACH ARTICLE in the CMS. (Perhaps even for each version of the article depending on the costs and structure of the CMS and of component #3!). The vulnerabilities of an open session (cookie hijacks, or even just physical control of the computer while the session is open) make a unique DID authentication at time of article commitment essential. By analogy to amazon (which requires password confirmation/re-entry before purchase), DID-pegged publishing should reaffirm/authenticate at time of finalizing any publication. Cell-phone-based SSI authentication (say, by QRCode) is our working draft thesis, although we want all the tires kicked and all the alternatives discussed if there's time! C) One SSI capability could be a \"Sign in with SSI button\" alongside other at point of CMS author login. We would like to hear from the SSI community if there are interoperable standards emerging of the kind proposed publically by uPort for various major CMS systems commonly used in journalism, such that a DID holder could verify their identity and account across publications. This is more important to UX and uptake of a finished solution than to the actual verification needs the broader system could meet, but there are also use-cases where a publication might want to attest that an author uploaded their own work directly/themselves, or when. For these reasons,","title":"1.) SSI capabilities for binding DIDs to articles in journalistic CMSs"},{"location":"rwot8/topics-and-advance-readings/journalistic-use-cases/#2-ssi-for-casual-journalists-and-occasional-contributors","text":"Similarly, many controversial and influential pieces of journalism are not written by professional journalists, but by experts and technicians from various other fields. For these kinds of authors only occasionally publishing open letters, \"op-ed\"s, and other public statements, it is sometimes necessary to be able to make verifiable claims about education, expertise, and employment, rather than linking back to a short trail of publications. For this reason, our group would like to check in with the Verified Claims for Education working group and the representatives of government on the status of various registries that soon be online for these purposes. Anything we design and pilot would need to be maximally interoperable with emerging Verified Claims (#VC) standards to be useful in this corner case. The idea is to imagine a minimal meta-ID system or middleware, along the model of \"Twitter verification,\" whether that be powered by DID #VCs or otherwise.","title":"2.) SSI for casual journalists and occasional contributors"},{"location":"rwot8/topics-and-advance-readings/journalistic-use-cases/#3-an-ssi-powered-immutable-publication-registry","text":"Immutable registries of articles (technically, of canonical versions of article, with later corrections stored elsewhere) are increasingly becoming popular projects among journalists for many reasons. The Ethereum-based startup Civic (and its subsidiary/partner, Popula) has been piloting a program to actually encode all of its published pieces immutably in the ethereum chain itself, while other publications have discussed other immutable and/or decentralized alternatives to traditional, hosted RSS or XML. An assessment of technical options and costs at different scales will probably be beyond the scale of a single RWoT meeting, but sketching out the parameters for such an assessment in consultation with the community might be a reasonable goal for this RWoT.","title":"3.) An SSI-powered immutable publication registry"},{"location":"rwot8/topics-and-advance-readings/journalistic-use-cases/#other-ramifications","text":"We are not yet at the stage of actively researching GDPR or eIDAS ramifications, but we are hoping to learn more about those from other, further along projects and welcome any input.","title":"Other ramifications"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./","text":"libp2p for DID Authentication and the exchange of Verifiable Credentials *Draft: Rebooting Web of Trust VIII Topic Paper by jonnycrunch Rebooting the Web of Trust, Spring 2019 Abstract Peer to peer protocols in general have a distributed application architecture whereby each participant have an equal privilege to participant in the system [1]. The concept was popularized with the music file sharing application Napster and more recently the Interplanetary File System (IPFS) . Since the emergence of the first draft of the DID specificaiton from the Fall 2016 Rebooting the Web of Trust [2] , numerous DID method specifications have appeared. Each DID method specification defines how to resolve a cryptographically-tied DID document given a method-specific identifier. In our last paper we presented how IPLD could be used as a generalized framework to repesent and resolve the DID document. In this paper, we will describe a way to perform DID authenication and thereby proove ownership of the private key that is presented in the DID document utilizing the libp2p protocol. Operating within one particular DID method simply requires applying specific protocols dictated in it's DID method specification and typically under the constraints of it's distributed ledger or network. However, authentication between DID methods, where may have implemented disperse cryptographic suite of tools will proove to be more challenging. In this paper, we would like to introduce the libp2p protocol to solve this problem. Why libp2p Libp2p is a modular networking framework that enables peer-to-peer applications. Originally part of the Interplanetary File system (IPFS), it has since broke out into it's own separate project and supports a variety of applications including Parity's Polkadot and is being evaluated for Ethereum 2.0 . In traditional client/server model a stateless communication channel is iniated via an IP address and port combination. Libp2p uses the concept of a multiaddress instead. Some examples: - /ip4/157.230.86.31/udp/9999 indicates the node with IP version 4 address of 157.230.86.31 and listening on UDP on port 9999 - /ip6/ffe80::21f:5bff:fe38:6d91/udp/1567/quic means that we should use the QUIC protocol on top of UDP port 1567 with an IPv6 address - /dns4/example.com/tcp/80/ws means that we should use the WebSocket protocol on top of TCP port 80, using DNS to resolve the hostname example.com DID authentication The term DID authenctication is still not currently well defined [1]. From a white paper from the Rebooting web of Trust in 2018, DID authenication is defined as a ceremony where the identity owner, with help of various components such as web browsers, mobile devices and other agent proves to a relying party that they are in control of a DID. This ceremony involves leveraging a number of pre-determined data formats, protocols and flows that transcend individual DID methods, but ultimately arrives at the end that the relaying party is sufficiently satified that the proving party is in control over the private key associated with the the public key published to the DID document specified as part of the DID method specific identifier. The DID specification allows for attribute declaration of multiple services including for DID authentication. After DID authentication (and thereby prooving control over the private key associated with the DID method specific identifer as declaried in the DID document), a multitude of other service and workflows can be performed including Authorization, Access control lists, exchange of Verifiable Credentials and invoking an Object Capability Systems . Example 1: DID authentication declaration inside an example DID document [3]: { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:example:123456789abcdefghi\", \"service\": { \"type\": \"DidAuthService\", \"serviceEndpoint\": \"https://auth.example.com/did:example:123456789abcdefg\" } } Example 2. DID authentication declaration using libp2p as a service endpoint { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"service\": { \"type\": \"DidAuthService\", \"serviceEndpoint\": \"/ip4/108.241.29.82/tcp/1265/ipfs/QmZTefe4V1KYwLUfhGVMMBbAkAa4E9vynzSNx5vPtrG4dv\", } } This paper will review the mechanism and workflow specifically for DID authentication in the form of data formats, challenges and responses to facilitate proving ownership of a private key published to the DID docuemnt for a speficic DID method specific identifer followed by issuing a Verifiable Credential. DID authencication may involved one-way interactions or a bi-directional protocol over each respective DID authentication server endpoints. We will explore both a unidirections DID authenications and bidirectional DID authentication over the libp2p protocol. DID authentication, at it's simplest interpretation, could be perceived as being the verification of a credential, and in this case the claim is control over the private key associated with the DID method specific identier ( or the prive key associated with the hash of the public key ). libp2p uses multiformats extensively discoverablilty and connectivity of peers process addressing ( when you connect that process you can authenicate ) future proof stack open source encrypted connections by default use in high latency scenarios work offline (offline first) protocol multiplexing works in the browser roaming ( independently coordinate the mobile edge client ) works in the browser ( webRTC ) used in Metamask for moving blocks of ethereum blockchain into the browser used by Parity networks Pokidot used by Filecoin Overview of circuit relay The circuit relay is a means to establish connectivity between libp2p nodes that wouldn't otherwise be able to establish a direct connection to each other. [4] Relays are generally used in situations where nodes are behind NAT, reverse proxies, firewalls and/or simply don't support the same transports (e.g. go-ipfs vs. browser-ipfs). Even though libp2p has modules for NAT traversal (go-libp2p-nat), piercing through NATs isn't always an option. The circuit relay protocol exists to overcome those scenarios. In the context of DID authentication circuit relays could be used to prevent correlation based on IPV4 or IPV6 addresses. The relay node short-circuits streams between the two nodes, enabling them to reach each other and prevent correlation. +-----+ /ip4/.../tcp/.../ws/p2p/QmRelay +-------+ /ip4/.../tcp/.../p2p/QmTwo +-----+ |QmOne| <------------------------------------>|QmRelay|<----------------------------------->|QmTwo| +-----+ (/libp2p/relay/circuit multistream) +-------+ (/libp2p/relay/circuit multistream) +-----+ ^ +-----+ ^ | /p2p-circuit/QmTwo | | | +-----------------------------------------+ +-----------------------------------------+ In the above example, the node labeled QmOne is connected to node QmTwo via a circuit relay QmRelay . QmRelay is configured to allow for relaying of connections between nodes and QmOne is configured to use QmRelay for relaying. All communication is encrypted between QmOne and QmTwo and the identity of the remote is verified, so the proxy QmRelay cannot act as a man-in-the-middle. DID authentication using circuit relaying is shown in Example 3. Example 3. DID authentication declaration using libp2p as a service endpoint with circuit relay. { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"service\": { \"type\": \"DidAuthService\", \"serviceEndpoint\": \"/p2p-circuit/QmTwo\", } } Problems with DNS problem with location addressing problem with root of trust Example flow for DID Authentication TBD Benefits Correlation can be prevented using circuit relays True peer-to-peer communication of Ability to connect edge devices to operate without other connectivity to the rest of the internet Drawbacks Unfortunately given the multiple levels of networking protocol, there are multiple layers of authenication and given the need for interoperability for other DID methods we need to add yet another on top of libp2p to allow for DID authentication. In a simple world, the DID authenications would happen at the protocol layer. QUIC implementation only supports RSA peer keys (Support for ed25519 and secp256k1 is pending ) Multiaddress is not yet standardized thru IETF Conclusion libp2p provides a robust framework for a lightweight protocol that faciliates cryptographic DID authentication over a variety of DID methods. Discussion and Future Work In this brief paper, we have introduced the application of libp2p as a general pattern for DID authentication and have highlighted its potential benefits and drawbacks and explained how it could be used across multiple DID method specifications. This technique enables a cost-effective and scalable solution for truly decentralized service endpoints to proove control over the private cryptographic key. This framework could be used to accelerate the adoption of truly self-sovereign digital identities. In the future, we hope to formalize this approach with additional stakeholders and standard bodies. Definitions Authentication : The ceremony where an identity owner proves to a relying party that the identity owner controls a DID, using a mechanism that is described in the DID's associated DID Document. Authorization : A process of establishing the rights and privileges of an identity owner to perform certain actions, including operations on a DID itself, or in another context. Decentralized Identifier (DID) : A globally unique identifier that does not require a centralized registration authority because it is registered with distributed ledger technology or another form of decentralized network. (see here ) DID Document : A structured document containing metadata that describes a DID, including authentication materials such as public keys and pseudonymous biometrics, that an entity can use to authenticate, i.e. to prove control of the DID. A DID Document may also contain other attributes or claims describing the entity. (see here ) DID Record : The combination of a DID and its associated DID Document. Identity Owner : The individual, organization or thing who created the DID, is identified by the DID that is the subject of the DID Document, and who has the ultimate authority to update or revoke the DID. Relying Party : The individual, organization or thing that authenticates an identity owner using a DID Auth protocol. Also called \"Verifier\" in other specifications. Verifiable Credentials : A set of one or more claims that are statements made by an issuer about a subject that is tamper-resistant and whose authorship can be cryptographically verified (see here ). References [1] Peer-to-peer protocols . [2] DID (Decentralized Identifier) Data Model and Generic Syntax 1.0 Implementer's Draft 01 [3] Rebooting Web of Trust Spring 2018 - Introduction to DID Auth","title":"libp2p for DID Authentication and the exchange of Verifiable Credentials"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#libp2p-for-did-authentication-and-the-exchange-of-verifiable-credentials","text":"*Draft: Rebooting Web of Trust VIII Topic Paper by jonnycrunch Rebooting the Web of Trust, Spring 2019","title":"libp2p for DID Authentication and the exchange of Verifiable Credentials"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#abstract","text":"Peer to peer protocols in general have a distributed application architecture whereby each participant have an equal privilege to participant in the system [1]. The concept was popularized with the music file sharing application Napster and more recently the Interplanetary File System (IPFS) . Since the emergence of the first draft of the DID specificaiton from the Fall 2016 Rebooting the Web of Trust [2] , numerous DID method specifications have appeared. Each DID method specification defines how to resolve a cryptographically-tied DID document given a method-specific identifier. In our last paper we presented how IPLD could be used as a generalized framework to repesent and resolve the DID document. In this paper, we will describe a way to perform DID authenication and thereby proove ownership of the private key that is presented in the DID document utilizing the libp2p protocol. Operating within one particular DID method simply requires applying specific protocols dictated in it's DID method specification and typically under the constraints of it's distributed ledger or network. However, authentication between DID methods, where may have implemented disperse cryptographic suite of tools will proove to be more challenging. In this paper, we would like to introduce the libp2p protocol to solve this problem.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#why-libp2p","text":"Libp2p is a modular networking framework that enables peer-to-peer applications. Originally part of the Interplanetary File system (IPFS), it has since broke out into it's own separate project and supports a variety of applications including Parity's Polkadot and is being evaluated for Ethereum 2.0 . In traditional client/server model a stateless communication channel is iniated via an IP address and port combination. Libp2p uses the concept of a multiaddress instead. Some examples: - /ip4/157.230.86.31/udp/9999 indicates the node with IP version 4 address of 157.230.86.31 and listening on UDP on port 9999 - /ip6/ffe80::21f:5bff:fe38:6d91/udp/1567/quic means that we should use the QUIC protocol on top of UDP port 1567 with an IPv6 address - /dns4/example.com/tcp/80/ws means that we should use the WebSocket protocol on top of TCP port 80, using DNS to resolve the hostname example.com","title":"Why libp2p"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#did-authentication","text":"The term DID authenctication is still not currently well defined [1]. From a white paper from the Rebooting web of Trust in 2018, DID authenication is defined as a ceremony where the identity owner, with help of various components such as web browsers, mobile devices and other agent proves to a relying party that they are in control of a DID. This ceremony involves leveraging a number of pre-determined data formats, protocols and flows that transcend individual DID methods, but ultimately arrives at the end that the relaying party is sufficiently satified that the proving party is in control over the private key associated with the the public key published to the DID document specified as part of the DID method specific identifier. The DID specification allows for attribute declaration of multiple services including for DID authentication. After DID authentication (and thereby prooving control over the private key associated with the DID method specific identifer as declaried in the DID document), a multitude of other service and workflows can be performed including Authorization, Access control lists, exchange of Verifiable Credentials and invoking an Object Capability Systems . Example 1: DID authentication declaration inside an example DID document [3]: { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:example:123456789abcdefghi\", \"service\": { \"type\": \"DidAuthService\", \"serviceEndpoint\": \"https://auth.example.com/did:example:123456789abcdefg\" } } Example 2. DID authentication declaration using libp2p as a service endpoint { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"service\": { \"type\": \"DidAuthService\", \"serviceEndpoint\": \"/ip4/108.241.29.82/tcp/1265/ipfs/QmZTefe4V1KYwLUfhGVMMBbAkAa4E9vynzSNx5vPtrG4dv\", } } This paper will review the mechanism and workflow specifically for DID authentication in the form of data formats, challenges and responses to facilitate proving ownership of a private key published to the DID docuemnt for a speficic DID method specific identifer followed by issuing a Verifiable Credential. DID authencication may involved one-way interactions or a bi-directional protocol over each respective DID authentication server endpoints. We will explore both a unidirections DID authenications and bidirectional DID authentication over the libp2p protocol. DID authentication, at it's simplest interpretation, could be perceived as being the verification of a credential, and in this case the claim is control over the private key associated with the DID method specific identier ( or the prive key associated with the hash of the public key ).","title":"DID authentication"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#libp2p","text":"uses multiformats extensively discoverablilty and connectivity of peers process addressing ( when you connect that process you can authenicate ) future proof stack open source encrypted connections by default use in high latency scenarios work offline (offline first) protocol multiplexing works in the browser roaming ( independently coordinate the mobile edge client ) works in the browser ( webRTC ) used in Metamask for moving blocks of ethereum blockchain into the browser used by Parity networks Pokidot used by Filecoin","title":"libp2p"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#overview-of-circuit-relay","text":"The circuit relay is a means to establish connectivity between libp2p nodes that wouldn't otherwise be able to establish a direct connection to each other. [4] Relays are generally used in situations where nodes are behind NAT, reverse proxies, firewalls and/or simply don't support the same transports (e.g. go-ipfs vs. browser-ipfs). Even though libp2p has modules for NAT traversal (go-libp2p-nat), piercing through NATs isn't always an option. The circuit relay protocol exists to overcome those scenarios. In the context of DID authentication circuit relays could be used to prevent correlation based on IPV4 or IPV6 addresses. The relay node short-circuits streams between the two nodes, enabling them to reach each other and prevent correlation. +-----+ /ip4/.../tcp/.../ws/p2p/QmRelay +-------+ /ip4/.../tcp/.../p2p/QmTwo +-----+ |QmOne| <------------------------------------>|QmRelay|<----------------------------------->|QmTwo| +-----+ (/libp2p/relay/circuit multistream) +-------+ (/libp2p/relay/circuit multistream) +-----+ ^ +-----+ ^ | /p2p-circuit/QmTwo | | | +-----------------------------------------+ +-----------------------------------------+ In the above example, the node labeled QmOne is connected to node QmTwo via a circuit relay QmRelay . QmRelay is configured to allow for relaying of connections between nodes and QmOne is configured to use QmRelay for relaying. All communication is encrypted between QmOne and QmTwo and the identity of the remote is verified, so the proxy QmRelay cannot act as a man-in-the-middle. DID authentication using circuit relaying is shown in Example 3. Example 3. DID authentication declaration using libp2p as a service endpoint with circuit relay. { \"@context\": \"https://w3id.org/did/v1\", \"id\": \"did:ipid:12D3KooWMHdrzcwpjbdrZs5GGqERAvcgqX3b5dpuPtPa9ot69yew\", \"service\": { \"type\": \"DidAuthService\", \"serviceEndpoint\": \"/p2p-circuit/QmTwo\", } }","title":"Overview of circuit relay"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#problems-with-dns","text":"problem with location addressing problem with root of trust","title":"Problems with DNS"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#example-flow-for-did-authentication","text":"TBD","title":"Example flow for DID Authentication"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#benefits","text":"Correlation can be prevented using circuit relays True peer-to-peer communication of Ability to connect edge devices to operate without other connectivity to the rest of the internet","title":"Benefits"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#drawbacks","text":"Unfortunately given the multiple levels of networking protocol, there are multiple layers of authenication and given the need for interoperability for other DID methods we need to add yet another on top of libp2p to allow for DID authentication. In a simple world, the DID authenications would happen at the protocol layer. QUIC implementation only supports RSA peer keys (Support for ed25519 and secp256k1 is pending ) Multiaddress is not yet standardized thru IETF","title":"Drawbacks"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#conclusion","text":"libp2p provides a robust framework for a lightweight protocol that faciliates cryptographic DID authentication over a variety of DID methods.","title":"Conclusion"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#discussion-and-future-work","text":"In this brief paper, we have introduced the application of libp2p as a general pattern for DID authentication and have highlighted its potential benefits and drawbacks and explained how it could be used across multiple DID method specifications. This technique enables a cost-effective and scalable solution for truly decentralized service endpoints to proove control over the private cryptographic key. This framework could be used to accelerate the adoption of truly self-sovereign digital identities. In the future, we hope to formalize this approach with additional stakeholders and standard bodies.","title":"Discussion and Future Work"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#definitions","text":"Authentication : The ceremony where an identity owner proves to a relying party that the identity owner controls a DID, using a mechanism that is described in the DID's associated DID Document. Authorization : A process of establishing the rights and privileges of an identity owner to perform certain actions, including operations on a DID itself, or in another context. Decentralized Identifier (DID) : A globally unique identifier that does not require a centralized registration authority because it is registered with distributed ledger technology or another form of decentralized network. (see here ) DID Document : A structured document containing metadata that describes a DID, including authentication materials such as public keys and pseudonymous biometrics, that an entity can use to authenticate, i.e. to prove control of the DID. A DID Document may also contain other attributes or claims describing the entity. (see here ) DID Record : The combination of a DID and its associated DID Document. Identity Owner : The individual, organization or thing who created the DID, is identified by the DID that is the subject of the DID Document, and who has the ultimate authority to update or revoke the DID. Relying Party : The individual, organization or thing that authenticates an identity owner using a DID Auth protocol. Also called \"Verifier\" in other specifications. Verifiable Credentials : A set of one or more claims that are statements made by an issuer about a subject that is tamper-resistant and whose authorship can be cryptographically verified (see here ).","title":"Definitions"},{"location":"rwot8/topics-and-advance-readings/libp2p_did_auth./#references","text":"[1] Peer-to-peer protocols . [2] DID (Decentralized Identifier) Data Model and Generic Syntax 1.0 Implementer's Draft 01 [3] Rebooting Web of Trust Spring 2018 - Introduction to DID Auth","title":"References"},{"location":"rwot8/topics-and-advance-readings/multiformat-superfriends/","text":"The Multibase, Multihash, and Hashlink Specifications By Manu Sporny and Ganesh Annan; Digital Bazaar This document outlines three specifications that have recently become work items of the W3C Credentials Community Group. The authors of these specifications desire feedback from the Rebooting the Web of Trust community to, if necessary, further expand the capabilities to address other use cases that are of importance to the individuals and organizations that attend RWoT8. Multibase The Multibase specification was published a few weeks ago as an IETF Internet-Draft. The specification is intended to be a joint work product of Protocol Labs (IPFS and Filecoin), the W3C Digital Verification Community Group, and the W3C Credentials Community Group. So, what problem does Multibase solve and why do we care? Our community encodes binary data in things like DID Documents and URLs. For example, both the Veres One and Sovrin DIDs encode the public key in the DID itself (these are legacy DIDs from past development versions, and are provided for illustration purposes only): did:v1:test:nym:z279ikfef3QbKRgnvTFmq5wEzZpe7s4fBiMb97hSEezXv3SL did:sov:WRfXPg8dantKVubE3HX8pw That bit at the end there is binary data (the public key) that is \"base-encoded\" so that it's easy (for developers) to copy/paste that stuff around and put it in emails when talking about technical things. The problem is that we don't know what base-encoding format was used above (base64 with padding, base64url, base58btc)? To solve this problem, we add a single byte to the beginning of the encoded value, so this: did:v1:test:nym:279ikfef3QbKRgnvTFmq5wEzZpe7s4fBiMb97hSEezXv3SL turns into this (note the addition of 'z', which means \"the data you are about to read is encoded in Bitcoin's base-58 encoding\"): did:v1:test:nym:z279ikfef3QbKRgnvTFmq5wEzZpe7s4fBiMb97hSEezXv3SL This is useful because this mechanism allows us to encode the same information in a variety of different ways. For example, Ethereum folks tend to use base16lower encoded values for their addresses while Bitcoin folks use a Bitcoin flavor of base58. At some level, these are sort of arbitrary decisions that developers fight over, but with Multibase, the decision is translated into a simple matter of value conversion. You don't have to fight over the base encoding format because Multibase encodes it in the resulting value, making conversion simple and deterministic and enabling future upgradability for your system (and compatibility with other systems). Multibase also allows developers to change the encoding format at a later date because the software was written with upgradability in mind from the beginning. The spec is a fairly short one, weighing in at 7 pages and can be found here: https://tools.ietf.org/html/draft-multiformats-multibase-01 Multihash The Multihash specification was drafted before/during the last RWoT and has been released as an IETF Internet-Draft. The specification is intended to be a joint work product of Protocol Labs (IPFS and Filecoin), the W3C Digital Verification Community Group, and the W3C Credentials Community Group. So, what problem does Multihash solve and why do we care? Our community depends on cryptographic hashes to generate short fingerprints for files and data. These fingerprints look like this as binary data: 0x0a4ec6f1629e49262d7093e2f82a3278 While that may look like gobbledygook to a human reader, it's enough for a computer to uniquely identify, for example, one picture among all of the other trillions of pictures on the Web. We use these fingerprints when working with Verifiable Credentials, DIDs, public keys, and evidence data (images, PDFs, etc.) that are linked to Verifiable Credentials. The problem is that we don't know what sort of cryptographic hash function generated the hash above, so we need to put an identifier at the front of that cryptographic hash. That's basically what Multihash does: it provides a mechanism to identify how cryptographic hashes were generated. This is useful because if we have a cryptographic hash identifier, we can design upgradability into the systems we're building today. Cryptographic hashing systems are broken every 10 years or so -- they have a shelf life and will eventually need to be upgraded. Building this concept of upgradability into our systems, especially Decentralized Identifiers and Verifiable Credentials, is a good engineering practice and the Multihash spec enables us to do just that. It's a fairly short 10 page spec and can be found here: https://tools.ietf.org/html/draft-multiformats-multihash-00 Hashlink The Hashlink specification is intended to be a way to enforce that the content at a given hyperlink has not changed since it was published. The specification is a joint work product of the W3C Digital Verification Community Group, JSON-LD WG (unofficial), VCWG (unofficial), and the W3C Credentials Community Group. Ok, so what's the problem and why do we care? We do the following things in this community: Link to JSON-LD Contexts Link to ZKP Schemas Link to evidence from Verifiable Credentials Link to external data from DID Documents ... and we have no way to tell if the content at the end of that link has changed since the Verifiable Credential was issued, or the DID Document was created. In the best case, verification of the cryptographic signature fails (because the content at the end of the link changed). In the average case, the signature doesn't fail, and we use data that may have been modified. In the worst case, the data was modified by an attacker and really bad things happen (like a developer being sloppy with how they use JSON-LD Contexts in a financial system and the source/destination fields being flipped in a financial transaction). There are mitigations for all of these issues, but they require the developer to be aware of the problem in the first place. What would be great is if we could trust that when we use a hyperlink in our systems (like a Verifiable Credential or a DID Document), and digitally sign that hyperlink, that we're also signing the content at the hyperlink. That's what the hashlink specification enables us to do. Here's a simple example for the JSON-LD Context for the current DID Spec. Here's the current link: https://w3id.org/did/v0.11 You have no idea if the version you retrieved and the version I retrieved are the same. Now, here's what it looks like when we secure it with a \"Legacy URL\" Hashlink (blake2b 8 byte multihash + base58btc multibase): https://w3id.org/did/v0.11?hl=z3aq31uzgnZBuWNzUB Now we both know that the version we download is the same (because of the little bit at the end starting with \"hl=\", which stands for Hashlink). Here's what it looks like as a (non-legacy) Hashlink URL: hl:z3aq31uzgnZBuWNzUB:zpr1Xd34f3NYqfr1yMzb6TBCrWWrvJeGVRJGsUMMyVWXS8 Yes, that looks awful, but there are a number of redeeming characteristics with the second form. The first is that the latter value is optional -- you can discard everything after the second colon and it's still a valid Hashlink URL. All you really need is \"hl:z3aq31uzgnZBuWNzUB\" and you can get the data from anywhere else (like different URLs, a local cache, etc.) and still thwart the attacks listed above. The other nice thing about Hashlink URLs, and this is the really exciting bit, is that you can create multi-sourced hashlinks that span completely different network architectures. Let's say that you had information that you really wanted to make sure didn't disappear (like the DID JSON-LD Context above), and you publish it at the following locations: https://w3id.org/did/v0.11 magnet:?xt=urn:btih:73C59D931B7E0C089C031D6CFE0D16AE ipfs://ipfs/QmR7W4GQUFWDPMVrQfmNE8xJC6LoVAyaWeRnDp4gS9/did-v0.11 onion://pq6kufupl4mc43g2/didv0.11 The Hashlink URL would be really long, something like this: hl:z3aq31uzgnZBuWNzUB:zeGVRWXS8TakFeJueF2bim3PaaDqbtqjkpxUc8ETS WXe6dQLWXQWvqiUdw8TJrncx3uKhwfc88MtM5xZbR27FhVRUKv9ogekamVtdE3U bXnXpMRT1AseCtoBUt1NE8x2SsnJxGfiZN45VVSCp6jh4dgcufL16tWrHREiSYE SEGP1J75yXCvAdvKPr7nb5aY ... but the Hashlink URL above 1) ensures the integrity of the thing you're linking to, and 2) still works if 3 out of the 4 networks listed above failed. To put it another way, the link above could survive (for example) the failure of the Web, BitTorrent, and Tor. This approach also solves the following problems that other communities in our orbit are having by: JSON-LD WG: Enabling backwards-compatible content integrity for JSON- LD Contexts. JSON-LD WG: Enabling non-Web-based, multi-sourced, compact, content integrity for JSON-LD Contexts. Sovrin: Enabling content integrity protected blockchain-based ZKP Schemas to be hosted on the Sovrin ledger (and Web-based locations) and referenced from Verifiable Credentials. IPFS: Enable organizations to dip their toes into IPFS as a \"backup mechanism\" w/o having to fully commit to jumping in with both feet. Verifiable Credentials WG: All the JSON-LD benefits above and the ability to digitally sign content integrity protected hyperlinks to evidence, terms of use, and any other information linked to from a Verifiable Credential. Credentials CG: Include links to data from DID Documents where the content at the links are secured by the cryptography of the ledger. The specification is 9 pages long and can be found here: https://tools.ietf.org/html/draft-sporny-hashlink-02 Collaboration A few of the authors and implementers of these specifications will be at RWoT8 and are very interested in educating the community about these specifications and getting feedback from the community about these specifications in order to improve them before large scale implementation begins.","title":"The Multibase, Multihash, and Hashlink Specifications"},{"location":"rwot8/topics-and-advance-readings/multiformat-superfriends/#the-multibase-multihash-and-hashlink-specifications","text":"By Manu Sporny and Ganesh Annan; Digital Bazaar This document outlines three specifications that have recently become work items of the W3C Credentials Community Group. The authors of these specifications desire feedback from the Rebooting the Web of Trust community to, if necessary, further expand the capabilities to address other use cases that are of importance to the individuals and organizations that attend RWoT8.","title":"The Multibase, Multihash, and Hashlink Specifications"},{"location":"rwot8/topics-and-advance-readings/multiformat-superfriends/#multibase","text":"The Multibase specification was published a few weeks ago as an IETF Internet-Draft. The specification is intended to be a joint work product of Protocol Labs (IPFS and Filecoin), the W3C Digital Verification Community Group, and the W3C Credentials Community Group. So, what problem does Multibase solve and why do we care? Our community encodes binary data in things like DID Documents and URLs. For example, both the Veres One and Sovrin DIDs encode the public key in the DID itself (these are legacy DIDs from past development versions, and are provided for illustration purposes only): did:v1:test:nym:z279ikfef3QbKRgnvTFmq5wEzZpe7s4fBiMb97hSEezXv3SL did:sov:WRfXPg8dantKVubE3HX8pw That bit at the end there is binary data (the public key) that is \"base-encoded\" so that it's easy (for developers) to copy/paste that stuff around and put it in emails when talking about technical things. The problem is that we don't know what base-encoding format was used above (base64 with padding, base64url, base58btc)? To solve this problem, we add a single byte to the beginning of the encoded value, so this: did:v1:test:nym:279ikfef3QbKRgnvTFmq5wEzZpe7s4fBiMb97hSEezXv3SL turns into this (note the addition of 'z', which means \"the data you are about to read is encoded in Bitcoin's base-58 encoding\"): did:v1:test:nym:z279ikfef3QbKRgnvTFmq5wEzZpe7s4fBiMb97hSEezXv3SL This is useful because this mechanism allows us to encode the same information in a variety of different ways. For example, Ethereum folks tend to use base16lower encoded values for their addresses while Bitcoin folks use a Bitcoin flavor of base58. At some level, these are sort of arbitrary decisions that developers fight over, but with Multibase, the decision is translated into a simple matter of value conversion. You don't have to fight over the base encoding format because Multibase encodes it in the resulting value, making conversion simple and deterministic and enabling future upgradability for your system (and compatibility with other systems). Multibase also allows developers to change the encoding format at a later date because the software was written with upgradability in mind from the beginning. The spec is a fairly short one, weighing in at 7 pages and can be found here: https://tools.ietf.org/html/draft-multiformats-multibase-01","title":"Multibase"},{"location":"rwot8/topics-and-advance-readings/multiformat-superfriends/#multihash","text":"The Multihash specification was drafted before/during the last RWoT and has been released as an IETF Internet-Draft. The specification is intended to be a joint work product of Protocol Labs (IPFS and Filecoin), the W3C Digital Verification Community Group, and the W3C Credentials Community Group. So, what problem does Multihash solve and why do we care? Our community depends on cryptographic hashes to generate short fingerprints for files and data. These fingerprints look like this as binary data: 0x0a4ec6f1629e49262d7093e2f82a3278 While that may look like gobbledygook to a human reader, it's enough for a computer to uniquely identify, for example, one picture among all of the other trillions of pictures on the Web. We use these fingerprints when working with Verifiable Credentials, DIDs, public keys, and evidence data (images, PDFs, etc.) that are linked to Verifiable Credentials. The problem is that we don't know what sort of cryptographic hash function generated the hash above, so we need to put an identifier at the front of that cryptographic hash. That's basically what Multihash does: it provides a mechanism to identify how cryptographic hashes were generated. This is useful because if we have a cryptographic hash identifier, we can design upgradability into the systems we're building today. Cryptographic hashing systems are broken every 10 years or so -- they have a shelf life and will eventually need to be upgraded. Building this concept of upgradability into our systems, especially Decentralized Identifiers and Verifiable Credentials, is a good engineering practice and the Multihash spec enables us to do just that. It's a fairly short 10 page spec and can be found here: https://tools.ietf.org/html/draft-multiformats-multihash-00","title":"Multihash"},{"location":"rwot8/topics-and-advance-readings/multiformat-superfriends/#hashlink","text":"The Hashlink specification is intended to be a way to enforce that the content at a given hyperlink has not changed since it was published. The specification is a joint work product of the W3C Digital Verification Community Group, JSON-LD WG (unofficial), VCWG (unofficial), and the W3C Credentials Community Group. Ok, so what's the problem and why do we care? We do the following things in this community: Link to JSON-LD Contexts Link to ZKP Schemas Link to evidence from Verifiable Credentials Link to external data from DID Documents ... and we have no way to tell if the content at the end of that link has changed since the Verifiable Credential was issued, or the DID Document was created. In the best case, verification of the cryptographic signature fails (because the content at the end of the link changed). In the average case, the signature doesn't fail, and we use data that may have been modified. In the worst case, the data was modified by an attacker and really bad things happen (like a developer being sloppy with how they use JSON-LD Contexts in a financial system and the source/destination fields being flipped in a financial transaction). There are mitigations for all of these issues, but they require the developer to be aware of the problem in the first place. What would be great is if we could trust that when we use a hyperlink in our systems (like a Verifiable Credential or a DID Document), and digitally sign that hyperlink, that we're also signing the content at the hyperlink. That's what the hashlink specification enables us to do. Here's a simple example for the JSON-LD Context for the current DID Spec. Here's the current link: https://w3id.org/did/v0.11 You have no idea if the version you retrieved and the version I retrieved are the same. Now, here's what it looks like when we secure it with a \"Legacy URL\" Hashlink (blake2b 8 byte multihash + base58btc multibase): https://w3id.org/did/v0.11?hl=z3aq31uzgnZBuWNzUB Now we both know that the version we download is the same (because of the little bit at the end starting with \"hl=\", which stands for Hashlink). Here's what it looks like as a (non-legacy) Hashlink URL: hl:z3aq31uzgnZBuWNzUB:zpr1Xd34f3NYqfr1yMzb6TBCrWWrvJeGVRJGsUMMyVWXS8 Yes, that looks awful, but there are a number of redeeming characteristics with the second form. The first is that the latter value is optional -- you can discard everything after the second colon and it's still a valid Hashlink URL. All you really need is \"hl:z3aq31uzgnZBuWNzUB\" and you can get the data from anywhere else (like different URLs, a local cache, etc.) and still thwart the attacks listed above. The other nice thing about Hashlink URLs, and this is the really exciting bit, is that you can create multi-sourced hashlinks that span completely different network architectures. Let's say that you had information that you really wanted to make sure didn't disappear (like the DID JSON-LD Context above), and you publish it at the following locations: https://w3id.org/did/v0.11 magnet:?xt=urn:btih:73C59D931B7E0C089C031D6CFE0D16AE ipfs://ipfs/QmR7W4GQUFWDPMVrQfmNE8xJC6LoVAyaWeRnDp4gS9/did-v0.11 onion://pq6kufupl4mc43g2/didv0.11 The Hashlink URL would be really long, something like this: hl:z3aq31uzgnZBuWNzUB:zeGVRWXS8TakFeJueF2bim3PaaDqbtqjkpxUc8ETS WXe6dQLWXQWvqiUdw8TJrncx3uKhwfc88MtM5xZbR27FhVRUKv9ogekamVtdE3U bXnXpMRT1AseCtoBUt1NE8x2SsnJxGfiZN45VVSCp6jh4dgcufL16tWrHREiSYE SEGP1J75yXCvAdvKPr7nb5aY ... but the Hashlink URL above 1) ensures the integrity of the thing you're linking to, and 2) still works if 3 out of the 4 networks listed above failed. To put it another way, the link above could survive (for example) the failure of the Web, BitTorrent, and Tor. This approach also solves the following problems that other communities in our orbit are having by: JSON-LD WG: Enabling backwards-compatible content integrity for JSON- LD Contexts. JSON-LD WG: Enabling non-Web-based, multi-sourced, compact, content integrity for JSON-LD Contexts. Sovrin: Enabling content integrity protected blockchain-based ZKP Schemas to be hosted on the Sovrin ledger (and Web-based locations) and referenced from Verifiable Credentials. IPFS: Enable organizations to dip their toes into IPFS as a \"backup mechanism\" w/o having to fully commit to jumping in with both feet. Verifiable Credentials WG: All the JSON-LD benefits above and the ability to digitally sign content integrity protected hyperlinks to evidence, terms of use, and any other information linked to from a Verifiable Credential. Credentials CG: Include links to data from DID Documents where the content at the links are secured by the cryptography of the ledger. The specification is 9 pages long and can be found here: https://tools.ietf.org/html/draft-sporny-hashlink-02","title":"Hashlink"},{"location":"rwot8/topics-and-advance-readings/multiformat-superfriends/#collaboration","text":"A few of the authors and implementers of these specifications will be at RWoT8 and are very interested in educating the community about these specifications and getting feedback from the community about these specifications in order to improve them before large scale implementation begins.","title":"Collaboration"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/","text":"Can Name Services provide Persistent Human-meaningful Names Linked To A Self Sovereign Identity DID Without Compromising SSI? A Topic Paper submitted to Rebooting The Web of Trust 8 by Toby Tremayne & Ryan Gill February 26, 2019, Melbourne Australia Overview The Zooko\u2019s Triangle Trilemma posits that only two of the desired three components of a modern digital identity system can successfully be implemented within a single system. Like \u201cGood, cheap fast; pick two\u201d, such a system can achieve security and decentralization, but without the third element of desirability \u2013 human meaningful names. The Topic for discussion is whether an externally managed name service can safely be used for deliberate identity persistence, allowing users to retain a name or handle and accrue reputation against it. Concept Figure 1: Diagram of conceptual authentication process using Idents Name Service A decentralized database or distributed ledger allows registration of \u201cAliases\u201d, a human readable arbitrary string, which can be attached to a a user specified sovereign identity DID (IE a Sovrin DID), creating an \u201cIdent\u201d. Behavioural or network contribution reputation within a moderation network or a consensus group can be assigned against the DID. This lets the user carry around their own \u201chandle\u201d or persona online much like a twitter handle, while still retaining the protections of SSI. Zero Knowledge Proof of Humanity (PoH) A proposed standard for proving a human controlled, legally liable yet publicly anonymous identity online. Simply, an agreed level of service based or in-person verification that is linked to verification of a government accepted form of identification, or sufficient equivalent proof. Proof of Humanity is a credential that could be offered at the time of verification with a Steward or Verifier on an SSI blockchain, or a recognised credential for any form of id that passes the requirement. This proof is used to assert that the user is a real human, identifiable from the rest of humanity. While the Steward or Verifier would retain knowledge of Personal Identity from the original verification, that knowledge is not passed on with credentials or in any other way to service providers. A Channel or service requiring PoH can allow anonymous or pseudonymous posting and transactions, while retaining the ability to action a behaviour on the account if there are valid applicable legal reasons to do so \u2013 IE providing a warrant or subpoena to a Steward or Verifier. Using PoH is consent to be legally liable for your interactions and transactions under that account, with the knowledge that Services do not record your real identity, but law enforcement can connect you to your activity should they have grounds. Details such as SLAs, security requirements and regulation are all important requirements for a successful PoH implementation, however we believe even a tacit agreement of currently available verification standards and a well drafted consent / usage form are enough to implement a proof of concept on existing implementations like Sovrin. Over time as the protocol is refined, proof requirements for a PoH credential could become more stringent. The ultimate aim of Poh is not to solve all online abuse and bad faith acts, but to provide a baseline to develop the ability for users to interact anonymously online while still being accountable to legal and community rules. Reputation Given an arbitrary reputational system, points could be accrued against the DID. Additionally, the use of the Alias in online social settings like games or social media accrue social reputation against the name. Concealed DID Instead of using the original SSI ID the user registered the Alias against on the Name Service, a new DID could be provided with a custom Ident method. \u201cdid:ident:2345\u201d might proxy a lookup to the original SSI DID, and pass the key detail accordingly, allowing for a step further obfuscation from data miners, while still being legally actionable. Collisions If a shared standard were to be developed, a game or social media site might use a list of name services, or allow you to sign up with any. Colliding names could be given automatically assigned modifiers for differentiation, based on locality and time of arrival. Subsequent users with the same alias registered on a different service might receive an appendix denoting their service and a number. Questions Standard How could a minimal standard for the use of names together with an SSI DID be developed to allow all such services to participate? How else might multiple name services co-exist and manage collisions for usernames for example, when everyone joins the latest VR world? Identity Cycling If not using the Concealed DID method, users could simply change the SSI ID attached to an Ident, thus \u201crinsing\u201d online reputation. Would the (globally speaking) limited number of Stewards, limit the feasibility of this action on a large scale? How can identity cycling be prevented without locking an Alias to a one time DID? If the user loses control of the Ident\u2019s SSI ID, how can they regain control of their online reputation without allowing the bad actor use of identity cycling? Proof of Humanity How could Proof of Humanity be developed as a standard among existing Stewards with current technology? How could a living standard be developed to grow and refine a list of acceptable forms of personal verification? Would PoH require legislation, IE criminalizing network accounts operating bots under false PoH? To what level would PoH need to be developed to be useful for anything from basic social media permissions to virtual environment and app authentication? Does the idea of layering names on top of SSI or providing the ability to create one portion of your Digital Identity to be your \u201cpresence\u201d in the online world work at odds to the intent of SSI?","title":"Name services and ssi"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#can-name-services-provide-persistent-human-meaningful-names-linked-to-a-self-sovereign-identity-did-without-compromising-ssi","text":"A Topic Paper submitted to Rebooting The Web of Trust 8 by Toby Tremayne & Ryan Gill February 26, 2019, Melbourne Australia","title":"Can Name Services provide Persistent Human-meaningful Names Linked To A Self Sovereign Identity DID Without Compromising SSI?"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#overview","text":"The Zooko\u2019s Triangle Trilemma posits that only two of the desired three components of a modern digital identity system can successfully be implemented within a single system. Like \u201cGood, cheap fast; pick two\u201d, such a system can achieve security and decentralization, but without the third element of desirability \u2013 human meaningful names. The Topic for discussion is whether an externally managed name service can safely be used for deliberate identity persistence, allowing users to retain a name or handle and accrue reputation against it.","title":"Overview"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#concept","text":"Figure 1: Diagram of conceptual authentication process using Idents","title":"Concept"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#name-service","text":"A decentralized database or distributed ledger allows registration of \u201cAliases\u201d, a human readable arbitrary string, which can be attached to a a user specified sovereign identity DID (IE a Sovrin DID), creating an \u201cIdent\u201d. Behavioural or network contribution reputation within a moderation network or a consensus group can be assigned against the DID. This lets the user carry around their own \u201chandle\u201d or persona online much like a twitter handle, while still retaining the protections of SSI.","title":"Name Service"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#zero-knowledge-proof-of-humanity-poh","text":"A proposed standard for proving a human controlled, legally liable yet publicly anonymous identity online. Simply, an agreed level of service based or in-person verification that is linked to verification of a government accepted form of identification, or sufficient equivalent proof. Proof of Humanity is a credential that could be offered at the time of verification with a Steward or Verifier on an SSI blockchain, or a recognised credential for any form of id that passes the requirement. This proof is used to assert that the user is a real human, identifiable from the rest of humanity. While the Steward or Verifier would retain knowledge of Personal Identity from the original verification, that knowledge is not passed on with credentials or in any other way to service providers. A Channel or service requiring PoH can allow anonymous or pseudonymous posting and transactions, while retaining the ability to action a behaviour on the account if there are valid applicable legal reasons to do so \u2013 IE providing a warrant or subpoena to a Steward or Verifier. Using PoH is consent to be legally liable for your interactions and transactions under that account, with the knowledge that Services do not record your real identity, but law enforcement can connect you to your activity should they have grounds. Details such as SLAs, security requirements and regulation are all important requirements for a successful PoH implementation, however we believe even a tacit agreement of currently available verification standards and a well drafted consent / usage form are enough to implement a proof of concept on existing implementations like Sovrin. Over time as the protocol is refined, proof requirements for a PoH credential could become more stringent. The ultimate aim of Poh is not to solve all online abuse and bad faith acts, but to provide a baseline to develop the ability for users to interact anonymously online while still being accountable to legal and community rules.","title":"Zero Knowledge Proof of Humanity (PoH)"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#reputation","text":"Given an arbitrary reputational system, points could be accrued against the DID. Additionally, the use of the Alias in online social settings like games or social media accrue social reputation against the name.","title":"Reputation"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#concealed-did","text":"Instead of using the original SSI ID the user registered the Alias against on the Name Service, a new DID could be provided with a custom Ident method. \u201cdid:ident:2345\u201d might proxy a lookup to the original SSI DID, and pass the key detail accordingly, allowing for a step further obfuscation from data miners, while still being legally actionable.","title":"Concealed DID"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#collisions","text":"If a shared standard were to be developed, a game or social media site might use a list of name services, or allow you to sign up with any. Colliding names could be given automatically assigned modifiers for differentiation, based on locality and time of arrival. Subsequent users with the same alias registered on a different service might receive an appendix denoting their service and a number.","title":"Collisions"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#questions","text":"","title":"Questions"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#standard","text":"How could a minimal standard for the use of names together with an SSI DID be developed to allow all such services to participate? How else might multiple name services co-exist and manage collisions for usernames for example, when everyone joins the latest VR world?","title":"Standard"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#identity-cycling","text":"If not using the Concealed DID method, users could simply change the SSI ID attached to an Ident, thus \u201crinsing\u201d online reputation. Would the (globally speaking) limited number of Stewards, limit the feasibility of this action on a large scale? How can identity cycling be prevented without locking an Alias to a one time DID? If the user loses control of the Ident\u2019s SSI ID, how can they regain control of their online reputation without allowing the bad actor use of identity cycling?","title":"Identity Cycling"},{"location":"rwot8/topics-and-advance-readings/name-services-and-ssi/#proof-of-humanity","text":"How could Proof of Humanity be developed as a standard among existing Stewards with current technology? How could a living standard be developed to grow and refine a list of acceptable forms of personal verification? Would PoH require legislation, IE criminalizing network accounts operating bots under false PoH? To what level would PoH need to be developed to be useful for anything from basic social media permissions to virtual environment and app authentication? Does the idea of layering names on top of SSI or providing the ability to create one portion of your Digital Identity to be your \u201cpresence\u201d in the online world work at odds to the intent of SSI?","title":"Proof of Humanity"},{"location":"rwot8/topics-and-advance-readings/notebook-workbench/","text":"A Notebook Workbench for SSI By Eric Welton (eric@korsimoro.com) The technical cost of experimenting with and evaluating web-of-trust technology, SSI and other, remains bewilderingly high. This situation hinders broad adoption. Impacts of this complexity include: - Biasing new projects towards build-from-scratch versus build-upon - Forcing focus on infrastructure and edge tech, rather than VC-ecosystem flows - Blocking effective cross-system evaluation and interoperability work - Pushing UX into products and delaying UX development I propose use of the Jupyterlab interactive computing environment as a means of: - facilitating broad adoption - promoting interoperability - supporting SSI UX development - expanding the number of documented use-cases Kool Aid Syndrome Spinning up a testnet for one of the SSI DLTs an evaluation framework for DID resolution via the universal resolver is non-trivial. When factoring in the device-locked nature of some SSI-tech, spinning up an evaluation environment may involve setting up iOS or Android simulation environments. Even setting up to develop UX components for DID, web-of-trust constructs, proof-and-credential exploration, and verifiable displays requires non-trivial setup. To be certain, setting up these environments is not, in itself, a roadblock. Many, if not most, of the technical participants at RWOT have done or routinely do exactly this. In fact, if you work primarily on a single platform you probably have a stable and comfortable local environment, based on code with which you are intimately familiar. The combination of in-depth knowledge and long term stability lets project developers focus on feature development and core bug fixes rather than removing obstacles which plague first time users, testers, and other evaluators. This is what I call the Kool-Aid Syndrome - everything becomes clear after the kool-aid kicks in, but the first step is drinking the project kool-aid. In order to join the party, you must first integrate with the community, explore the chat channels and forums (often in a foreign language), track down the relevant technical documentation, (perhaps translate them into your language), and begin working the kinks out of your local setup as you work through the getting started guide. This is reasonable for early stage development of new technology, but consider someone kicking around the idea of how to solve a non-technical problem where a strong SSI infrastructure would be a game changer. During the build-out of the business and product model, the project owners ask \u201cshould we use one of these existing systems, and if so which one, how would that work and how does that change what we do?\u201d. During this formative exploratory time there is a rich interplay between \u201ccan system X do Y?\u201d and \u201csystem X supports Z, how can we leverage that?\u201d The people involved are not starting from a position of years of identity-tech immersion, rather, they are trying to understand identity-tech at the same time as they are trying to solve their specific problem. This is why it is critical for developers to be able to do hands-on exploration of a system, in order to bring value to the solution development discussions. Unfortunately, the answer to \u201ccan or how does X do Y?\u201d is not often \u201clet me get you an answer in an hour\u201d . Instead, more often than not, the answer is \u201cI\u2019ll work on that, I\u2019m still trying to set up X, but the part that is supposed to go bing went bong. I\u2019ll check the chat channels, and I\u2019ll check with Sally, maybe she had luck upgrading irrelevant infrastructure part Z over the weekend - that might get us a bing, i think my local firewall was blocking internode traffic, let me just\u2026..\u201d. The result of Kool-Aid Syndrome are outcomes such as: - Let\u2019s just build our own system from scratch, I\u2019m sure we can get it to do exactly what we want, i\u2019ve got some really good ideas about key recovery from this bitcoin youtube video I watched last week\u2026.. - Let\u2019s pick one and put all of our eggs in that basket - we will be an X project, not an SSI project, and if we need features we can just fork the codebase. - Let\u2019s just use standard social auth, we\u2019ll add SSI later, SSI is really just another version of a site-login and user-profile, right? Let\u2019s add a DID column to the database, would that work? - The phone fingerprint is popular, let\u2019s just protect all the data that way and tell Jerry to add \u201cbiometrically secured personal data\u201d to the pitch decks, problem solved! Education and Use-Cases The value proposition of SSI is not in the technical details of any specific system, rather it is in the credentialing dance, where multiple SSI and non-SSI systems exchange information and enable individuals to achieve something they could not previously achieve. There are some textually documented use-case libraries, but while necessary, these are not sufficient when it comes to educating the adopting community. Specific cases in which I have had personal experience include interbank-KYC-markets, education document verification, and immigration-and-citizen systems. In these cases, the stakeholders are quickly confused in a sea of Alice, Bob, Example Bank, Example University, Example Government, DIDs, Verifiable Credentials, Zero-Knowledge Proofs, and Blockchains. Sometimes the take-away is only \u201cit uses blockchain, we need that, right?\u201d or \u201cit uses blockchain, but we can\u2019t be a bitcoin system, let\u2019s stick with [favorite enterprise vendor]\u201d. To mitigate this, substantial effort is required to build pitch decks and to walk through explanations and logic that are already deeply familiar to much of the RWOT cohort. The challenge is to do this quickly, efficiently, and respectfully - as the decision makers and stakeholders are not purposefully resistant - rather they have 150 other high priority tasks and they are trying to balance making it happen with doing it right . In cases where the presentations leave the decision makers intrigued, the next step is inevitably setting up a \u201ctechnical deep dive\u201d with the engineers. This sort of session is where you sit down and really evaluate the technology in action, perhaps collaboratively working through a getting started example and hoping that the Kool-Aid starts to kick in. In one particularly telling case, evaluation of Verifiable Credentials was seen as so far afield, that the decision was made to release a digital employment credential in the form of a \u201csmart-phone app\u201d which displayed a PDF scan of the paper document it replaced. No cryptography or security, of any kind, other than the smart-phone device security was in play - not even HTTPS for REST calls. UX & Playtime The call for a robust UX for web of trust components, like DIDs, VCs, proofs, trust web navigation, and assurance visualization continues to gain steam. Given the scope of identity-tech, the development of a strong visual language which can engage the world\u2019s population is critical. Imagine how much less Kool-Aid one has to drink if the above presentations and pitches could draw from a common UX language. Typical UX development is product and project driven. It only emerges as a common lexicon after sufficient adoption. In terms of identity-tech, we may be able to invert this pattern and spur adoption by developing a UX lexicon first. But how does one build UX components for identity-tech outside of the scope of a specific project? I think that what is needed is the ability to play with credential flows and to both explore and generate a wider range of richly documented use-cases. If the Kool-Aid can be removed then people who are interested in issues of usability and credential flow can work directly on the shared UX lexicon and can expand the documented use-case library to the point where that library itself becomes less of a \u2018for example\u2019 and more of a \u2018let\u2019s see how this is done elsewhere\u2019. Let's fly to Jupyter I have been working on an SSI tech workbench based on the Jupyter notebook environment popular in data science communities. Jupyter is an interactive computing environment which separates the UI from the background compute, and which can be frozen and re-distributed in the form of educational notebooks. In an interactive computing environment, commands can generate rich output. It is not a simple terminal session. This is why such notebooks have become a mainstay in the data-science world - where many projects are ad-hoc and every data set needs a little bit of programmatic massaging before it is useful. With a user base of over 5M, a notebook presence of over 10M, and support from major vendors like Azure, this sort of interactive computing is neither a passing fad nor a far flung future vision - it is now, and it is mature - it is just not yet a part of the SSI world. The sort of interaction that interests me, and I think is of interest to the identity-tech development community, looks something like this: # start up a local blockchain launch veres.one on v1.local # start up another local blockchain launch sovrin on sovrin.local # create test users alice = create_actor(\u2018alice\u2019,{... payment definition }) bob = create_actor(\u2018bob\u2019,{... payment definition}) # grab some did information (did_x,ddo_x) = grab_new_did(\u2018v1.local\u2019,alice.payment_info) # run evaluation/verification tests on a DID Document validate(ddo_x) assign(alice,did_x) # grab bob\u2019s did information (did_y,ddo_y) = grab_new_did(\u2018sovrin.local\u2019,bob.payment_info) assign(bob,did_y) # and so on, until we can get to something really sexy like run_w3c_vcwg_use_case_suite({student:bob,mortgage:a1Mortgage\u2026.}) The W3C VC working group, for example, provides a set of use-cases in the form of text in a github repository. Wouldn\u2019t it be great if we could provide a \u2018live\u2019 notebook which allows someone to quickly execute the actual use case? In so doing: - Use-Case Notebooks can be distributed to demonstrate, not as theory, but as running code, the applicability of SSI technology - it can be handed to anyone, anywhere, who can actually \u201cuse\u201d SSI technology, instead of reading \u201cabout\u201d the technology - Use-Cases can be explored, actively, in the context of interoperability and edge-case investigation - the ability to start with a reproducible, active baseline, and then tweak the environment slightly and re-run the scripts, promotes interoperability. Such notebooks can be easily redistributed, perhaps as documents in the same repository as the current text-based use cases. When it comes to the UX - if it is easy to \u201cspin up some actors, holders, issuers, verifiers, etc\u201d, then we can focus on the UX. Consider a # issue a digital driving license DL = issue({...},alice,landTransportOffice) present(DL) <output is a React Widget, or a Verifiable HTML widget, etc.> The output produced is a re-usable UX component, placed in a wrapper which adapts the component to the ZMQ based Jupyter notebook/kernel environment. This allows a UX developer to focus on the UX itself, and on the UX relationship to the VC environment, without being bogged down in the technical details of setting up the development harness, or even in product details. The product, in this case, is the interactive library of use-cases. There is no Kool-Aid on Jupyter The workbench, as it is envisioned, is distributed as an electron app (e.g. jupyterlab_app), so that it can be \u201cdownloaded and run\u201d as any other desktop application. Dependencies are limited to external language installations (python, rust, go, etc.) and docker. This is part of the existing jupyterlab framework, and would need to be expanded slightly to accommodate common SSI elements. In Jupyter terms, the SSI specific runtimes are known as Kernels, and with a current coverage of over 100 languages, adding a few SSI specific systems means following a well-traveled pathway and is not prohibitive. Once downloaded, the ability to open up \u201cnotebooks\u201d which contain reproducible investigations of specific use-cases is as simple as \u201cclicking on the file\u201d. In terms of presenting to stakeholders and decision makers, and in terms of exploring \u201chow can we leverage SSI to solve our problem\u201d, the requirement of deep investment within a specific system and the requirement of strong prior-knowledge of the last years of SSI technology is replaced by a focus on \u201cdownload, explore, and engage\u201d The ability to connect this sort of easily accessible workbench to engineering teams, both before and after engaging decision makers and product teams, substantially reduces the need for long, Kool-Aid fueled technical deep dives involving specific systems. Using this approach, fostering SSI adoption can focus on the dance of credentials, issuers, holders, verifiers, relying parties, assurance mavens, compliance officers, audit specialists, governance authorities, and end user abilities - rather than on the technical details of making sure the right version of node or libsodium is installed or that the docker environment is correctly configured to connect with the local test ipads and androids. Furthermore, the development of appropriate UX technology can be combined with the expansion of the use case library, in the form of redistributable educational notebooks.","title":"A Notebook Workbench for SSI"},{"location":"rwot8/topics-and-advance-readings/notebook-workbench/#a-notebook-workbench-for-ssi","text":"By Eric Welton (eric@korsimoro.com) The technical cost of experimenting with and evaluating web-of-trust technology, SSI and other, remains bewilderingly high. This situation hinders broad adoption. Impacts of this complexity include: - Biasing new projects towards build-from-scratch versus build-upon - Forcing focus on infrastructure and edge tech, rather than VC-ecosystem flows - Blocking effective cross-system evaluation and interoperability work - Pushing UX into products and delaying UX development I propose use of the Jupyterlab interactive computing environment as a means of: - facilitating broad adoption - promoting interoperability - supporting SSI UX development - expanding the number of documented use-cases","title":"A Notebook Workbench for SSI"},{"location":"rwot8/topics-and-advance-readings/notebook-workbench/#kool-aid-syndrome","text":"Spinning up a testnet for one of the SSI DLTs an evaluation framework for DID resolution via the universal resolver is non-trivial. When factoring in the device-locked nature of some SSI-tech, spinning up an evaluation environment may involve setting up iOS or Android simulation environments. Even setting up to develop UX components for DID, web-of-trust constructs, proof-and-credential exploration, and verifiable displays requires non-trivial setup. To be certain, setting up these environments is not, in itself, a roadblock. Many, if not most, of the technical participants at RWOT have done or routinely do exactly this. In fact, if you work primarily on a single platform you probably have a stable and comfortable local environment, based on code with which you are intimately familiar. The combination of in-depth knowledge and long term stability lets project developers focus on feature development and core bug fixes rather than removing obstacles which plague first time users, testers, and other evaluators. This is what I call the Kool-Aid Syndrome - everything becomes clear after the kool-aid kicks in, but the first step is drinking the project kool-aid. In order to join the party, you must first integrate with the community, explore the chat channels and forums (often in a foreign language), track down the relevant technical documentation, (perhaps translate them into your language), and begin working the kinks out of your local setup as you work through the getting started guide. This is reasonable for early stage development of new technology, but consider someone kicking around the idea of how to solve a non-technical problem where a strong SSI infrastructure would be a game changer. During the build-out of the business and product model, the project owners ask \u201cshould we use one of these existing systems, and if so which one, how would that work and how does that change what we do?\u201d. During this formative exploratory time there is a rich interplay between \u201ccan system X do Y?\u201d and \u201csystem X supports Z, how can we leverage that?\u201d The people involved are not starting from a position of years of identity-tech immersion, rather, they are trying to understand identity-tech at the same time as they are trying to solve their specific problem. This is why it is critical for developers to be able to do hands-on exploration of a system, in order to bring value to the solution development discussions. Unfortunately, the answer to \u201ccan or how does X do Y?\u201d is not often \u201clet me get you an answer in an hour\u201d . Instead, more often than not, the answer is \u201cI\u2019ll work on that, I\u2019m still trying to set up X, but the part that is supposed to go bing went bong. I\u2019ll check the chat channels, and I\u2019ll check with Sally, maybe she had luck upgrading irrelevant infrastructure part Z over the weekend - that might get us a bing, i think my local firewall was blocking internode traffic, let me just\u2026..\u201d. The result of Kool-Aid Syndrome are outcomes such as: - Let\u2019s just build our own system from scratch, I\u2019m sure we can get it to do exactly what we want, i\u2019ve got some really good ideas about key recovery from this bitcoin youtube video I watched last week\u2026.. - Let\u2019s pick one and put all of our eggs in that basket - we will be an X project, not an SSI project, and if we need features we can just fork the codebase. - Let\u2019s just use standard social auth, we\u2019ll add SSI later, SSI is really just another version of a site-login and user-profile, right? Let\u2019s add a DID column to the database, would that work? - The phone fingerprint is popular, let\u2019s just protect all the data that way and tell Jerry to add \u201cbiometrically secured personal data\u201d to the pitch decks, problem solved!","title":"Kool Aid Syndrome"},{"location":"rwot8/topics-and-advance-readings/notebook-workbench/#education-and-use-cases","text":"The value proposition of SSI is not in the technical details of any specific system, rather it is in the credentialing dance, where multiple SSI and non-SSI systems exchange information and enable individuals to achieve something they could not previously achieve. There are some textually documented use-case libraries, but while necessary, these are not sufficient when it comes to educating the adopting community. Specific cases in which I have had personal experience include interbank-KYC-markets, education document verification, and immigration-and-citizen systems. In these cases, the stakeholders are quickly confused in a sea of Alice, Bob, Example Bank, Example University, Example Government, DIDs, Verifiable Credentials, Zero-Knowledge Proofs, and Blockchains. Sometimes the take-away is only \u201cit uses blockchain, we need that, right?\u201d or \u201cit uses blockchain, but we can\u2019t be a bitcoin system, let\u2019s stick with [favorite enterprise vendor]\u201d. To mitigate this, substantial effort is required to build pitch decks and to walk through explanations and logic that are already deeply familiar to much of the RWOT cohort. The challenge is to do this quickly, efficiently, and respectfully - as the decision makers and stakeholders are not purposefully resistant - rather they have 150 other high priority tasks and they are trying to balance making it happen with doing it right . In cases where the presentations leave the decision makers intrigued, the next step is inevitably setting up a \u201ctechnical deep dive\u201d with the engineers. This sort of session is where you sit down and really evaluate the technology in action, perhaps collaboratively working through a getting started example and hoping that the Kool-Aid starts to kick in. In one particularly telling case, evaluation of Verifiable Credentials was seen as so far afield, that the decision was made to release a digital employment credential in the form of a \u201csmart-phone app\u201d which displayed a PDF scan of the paper document it replaced. No cryptography or security, of any kind, other than the smart-phone device security was in play - not even HTTPS for REST calls.","title":"Education and Use-Cases"},{"location":"rwot8/topics-and-advance-readings/notebook-workbench/#ux-playtime","text":"The call for a robust UX for web of trust components, like DIDs, VCs, proofs, trust web navigation, and assurance visualization continues to gain steam. Given the scope of identity-tech, the development of a strong visual language which can engage the world\u2019s population is critical. Imagine how much less Kool-Aid one has to drink if the above presentations and pitches could draw from a common UX language. Typical UX development is product and project driven. It only emerges as a common lexicon after sufficient adoption. In terms of identity-tech, we may be able to invert this pattern and spur adoption by developing a UX lexicon first. But how does one build UX components for identity-tech outside of the scope of a specific project? I think that what is needed is the ability to play with credential flows and to both explore and generate a wider range of richly documented use-cases. If the Kool-Aid can be removed then people who are interested in issues of usability and credential flow can work directly on the shared UX lexicon and can expand the documented use-case library to the point where that library itself becomes less of a \u2018for example\u2019 and more of a \u2018let\u2019s see how this is done elsewhere\u2019.","title":"UX &amp; Playtime"},{"location":"rwot8/topics-and-advance-readings/notebook-workbench/#lets-fly-to-jupyter","text":"I have been working on an SSI tech workbench based on the Jupyter notebook environment popular in data science communities. Jupyter is an interactive computing environment which separates the UI from the background compute, and which can be frozen and re-distributed in the form of educational notebooks. In an interactive computing environment, commands can generate rich output. It is not a simple terminal session. This is why such notebooks have become a mainstay in the data-science world - where many projects are ad-hoc and every data set needs a little bit of programmatic massaging before it is useful. With a user base of over 5M, a notebook presence of over 10M, and support from major vendors like Azure, this sort of interactive computing is neither a passing fad nor a far flung future vision - it is now, and it is mature - it is just not yet a part of the SSI world. The sort of interaction that interests me, and I think is of interest to the identity-tech development community, looks something like this: # start up a local blockchain launch veres.one on v1.local # start up another local blockchain launch sovrin on sovrin.local # create test users alice = create_actor(\u2018alice\u2019,{... payment definition }) bob = create_actor(\u2018bob\u2019,{... payment definition}) # grab some did information (did_x,ddo_x) = grab_new_did(\u2018v1.local\u2019,alice.payment_info) # run evaluation/verification tests on a DID Document validate(ddo_x) assign(alice,did_x) # grab bob\u2019s did information (did_y,ddo_y) = grab_new_did(\u2018sovrin.local\u2019,bob.payment_info) assign(bob,did_y) # and so on, until we can get to something really sexy like run_w3c_vcwg_use_case_suite({student:bob,mortgage:a1Mortgage\u2026.}) The W3C VC working group, for example, provides a set of use-cases in the form of text in a github repository. Wouldn\u2019t it be great if we could provide a \u2018live\u2019 notebook which allows someone to quickly execute the actual use case? In so doing: - Use-Case Notebooks can be distributed to demonstrate, not as theory, but as running code, the applicability of SSI technology - it can be handed to anyone, anywhere, who can actually \u201cuse\u201d SSI technology, instead of reading \u201cabout\u201d the technology - Use-Cases can be explored, actively, in the context of interoperability and edge-case investigation - the ability to start with a reproducible, active baseline, and then tweak the environment slightly and re-run the scripts, promotes interoperability. Such notebooks can be easily redistributed, perhaps as documents in the same repository as the current text-based use cases. When it comes to the UX - if it is easy to \u201cspin up some actors, holders, issuers, verifiers, etc\u201d, then we can focus on the UX. Consider a # issue a digital driving license DL = issue({...},alice,landTransportOffice) present(DL) <output is a React Widget, or a Verifiable HTML widget, etc.> The output produced is a re-usable UX component, placed in a wrapper which adapts the component to the ZMQ based Jupyter notebook/kernel environment. This allows a UX developer to focus on the UX itself, and on the UX relationship to the VC environment, without being bogged down in the technical details of setting up the development harness, or even in product details. The product, in this case, is the interactive library of use-cases.","title":"Let's fly to Jupyter"},{"location":"rwot8/topics-and-advance-readings/notebook-workbench/#there-is-no-kool-aid-on-jupyter","text":"The workbench, as it is envisioned, is distributed as an electron app (e.g. jupyterlab_app), so that it can be \u201cdownloaded and run\u201d as any other desktop application. Dependencies are limited to external language installations (python, rust, go, etc.) and docker. This is part of the existing jupyterlab framework, and would need to be expanded slightly to accommodate common SSI elements. In Jupyter terms, the SSI specific runtimes are known as Kernels, and with a current coverage of over 100 languages, adding a few SSI specific systems means following a well-traveled pathway and is not prohibitive. Once downloaded, the ability to open up \u201cnotebooks\u201d which contain reproducible investigations of specific use-cases is as simple as \u201cclicking on the file\u201d. In terms of presenting to stakeholders and decision makers, and in terms of exploring \u201chow can we leverage SSI to solve our problem\u201d, the requirement of deep investment within a specific system and the requirement of strong prior-knowledge of the last years of SSI technology is replaced by a focus on \u201cdownload, explore, and engage\u201d The ability to connect this sort of easily accessible workbench to engineering teams, both before and after engaging decision makers and product teams, substantially reduces the need for long, Kool-Aid fueled technical deep dives involving specific systems. Using this approach, fostering SSI adoption can focus on the dance of credentials, issuers, holders, verifiers, relying parties, assurance mavens, compliance officers, audit specialists, governance authorities, and end user abilities - rather than on the technical details of making sure the right version of node or libsodium is installed or that the docker environment is correctly configured to connect with the local test ipads and androids. Furthermore, the development of appropriate UX technology can be combined with the expansion of the use case library, in the form of redistributable educational notebooks.","title":"There is no Kool-Aid on Jupyter"},{"location":"rwot8/topics-and-advance-readings/oidc-profile-for-ssi/","text":"OIDC Profile for SSI By Oliver Terbu, Andres Junge Introduction The current online authentication and identity landscape is dominated by the typical Identity Provider (IdP) pattern. Protocols like OpenID Connect (OIDC) [OIDC] respectively OAuth2 and the Security Markup Language (SAML) Web Browser SSO profile is mostly used in this area. For example, behind the scenes, Google uses the OIDC protocol in their sign-in button. After user authentication and consent, and depending on the flow, clients, i.e., Relying Parties (RP), will receive up to three different tokens, i.e., id token, refresh token, access token. The RP will use these tokens with the IdP to obtain data about the user: The id token can already contain personally identifiable information, i.e., the requested data about the user. The access token (typically a bearer token) provides access to the user info endpoint, which allows the RP to request or refresh the required data. The IdP usually hosts the user info endpoint. Because access tokens are valid for a shorter period, the refresh token could be used to obtain a new access token. This approach typically has an impact on the user\u2019s privacy by allowing the OpenID Connect Provider (OP) to track the user's behavior. Furthermore, relying on a bearer token has also security implications. Of course, mechanisms like token binding exist to mitigate the security issues, but they are still not common practice. While SAML loses its importance, OIDC is still there, actively maintained, and has a lot of support from the industry and many independent OIDC client implementations are available. Depending on the area, e.g., telcos or government, dedicated OIDC profiles were created that specifically address their requirements. An OIDC profile is a specific conformity configuration of pieces of the OIDC specifications, e.g., flows, parameters. While the traditional IdP approach has a lot of downsides, the underlying OIDC protocol could provide enough flexibility to support SSI. Goal The goal is to continue the work on an OIDC profile for SSI based on [NS18] and [II18] and finalize the first version of it. The profile should support SSI wallets without needing to have a central IdP. It is assumed that the user controls and provides consent with an SSI wallet. We anticipate that this will likely be a mobile app on Android and iOS. Of course, an online wallet should also be considered. While it is possible that the SSI wallet directly provides the data, it should still be possible for an RP to obtain data about the user while the SSI wallet is offline. During RWOT6, people from the SSI community already identified that OIDC due to its flexibility it could play a specific role in DID Authn [RW18]. Nat Sakimura, Chair of the OpenID Foundation, started an article how OIDC could be used in an SSI system [NS18]. The main idea was to make use of the Self-Issued OP (SIOP) specification that is part of the OIDC core specification. An SSI wallet on an edge device like a smartphone could act as a SIOP. However, DID support and how to leverage DIF concepts was not described in this context. At IIW IIWXXVII, a session on the feasibility of an OIDC profile for DID Authn was hosted to validate a similar approach based on DIDs and concepts developed by the SSI community [II18]. OIDC Profile The goal is to continue the work on an OIDC profile for SSI based on [NS18] and [II18], and finalize the first version of it. The profile should support SSI wallets without needing to have a central IdP. It is assumed that the user controls and provides consent with an SSI wallet. We anticipate that this will likely be a mobile app on Android and iOS. Of course, an online wallet should also be considered. While it is possible that the data is directly provided by the SSI wallet, it should still be possible for an RP to obtain data about the user while the SSI wallet is offline. The approach entails the following: - Leverage existing OIDC flows, i.e., request/response message, and data structures - Use the SIOP specification to solve OP discovery - Use the aggregated and distributed claims OIDC specification to support multiple verifiable credentials issuer. Essentially, this means the id token contains a multitude of JWT issued by different issuers. - The SSI holder signs the id token itself. - Add DID support for request and response message, i.e., JWT. This was already addressed as part of the ongoing W3C Verifiable Credentials specification [W3C]. - Client-side DID Auth could additionally protect user info endpoint. The following challenges need to be tackled by the OIDC profile: - How to align the id token with the DID specification? - How to align the id token format with the W3C VC specification? - How to provide access to the data when the user is offline? - How to ensure and improve privacy and mitigate mining the user? - How to deal with client registration? - How to achieve end to end encryption? - How to enable encrypted data stores? - ... References https://openid.net/specs/openid-connect-core-1_0.html https://www.youtube.com/watch?v=tTeN_SxQ_OI, [NS18] https://nat.sakimura.org/2018/12/11/todo-list-for-self-issued-op/, [NS18] https://iiw.idcommons.net/OIDC_DID-Auth_Profile, [II18] https://github.com/WebOfTrustInfo/rwot6-santabarbara/blob/master/final-documents/did-auth.md, [RW18] https://iiw.idcommons.net/Open_ID_v._FIDO_v._SSI https://w3c.github.io/vc-data-model/#json-web-token, [W3C]","title":"OIDC Profile for SSI"},{"location":"rwot8/topics-and-advance-readings/oidc-profile-for-ssi/#oidc-profile-for-ssi","text":"By Oliver Terbu, Andres Junge","title":"OIDC Profile for SSI"},{"location":"rwot8/topics-and-advance-readings/oidc-profile-for-ssi/#introduction","text":"The current online authentication and identity landscape is dominated by the typical Identity Provider (IdP) pattern. Protocols like OpenID Connect (OIDC) [OIDC] respectively OAuth2 and the Security Markup Language (SAML) Web Browser SSO profile is mostly used in this area. For example, behind the scenes, Google uses the OIDC protocol in their sign-in button. After user authentication and consent, and depending on the flow, clients, i.e., Relying Parties (RP), will receive up to three different tokens, i.e., id token, refresh token, access token. The RP will use these tokens with the IdP to obtain data about the user: The id token can already contain personally identifiable information, i.e., the requested data about the user. The access token (typically a bearer token) provides access to the user info endpoint, which allows the RP to request or refresh the required data. The IdP usually hosts the user info endpoint. Because access tokens are valid for a shorter period, the refresh token could be used to obtain a new access token. This approach typically has an impact on the user\u2019s privacy by allowing the OpenID Connect Provider (OP) to track the user's behavior. Furthermore, relying on a bearer token has also security implications. Of course, mechanisms like token binding exist to mitigate the security issues, but they are still not common practice. While SAML loses its importance, OIDC is still there, actively maintained, and has a lot of support from the industry and many independent OIDC client implementations are available. Depending on the area, e.g., telcos or government, dedicated OIDC profiles were created that specifically address their requirements. An OIDC profile is a specific conformity configuration of pieces of the OIDC specifications, e.g., flows, parameters. While the traditional IdP approach has a lot of downsides, the underlying OIDC protocol could provide enough flexibility to support SSI.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/oidc-profile-for-ssi/#goal","text":"The goal is to continue the work on an OIDC profile for SSI based on [NS18] and [II18] and finalize the first version of it. The profile should support SSI wallets without needing to have a central IdP. It is assumed that the user controls and provides consent with an SSI wallet. We anticipate that this will likely be a mobile app on Android and iOS. Of course, an online wallet should also be considered. While it is possible that the SSI wallet directly provides the data, it should still be possible for an RP to obtain data about the user while the SSI wallet is offline. During RWOT6, people from the SSI community already identified that OIDC due to its flexibility it could play a specific role in DID Authn [RW18]. Nat Sakimura, Chair of the OpenID Foundation, started an article how OIDC could be used in an SSI system [NS18]. The main idea was to make use of the Self-Issued OP (SIOP) specification that is part of the OIDC core specification. An SSI wallet on an edge device like a smartphone could act as a SIOP. However, DID support and how to leverage DIF concepts was not described in this context. At IIW IIWXXVII, a session on the feasibility of an OIDC profile for DID Authn was hosted to validate a similar approach based on DIDs and concepts developed by the SSI community [II18].","title":"Goal"},{"location":"rwot8/topics-and-advance-readings/oidc-profile-for-ssi/#oidc-profile","text":"The goal is to continue the work on an OIDC profile for SSI based on [NS18] and [II18], and finalize the first version of it. The profile should support SSI wallets without needing to have a central IdP. It is assumed that the user controls and provides consent with an SSI wallet. We anticipate that this will likely be a mobile app on Android and iOS. Of course, an online wallet should also be considered. While it is possible that the data is directly provided by the SSI wallet, it should still be possible for an RP to obtain data about the user while the SSI wallet is offline. The approach entails the following: - Leverage existing OIDC flows, i.e., request/response message, and data structures - Use the SIOP specification to solve OP discovery - Use the aggregated and distributed claims OIDC specification to support multiple verifiable credentials issuer. Essentially, this means the id token contains a multitude of JWT issued by different issuers. - The SSI holder signs the id token itself. - Add DID support for request and response message, i.e., JWT. This was already addressed as part of the ongoing W3C Verifiable Credentials specification [W3C]. - Client-side DID Auth could additionally protect user info endpoint. The following challenges need to be tackled by the OIDC profile: - How to align the id token with the DID specification? - How to align the id token format with the W3C VC specification? - How to provide access to the data when the user is offline? - How to ensure and improve privacy and mitigate mining the user? - How to deal with client registration? - How to achieve end to end encryption? - How to enable encrypted data stores? - ...","title":"OIDC Profile"},{"location":"rwot8/topics-and-advance-readings/oidc-profile-for-ssi/#references","text":"https://openid.net/specs/openid-connect-core-1_0.html https://www.youtube.com/watch?v=tTeN_SxQ_OI, [NS18] https://nat.sakimura.org/2018/12/11/todo-list-for-self-issued-op/, [NS18] https://iiw.idcommons.net/OIDC_DID-Auth_Profile, [II18] https://github.com/WebOfTrustInfo/rwot6-santabarbara/blob/master/final-documents/did-auth.md, [RW18] https://iiw.idcommons.net/Open_ID_v._FIDO_v._SSI https://w3c.github.io/vc-data-model/#json-web-token, [W3C]","title":"References"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/","text":"On interpersonal data Philip Sheldrake. For Rebooting Web of Trust, 1-3 March 2019. A precis of a three-part series, published on the AKASHA Foundation blog, January 2019: The misleading name, metaphor defiance, and awesome potential of \u201cpersonal data\u201d We believe that humans are not problems waiting to be solved, but potential waiting to unfold. This is why we nurture projects helping individuals amplify their potential through open systems that expand our collective mind at local, regional and global scales. We have a problem and an opportunity currently labelled \"personal data\". The opportunity encompasses nothing less than a complete redesign of our lives and societies and our collective ability to grapple with complex adaptive systems including super-wicked problems \u2014 but this will remain elusive until we've wrestled with the \"personal data\" problem, including the problem of the way we frame the opportunity and problem. The \u201cpersonal data\u201d problem Simply speaking, the problem is this ... the I and the we are not really separable. For social philosophical context and brevity, it\u2019s sufficient to note that I\u2019m locating this between the extremes of fanatical individuality and the total subjugation of the individual in favour of the collective. It feels to me that this is compatible with the majority view, at least in terms of the European cultures with which I\u2019m most familiar. It turns out that there is very few data that may be described as purely personal data. That lunch date, that genome map, those photos, that joint bank account \u2014 all turn out to be interpersonal data. In fact, a personal bank account also records the relationship between two parties / persons (the bank and its customer), and lists transactions between the account owner and other parties / persons. We are actually contemplating interpersonal data . The prefix inter is familiar and requires an apparently simple shift in locus, and yet deeper down it offers a way forward to engineer the complexity needed to respond to the complexity (per the law of requisite variety). Maintaining the word person just about stretches to our needs in terms of personhood, encompassing legal persons (organizations recognized as having privileges and obligations under law) and quite reasonably transhumans. To press the point home fully however, any organizing (assemblage, however ephemeral or enduring) would also need to be a person. I am a person Alphabet Inc. is a person, as are its subsidiaries, e.g. Google Her Majesty's Government is a person, as are its Departments A family is a person of persons Molly Millions is a person Hawking Incorporated was a person. Now let's consider persons living, i.e. relating, interdepending, responding, learning. In other words, exchanging information \u2014 interpersonal data. We are contemplating interpersonal data with no scale at every scale. The problem of the way we frame the opportunity and problem Data is data. While accepting that language is metaphor, there doesn't appear to be a wholly useful metaphor that can apply to anything but a fragment or two of the required architecture. It seems that there's never been anything like personal or interpersonal data. No metaphor can be extended to analogy that's for certain. I review five common attempts. Data-as-property Property is rivalrous. Property is monopoly. Property, unlike human rights, is not universal, indivisible, or inalienable. Personal data is not property. Personal data isn\u2019t money as we know it. Simplistic. Transactional. Binary. It is more akin to the rich, varied and complex information flows present in rainforests, in oceans, in human cultures. It is worth a lot more than six cents a day Facebook makes out of you when markets don\u2019t reduce it and its application to a mere transactional exchange. At the very moment we might conceive an awesome, distributed and continuous crystallization of collective knowledge, the signalling known as price entails monumental information loss and inevitable consequential inequalities. Data-as-labour Data-as-labour is effectively a rebadging of data as property \u2014 one owns one's labour after all. Labour is remunerated based on time and/or output at a market value. Here, the value is contingent on factors way beyond any conceptualization of work that I know. The shoehorning simply serves to connect the treatment of personal and similar data to concerns for the impact on employment of all variety of digital technologies, not least artificial intelligence. That might be honourable, but it\u2019s also tenuous at best. Data-as-reputation Programmatic quantification of reputation is an inevitable evil due to the unavoidable self-moderation and modulation it inflicts on its subjects beyond that which might be argued as 'good for society'. To be clear, the social accretion of local, contextually relevant reputation with forgiving opportunities for reparation has served communities for millennia. We are however considering universal, non-contextual and irremediable scoring and algorithmic assessment. Context is a keyword. I want to know if Alice is trustworthy to drive me safely from A to B. I couldn't care less if she's up on her mortgage payments, or has been dropping litter (I do care about that generally, but not in this context), or has been 'dutifully' supportive of the current government, which of course would be an abhorrent context here and one we must ensure we don't engender accidentally. If you resist censorship, it seems you must also resist reputation scoring. Fortunately, we're not actually interested in reputation per se, only as a proxy for trustworthiness and accountability. With some forethought we can engineer our way around these challenges and avoid a (Black Mirror) Nosedive. I suspect the architecture will resemble the past in terms of our having to re-engineer appropriate contextual and social frictions. Data-as-public-good Academic authors such as Mariana Mazzucato and Viktor Mayer-Sch\u00f6nberger grapple for ways to regulate the run-away oligopolistic success of the centralizing data-hungry behemoths such as Facebook and Google \u2014 getting them to share some of their informational hoard. There responses predominantly take a regulatory form. And yet at the end of the day market regulators can only regulate markets, and markets are dedicated to property ownership. Data can be a public good, of course!, but this doesn\u2019t strike me as anywhere near an optimal way to go about it. Data-as-me Informational privacy requires radical re-interpretation, one that takes into account the essentially informational nature of human beings and of their operations as informational social agents. Such re-interpretation is achieved by considering each person as constituted by his or her information, and hence by understanding a breach of one's informational privacy as a form of aggression towards one's personal identity. Luciano Floridi This is an exciting vista. Nevertheless, it requires some modification to encourage the emergence of collective intelligence and our anti-rivalrous flourishing. How might we realise this value while respecting personal dignity and agency? Indeed while respecting personal privacy, because what agency can anyone be said to have if they cannot maintain personally desired and contextually appropriate privacy? Interpersonal data architectural principles Humans, not data subjects as the GDPR has it Edge-centric, not node-centric, as this is interpersonal Agency, not control as it seems nearly everyone would have it! Rhizomes, not trees as our sciences know it Cache, not facsimile as \u2018stores\u2019, \u2018vaults\u2019 and \u2018wallets\u2019 would have it. On that last point, if we get this right, your bank's cloud (should they continue to maintain one, and should banks continue to be necessary to banking) will be the facsimile. We are dealing with a phenomenon here that appears atomistically simple but in fact forms, informs and infects everything. It's the original complex. Identity is information (and, I suspect, digitally emergent from interpersonal data). Relationships is information. Reputation is information. Exchange is information. Organizing is information. Life is information. We should then proceed with due caution \u2014 ethically and technically. Ethically, we need to take appropriate time for due diligence. Ethically, we cannot delay and allow the data-as-property protagonists time to establish the mother of all Nash equilibria.","title":"On interpersonal data"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/#on-interpersonal-data","text":"Philip Sheldrake. For Rebooting Web of Trust, 1-3 March 2019. A precis of a three-part series, published on the AKASHA Foundation blog, January 2019: The misleading name, metaphor defiance, and awesome potential of \u201cpersonal data\u201d We believe that humans are not problems waiting to be solved, but potential waiting to unfold. This is why we nurture projects helping individuals amplify their potential through open systems that expand our collective mind at local, regional and global scales. We have a problem and an opportunity currently labelled \"personal data\". The opportunity encompasses nothing less than a complete redesign of our lives and societies and our collective ability to grapple with complex adaptive systems including super-wicked problems \u2014 but this will remain elusive until we've wrestled with the \"personal data\" problem, including the problem of the way we frame the opportunity and problem.","title":"On interpersonal data"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/#the-personal-data-problem","text":"Simply speaking, the problem is this ... the I and the we are not really separable. For social philosophical context and brevity, it\u2019s sufficient to note that I\u2019m locating this between the extremes of fanatical individuality and the total subjugation of the individual in favour of the collective. It feels to me that this is compatible with the majority view, at least in terms of the European cultures with which I\u2019m most familiar. It turns out that there is very few data that may be described as purely personal data. That lunch date, that genome map, those photos, that joint bank account \u2014 all turn out to be interpersonal data. In fact, a personal bank account also records the relationship between two parties / persons (the bank and its customer), and lists transactions between the account owner and other parties / persons. We are actually contemplating interpersonal data . The prefix inter is familiar and requires an apparently simple shift in locus, and yet deeper down it offers a way forward to engineer the complexity needed to respond to the complexity (per the law of requisite variety). Maintaining the word person just about stretches to our needs in terms of personhood, encompassing legal persons (organizations recognized as having privileges and obligations under law) and quite reasonably transhumans. To press the point home fully however, any organizing (assemblage, however ephemeral or enduring) would also need to be a person. I am a person Alphabet Inc. is a person, as are its subsidiaries, e.g. Google Her Majesty's Government is a person, as are its Departments A family is a person of persons Molly Millions is a person Hawking Incorporated was a person. Now let's consider persons living, i.e. relating, interdepending, responding, learning. In other words, exchanging information \u2014 interpersonal data. We are contemplating interpersonal data with no scale at every scale.","title":"The \u201cpersonal data\u201d problem"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/#the-problem-of-the-way-we-frame-the-opportunity-and-problem","text":"Data is data. While accepting that language is metaphor, there doesn't appear to be a wholly useful metaphor that can apply to anything but a fragment or two of the required architecture. It seems that there's never been anything like personal or interpersonal data. No metaphor can be extended to analogy that's for certain. I review five common attempts.","title":"The problem of the way we frame the opportunity and problem"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/#data-as-property","text":"Property is rivalrous. Property is monopoly. Property, unlike human rights, is not universal, indivisible, or inalienable. Personal data is not property. Personal data isn\u2019t money as we know it. Simplistic. Transactional. Binary. It is more akin to the rich, varied and complex information flows present in rainforests, in oceans, in human cultures. It is worth a lot more than six cents a day Facebook makes out of you when markets don\u2019t reduce it and its application to a mere transactional exchange. At the very moment we might conceive an awesome, distributed and continuous crystallization of collective knowledge, the signalling known as price entails monumental information loss and inevitable consequential inequalities.","title":"Data-as-property"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/#data-as-labour","text":"Data-as-labour is effectively a rebadging of data as property \u2014 one owns one's labour after all. Labour is remunerated based on time and/or output at a market value. Here, the value is contingent on factors way beyond any conceptualization of work that I know. The shoehorning simply serves to connect the treatment of personal and similar data to concerns for the impact on employment of all variety of digital technologies, not least artificial intelligence. That might be honourable, but it\u2019s also tenuous at best.","title":"Data-as-labour"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/#data-as-reputation","text":"Programmatic quantification of reputation is an inevitable evil due to the unavoidable self-moderation and modulation it inflicts on its subjects beyond that which might be argued as 'good for society'. To be clear, the social accretion of local, contextually relevant reputation with forgiving opportunities for reparation has served communities for millennia. We are however considering universal, non-contextual and irremediable scoring and algorithmic assessment. Context is a keyword. I want to know if Alice is trustworthy to drive me safely from A to B. I couldn't care less if she's up on her mortgage payments, or has been dropping litter (I do care about that generally, but not in this context), or has been 'dutifully' supportive of the current government, which of course would be an abhorrent context here and one we must ensure we don't engender accidentally. If you resist censorship, it seems you must also resist reputation scoring. Fortunately, we're not actually interested in reputation per se, only as a proxy for trustworthiness and accountability. With some forethought we can engineer our way around these challenges and avoid a (Black Mirror) Nosedive. I suspect the architecture will resemble the past in terms of our having to re-engineer appropriate contextual and social frictions.","title":"Data-as-reputation"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/#data-as-public-good","text":"Academic authors such as Mariana Mazzucato and Viktor Mayer-Sch\u00f6nberger grapple for ways to regulate the run-away oligopolistic success of the centralizing data-hungry behemoths such as Facebook and Google \u2014 getting them to share some of their informational hoard. There responses predominantly take a regulatory form. And yet at the end of the day market regulators can only regulate markets, and markets are dedicated to property ownership. Data can be a public good, of course!, but this doesn\u2019t strike me as anywhere near an optimal way to go about it.","title":"Data-as-public-good"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/#data-as-me","text":"Informational privacy requires radical re-interpretation, one that takes into account the essentially informational nature of human beings and of their operations as informational social agents. Such re-interpretation is achieved by considering each person as constituted by his or her information, and hence by understanding a breach of one's informational privacy as a form of aggression towards one's personal identity. Luciano Floridi This is an exciting vista. Nevertheless, it requires some modification to encourage the emergence of collective intelligence and our anti-rivalrous flourishing. How might we realise this value while respecting personal dignity and agency? Indeed while respecting personal privacy, because what agency can anyone be said to have if they cannot maintain personally desired and contextually appropriate privacy?","title":"Data-as-me"},{"location":"rwot8/topics-and-advance-readings/on-interpersonal-data/#interpersonal-data-architectural-principles","text":"Humans, not data subjects as the GDPR has it Edge-centric, not node-centric, as this is interpersonal Agency, not control as it seems nearly everyone would have it! Rhizomes, not trees as our sciences know it Cache, not facsimile as \u2018stores\u2019, \u2018vaults\u2019 and \u2018wallets\u2019 would have it. On that last point, if we get this right, your bank's cloud (should they continue to maintain one, and should banks continue to be necessary to banking) will be the facsimile. We are dealing with a phenomenon here that appears atomistically simple but in fact forms, informs and infects everything. It's the original complex. Identity is information (and, I suspect, digitally emergent from interpersonal data). Relationships is information. Reputation is information. Exchange is information. Organizing is information. Life is information. We should then proceed with due caution \u2014 ethically and technically. Ethically, we need to take appropriate time for due diligence. Ethically, we cannot delay and allow the data-as-property protagonists time to establish the mother of all Nash equilibria.","title":"Interpersonal data architectural principles"},{"location":"rwot8/topics-and-advance-readings/psdad/","text":"PSDAD - A new data format with secure semantics Submission to: Rebooting the Web of Trust #8 (March 2019, Barcelona) Author: Sandro Hawke < sandro@w3.org > Abstract Existing data serialization formats like JSON, JSON-LD, XML, and even ASN.1 (with its various encoding rules) work well enough for conventional computing environments, but they fall short when high levels of both trust and decentralization are required. PSDAD (plaintext self-describing assertional data) is a proposed new approach which uses natural language strings simultaneously as identifiers, delimiters, and documentation, resulting in a surprisingly simple and robust system with distinct advantages over known approaches in certain environments. PSDAD has a partial spec and partial implementation available. Motivation Consider the scenario where Alice wants to send Bob a machine-readable message saying that some thermal probe (probe number 6) is currently reading 34\u00b0C. With JSON , she might encode it like this: { \"probe\": 6, \"temp\": 34 } To properly understand this message, Bob needs to know its syntax and semantics. In particular, in this example, how can he tell whether the temperature is in Celsius, Fahrenheit, Kelvin, or some application-specific scale? This has to be communicated out-of-band, via external means. Perhaps Alice and Bob were at a hackathon together while building this system. Or maybe Alice wrote a blog post about it, which Bob found via search engine. Or maybe it's part of some formal standard, or this is some proprietary software talking to another instance of itself. In any case, for Bob to be confident he is correctly understanding Alice's message, there has to be some clear external signaling of how Alice intends each of the data fields in her message to be understood, in the context of the message. With JSON-LD , Alice might encode her message like this: { \"@context\": { \"probe\": \"https://thermals.example/schema#probe\", \"temp\": \"https://thermals.example/schema#temp\" }, \"probe\": 6, \"temp\": 34 } Now, there is some in-band signaling of the semantics. The URIs act as global identifier (in a way \"probe\" and \"temp\" do not). And now Bob has the additional option of visiting https://thermals.example/schema in a browser and perhaps finding some useful information about the semantics of the data. That sounds pretty good. These familiar techniques fall short, however, if there is confusion about which definitions to use. That confusion might come from an innocent mistake (such as poor coordination during evolution of the system), or it might come from a disinformation attack, with someone deliberately spreading misleading information about the semantics of Alice's data. As long as Alice and Bob know each other, are using the same software, or are using data formats defined by large standards organizations, the risk is probably small. The risk becomes significant, however, in a dynamic market around an open data bus (such as the web), with many different components and systems passing data around, with increasingly complexity, and new formats or extensions appearing over time. Some people suggest the RDF (and XML) namespace architectures, as shown above in JSON-LD, solve this problem because one can use the namespace URL content as the master definition source. In theory this could work, but in practice it has three serious problems: The W3C RDF specifications and current practice do not actually give the namespace content any special privileged status. If the content disagrees with search engine results or word-of-mouth, it's unclear which should be taken as correct. It is also unclear whether there is any way to add such special status now, long after the specs have been published. This makes the namespace host a part of the critical security infrastructure, since it gets to say what the terms mean. Now, when Alice and Bob communicate, they must also trust the namespace host (thermal.example, in the above example). This could make it significantly harder to build secure data communication channels, and it increases the cost and liability for running a namespace host. Arguably, it increases centralization around major namespaces. Not everybody is willing to use RDF, even in the form of JSON-LD. These concerns are becoming more pressing as the Credible Web Community Group moves toward creating a data ecosystem to help combat online misinformation. There may be others in the larger community with similar concerns. Alternatives I am not aware of anyone else having done work on this problem. I have developed another solution which applies only to the RDF space, Movable Schemas , possibly with version-integrity . PSDAD is the extension of Movable Schemas idea to the larger field beyond RDF. Proposal PSDAD starts with the observation that the semantics of each field in a data format are in practice defined by a short paragraph in some specification. In the example above, Alice and Bob might be using a spec which contains this text: ... probe : The \"probe\" field specifies the integer index of the probe whose temperature is specified in the \"temp\" field. temp : The \"temp\" field specifies the temperature measured at the given probe, rounded off to the nearest integer, in degrees Celsius. ... We want that spec text to be connected to the bits on the wire, in a way that is secure, scalable, and maintainable. Operationally, we want the people writing any software which might be working with the data in those fields to be looking at the same spec text, with minimal chance for error or disinformation. The PSDAD approach is to send that spec text on the wire wrapped around the data fields. For example, Alice might transmit: The temperature at probe number 6 is 34\u00b0C. To a human who knows English, this is fairly meaningful. To the software at either end, this looks pretty much the same as the JSON expression given above, {\"probe\":6,\"temp\": 34} . It's a few bytes longer, but it's still \"stuff 6 stuff 34 stuff\". The PSDAD trick is to make the delimiters be the documentation by writing the documentation as template assertions, then filling in the templates with the field data values. The template in use here is The temperature at probe number [] is []\u00b0C. Given that template, software at either end can easily serialize and deserialize probe temperature data. Applicability The primary use case here is to have decentralized extensibility be robust against disinformation attacks, but this technique also has potential to address schema and documentation management issues in conventional deployment environments. In particular, PSDAD allows developers to publish and/or consume machine-readable data with semantic clarity and minimal procedural work or need for prior agreement. If you want to publish some data, just think about how you would express each item in that data clearly, pedantically, as a statement in your preferred natural language. If you want to consume some data, look at some examples and deduce the template. If it's not obvious, there's probably an issue. Here's another example. Alice wants to help build a web of trust, so she publish this text at https://alice.example/endorsements : I have met and know the person \"Susan X. Lastname\". Susan X. Lastname controls and is the sole user of the website https://susan.example. Susan X. Lastname has the ssh key with public part ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDQnL/9t1t+w8U9bBOn3OeM568BkiejgK37ITAgaRHyGw0+vYCqinMKeswzv0YFar8n9M+Rwi78Evk72RuOlQldcw5cmVfrgwey3U7k/0cJE5ecnO2CEBU4zqfBiKlGnlRsjQ1UcsY2e396ScBGrzm02UYBmtRa09fA+vbSxN4O6nHVDdR1cscbOa3TXhfPpp009LdDzUIt5AgiwXPdBYOBBrvRnzULhGMWiTy8HEXpueRpBOa90Q7yEhL/zZluZQo8tMRUYkk6EwTvJUmz3TjrYKI5DO5p5q1mPR1/wMjVRUEwYG7HET6PgQuJj4aQiIrPQZbVftd+AVOjJyvenkW9 susan@susan.example. I have met and know the person Tommy Lastname. Tommy Lastname controls and is the sole user of the website https://tommy.example. These strings can be serialized and/or deserialized easily, given these templates: * I have met and know the person [name]. * [name] controls and is the sole user of the website [url]. * [name] has the ssh key with the public part [ssh_pub]. Details As with any data format, there are many details. They are being addressed in the PSDAD spec A few specific points: PSDAD text SHOULD be compressed using the standard compression features of the underlying layer(s). After compression, the size is likely to converge on the same size as JSON serialized data. Field value serializations MAY be quoted (JSON-style) and MUST be quoted if the value happens to include a substring matching the ending delimiter of that field in that template. (We had to quote \"Susan X. Lastname\" above because the string contained period+whitespace, which was also the ending delimiter. Unknown text in input MUST be ignored, to allow for extensibility. If there are multiple different templates for the same data (perhaps because of schema evolution), and you don't have a negotiation mechanism to narrow down which ones the other end might be using, you MUST use them all. Writers must write them all, readers must attempt to recognize them all. This provides backward and forward compatibility. The semantics of transmitted data are assertional (like RDF), so duplicate data/statements are ignored Writing good template statements is hard, as is writing a good spec. If you err on the long side, it will at least serve to identify and delimit, and then you can iteratively improve the wording as issues arise. Conclusion PSDAD is a new data serialization technique, which appears to have significant advantages in dynamic, decentralized, trust-sensitive environments. Feedback and improvements are welcome via github .","title":"PSDAD - A new data format with secure semantics"},{"location":"rwot8/topics-and-advance-readings/psdad/#psdad-a-new-data-format-with-secure-semantics","text":"Submission to: Rebooting the Web of Trust #8 (March 2019, Barcelona) Author: Sandro Hawke < sandro@w3.org >","title":"PSDAD - A new data format with secure semantics"},{"location":"rwot8/topics-and-advance-readings/psdad/#abstract","text":"Existing data serialization formats like JSON, JSON-LD, XML, and even ASN.1 (with its various encoding rules) work well enough for conventional computing environments, but they fall short when high levels of both trust and decentralization are required. PSDAD (plaintext self-describing assertional data) is a proposed new approach which uses natural language strings simultaneously as identifiers, delimiters, and documentation, resulting in a surprisingly simple and robust system with distinct advantages over known approaches in certain environments. PSDAD has a partial spec and partial implementation available.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/psdad/#motivation","text":"Consider the scenario where Alice wants to send Bob a machine-readable message saying that some thermal probe (probe number 6) is currently reading 34\u00b0C. With JSON , she might encode it like this: { \"probe\": 6, \"temp\": 34 } To properly understand this message, Bob needs to know its syntax and semantics. In particular, in this example, how can he tell whether the temperature is in Celsius, Fahrenheit, Kelvin, or some application-specific scale? This has to be communicated out-of-band, via external means. Perhaps Alice and Bob were at a hackathon together while building this system. Or maybe Alice wrote a blog post about it, which Bob found via search engine. Or maybe it's part of some formal standard, or this is some proprietary software talking to another instance of itself. In any case, for Bob to be confident he is correctly understanding Alice's message, there has to be some clear external signaling of how Alice intends each of the data fields in her message to be understood, in the context of the message. With JSON-LD , Alice might encode her message like this: { \"@context\": { \"probe\": \"https://thermals.example/schema#probe\", \"temp\": \"https://thermals.example/schema#temp\" }, \"probe\": 6, \"temp\": 34 } Now, there is some in-band signaling of the semantics. The URIs act as global identifier (in a way \"probe\" and \"temp\" do not). And now Bob has the additional option of visiting https://thermals.example/schema in a browser and perhaps finding some useful information about the semantics of the data. That sounds pretty good. These familiar techniques fall short, however, if there is confusion about which definitions to use. That confusion might come from an innocent mistake (such as poor coordination during evolution of the system), or it might come from a disinformation attack, with someone deliberately spreading misleading information about the semantics of Alice's data. As long as Alice and Bob know each other, are using the same software, or are using data formats defined by large standards organizations, the risk is probably small. The risk becomes significant, however, in a dynamic market around an open data bus (such as the web), with many different components and systems passing data around, with increasingly complexity, and new formats or extensions appearing over time. Some people suggest the RDF (and XML) namespace architectures, as shown above in JSON-LD, solve this problem because one can use the namespace URL content as the master definition source. In theory this could work, but in practice it has three serious problems: The W3C RDF specifications and current practice do not actually give the namespace content any special privileged status. If the content disagrees with search engine results or word-of-mouth, it's unclear which should be taken as correct. It is also unclear whether there is any way to add such special status now, long after the specs have been published. This makes the namespace host a part of the critical security infrastructure, since it gets to say what the terms mean. Now, when Alice and Bob communicate, they must also trust the namespace host (thermal.example, in the above example). This could make it significantly harder to build secure data communication channels, and it increases the cost and liability for running a namespace host. Arguably, it increases centralization around major namespaces. Not everybody is willing to use RDF, even in the form of JSON-LD. These concerns are becoming more pressing as the Credible Web Community Group moves toward creating a data ecosystem to help combat online misinformation. There may be others in the larger community with similar concerns.","title":"Motivation"},{"location":"rwot8/topics-and-advance-readings/psdad/#alternatives","text":"I am not aware of anyone else having done work on this problem. I have developed another solution which applies only to the RDF space, Movable Schemas , possibly with version-integrity . PSDAD is the extension of Movable Schemas idea to the larger field beyond RDF.","title":"Alternatives"},{"location":"rwot8/topics-and-advance-readings/psdad/#proposal","text":"PSDAD starts with the observation that the semantics of each field in a data format are in practice defined by a short paragraph in some specification. In the example above, Alice and Bob might be using a spec which contains this text: ... probe : The \"probe\" field specifies the integer index of the probe whose temperature is specified in the \"temp\" field. temp : The \"temp\" field specifies the temperature measured at the given probe, rounded off to the nearest integer, in degrees Celsius. ... We want that spec text to be connected to the bits on the wire, in a way that is secure, scalable, and maintainable. Operationally, we want the people writing any software which might be working with the data in those fields to be looking at the same spec text, with minimal chance for error or disinformation. The PSDAD approach is to send that spec text on the wire wrapped around the data fields. For example, Alice might transmit: The temperature at probe number 6 is 34\u00b0C. To a human who knows English, this is fairly meaningful. To the software at either end, this looks pretty much the same as the JSON expression given above, {\"probe\":6,\"temp\": 34} . It's a few bytes longer, but it's still \"stuff 6 stuff 34 stuff\". The PSDAD trick is to make the delimiters be the documentation by writing the documentation as template assertions, then filling in the templates with the field data values. The template in use here is The temperature at probe number [] is []\u00b0C. Given that template, software at either end can easily serialize and deserialize probe temperature data.","title":"Proposal"},{"location":"rwot8/topics-and-advance-readings/psdad/#applicability","text":"The primary use case here is to have decentralized extensibility be robust against disinformation attacks, but this technique also has potential to address schema and documentation management issues in conventional deployment environments. In particular, PSDAD allows developers to publish and/or consume machine-readable data with semantic clarity and minimal procedural work or need for prior agreement. If you want to publish some data, just think about how you would express each item in that data clearly, pedantically, as a statement in your preferred natural language. If you want to consume some data, look at some examples and deduce the template. If it's not obvious, there's probably an issue. Here's another example. Alice wants to help build a web of trust, so she publish this text at https://alice.example/endorsements : I have met and know the person \"Susan X. Lastname\". Susan X. Lastname controls and is the sole user of the website https://susan.example. Susan X. Lastname has the ssh key with public part ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDQnL/9t1t+w8U9bBOn3OeM568BkiejgK37ITAgaRHyGw0+vYCqinMKeswzv0YFar8n9M+Rwi78Evk72RuOlQldcw5cmVfrgwey3U7k/0cJE5ecnO2CEBU4zqfBiKlGnlRsjQ1UcsY2e396ScBGrzm02UYBmtRa09fA+vbSxN4O6nHVDdR1cscbOa3TXhfPpp009LdDzUIt5AgiwXPdBYOBBrvRnzULhGMWiTy8HEXpueRpBOa90Q7yEhL/zZluZQo8tMRUYkk6EwTvJUmz3TjrYKI5DO5p5q1mPR1/wMjVRUEwYG7HET6PgQuJj4aQiIrPQZbVftd+AVOjJyvenkW9 susan@susan.example. I have met and know the person Tommy Lastname. Tommy Lastname controls and is the sole user of the website https://tommy.example. These strings can be serialized and/or deserialized easily, given these templates: * I have met and know the person [name]. * [name] controls and is the sole user of the website [url]. * [name] has the ssh key with the public part [ssh_pub].","title":"Applicability"},{"location":"rwot8/topics-and-advance-readings/psdad/#details","text":"As with any data format, there are many details. They are being addressed in the PSDAD spec A few specific points: PSDAD text SHOULD be compressed using the standard compression features of the underlying layer(s). After compression, the size is likely to converge on the same size as JSON serialized data. Field value serializations MAY be quoted (JSON-style) and MUST be quoted if the value happens to include a substring matching the ending delimiter of that field in that template. (We had to quote \"Susan X. Lastname\" above because the string contained period+whitespace, which was also the ending delimiter. Unknown text in input MUST be ignored, to allow for extensibility. If there are multiple different templates for the same data (perhaps because of schema evolution), and you don't have a negotiation mechanism to narrow down which ones the other end might be using, you MUST use them all. Writers must write them all, readers must attempt to recognize them all. This provides backward and forward compatibility. The semantics of transmitted data are assertional (like RDF), so duplicate data/statements are ignored Writing good template statements is hard, as is writing a good spec. If you err on the long side, it will at least serve to identify and delimit, and then you can iteratively improve the wording as issues arise.","title":"Details"},{"location":"rwot8/topics-and-advance-readings/psdad/#conclusion","text":"PSDAD is a new data serialization technique, which appears to have significant advantages in dynamic, decentralized, trust-sensitive environments. Feedback and improvements are welcome via github .","title":"Conclusion"},{"location":"rwot8/topics-and-advance-readings/rightsframeworks/","text":"Legal Frameworks for Humanity in the Digital Age Elizabeth M. Renieris (Twitter @hackylawyER), For Rebooting Web of Trust, 1-3 March 2019 Elizabeth is Global Policy Counsel at digital ID startup Evernym. She is a law and policy expert who advises on data protection and privacy in the context of emerging technologies like blockchain, AI, and machine learning. Elizabeth is a Certified Information Privacy Professional in the US and Europe (CIPP/US, CIPP/E) and has advised the European Commission, UK Parliament, and other governments on the intersection of blockchain and data protection/privacy, particularly in the context of digital and self-sovereign identity. TL;DR: We are allowing the tides of technology and commerce to haphazardly turn everything into commodifiable data. But will we allow ourselves to be reduced to data points and, worse yet, commodified? If we are not deliberate about designing the correct legal frameworks for humanity in the data-driven age, we risk losing sight of our fundamental rights as humans. With more than half of the world's population now online and doing more than ever before, personal data is proliferating at an unprecedented rate. Our personal data exhaust is no longer confined to \u201csurfing the Web\u201d or traditional \u201conline\u201d activities. As our lives get \"smarter\" (through smart phones, smart homes, smart cars, smart devices, smart cities, and eventually smart or augmented humans), every second of our lives becomes a data point. Despite the seeming inevitability of all things digital fueled by our data, usage does not connote trust. On the contrary, our trust and confidence in the entities handling our personal data is at an all-time low. Yet, despite an emerging consensus that the advertising revenue-based business model of the Internet is largely to blame, the most popular solutions on the table risk extending this broken framework to our lives at large. One popular view taking hold and gaining momentum in the public discourse is what I call the \"data-as-property\" approach. In a recently authored op-ed in the Economist, musician will.i.am argued that personal data should be regarded as property and that people should be compensated for the use of their data as property. He is not alone. Many loud voices are jumping on the \u201cown your data\u201d bandwagon, as if owning our data (if that were even possible), will correct the breaches of trust and confidence we have suffered at the hands of big tech. For the record, it will not. As we saw with one of Facebook's more recent scandals around its market research mobile app, compensating individuals for access to their data does little to solve the underlying problem. Rather, business models enabled by legal frameworks that treat our data as property capable of being bought and sold via contract is the root of the problem. In order to course-correct the public discourse and help design and develop better legal frameworks, let's first address why we are tempted to treat personal data as property by exploring the complex nature of data, which makes it at once feel like a commodity but actually ill-suited for commodification: all data points are relatively indistinguishable, consisting of binary bits and bytes; the word \u201cdata\u201d itself, derived from the Latin verb meaning \u201cto give,\u201d conveys liquidity or transferability; and, once digitized, data becomes \u201cmoney-like.\u201d This is perhaps why our personal data is increasingly described as \u201cthe new currency\u201d or \u201cnew oil\u201d and why many espouse a property or commodity view of data as something we can own and sell. However, because data is a mere representation of other things, e.g. property, money, speech, etc., it cannot fall under a single legal framework or body of law. The significance and value of any given data point is different from one context to another. Treating all data as property strips it from its context, from the environment that gives it meaning. This nature of data challenges our existing legal frameworks, which is why we need to consciously design better alternatives. Our existing legal frameworks are inadequate for the societal sea-change that is transpiring. As we haphazardly fuse the physical, digital, and biological worlds through emerging technologies, everything is converging as data\u2014everything apart from our laws. Our approach to law has tried to maintain neat swim lanes, regulating, e.g., property as property, money as money, speech as speech, and (more recently) data as data. But as everything is becoming data, these lanes are breaking down. Domain-specific legal frameworks ignore how the digital versions of things are qualitatively different from their analog counterparts, while data-specific regulations often neglect the consumptive, expressive, or other contextual qualities of the source of a given data point or data set\u2014contexts in which we may have important rights as citizens, patients, and humans, rather than mere customers or consumers in a data-industrial complex. To better illustrate the way we have to think about designing better legal frameworks, take a concrete example of this digital convergence--the example of money and speech. Money and speech have traditionally faced distinct legal frameworks with different, often oppositional, objectives. Financial regulations are generally designed to curtail and control, while speech laws are meant to presumptively protect and promote the freedom of expression. But money is increasingly digital, with businesses and localities going cashless, the explosion of \u201cfintechs,\u201d programmable money in the form of cryptocurrencies and tokens, and digital record-keeping of value through blockchains and distributed ledgers. At the same time, human behavior and expression itself is being turned into data as we are tracked online and off in what Shoshana Zuboff calls \u201csurveillance capitalism.\u201d This convergence threatens the fundamental rights of individuals. If we regulate digital money without considering its expressive qualities, we risk censoring certain individuals and vulnerable populations and, conversely, if we regulate digital speech without considering its consumptive qualities, we risk curtailing their bargaining or purchasing power too. Using examples like this one, I argue for legal frameworks that treat personal data as an extension of our identity, calling for inalienable, intrinsic rights in our data, a so-called \u201crights-based approach.\u201d The pace and nature of technological change, coupled with the seduction of reductive approaches in the face of this complex challenge, is accelerating the urgency of designing viable frameworks for humanity in the data-driven age. Per the fundamental rule of technological innovation, what can be done will be done, and market forces will also nudge us down a dangerous path. The time to protect our rights by mindfully addressing and deciding how we will treat our data in the data-driven age is now\u2014our choices will determine how we interact with technology, how we relate to each other, and even how we order society. This is a (if not the) critical question of our time\u2014as our lives and our humanity are increasingly represented by data, the way we treat data will become the way we treat our humanity. It is also an opportunity to renew and reaffirm our core values and ideals as we move into Web 3.0.","title":"Rightsframeworks"},{"location":"rwot8/topics-and-advance-readings/rightsframeworks/#legal-frameworks-for-humanity-in-the-digital-age","text":"Elizabeth M. Renieris (Twitter @hackylawyER), For Rebooting Web of Trust, 1-3 March 2019 Elizabeth is Global Policy Counsel at digital ID startup Evernym. She is a law and policy expert who advises on data protection and privacy in the context of emerging technologies like blockchain, AI, and machine learning. Elizabeth is a Certified Information Privacy Professional in the US and Europe (CIPP/US, CIPP/E) and has advised the European Commission, UK Parliament, and other governments on the intersection of blockchain and data protection/privacy, particularly in the context of digital and self-sovereign identity. TL;DR: We are allowing the tides of technology and commerce to haphazardly turn everything into commodifiable data. But will we allow ourselves to be reduced to data points and, worse yet, commodified? If we are not deliberate about designing the correct legal frameworks for humanity in the data-driven age, we risk losing sight of our fundamental rights as humans. With more than half of the world's population now online and doing more than ever before, personal data is proliferating at an unprecedented rate. Our personal data exhaust is no longer confined to \u201csurfing the Web\u201d or traditional \u201conline\u201d activities. As our lives get \"smarter\" (through smart phones, smart homes, smart cars, smart devices, smart cities, and eventually smart or augmented humans), every second of our lives becomes a data point. Despite the seeming inevitability of all things digital fueled by our data, usage does not connote trust. On the contrary, our trust and confidence in the entities handling our personal data is at an all-time low. Yet, despite an emerging consensus that the advertising revenue-based business model of the Internet is largely to blame, the most popular solutions on the table risk extending this broken framework to our lives at large. One popular view taking hold and gaining momentum in the public discourse is what I call the \"data-as-property\" approach. In a recently authored op-ed in the Economist, musician will.i.am argued that personal data should be regarded as property and that people should be compensated for the use of their data as property. He is not alone. Many loud voices are jumping on the \u201cown your data\u201d bandwagon, as if owning our data (if that were even possible), will correct the breaches of trust and confidence we have suffered at the hands of big tech. For the record, it will not. As we saw with one of Facebook's more recent scandals around its market research mobile app, compensating individuals for access to their data does little to solve the underlying problem. Rather, business models enabled by legal frameworks that treat our data as property capable of being bought and sold via contract is the root of the problem. In order to course-correct the public discourse and help design and develop better legal frameworks, let's first address why we are tempted to treat personal data as property by exploring the complex nature of data, which makes it at once feel like a commodity but actually ill-suited for commodification: all data points are relatively indistinguishable, consisting of binary bits and bytes; the word \u201cdata\u201d itself, derived from the Latin verb meaning \u201cto give,\u201d conveys liquidity or transferability; and, once digitized, data becomes \u201cmoney-like.\u201d This is perhaps why our personal data is increasingly described as \u201cthe new currency\u201d or \u201cnew oil\u201d and why many espouse a property or commodity view of data as something we can own and sell. However, because data is a mere representation of other things, e.g. property, money, speech, etc., it cannot fall under a single legal framework or body of law. The significance and value of any given data point is different from one context to another. Treating all data as property strips it from its context, from the environment that gives it meaning. This nature of data challenges our existing legal frameworks, which is why we need to consciously design better alternatives. Our existing legal frameworks are inadequate for the societal sea-change that is transpiring. As we haphazardly fuse the physical, digital, and biological worlds through emerging technologies, everything is converging as data\u2014everything apart from our laws. Our approach to law has tried to maintain neat swim lanes, regulating, e.g., property as property, money as money, speech as speech, and (more recently) data as data. But as everything is becoming data, these lanes are breaking down. Domain-specific legal frameworks ignore how the digital versions of things are qualitatively different from their analog counterparts, while data-specific regulations often neglect the consumptive, expressive, or other contextual qualities of the source of a given data point or data set\u2014contexts in which we may have important rights as citizens, patients, and humans, rather than mere customers or consumers in a data-industrial complex. To better illustrate the way we have to think about designing better legal frameworks, take a concrete example of this digital convergence--the example of money and speech. Money and speech have traditionally faced distinct legal frameworks with different, often oppositional, objectives. Financial regulations are generally designed to curtail and control, while speech laws are meant to presumptively protect and promote the freedom of expression. But money is increasingly digital, with businesses and localities going cashless, the explosion of \u201cfintechs,\u201d programmable money in the form of cryptocurrencies and tokens, and digital record-keeping of value through blockchains and distributed ledgers. At the same time, human behavior and expression itself is being turned into data as we are tracked online and off in what Shoshana Zuboff calls \u201csurveillance capitalism.\u201d This convergence threatens the fundamental rights of individuals. If we regulate digital money without considering its expressive qualities, we risk censoring certain individuals and vulnerable populations and, conversely, if we regulate digital speech without considering its consumptive qualities, we risk curtailing their bargaining or purchasing power too. Using examples like this one, I argue for legal frameworks that treat personal data as an extension of our identity, calling for inalienable, intrinsic rights in our data, a so-called \u201crights-based approach.\u201d The pace and nature of technological change, coupled with the seduction of reductive approaches in the face of this complex challenge, is accelerating the urgency of designing viable frameworks for humanity in the data-driven age. Per the fundamental rule of technological innovation, what can be done will be done, and market forces will also nudge us down a dangerous path. The time to protect our rights by mindfully addressing and deciding how we will treat our data in the data-driven age is now\u2014our choices will determine how we interact with technology, how we relate to each other, and even how we order society. This is a (if not the) critical question of our time\u2014as our lives and our humanity are increasingly represented by data, the way we treat data will become the way we treat our humanity. It is also an opportunity to renew and reaffirm our core values and ideals as we move into Web 3.0.","title":"Legal Frameworks for Humanity in the Digital Age"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/","text":"Security considerations for Shamir's secret sharing This article comes from research conducted during development of Dark-Crystal, a social backup and recovery system for secrets such as cryptographic keys, built on the peer-to-peer protocol Secure Scuttlebutt. Issues with private key management often pose barriers to the adoption of empowering decentralised technologies and this is exactly what this project aims to address. Additional research as well as our source code is available at github.com/blockades . Introduction In general, Shamir's scheme is considered information-theoretically secure. That is, individual shares contain absolutely no semantic information about the secret, and it can be said to be 'post quantum' cryptography. An interesting anecdote, the root key for ICANN DNS security, effectively the key which secures the naming system of the internet, is held by seven parties, based in Britain, the U.S., Burkina Faso, Trinidad and Tobago, Canada, China, and the Czech Republic. Cryptographer Bruce Schneier has alleged that they are holders of Shamir's secret shares, which indicates the scheme is taken quite seriously. However, it is not without its problems: The need for verification of individual shares Harn and Lin consider the situation in which 'cheaters' claiming to be holders of shares introduce 'fake' shares, causing the incorrect secret to be recovered. Of course without having the other shares they have no control over the content of the 'incorrect' secret. This is not so much of a concern for our project as we already have a way to validate who is a custodian. However we also considered the possibility that genuine holders of shares might have a motivation for not wanting the secret to be recovered, and could maliciously modify their share. Furthermore, the shares might be modified by some accidental or external cause, and it is important to be able to determine which share is causing the problem. It might be very easy to determine that we have recovered the wrong secret. Either because we have some idea of how we expect it to look, or as we have recently implemented in dark crystal, an identifier is added to the secret to allow the correct secret to be automatically recognised. (We concatonate the secret with the last 16 bytes of its SHA256 hash). However, the problem here is that although we might know for sure that we have not successfully restored our secret, we have no way of telling which share(s) have caused the problem, meaning we do not know who is responsible. The solution is to introduce some verification of shares, and a number of different methods of doing this have been proposed. Typically, they rely on publicly publishing some information which allows verification of a given share. Here are some possible solutions: Publicly publishing the encrypted shares This is what we were originally planning to do, but this only helps in this context if the encryption scheme used is deterministic. That is to say encrypting the same message with the same key twice will reliably give the same output. The problem here is that such encryption schemes are vulnerable to replay attacks. Most modern asymmetric schemes introduce some random nonce to evade this problem. The scheme we are using (libsodium's secret box) typically takes a 24 byte random nonce. So this is not a good option. However we actually already 'wrap' shares in a second level of encryption to to allow the secret owner to publish their own encrypted copy of the share message with its associated metadata as a way to keep a personal record of what was sent to who. When we looked at other schemes for verifiable secret sharing we found they involved a similar practice, and we decided to use an existing well-documented scheme. Publicly publishing the hash of each share This is also something we considered, but feel that it gives custodians more unnecessary extra information and less accountability compared to other methods. Feldman's scheme Paul Feldman proposed a scheme in 1987 which allows custodians to verify their own shares, using homomorphic encryption (an encryption scheme where computation can be done on encrypted data which when decrypted gives the same result as doing that computation on the original data) on top of Shamir's original scheme. Schoenmakers scheme More recently Berry Schoenmaker proposed a scheme which is publicly verifiable (originally introduced by Stadler, 1996). That is, not only custodians, but anybody is able to verify that the correct shares were given. The scheme is described in the context of an electronic voting application and focusses on validating the behaviour of the 'dealer' (the author of the secret). But it can just as well be used to verify that returned shares have not been modified, which is what we are most interested in. Implementations We have assessed the following implementations: https://github.com/songgeng87/PubliclyVerifiableSecretSharing - C implementation built on secp256k1 used by EOS https://github.com/FabioTacke/PubliclyVerifiableSecretSharing - A Swift implementation https://github.com/dfinity/vss - Dfinity's NodeJS implementation built on BLS, and used for their distributed key generation However, these do not give a drop-in replacement for the secrets library we currently use. Adopting verifiable secret sharing would require a large change to our codebase and mean we need to reconsider several aspects of our model. But it would bring a great advantage in terms of security. Share size has a linear relationship with secret size Anyone holding a share is able to determine the length of the secret. Particular kind of cryptographic keys have a characteristic length. So the scheme gives away more information to custodians than is necessary. Our solution to this is to add padding to the secret to increase share length to a standard amount. Revoking shares if trust in a custodian is lost Suppose we loose trust in one person holding a share. This might be because they had their computer stolen. Or maybe we had a really bad argument with them. Or maybe we found out they weren't the person they were claiming to be. In Shamir's original paper he states that one of the great advantages of the scheme is that it is possible to create as many distinct sets of shares as you like without needing to modify the secret. Each set of shares is incompatible with the other sets. Using Glenn Rempe's implementation , if we run the share method several times with the same secret, we get each time a different set of shares. When generating a new set, an extra check could be done to rule out the extremely improbable case that an identical set had been generated. This means in a conventional secret sharing scenario (imagine the shares are written on paper and given to the custodians), we could simply give new shares to the custodians we do still trust and ask them to destroy the old ones. This would make the share belonging to the untrusted person become useless. In our case, we are using Secure-Scuttlebutt's immutable log, and have no way of destroying a message. Our solution is to use ephemeral keys which are used only one time for a particular share and can be deleted when a new set of shares is issued. This gives greatly increased security, but the cost is more keys to manage and increased complexity of the model. Secure computation Having a good system of encryption does not give us security if the host system is compromised. We are considering using a dedicated virtual machine for secure computation, such as Dyne.org's Zenroom . However there, are many more considerations one needs to make, especially if secret is initially stored on disk. But this goes beyond the scope of assessing the security of Shamir's scheme. Conclusion Threshold-based secret sharing schemes provide a powerful tool to address the private-key custody problem. There are promising solutions to the issues explored in this article. However, we have focussed here mainly on technical limitations of such schemes. There are many other social aspects which pose threats, and the variety of use-cases makes this a broad field of research. References Beimel, Amos (2011). \"Secret-Sharing Schemes: A Survey\" http://www.cs.bgu.ac.il/~beimel/Papers/Survey.pdf Blakley, G.R. (1979). \"Safeguarding Cryptographic Keys\". Managing Requirements Knowledge, International Workshop on (AFIPS). 48: 313\u2013317. doi:10.1109-/AFIPS.1979.98. Feldman, Paul (1987) \"A practical scheme for non-interactive Verifiable Secret Sharing\" Proceedings of the 28th Annual Symposium on Foundations of Computer Science Harn, L. & Lin, C. Detection and identification of cheaters in (t, n) secret sharing scheme, Des. Codes Cryptogr. (2009) 52: 15. https://link.springer.com/article/10.1007/s10623-008-9265-8 Schneier, Bruce (2010) - DNSSEC Root Key held by 7 parties worldwide https://www.schneier.com/blog/archives/2010/07/dnssec_root_key.html Schoenmakers, Berry (1999) \"A Simple Publicly Verifiable Secret Sharing Scheme and its Application to Electronic Voting\" Advances in Cryptology-CRYPTO'99, volume 1666 of Lecture Notes in Computer Science, pages 148-164, Berlin, 1999. Springer-Verlag. Shamir, Adi (1979). \"How to share a secret\". Communications of the ACM. 22 (11): 612\u2013613. doi:10.1145/359168.359176. Zenroom, a virtual machine for fast cryptographic operations on elliptic curves, https://zenroom.dyne.org/ See Also... Our list of applications and articles on Shamir's secret sharing Brainstorming Coconut-related scenarios ('Coconut death' refers to a role playing game we did as part of our research where we tried to recover the keys of members of the group who had been hit by coconuts) Thoughts on verifying received shards in dark crystal You can also follow our development on Secure Scuttlebutt channels #darkcrystal #dark-crystal and #mmt","title":"Security shamirs"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#security-considerations-for-shamirs-secret-sharing","text":"This article comes from research conducted during development of Dark-Crystal, a social backup and recovery system for secrets such as cryptographic keys, built on the peer-to-peer protocol Secure Scuttlebutt. Issues with private key management often pose barriers to the adoption of empowering decentralised technologies and this is exactly what this project aims to address. Additional research as well as our source code is available at github.com/blockades .","title":"Security considerations for Shamir's secret sharing"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#introduction","text":"In general, Shamir's scheme is considered information-theoretically secure. That is, individual shares contain absolutely no semantic information about the secret, and it can be said to be 'post quantum' cryptography. An interesting anecdote, the root key for ICANN DNS security, effectively the key which secures the naming system of the internet, is held by seven parties, based in Britain, the U.S., Burkina Faso, Trinidad and Tobago, Canada, China, and the Czech Republic. Cryptographer Bruce Schneier has alleged that they are holders of Shamir's secret shares, which indicates the scheme is taken quite seriously. However, it is not without its problems:","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#the-need-for-verification-of-individual-shares","text":"Harn and Lin consider the situation in which 'cheaters' claiming to be holders of shares introduce 'fake' shares, causing the incorrect secret to be recovered. Of course without having the other shares they have no control over the content of the 'incorrect' secret. This is not so much of a concern for our project as we already have a way to validate who is a custodian. However we also considered the possibility that genuine holders of shares might have a motivation for not wanting the secret to be recovered, and could maliciously modify their share. Furthermore, the shares might be modified by some accidental or external cause, and it is important to be able to determine which share is causing the problem. It might be very easy to determine that we have recovered the wrong secret. Either because we have some idea of how we expect it to look, or as we have recently implemented in dark crystal, an identifier is added to the secret to allow the correct secret to be automatically recognised. (We concatonate the secret with the last 16 bytes of its SHA256 hash). However, the problem here is that although we might know for sure that we have not successfully restored our secret, we have no way of telling which share(s) have caused the problem, meaning we do not know who is responsible. The solution is to introduce some verification of shares, and a number of different methods of doing this have been proposed. Typically, they rely on publicly publishing some information which allows verification of a given share. Here are some possible solutions:","title":"The need for verification of individual shares"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#publicly-publishing-the-encrypted-shares","text":"This is what we were originally planning to do, but this only helps in this context if the encryption scheme used is deterministic. That is to say encrypting the same message with the same key twice will reliably give the same output. The problem here is that such encryption schemes are vulnerable to replay attacks. Most modern asymmetric schemes introduce some random nonce to evade this problem. The scheme we are using (libsodium's secret box) typically takes a 24 byte random nonce. So this is not a good option. However we actually already 'wrap' shares in a second level of encryption to to allow the secret owner to publish their own encrypted copy of the share message with its associated metadata as a way to keep a personal record of what was sent to who. When we looked at other schemes for verifiable secret sharing we found they involved a similar practice, and we decided to use an existing well-documented scheme.","title":"Publicly publishing the encrypted shares"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#publicly-publishing-the-hash-of-each-share","text":"This is also something we considered, but feel that it gives custodians more unnecessary extra information and less accountability compared to other methods.","title":"Publicly publishing the hash of each share"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#feldmans-scheme","text":"Paul Feldman proposed a scheme in 1987 which allows custodians to verify their own shares, using homomorphic encryption (an encryption scheme where computation can be done on encrypted data which when decrypted gives the same result as doing that computation on the original data) on top of Shamir's original scheme.","title":"Feldman's scheme"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#schoenmakers-scheme","text":"More recently Berry Schoenmaker proposed a scheme which is publicly verifiable (originally introduced by Stadler, 1996). That is, not only custodians, but anybody is able to verify that the correct shares were given. The scheme is described in the context of an electronic voting application and focusses on validating the behaviour of the 'dealer' (the author of the secret). But it can just as well be used to verify that returned shares have not been modified, which is what we are most interested in.","title":"Schoenmakers scheme"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#implementations","text":"We have assessed the following implementations: https://github.com/songgeng87/PubliclyVerifiableSecretSharing - C implementation built on secp256k1 used by EOS https://github.com/FabioTacke/PubliclyVerifiableSecretSharing - A Swift implementation https://github.com/dfinity/vss - Dfinity's NodeJS implementation built on BLS, and used for their distributed key generation However, these do not give a drop-in replacement for the secrets library we currently use. Adopting verifiable secret sharing would require a large change to our codebase and mean we need to reconsider several aspects of our model. But it would bring a great advantage in terms of security.","title":"Implementations"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#share-size-has-a-linear-relationship-with-secret-size","text":"Anyone holding a share is able to determine the length of the secret. Particular kind of cryptographic keys have a characteristic length. So the scheme gives away more information to custodians than is necessary. Our solution to this is to add padding to the secret to increase share length to a standard amount.","title":"Share size has a linear relationship with secret size"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#revoking-shares-if-trust-in-a-custodian-is-lost","text":"Suppose we loose trust in one person holding a share. This might be because they had their computer stolen. Or maybe we had a really bad argument with them. Or maybe we found out they weren't the person they were claiming to be. In Shamir's original paper he states that one of the great advantages of the scheme is that it is possible to create as many distinct sets of shares as you like without needing to modify the secret. Each set of shares is incompatible with the other sets. Using Glenn Rempe's implementation , if we run the share method several times with the same secret, we get each time a different set of shares. When generating a new set, an extra check could be done to rule out the extremely improbable case that an identical set had been generated. This means in a conventional secret sharing scenario (imagine the shares are written on paper and given to the custodians), we could simply give new shares to the custodians we do still trust and ask them to destroy the old ones. This would make the share belonging to the untrusted person become useless. In our case, we are using Secure-Scuttlebutt's immutable log, and have no way of destroying a message. Our solution is to use ephemeral keys which are used only one time for a particular share and can be deleted when a new set of shares is issued. This gives greatly increased security, but the cost is more keys to manage and increased complexity of the model.","title":"Revoking shares if trust in a custodian is lost"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#secure-computation","text":"Having a good system of encryption does not give us security if the host system is compromised. We are considering using a dedicated virtual machine for secure computation, such as Dyne.org's Zenroom . However there, are many more considerations one needs to make, especially if secret is initially stored on disk. But this goes beyond the scope of assessing the security of Shamir's scheme.","title":"Secure computation"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#conclusion","text":"Threshold-based secret sharing schemes provide a powerful tool to address the private-key custody problem. There are promising solutions to the issues explored in this article. However, we have focussed here mainly on technical limitations of such schemes. There are many other social aspects which pose threats, and the variety of use-cases makes this a broad field of research.","title":"Conclusion"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#references","text":"Beimel, Amos (2011). \"Secret-Sharing Schemes: A Survey\" http://www.cs.bgu.ac.il/~beimel/Papers/Survey.pdf Blakley, G.R. (1979). \"Safeguarding Cryptographic Keys\". Managing Requirements Knowledge, International Workshop on (AFIPS). 48: 313\u2013317. doi:10.1109-/AFIPS.1979.98. Feldman, Paul (1987) \"A practical scheme for non-interactive Verifiable Secret Sharing\" Proceedings of the 28th Annual Symposium on Foundations of Computer Science Harn, L. & Lin, C. Detection and identification of cheaters in (t, n) secret sharing scheme, Des. Codes Cryptogr. (2009) 52: 15. https://link.springer.com/article/10.1007/s10623-008-9265-8 Schneier, Bruce (2010) - DNSSEC Root Key held by 7 parties worldwide https://www.schneier.com/blog/archives/2010/07/dnssec_root_key.html Schoenmakers, Berry (1999) \"A Simple Publicly Verifiable Secret Sharing Scheme and its Application to Electronic Voting\" Advances in Cryptology-CRYPTO'99, volume 1666 of Lecture Notes in Computer Science, pages 148-164, Berlin, 1999. Springer-Verlag. Shamir, Adi (1979). \"How to share a secret\". Communications of the ACM. 22 (11): 612\u2013613. doi:10.1145/359168.359176. Zenroom, a virtual machine for fast cryptographic operations on elliptic curves, https://zenroom.dyne.org/","title":"References"},{"location":"rwot8/topics-and-advance-readings/security_shamirs/#see-also","text":"Our list of applications and articles on Shamir's secret sharing Brainstorming Coconut-related scenarios ('Coconut death' refers to a role playing game we did as part of our research where we tried to recover the keys of members of the group who had been hit by coconuts) Thoughts on verifying received shards in dark crystal You can also follow our development on Secure Scuttlebutt channels #darkcrystal #dark-crystal and #mmt","title":"See Also..."},{"location":"rwot8/topics-and-advance-readings/social-key-recovery/","text":"A New Approach to Social Key Recovery by Christopher Allen & Mark Friedenbach The goal of social key recovery is for the user to specify groups of individuals that together possess the ability to recover the root secret of a wallet. A good social key recovery protocol should not just reflect what cryptographic primitives happen to be available for use, but rather instead should be designed to correspond with the structure of trust in the user\u2019s social network while balancing the technical tradeoffs involved under the hood. The most popular social key recovery algorithm, Shamir Secret Sharing, is considered information-theoretically secure. That is, any combination of shares less than the necessary threshold convey absolutely no information about the secret. However, all secrets have equal weight and once a sufficient threshold is achieved the secret can be reconstructed. In social contexts this can cause a number of problems in common real-world scenarios. In addition, Shamir Secret Sharing has a history of being naively implemented including a number of serious vulnerabilities. To quote Bitcoin Core Developer Greg Maxwell: \"I think Shamir Secret Sharing (and a number of other things, RNGs for example), suffer from a property where they are just complex enough that people are excited to implement them often for little good reason, and then they are complex enough (or have few enough reasons to invest significant time) they implement them poorly.\u201d Ideally an implementation of social key recovery should balancing numerous competing goals: Key recovery should require approval of many individuals so as to minimize potential for theft or deliberate key compromise by a small malicious subset of users. Key recovery should require the smallest acceptable threshold so as to prevent loss of funds from destroyed/lost/inaccessible shares. Key recovery requiring interaction with too many individuals is undesirable, as each interaction must involve re-authentication at the inconvenience of all involved. Social trust is not uniformly distributed among different social circles, such as friends, business acquaintances, and family. Individuals may be fungible within a certain circle of trust, but not more broadly. A \u201cfamily member\u201d is not the same as a \u201cbusiness partner.\u201d For example, Alice might want a minimum of three people to sign off on a key recovery attempt, with individuals chosen among her friends, family, and close business partners. More than 3 would be inconvenient and risk loss of funds, while any combination of less than 3 individuals would not be trustworthy. However some combinations of 3 individuals drawn from this entire set would not be reliable: she wouldn\u2019t want her 3 business partners alone having control over her funds, as they may act maliciously in their shared business interests and not for her. One solution for Alice is to require 3 individuals for social key recovery, but also require that these three individuals include AT LEAST one friend and one family member. This can be accomplished by constructing a three-way linear key split with shares X, Y, and Z. X is given to family members, Y is given to friends, and Z is further split using 3-of-N Shamir secret sharing, with unique shares given to each family member, friend, and trusted business partner. Thus each family member knows X and one share of Z, each friend knows Y and one share of Z, and business partners only know their share of Z. As the original key is constructed as X+Y+Z, all three must be reconstructed, which requires the assistance of at least one family member, at least one friend, and a total of three individuals drawn from all three sets. We suggest calling these separate groups of individuals \u201ccircles,\u201d and we suggest designing a social key recovery system where users are allowed to specify participation thresholds for the recovery of the key split associated with each circle (3 \u201cfriends\u201d are required, 2 \u201cbusiness partners\u201d, etc.), and then also specify which circle thresholds are required in disjunctive normal form, e.g. \u201cFriends AND Family\u201d OR \u201cFamily AND Coworkers\u201d. Under the hood this is translated into a set of linear key splits and Shamir secret shares that are encrypted and transmitted to each participant. Longer term on the hardware side, an HSM with secure I/O for user authentication could be used to perform the social key recovery. A potential early place to implement this is the HTC Exodus cryptocurrency phone. When a user attempts key recovery, they present a fresh set of identity keys to their friend/family/coworker/etc. and authenticate themselves to the individual. If the individual is convinced to participate, they authorize their device to reveal their shares, which is done by decrypting on the HSM and then re-encrypting to the temporary identity keys of the user\u2019s new or wiped device. When a user does this with enough shares to reconstruct the original key, their device automatically does so and retires the temporary identity, replacing with the recovered master key. In terms of cryptographic implementation, this requires combining Shamir secret sharing with linear key splits, and then building a social key recovery API centered around the recovery protocol rather than the cryptographic primitives. It will also require some work in defining serialization formats and web-of-trust public key infrastructure for encryption and authentication of the key splits both at the time of distribution and recovery, as well as some thoughts on best ways to store keyshares offline. Milestones Stage 1 would be a review of newer academic papers on improvements to Shamir Secret Sharing as well as linear key splits. Stage 2 would be a specific proposal that we can submit to arXiv and/or academic conferences regarding our specific new proposal. The remaining stages are code. In Stage 3, a preliminary secure implementation of the underlying cryptography for this proposal such that we can get code review by various parties. In Stage 4, we reqiest a formal independent review of the code by an outside party. In Stage 5, we implement a UI version of this for the iPhone. None of these stages would be for secure hardware such as for TrustZone (HTC) or TinyPython for Ledger, or other secure hardware, but would include architectural considerations for such in the future. References Shamir, Adi (1979). \u201cHow to share a secret\u201d. Communications of the ACM. 22 (11): 612\u2013613. doi:10.1145/359168.359176. https://cs.jhu.edu/~sdoshi/crypto/papers/shamirturing.pdf Beimel A. (2011) Secret-Sharing Schemes: A Survey. In: Chee Y.M. et al. (eds) Coding and Cryptology. IWCC 2011. Lecture Notes in Computer Science, vol 6639. Springer, Berlin, Heidelberg https://www.cs.bgu.ac.il/~beimel/Papers/Survey.pdf Rait, Seth (2016). \u201cShamir Secret Sharing and Threshold Cryptography\u201d https://sethrait.com/Shamir-Secret-Sharing-and-Threshold-Cryptography Dautrich J.L., Ravishankar C.V. (2012) \u201cSecurity Limitations of Using Secret Sharing for Data Outsourcing. In: Cuppens-Boulahia\u201d N., Cuppens F., Garcia-Alfaro J. (eds) Data and Applications Security and Privacy XXVI. DBSec 2012. Lecture Notes in Computer Science, vol 7371. Springer, Berlin, Heidelberg http://www.cs.ucr.edu/~ravi/Papers/DBConf/secret_sharing.pdf ) Komargodski I., Naor M., Yogev E. (2016) How to Share a Secret, Infinitely. In: Hirt M., Smith A. (eds) Theory of Cryptography. TCC 2016. Lecture Notes in Computer Science, vol 9986. Springer, Berlin, Heidelberg https://eprint.iacr.org/2016/194.pdf Coron JS., Prouff E., Roche T. (2013) On the Use of Shamir\u2019s Secret Sharing against Side-Channel Analysis. In: Mangard S. (eds) Smart Card Research and Advanced Applications. CARDIS 2012. Lecture Notes in Computer Science, vol 7771. Springer, Berlin, Heidelberg https://www.ssi.gouv.fr/uploads/IMG/pdf/aesshamir_Coron_Prouff_Roche.pdf Blakley, G.R. (1979). \u201cSafeguarding Cryptographic Keys\u201d. Managing Requirements Knowledge, International Workshop on (AFIPS). 48: 313\u2013317. doi:10.1109-/AFIPS.1979.98. https://pdfs.semanticscholar.org/32d2/1ccc21a807627fcb21ea829d1acdab23be12.pdf Feldman, Paul (1987) \u201cA practical scheme for non-interactive Verifiable Secret Sharing\u201d Proceedings of the 28th Annual Symposium on Foundations of Computer Science https://www.cs.umd.edu/~gasarch/TOPICS/secretsharing/feldmanVSS.pdf Harn, e, Changlu L (2009). \u201cDetection and identification of cheaters in (t, n) secret sharing scheme\u201d Des. Codes Cryptography 52, 1 (July 2009), 15-24. DOI=10.1007/s10623-008-9265-8 http://dx.doi.org/10.1007/s10623-008-9265-8 Schoenmakers, Berry (1999) \u201cA Simple Publicly Verifiable Secret Sharing Scheme and its Application to Electronic Voting\u201d Advances in Cryptology-CRYPTO\u201999, volume 1666 of Lecture Notes in Computer Science, pages 148-164, Berlin, 1999. Springer-Verlag. https://www.win.tue.nl/~berry/papers/crypto99.pdf Rusnak, P, et. al (2018) \u201cSLIP-0039 : Shamir\u2019s Secret-Sharing for Mnemonic Codes\u201d Satoshi Labs Github. https://github.com/satoshilabs/slips/blob/master/slip-0039.md Stack Exchange (2016) \u201cWhy is Shamir Secret Sharing not secure against active adversaries out-of-the-box?\u201d Stack Exchange https://crypto.stackexchange.com/questions/41994/why-is-shamir-secret-sharing-not-secure-against-active-adversaries-out-of-the-bo","title":"A New Approach to Social Key Recovery"},{"location":"rwot8/topics-and-advance-readings/social-key-recovery/#a-new-approach-to-social-key-recovery","text":"","title":"A New Approach to Social Key Recovery"},{"location":"rwot8/topics-and-advance-readings/social-key-recovery/#by-christopher-allen-mark-friedenbach","text":"The goal of social key recovery is for the user to specify groups of individuals that together possess the ability to recover the root secret of a wallet. A good social key recovery protocol should not just reflect what cryptographic primitives happen to be available for use, but rather instead should be designed to correspond with the structure of trust in the user\u2019s social network while balancing the technical tradeoffs involved under the hood. The most popular social key recovery algorithm, Shamir Secret Sharing, is considered information-theoretically secure. That is, any combination of shares less than the necessary threshold convey absolutely no information about the secret. However, all secrets have equal weight and once a sufficient threshold is achieved the secret can be reconstructed. In social contexts this can cause a number of problems in common real-world scenarios. In addition, Shamir Secret Sharing has a history of being naively implemented including a number of serious vulnerabilities. To quote Bitcoin Core Developer Greg Maxwell: \"I think Shamir Secret Sharing (and a number of other things, RNGs for example), suffer from a property where they are just complex enough that people are excited to implement them often for little good reason, and then they are complex enough (or have few enough reasons to invest significant time) they implement them poorly.\u201d Ideally an implementation of social key recovery should balancing numerous competing goals: Key recovery should require approval of many individuals so as to minimize potential for theft or deliberate key compromise by a small malicious subset of users. Key recovery should require the smallest acceptable threshold so as to prevent loss of funds from destroyed/lost/inaccessible shares. Key recovery requiring interaction with too many individuals is undesirable, as each interaction must involve re-authentication at the inconvenience of all involved. Social trust is not uniformly distributed among different social circles, such as friends, business acquaintances, and family. Individuals may be fungible within a certain circle of trust, but not more broadly. A \u201cfamily member\u201d is not the same as a \u201cbusiness partner.\u201d For example, Alice might want a minimum of three people to sign off on a key recovery attempt, with individuals chosen among her friends, family, and close business partners. More than 3 would be inconvenient and risk loss of funds, while any combination of less than 3 individuals would not be trustworthy. However some combinations of 3 individuals drawn from this entire set would not be reliable: she wouldn\u2019t want her 3 business partners alone having control over her funds, as they may act maliciously in their shared business interests and not for her. One solution for Alice is to require 3 individuals for social key recovery, but also require that these three individuals include AT LEAST one friend and one family member. This can be accomplished by constructing a three-way linear key split with shares X, Y, and Z. X is given to family members, Y is given to friends, and Z is further split using 3-of-N Shamir secret sharing, with unique shares given to each family member, friend, and trusted business partner. Thus each family member knows X and one share of Z, each friend knows Y and one share of Z, and business partners only know their share of Z. As the original key is constructed as X+Y+Z, all three must be reconstructed, which requires the assistance of at least one family member, at least one friend, and a total of three individuals drawn from all three sets. We suggest calling these separate groups of individuals \u201ccircles,\u201d and we suggest designing a social key recovery system where users are allowed to specify participation thresholds for the recovery of the key split associated with each circle (3 \u201cfriends\u201d are required, 2 \u201cbusiness partners\u201d, etc.), and then also specify which circle thresholds are required in disjunctive normal form, e.g. \u201cFriends AND Family\u201d OR \u201cFamily AND Coworkers\u201d. Under the hood this is translated into a set of linear key splits and Shamir secret shares that are encrypted and transmitted to each participant. Longer term on the hardware side, an HSM with secure I/O for user authentication could be used to perform the social key recovery. A potential early place to implement this is the HTC Exodus cryptocurrency phone. When a user attempts key recovery, they present a fresh set of identity keys to their friend/family/coworker/etc. and authenticate themselves to the individual. If the individual is convinced to participate, they authorize their device to reveal their shares, which is done by decrypting on the HSM and then re-encrypting to the temporary identity keys of the user\u2019s new or wiped device. When a user does this with enough shares to reconstruct the original key, their device automatically does so and retires the temporary identity, replacing with the recovered master key. In terms of cryptographic implementation, this requires combining Shamir secret sharing with linear key splits, and then building a social key recovery API centered around the recovery protocol rather than the cryptographic primitives. It will also require some work in defining serialization formats and web-of-trust public key infrastructure for encryption and authentication of the key splits both at the time of distribution and recovery, as well as some thoughts on best ways to store keyshares offline.","title":"by Christopher Allen &amp; Mark Friedenbach"},{"location":"rwot8/topics-and-advance-readings/social-key-recovery/#milestones","text":"Stage 1 would be a review of newer academic papers on improvements to Shamir Secret Sharing as well as linear key splits. Stage 2 would be a specific proposal that we can submit to arXiv and/or academic conferences regarding our specific new proposal. The remaining stages are code. In Stage 3, a preliminary secure implementation of the underlying cryptography for this proposal such that we can get code review by various parties. In Stage 4, we reqiest a formal independent review of the code by an outside party. In Stage 5, we implement a UI version of this for the iPhone. None of these stages would be for secure hardware such as for TrustZone (HTC) or TinyPython for Ledger, or other secure hardware, but would include architectural considerations for such in the future.","title":"Milestones"},{"location":"rwot8/topics-and-advance-readings/social-key-recovery/#references","text":"Shamir, Adi (1979). \u201cHow to share a secret\u201d. Communications of the ACM. 22 (11): 612\u2013613. doi:10.1145/359168.359176. https://cs.jhu.edu/~sdoshi/crypto/papers/shamirturing.pdf Beimel A. (2011) Secret-Sharing Schemes: A Survey. In: Chee Y.M. et al. (eds) Coding and Cryptology. IWCC 2011. Lecture Notes in Computer Science, vol 6639. Springer, Berlin, Heidelberg https://www.cs.bgu.ac.il/~beimel/Papers/Survey.pdf Rait, Seth (2016). \u201cShamir Secret Sharing and Threshold Cryptography\u201d https://sethrait.com/Shamir-Secret-Sharing-and-Threshold-Cryptography Dautrich J.L., Ravishankar C.V. (2012) \u201cSecurity Limitations of Using Secret Sharing for Data Outsourcing. In: Cuppens-Boulahia\u201d N., Cuppens F., Garcia-Alfaro J. (eds) Data and Applications Security and Privacy XXVI. DBSec 2012. Lecture Notes in Computer Science, vol 7371. Springer, Berlin, Heidelberg http://www.cs.ucr.edu/~ravi/Papers/DBConf/secret_sharing.pdf ) Komargodski I., Naor M., Yogev E. (2016) How to Share a Secret, Infinitely. In: Hirt M., Smith A. (eds) Theory of Cryptography. TCC 2016. Lecture Notes in Computer Science, vol 9986. Springer, Berlin, Heidelberg https://eprint.iacr.org/2016/194.pdf Coron JS., Prouff E., Roche T. (2013) On the Use of Shamir\u2019s Secret Sharing against Side-Channel Analysis. In: Mangard S. (eds) Smart Card Research and Advanced Applications. CARDIS 2012. Lecture Notes in Computer Science, vol 7771. Springer, Berlin, Heidelberg https://www.ssi.gouv.fr/uploads/IMG/pdf/aesshamir_Coron_Prouff_Roche.pdf Blakley, G.R. (1979). \u201cSafeguarding Cryptographic Keys\u201d. Managing Requirements Knowledge, International Workshop on (AFIPS). 48: 313\u2013317. doi:10.1109-/AFIPS.1979.98. https://pdfs.semanticscholar.org/32d2/1ccc21a807627fcb21ea829d1acdab23be12.pdf Feldman, Paul (1987) \u201cA practical scheme for non-interactive Verifiable Secret Sharing\u201d Proceedings of the 28th Annual Symposium on Foundations of Computer Science https://www.cs.umd.edu/~gasarch/TOPICS/secretsharing/feldmanVSS.pdf Harn, e, Changlu L (2009). \u201cDetection and identification of cheaters in (t, n) secret sharing scheme\u201d Des. Codes Cryptography 52, 1 (July 2009), 15-24. DOI=10.1007/s10623-008-9265-8 http://dx.doi.org/10.1007/s10623-008-9265-8 Schoenmakers, Berry (1999) \u201cA Simple Publicly Verifiable Secret Sharing Scheme and its Application to Electronic Voting\u201d Advances in Cryptology-CRYPTO\u201999, volume 1666 of Lecture Notes in Computer Science, pages 148-164, Berlin, 1999. Springer-Verlag. https://www.win.tue.nl/~berry/papers/crypto99.pdf Rusnak, P, et. al (2018) \u201cSLIP-0039 : Shamir\u2019s Secret-Sharing for Mnemonic Codes\u201d Satoshi Labs Github. https://github.com/satoshilabs/slips/blob/master/slip-0039.md Stack Exchange (2016) \u201cWhy is Shamir Secret Sharing not secure against active adversaries out-of-the-box?\u201d Stack Exchange https://crypto.stackexchange.com/questions/41994/why-is-shamir-secret-sharing-not-secure-against-active-adversaries-out-of-the-bo","title":"References"},{"location":"rwot8/topics-and-advance-readings/ssi20XX-model-use-case-def/","text":"#SSI20XX - Model Use-Case Definitions and Implementation Commitment for higher Solution Interoperability and Robust Specifications. Preface: Contribution by Martin Riedel / Identity.com as a Topic paper for the Rebooting of the Web-of-Trust workshop held in Barcelona, Spain from March 1st to 3rd. Summary \"The SSI community has developed several products around existing (draft) standards, like Decentralized Identifiers (DIDs) or Verifiable Credentials; however, there have been few interoperable implementations around SSI across different providers or companies even though there is often a high-level consensus within the community groups. This paper proposes that a vision statement (for example #SSI20XX) containing a very limited set of highly specified use-cases and UX descriptions will allow solution providers to commit to a common implementation goal that can greatly benefit solution interoperability and provide valuable feedback to existing specifications in the field of SSI.\" Introduction The genesis block of Bitcoin just celebrated its 10 year anniversary on January, 3rd 2019, and since the genie of that new technology was let out of the bottle, the community has seen tremendous advancement in the adoption and the development around Distributed Ledger Technology (DLT). The application of blockchain in the identity and access management (IAM) space quickly became apparent, especially since the existing centralized solutions were prone to hacks or implemented data management processes that allowed for easy identity fraud (e.g. social hacking). Since then, numerous companies or organizations have developed solutions that utilize a DLT in some way or another to anchor identity information in a decentralized way. Their products and implementations have proven that the technology could practically be applied. The introduction of Decentralized Identifiers (DIDs) was a significant milestone because it started to standardize the foundation layer around an interoperable self-sovereign identity ecosystem. Solution providers have greatly adopted the utilization of DIDs within their existing products, and as a foundational technology, it does not come with too many dependent technologies that need to be adopted due to a transitive relation. However, within recent years, the community has built further upon that foundation and divergent patterns are starting to arise again. Within an emerging field, that is generally a beneficial trend because it allows for new innovation to arise quickly. The exploratory spikes become problematic as soon as new designs or specification are based on implementations that a whole part of the community has not developed consensus over yet. For example, it is very hard to standardize on a Proofing Request protocol if there is no high-level agreement over the underlying communication elements. Solution providers within the decentralized identity space should strive for a high degree of interoperability in order to demonstrate the anti-monopoly patterns that arise from the use of that technology. Therefore this paper proposes that a vision statement (for example #SSI20XX) containing a very limited set of highly specified use-cases and UX descriptions will allow solution providers to commit to a common implementation goal, which will greatly benefit solution interoperability and provide valuable feedback to existing specifications in the field of SSI. Status Quo Most solution providers fill in the \u201cgaps\u201d that arise during the implementation of existing SSI specification on their own. Generally, these missing pieces would provide valuable input back to the original standard bodies, but often, the thought process and design around the custom implementation will not actively be shared externally. Usually, this is not due to any ill intentions by the implementer, but rather due to the fact the changes are not deemed important enough, or, are highly linked to a dependent technology of the custom implementation that does not want to be shared publicly. An aligned vision statement around solving a predefined use-case in an interoperable way would allow solutions providers to fill in the \u201cmissing\u201d pieces of implementation and specification in a collaborative way. Requirements for the (Implementation) Vision paper: Solution providers within the SSI Community need to estimate and prioritize implementation demand that would arise of executing a common SSI vision use-case in competition with other internal development requirements. Therefore, the SSI vision paper should be a very detailed description of the use-case (or use-cases) that are planned to be achieved. This should include functional diagrams (e.g. sequence flows, UI/UX designs, data formats, and technology stacks) that allow for a better estimation around the expected complexity that may arise. Furthermore, the described use-cases should not have a strong link to any specific functional domain, in order to not discourage providers that have business interests in other fields. Or, reversely, no single contributor should benefit in a disproportionate way. Most certainly this would lead towards asynchronous support of the project. Example Flow: In order to initiate the discussion about well suited universal use-cases that can potentially be part of that implementation vision paper, an exemplary UX Flow for an Appointment and Access Management is attached. Challenges There is an implicit trade-off when deciding on the specificity of a vision paper. On the one hand, it needs to be specific enough for companies to be able to commit towards it and provide an implementation within a predefined timeline. On the other hand, it should not take over the role of a priori specification, where low level technical and functional problems need to be solved. Readers need to be able to understand what the \u201cdestination\u201d looks like, rather than knowing the path that leads there. Outlook An Interoperability Project has been initialized within the DIF (Decentralized Identity Foundation) that aims at executing on that plan and designing the use-case scenario(s) in detail so that several implementation partners are able to commit towards it.","title":"SSI20XX - Model Use-Case Definitions and Implementation Commitment for higher Solution Interoperability and Robust Specifications."},{"location":"rwot8/topics-and-advance-readings/ssi20XX-model-use-case-def/#ssi20xx-model-use-case-definitions-and-implementation-commitment-for-higher-solution-interoperability-and-robust-specifications","text":"Preface: Contribution by Martin Riedel / Identity.com as a Topic paper for the Rebooting of the Web-of-Trust workshop held in Barcelona, Spain from March 1st to 3rd.","title":"#SSI20XX - Model Use-Case Definitions and Implementation Commitment for higher Solution Interoperability and Robust Specifications."},{"location":"rwot8/topics-and-advance-readings/ssi20XX-model-use-case-def/#summary","text":"\"The SSI community has developed several products around existing (draft) standards, like Decentralized Identifiers (DIDs) or Verifiable Credentials; however, there have been few interoperable implementations around SSI across different providers or companies even though there is often a high-level consensus within the community groups. This paper proposes that a vision statement (for example #SSI20XX) containing a very limited set of highly specified use-cases and UX descriptions will allow solution providers to commit to a common implementation goal that can greatly benefit solution interoperability and provide valuable feedback to existing specifications in the field of SSI.\"","title":"Summary"},{"location":"rwot8/topics-and-advance-readings/ssi20XX-model-use-case-def/#introduction","text":"The genesis block of Bitcoin just celebrated its 10 year anniversary on January, 3rd 2019, and since the genie of that new technology was let out of the bottle, the community has seen tremendous advancement in the adoption and the development around Distributed Ledger Technology (DLT). The application of blockchain in the identity and access management (IAM) space quickly became apparent, especially since the existing centralized solutions were prone to hacks or implemented data management processes that allowed for easy identity fraud (e.g. social hacking). Since then, numerous companies or organizations have developed solutions that utilize a DLT in some way or another to anchor identity information in a decentralized way. Their products and implementations have proven that the technology could practically be applied. The introduction of Decentralized Identifiers (DIDs) was a significant milestone because it started to standardize the foundation layer around an interoperable self-sovereign identity ecosystem. Solution providers have greatly adopted the utilization of DIDs within their existing products, and as a foundational technology, it does not come with too many dependent technologies that need to be adopted due to a transitive relation. However, within recent years, the community has built further upon that foundation and divergent patterns are starting to arise again. Within an emerging field, that is generally a beneficial trend because it allows for new innovation to arise quickly. The exploratory spikes become problematic as soon as new designs or specification are based on implementations that a whole part of the community has not developed consensus over yet. For example, it is very hard to standardize on a Proofing Request protocol if there is no high-level agreement over the underlying communication elements. Solution providers within the decentralized identity space should strive for a high degree of interoperability in order to demonstrate the anti-monopoly patterns that arise from the use of that technology. Therefore this paper proposes that a vision statement (for example #SSI20XX) containing a very limited set of highly specified use-cases and UX descriptions will allow solution providers to commit to a common implementation goal, which will greatly benefit solution interoperability and provide valuable feedback to existing specifications in the field of SSI.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/ssi20XX-model-use-case-def/#status-quo","text":"Most solution providers fill in the \u201cgaps\u201d that arise during the implementation of existing SSI specification on their own. Generally, these missing pieces would provide valuable input back to the original standard bodies, but often, the thought process and design around the custom implementation will not actively be shared externally. Usually, this is not due to any ill intentions by the implementer, but rather due to the fact the changes are not deemed important enough, or, are highly linked to a dependent technology of the custom implementation that does not want to be shared publicly. An aligned vision statement around solving a predefined use-case in an interoperable way would allow solutions providers to fill in the \u201cmissing\u201d pieces of implementation and specification in a collaborative way.","title":"Status Quo"},{"location":"rwot8/topics-and-advance-readings/ssi20XX-model-use-case-def/#requirements-for-the-implementation-vision-paper","text":"Solution providers within the SSI Community need to estimate and prioritize implementation demand that would arise of executing a common SSI vision use-case in competition with other internal development requirements. Therefore, the SSI vision paper should be a very detailed description of the use-case (or use-cases) that are planned to be achieved. This should include functional diagrams (e.g. sequence flows, UI/UX designs, data formats, and technology stacks) that allow for a better estimation around the expected complexity that may arise. Furthermore, the described use-cases should not have a strong link to any specific functional domain, in order to not discourage providers that have business interests in other fields. Or, reversely, no single contributor should benefit in a disproportionate way. Most certainly this would lead towards asynchronous support of the project.","title":"Requirements for the (Implementation) Vision paper:"},{"location":"rwot8/topics-and-advance-readings/ssi20XX-model-use-case-def/#example-flow","text":"In order to initiate the discussion about well suited universal use-cases that can potentially be part of that implementation vision paper, an exemplary UX Flow for an Appointment and Access Management is attached.","title":"Example Flow:"},{"location":"rwot8/topics-and-advance-readings/ssi20XX-model-use-case-def/#challenges","text":"There is an implicit trade-off when deciding on the specificity of a vision paper. On the one hand, it needs to be specific enough for companies to be able to commit towards it and provide an implementation within a predefined timeline. On the other hand, it should not take over the role of a priori specification, where low level technical and functional problems need to be solved. Readers need to be able to understand what the \u201cdestination\u201d looks like, rather than knowing the path that leads there.","title":"Challenges"},{"location":"rwot8/topics-and-advance-readings/ssi20XX-model-use-case-def/#outlook","text":"An Interoperability Project has been initialized within the DIF (Decentralized Identity Foundation) that aims at executing on that plan and designing the use-case scenario(s) in detail so that several implementation partners are able to commit towards it.","title":"Outlook"},{"location":"rwot8/topics-and-advance-readings/supporting-btcr/","text":"Querying Bitcoin blockchain for BTCR support Kulpreet Singh, kulpreet@opdup.com Abstract I describe the progress made during the BTCR hackathon towards providing a community service for querying the bitcoin blockchain as a means to support clients building BTCR DID resolvers using the BTCR DID method. BTCR DID method enables bitcoin users to generate DIDs corresponding to their bitcoin key pairs. By generating DIDs using bitcoin key pairs we can leverage the high level of guarantees provided by the bitcoin blockchain against manipulation of published data. I provide a brief overview of the BTCR DID resolution method, point to the problem of querying third party services and how we provided our own community service as part of a hackathon in 2018. BTCR Overview Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, \"self-sovereign\" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. The BTCR DID method specifies how the transaction output identified above can be used to generate a DID Document matching the key pair used to sign that transaction. BTCR DID method uses a reference to a transaction provided by a user who wants to generate a BTCR DID. This reference to a transaction includes the type of the network (testnet or mainnet), the height of the block that contains the transaction, the index of the transaction in the block and finally the index of the output. See Extended TxRef for details on how the reference to a transaction can be encoded and easily exchanged between humans. The BTCR DID method requires that we check if the transaction output specified has been spent or not. Depending on the spend status the DID method generates a DID pointing to the public key found in the transaction output or follows the spends to see if a future transaction output can be be found that has not been spent - this enables key rotation in the BTCR DID method. The challenge facing BTCR DID resolution is the ability to query the bitcoin blockchain for any transactions referencing a given address. In our case, it is the address in the P2PKH script referenced by the user transaction. The bitcoin reference client, bitcoind, does not maintain an address history and therefore as a community the BTCR developers were dependent on third party block explorers. Using Block Explorers Third party block explorers run their own databases indexing transactions by addresses referenced in those transactions. This allowed the BTCR developer community to quickly develop some MVPs and demo them to the larger community. However, third party block explorers are not a long term solution because of concerns about privacy leaks and more immediately the rate limits imposed by them. Community Run Query Endpoint To get past the problems of of using third part block explorers and also to validate potential implementation solutions we deployed a BTCD instance that provides an endpoint to query all transactions referencing and address. We also provided a very simple API to query all transactions referencing an address. The service is built in Go and invokes the BTCD API endpoints. It accept TxRefs as inputs and responds with a JSON array of transactions referencing the address. For example, the URL https://btcr-service.opdup.com/txref/txtest1:xz35-jzv2-qqs2-9wjt/resolve?spendsOnly=false queries the service for TxRef xz35-jzv2-qqs2-9wjt on the testnet network and responds with all transactions referencing that address. With access to the list of transactions, BTCR developers can continue to experiment and further develop the BTCR DID method specification. The source code for the service is available on the GitHub repo https://github.com/kulpreet/btcr-service . The README on the repo describes the various API endpoints and how to deploy the service. Current and Future Work As a proof of concept we are using BTCR Service to build a BTCR Android Wallet. The BTCR Android Wallet includes a Java client library for the BTCR Service API endpoints. For now, the wallet accepts a TxRef and shows the public key that can be used to generate an implicit DID. Our plan is to support DID generation using key pairs managed by the Android wallet. The wallet source code is available at https://github.com/opdup/btcr-android-wallet . As the BTCR DID method spec evolves, we also plan to continue adding API endpoints that will be helpful to the community.","title":"Querying Bitcoin blockchain for BTCR support"},{"location":"rwot8/topics-and-advance-readings/supporting-btcr/#querying-bitcoin-blockchain-for-btcr-support","text":"Kulpreet Singh, kulpreet@opdup.com","title":"Querying Bitcoin blockchain for BTCR support"},{"location":"rwot8/topics-and-advance-readings/supporting-btcr/#abstract","text":"I describe the progress made during the BTCR hackathon towards providing a community service for querying the bitcoin blockchain as a means to support clients building BTCR DID resolvers using the BTCR DID method. BTCR DID method enables bitcoin users to generate DIDs corresponding to their bitcoin key pairs. By generating DIDs using bitcoin key pairs we can leverage the high level of guarantees provided by the bitcoin blockchain against manipulation of published data. I provide a brief overview of the BTCR DID resolution method, point to the problem of querying third party services and how we provided our own community service as part of a hackathon in 2018.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/supporting-btcr/#btcr-overview","text":"Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, \"self-sovereign\" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. The BTCR DID method specifies how the transaction output identified above can be used to generate a DID Document matching the key pair used to sign that transaction. BTCR DID method uses a reference to a transaction provided by a user who wants to generate a BTCR DID. This reference to a transaction includes the type of the network (testnet or mainnet), the height of the block that contains the transaction, the index of the transaction in the block and finally the index of the output. See Extended TxRef for details on how the reference to a transaction can be encoded and easily exchanged between humans. The BTCR DID method requires that we check if the transaction output specified has been spent or not. Depending on the spend status the DID method generates a DID pointing to the public key found in the transaction output or follows the spends to see if a future transaction output can be be found that has not been spent - this enables key rotation in the BTCR DID method. The challenge facing BTCR DID resolution is the ability to query the bitcoin blockchain for any transactions referencing a given address. In our case, it is the address in the P2PKH script referenced by the user transaction. The bitcoin reference client, bitcoind, does not maintain an address history and therefore as a community the BTCR developers were dependent on third party block explorers.","title":"BTCR Overview"},{"location":"rwot8/topics-and-advance-readings/supporting-btcr/#using-block-explorers","text":"Third party block explorers run their own databases indexing transactions by addresses referenced in those transactions. This allowed the BTCR developer community to quickly develop some MVPs and demo them to the larger community. However, third party block explorers are not a long term solution because of concerns about privacy leaks and more immediately the rate limits imposed by them.","title":"Using Block Explorers"},{"location":"rwot8/topics-and-advance-readings/supporting-btcr/#community-run-query-endpoint","text":"To get past the problems of of using third part block explorers and also to validate potential implementation solutions we deployed a BTCD instance that provides an endpoint to query all transactions referencing and address. We also provided a very simple API to query all transactions referencing an address. The service is built in Go and invokes the BTCD API endpoints. It accept TxRefs as inputs and responds with a JSON array of transactions referencing the address. For example, the URL https://btcr-service.opdup.com/txref/txtest1:xz35-jzv2-qqs2-9wjt/resolve?spendsOnly=false queries the service for TxRef xz35-jzv2-qqs2-9wjt on the testnet network and responds with all transactions referencing that address. With access to the list of transactions, BTCR developers can continue to experiment and further develop the BTCR DID method specification. The source code for the service is available on the GitHub repo https://github.com/kulpreet/btcr-service . The README on the repo describes the various API endpoints and how to deploy the service.","title":"Community Run Query Endpoint"},{"location":"rwot8/topics-and-advance-readings/supporting-btcr/#current-and-future-work","text":"As a proof of concept we are using BTCR Service to build a BTCR Android Wallet. The BTCR Android Wallet includes a Java client library for the BTCR Service API endpoints. For now, the wallet accepts a TxRef and shows the public key that can be used to generate an implicit DID. Our plan is to support DID generation using key pairs managed by the Android wallet. The wallet source code is available at https://github.com/opdup/btcr-android-wallet . As the BTCR DID method spec evolves, we also plan to continue adding API endpoints that will be helpful to the community.","title":"Current and Future Work"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/","text":"Multi-Factor Attribute Trust By Will Abramson Developing flexible mechanisms for judging the trust of an attribute presentation is going to be essential to driving the network affects needed for widespread adoption of Verifiable Credential based identity networks. The question that a verifier needs to answer is why should I trust these attribute presenations? I know Sovrin have the concept of Governance Frameworks , which define the rules for actors within a certain identity ecosystem. While I am not aware of the technical implementation of these frameworks, defining trust from a legal, governance point of view makes sense. However, I wonder about the ability of these frameworks ability to provide trust when credentials are used across ecosystems. One of the goals of Verifiable Credentials is to empower individuals with their own set of credentials which they can present to any entity they choose to. Enabling them to prove aspects of their identity. For this to work a credential needs to be accepted in as many contexts and by as many different verifiers as possible. Within a well defined credential ecosystem it is easy to envision this working through shared knowledge of the trusted issuers' public decentralised identifiers, but how does a verifier from outside this ecosystem without this knowledge trust attributes from these credentials? Defining robust methods for gauging the trustworthiness of an attribute presentation will be essential for ensuring these credentials can be used as extensively as we would like. Throughout the rest of this article, I outline potential mechanisms for calculating this trust. Mechanisms for Judging Trust Open Access to an Ecosystems Trusted Issuer DIDs This option would seem to be the most obvious. Suppose within an identity network an ecosystem forms for Healthcare in the UK. As part of that formation, they define a framework outlining the types of credentials needed within the ecosystem and the public DIDs trusted to issue them. These DIDs are shared with all members of the ecosystem. For example, all hospitals are trusted issuers of doctor employment credentials and know all the other trusted DIDs able to issue these employment credentials. This works within an ecosystem where dissemination of trusted DIDs is easy and could be built into the services used by this ecosystem, but how does a verifier outside this ecosystem check a doctor employment credential was issued by a DID trusted within this health framework? How can they do it in real time? How can they do it without compromising the privacy of the entity presenting the claim? One option might be for that ecosystem to host an endpoint allowing queries to check the issuing capabilities of a specific DID. This has its problems though. Who would be in charge of hosting the endpoint? How can you be sure the endpoint has not been compromised? What happens if the endpoint is down? Is this a privacy concern - if every time a credential is verified, the verifier has to hit someone's endpoint? A solution to this could be to store these registries on the ledger much the same as revocation registries. These use dynamic accumulators to combine the hashes of all trusted DID's for a certain credential definition. This would provide an open, highly available registry for verifiers to query without the same privacy concerns as an endpoint. The only problem I can see is which entities are capable of dynamically updating the registry. This could be managed through threshold signatures with key sharing defined as part of the Governance Framework. Public Credentials/Attestations A different approach could be to issue credentials on chain to the trusted issuers in a certain ecosystem. Then when verifying a credential presentation by resolving the DID on chain the verifier would also check for a credential to support the DID as a trusted issuer. The obvious problem with this is who issued that credential and how can you trust them. Trust could be improved with public credentials issued from top to bottom of a governance framework. These credentials are basically attestations to the trust of a public DID. If all members of an ecosystem attest to the trust of all other members on chain creating a verifiable web of trust for that ecosystem. This would make it much harder for a bad actor to imitate a trusted issuer. On reading some of the articles for this RWoT I came across the idea of DID Namespace records . An interesting read, suggesting creating a web of trust through name spaced DIDs. Contextual Evidence Providing context could be another mechanism for gauging the trust of an attribute. For example, a verifier receives a credential presentation of an attribute issued by an issuer they have low trust in. The verifier could ask for further corroborating proof of that attribute. The challenge here would be to enable a prover to provide this proof without giving away any more information about themselves than necessary, following the data minimisation principle. An example could be when attesting to work experience in a foreign country, the verifier requests proof that you indeed were in that country for the stated amount of time which the prover can present through proofs of digital passport stamps from that country. Quantity of Attesting Entities The number of different issuers a prover can show attest to the same attribute they are presenting the greater the trust a verifier can place in that attribute. A simple example of this might be attendance at an event. Attestation from 10 different low trust DID's could be as trustworthy as a single attestation from a trusted DID. Leveraging Time Time is an interesting one. I think there are multiple ways to include time into a trustworthiness calculation. The most obvious is how long has the issuing DID been registered on the public ledger. However, I think particularly in low trust environments, if there was some way to leverage the length of time the prover has had a private connection with an issuer or an entity attesting to the credibility of a claim in such a way that the prover could combine this into the presentation then time could be a really useful measure of trust. I am not sure exactly how this would work, but it mirrors how we act in the physical world. The longer I know someone the more confident I feel I can attest to their trustworthiness. I believe there must be some way to translate this into the anonymous credential world. Closing Thoughts For a verifier, making the decision to trust or reject the attributes presented to them will be critical to their organisation. At the same time, the amount of trust required and the initial trust conditions can vary widely depending on the context of the identity interaction. Mechanisms to allow verifiers to make real-time decisions on whether or not to trust the presentation of attributes presented to them will be important to the development of Self-Sovereign Identity and the network effects that will make it the default approach for digital identity. This is an initial attempt to outline ways to judge the trustworthiness of an attestation. I believe a combination of these methods could provide multi-factor trust needed, particularly for individuals from areas of the world without trusted entities capable of bootstrapping a decentralised trusted credential network. Finally, these are some questions I am thinking about: Are there any other approaches that could be used to assess the trust of an attribute presentation? How might these methods be implemented & combined within an agent in such a way to provide a trust score for an attribute? How can the initial trust conditions of a verifier be included in these methods to provide an interoperable solution any verifier could use? How might we combine trust assumptions gained from context, quantity and time be used to provide high trust credentials to people from areas of the world with low trust organisations? Next Steps I would like to discuss ongoing work within the community to solve this trust problem. How do people see it working? Other than hard coding in trusted DID issuers how can a developer build applications that verify credential presentations meet the trust requirements of the application in as flexible a way as possible? This discussion will be closely tied to the reading posted by Matt Stone and Dan Burnett, who ask the question How do we bootstrap the web of trust in Verifiable Claims .","title":"Multi-Factor Attribute Trust"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#multi-factor-attribute-trust","text":"","title":"Multi-Factor Attribute Trust"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#by-will-abramson","text":"Developing flexible mechanisms for judging the trust of an attribute presentation is going to be essential to driving the network affects needed for widespread adoption of Verifiable Credential based identity networks. The question that a verifier needs to answer is why should I trust these attribute presenations? I know Sovrin have the concept of Governance Frameworks , which define the rules for actors within a certain identity ecosystem. While I am not aware of the technical implementation of these frameworks, defining trust from a legal, governance point of view makes sense. However, I wonder about the ability of these frameworks ability to provide trust when credentials are used across ecosystems. One of the goals of Verifiable Credentials is to empower individuals with their own set of credentials which they can present to any entity they choose to. Enabling them to prove aspects of their identity. For this to work a credential needs to be accepted in as many contexts and by as many different verifiers as possible. Within a well defined credential ecosystem it is easy to envision this working through shared knowledge of the trusted issuers' public decentralised identifiers, but how does a verifier from outside this ecosystem without this knowledge trust attributes from these credentials? Defining robust methods for gauging the trustworthiness of an attribute presentation will be essential for ensuring these credentials can be used as extensively as we would like. Throughout the rest of this article, I outline potential mechanisms for calculating this trust.","title":"By Will Abramson"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#mechanisms-for-judging-trust","text":"","title":"Mechanisms for Judging Trust"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#open-access-to-an-ecosystems-trusted-issuer-dids","text":"This option would seem to be the most obvious. Suppose within an identity network an ecosystem forms for Healthcare in the UK. As part of that formation, they define a framework outlining the types of credentials needed within the ecosystem and the public DIDs trusted to issue them. These DIDs are shared with all members of the ecosystem. For example, all hospitals are trusted issuers of doctor employment credentials and know all the other trusted DIDs able to issue these employment credentials. This works within an ecosystem where dissemination of trusted DIDs is easy and could be built into the services used by this ecosystem, but how does a verifier outside this ecosystem check a doctor employment credential was issued by a DID trusted within this health framework? How can they do it in real time? How can they do it without compromising the privacy of the entity presenting the claim? One option might be for that ecosystem to host an endpoint allowing queries to check the issuing capabilities of a specific DID. This has its problems though. Who would be in charge of hosting the endpoint? How can you be sure the endpoint has not been compromised? What happens if the endpoint is down? Is this a privacy concern - if every time a credential is verified, the verifier has to hit someone's endpoint? A solution to this could be to store these registries on the ledger much the same as revocation registries. These use dynamic accumulators to combine the hashes of all trusted DID's for a certain credential definition. This would provide an open, highly available registry for verifiers to query without the same privacy concerns as an endpoint. The only problem I can see is which entities are capable of dynamically updating the registry. This could be managed through threshold signatures with key sharing defined as part of the Governance Framework.","title":"Open Access to an Ecosystems Trusted Issuer DIDs"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#public-credentialsattestations","text":"A different approach could be to issue credentials on chain to the trusted issuers in a certain ecosystem. Then when verifying a credential presentation by resolving the DID on chain the verifier would also check for a credential to support the DID as a trusted issuer. The obvious problem with this is who issued that credential and how can you trust them. Trust could be improved with public credentials issued from top to bottom of a governance framework. These credentials are basically attestations to the trust of a public DID. If all members of an ecosystem attest to the trust of all other members on chain creating a verifiable web of trust for that ecosystem. This would make it much harder for a bad actor to imitate a trusted issuer. On reading some of the articles for this RWoT I came across the idea of DID Namespace records . An interesting read, suggesting creating a web of trust through name spaced DIDs.","title":"Public Credentials/Attestations"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#contextual-evidence","text":"Providing context could be another mechanism for gauging the trust of an attribute. For example, a verifier receives a credential presentation of an attribute issued by an issuer they have low trust in. The verifier could ask for further corroborating proof of that attribute. The challenge here would be to enable a prover to provide this proof without giving away any more information about themselves than necessary, following the data minimisation principle. An example could be when attesting to work experience in a foreign country, the verifier requests proof that you indeed were in that country for the stated amount of time which the prover can present through proofs of digital passport stamps from that country.","title":"Contextual Evidence"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#quantity-of-attesting-entities","text":"The number of different issuers a prover can show attest to the same attribute they are presenting the greater the trust a verifier can place in that attribute. A simple example of this might be attendance at an event. Attestation from 10 different low trust DID's could be as trustworthy as a single attestation from a trusted DID.","title":"Quantity of Attesting Entities"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#leveraging-time","text":"Time is an interesting one. I think there are multiple ways to include time into a trustworthiness calculation. The most obvious is how long has the issuing DID been registered on the public ledger. However, I think particularly in low trust environments, if there was some way to leverage the length of time the prover has had a private connection with an issuer or an entity attesting to the credibility of a claim in such a way that the prover could combine this into the presentation then time could be a really useful measure of trust. I am not sure exactly how this would work, but it mirrors how we act in the physical world. The longer I know someone the more confident I feel I can attest to their trustworthiness. I believe there must be some way to translate this into the anonymous credential world.","title":"Leveraging Time"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#closing-thoughts","text":"For a verifier, making the decision to trust or reject the attributes presented to them will be critical to their organisation. At the same time, the amount of trust required and the initial trust conditions can vary widely depending on the context of the identity interaction. Mechanisms to allow verifiers to make real-time decisions on whether or not to trust the presentation of attributes presented to them will be important to the development of Self-Sovereign Identity and the network effects that will make it the default approach for digital identity. This is an initial attempt to outline ways to judge the trustworthiness of an attestation. I believe a combination of these methods could provide multi-factor trust needed, particularly for individuals from areas of the world without trusted entities capable of bootstrapping a decentralised trusted credential network. Finally, these are some questions I am thinking about: Are there any other approaches that could be used to assess the trust of an attribute presentation? How might these methods be implemented & combined within an agent in such a way to provide a trust score for an attribute? How can the initial trust conditions of a verifier be included in these methods to provide an interoperable solution any verifier could use? How might we combine trust assumptions gained from context, quantity and time be used to provide high trust credentials to people from areas of the world with low trust organisations?","title":"Closing Thoughts"},{"location":"rwot8/topics-and-advance-readings/trusting-attributes/#next-steps","text":"I would like to discuss ongoing work within the community to solve this trust problem. How do people see it working? Other than hard coding in trusted DID issuers how can a developer build applications that verify credential presentations meet the trust requirements of the application in as flexible a way as possible? This discussion will be closely tied to the reading posted by Matt Stone and Dan Burnett, who ask the question How do we bootstrap the web of trust in Verifiable Claims .","title":"Next Steps"},{"location":"rwot8/topics-and-advance-readings/united-humans-critiqal-questions/","text":"The United Humans - can it really work? Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: short_topic_paper_to_open_discussion, critique of The United Humans project This paper is an attempt to elicit critique, and start discussion to explore weak points of the United Humans idea, project [1] and to encourage people to join its boostraping effort [2]. Existential Do we need organization of the United Humans? Will it duplicate the the United Nations? Are the United Humans' services going to be net positive for humanity? There might be unforeseen negative consequences caused by emergence of Kudos and the United Humans. Is risk of doing it, worth it? Can we not do the United Humans or will it emerge anyway, given the existence of Kudos revenue model? Governance How it will be governed? Will it be representative of all people on the Earth? What power the organization of the United Humans will have? Will it have too much power? Will the United Humans become corrupt and/or ineffective eventually (see issues with UN, FIFA etc)? Technichal / SURLHI Arbitration (web of trust analysis). Can SURLHI arbitration (analysis of SURLHI's web of trust) be successful? Will SURLHI (Statements of Unique Representation of Living Human Individual) arbitration be reliable? Will SURLHI claims be acknowledged as valid/honest correctly? How many successful Sybil identities (identities with fake SURLHI acknowledged as valid) will be out there? Do we need human arbiters? Will there be enough arbiters? Funding How The UH organization is going to be funded in the long term? Is revenue model based on Kudos issuance and lottery sustainable? What will be the price of Kudos at different stages of the UH development? Is it going to be high enough to incentivise the UH jump-start and continuous development? Products Are The UH services going to be any better than services already provided by the market (Facebook, Google)? Why market cannot provide these services? Government (not-for-profit) provided products tend to be subpar comparing to the market provided products, why The UH services be any better? References [1] The United Humans paper - https://github.com/WebOfTrustInfo/rwot7-toronto/blob/master/topics-and-advance-readings/united-humans.md) [2] The United Humans investment deck - https://drive.google.com/file/d/1Rk32GV8BoEPQPgZ16AUNSj6zI04Kgxyo/view","title":"The United Humans - can it really work?"},{"location":"rwot8/topics-and-advance-readings/united-humans-critiqal-questions/#the-united-humans-can-it-really-work","text":"Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: short_topic_paper_to_open_discussion, critique of The United Humans project This paper is an attempt to elicit critique, and start discussion to explore weak points of the United Humans idea, project [1] and to encourage people to join its boostraping effort [2]. Existential Do we need organization of the United Humans? Will it duplicate the the United Nations? Are the United Humans' services going to be net positive for humanity? There might be unforeseen negative consequences caused by emergence of Kudos and the United Humans. Is risk of doing it, worth it? Can we not do the United Humans or will it emerge anyway, given the existence of Kudos revenue model? Governance How it will be governed? Will it be representative of all people on the Earth? What power the organization of the United Humans will have? Will it have too much power? Will the United Humans become corrupt and/or ineffective eventually (see issues with UN, FIFA etc)? Technichal / SURLHI Arbitration (web of trust analysis). Can SURLHI arbitration (analysis of SURLHI's web of trust) be successful? Will SURLHI (Statements of Unique Representation of Living Human Individual) arbitration be reliable? Will SURLHI claims be acknowledged as valid/honest correctly? How many successful Sybil identities (identities with fake SURLHI acknowledged as valid) will be out there? Do we need human arbiters? Will there be enough arbiters? Funding How The UH organization is going to be funded in the long term? Is revenue model based on Kudos issuance and lottery sustainable? What will be the price of Kudos at different stages of the UH development? Is it going to be high enough to incentivise the UH jump-start and continuous development? Products Are The UH services going to be any better than services already provided by the market (Facebook, Google)? Why market cannot provide these services? Government (not-for-profit) provided products tend to be subpar comparing to the market provided products, why The UH services be any better?","title":"The United Humans - can it really work?"},{"location":"rwot8/topics-and-advance-readings/united-humans-critiqal-questions/#references","text":"[1] The United Humans paper - https://github.com/WebOfTrustInfo/rwot7-toronto/blob/master/topics-and-advance-readings/united-humans.md) [2] The United Humans investment deck - https://drive.google.com/file/d/1Rk32GV8BoEPQPgZ16AUNSj6zI04Kgxyo/view","title":"References"},{"location":"rwot8/topics-and-advance-readings/universal-id-framework/","text":"Universal ID Framework By Dr. Shigeya Suzuki, Keio University Submitted to Rebooting Web of Trust 8 January 31, 2019, Tokyo Japan Problem Suppose we configure an access control list for a server software using some certificate-based client authentication over a secure connection. We need to configure a list of identity references such as the certificate's subjects or issuers. To describe the identity reference with PKI, we use X.500 Distinguished Name style strings. Suppose we want to add support for DID in a client authentication server code like the above. We need to modify the access control code of the server to add support for DID resolution. If we have a DID resolver library to implement this, it is not a difficult task. However, if we want broader deployment of DID, we need to develop a better way to add support for DID to help application developers. If we look at how to treat identity reference in an application, it is natural to develop a standard abstract data type which may store a different kind of identity references. Key Idea One of the ways to support references for multiple ID scheme everywhere is developing a general Universal ID Reference scheme, which covers not only the references to DID but also Distinguish Name-based scheme or possibly others. It is possible to develop a Universal ID framework consists of standard documents, open source libraries, and other necessary components to help using Universal ID-based system. Possible outcomes If the Universal ID based framework is ready and the traditional single-framework component is replaced with mutiple-framework capable component, it will be able to seamlessly support both DID and PKI based trust system. This change effectively enables the use of DIDs in various places. Relationship with URI Designing Universal ID reference syntax based on URI is natural. However, restrict the syntax of the Universal ID within the URI scheme or relaxing the syntax require detailed use case analysis.","title":"Universal id framework"},{"location":"rwot8/topics-and-advance-readings/universal-id-framework/#universal-id-framework","text":"By Dr. Shigeya Suzuki, Keio University Submitted to Rebooting Web of Trust 8 January 31, 2019, Tokyo Japan","title":"Universal ID Framework"},{"location":"rwot8/topics-and-advance-readings/universal-id-framework/#problem","text":"Suppose we configure an access control list for a server software using some certificate-based client authentication over a secure connection. We need to configure a list of identity references such as the certificate's subjects or issuers. To describe the identity reference with PKI, we use X.500 Distinguished Name style strings. Suppose we want to add support for DID in a client authentication server code like the above. We need to modify the access control code of the server to add support for DID resolution. If we have a DID resolver library to implement this, it is not a difficult task. However, if we want broader deployment of DID, we need to develop a better way to add support for DID to help application developers. If we look at how to treat identity reference in an application, it is natural to develop a standard abstract data type which may store a different kind of identity references.","title":"Problem"},{"location":"rwot8/topics-and-advance-readings/universal-id-framework/#key-idea","text":"One of the ways to support references for multiple ID scheme everywhere is developing a general Universal ID Reference scheme, which covers not only the references to DID but also Distinguish Name-based scheme or possibly others. It is possible to develop a Universal ID framework consists of standard documents, open source libraries, and other necessary components to help using Universal ID-based system.","title":"Key Idea"},{"location":"rwot8/topics-and-advance-readings/universal-id-framework/#possible-outcomes","text":"If the Universal ID based framework is ready and the traditional single-framework component is replaced with mutiple-framework capable component, it will be able to seamlessly support both DID and PKI based trust system. This change effectively enables the use of DIDs in various places.","title":"Possible outcomes"},{"location":"rwot8/topics-and-advance-readings/universal-id-framework/#relationship-with-uri","text":"Designing Universal ID reference syntax based on URI is natural. However, restrict the syntax of the Universal ID within the URI scheme or relaxing the syntax require detailed use case analysis.","title":"Relationship with URI"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/","text":"The universal ledger agent: a logical result of Rabobank's journey in blockchain based self-sovereign identity By David Lamers < david.lamers@rabobank.nl > - v1.2 Foreword Within Rabobank we are doing research on self-sovereign identity using blockchain technology since 2016. Within a wide range of partnerships, we explore together with partners the opportunities for users and businesses as well as the technical standards and solutions. An interesting keynote of our CEO about the importance of SSI can be found on Youtube . Currently we are working on supporting the W3C verifiable credentials (editor's draft January 2019) and DIDs , and have designed the Universal Ledger Agent. With this paper we intended to share our work and to open the discussion on the design and implementation. A demo of the current solution can be found on https://youtu.be/ZV52KpE_yaU Fundamentals In this section the fundamental concepts of our SSI solution are explained. Holder, issuer and verifier : We developed a mobile application (using Ionic) for the holder and NodeJS web applications for the issuer and verifier demo\u2019s. For the issuing process, the issuer sends a challenge request to the holder - asking for a DID for each credential so they can be revoked independently whilst maintaining privacy. The holder sends the DID\u2019s back in the form of self-issued verifiable credentials encapsulated in a verifiable presentation . The issuer then issues credentials on those DID\u2019s and sends a verifiable presentation back to the holder. The verifier uses the same process, but the challenge request has different content. So the holder sends a verifiable presentation with credentials retrieved from the issuer in the previous step. Decentralized (pure SSI) : Pure SSI, without any centralized dependencies and providing all functionalities, is probably only possible with blockchain. Blockchain mainly provides functionalities for timestamping, the trust registry and revoking credentials. In each solution being built we maintain the design principles for self-sovereign identity as explained by Christopher Allen . Trust registry/public claims : In order to rely on verifiable credentials, the verifier should be able to verify the signature from the issuer. We foresee a role for the government or a chamber of commerce to maintain these trust registries and maybe also provide public claims about those companies. JSON-LD (signatures) : To create semantic interoperability we use JSON-LD and schema.org definitions when data is exchanged. Signatures are created using the implementation from Digital Bazaar . Personal identity : We use iDIN to retrieve general information of someone\u2019s personal identity in the mobile app. iDIN is supported by all Dutch banks and provides a basic personal identity of the user. Templates : In the past we worked on JSON-LD templates for communication between holder and verifier/issuer. However, over time we wanted to exchange more data which made templates inefficient. Therefore we are now using challenge requests for the issuing and verify processes. The challenge requests contains information about which credentials the holder will receive, but also which credentials the holder must include in the response. This message can contain additional information, for instance the allowed type of verifiable credentials when multiple types are supported (see also universal ledger agent below). Collaboration environments : The fundamentals and use cases have been developed within collaboration environments Rabobank is part of, for example Techruption and the Dutch Blockchain Coalition . The development of SSI should be a common process to create a widely supported and interoperable solution. BIP32 : In the first designs we used Bitcoin's BIP32 to share derived public extended keys with issuers from which a public key was used as salt in hashes to have an high level of entropy. However, hashed personal data (with salt) still has to comply with GDPR regulations. To comply, we changed the design to not use hashes anymore but only use BIP32 to create DIDs. With the possibilities of key derivation, one has to maintain only one master key. Use cases within Rabobank The development of an SSI solution can provide opportunities for the Rabobank within for instance the following business lines. The added value for the business lines, our customers and employees is the reason we work on SSI. KYC : Currently, we have extensive know your customer (KYC) and customer due diligence (CDD) processes for retail and wholesale. With hundreds of possible data points these processes are based on, it's complex to retrieve the data from a (direct) verifiable source. SSI could provide directly verifiable data and as bank we have valuable data for the customer for third parties. Research within Rabobank has shown the added value of SSI within KYC. In the first proof of concept we retrieved data from the chamber of commerce to the identity app. Mortgage : The average mortgage flow requires a lot of time from the customer. Documents from a lot of resources are required and most of them are not verifiable. The application process takes long which can be improved with SSI. By supplying the data using the identity app we can immediately check and verify validity and origin of the data. HR and onboarding of employees : we want to empower employees with their own data. It should be possible to reuse certificates or assessments they achieved or did at Rabobank everywhere else. Therefore we do projects in order to save certificates, diplomas, trainings and employment credentials. This means also supporting open badges within W3C verifiable credentials. If we can receive data this way during onboarding, we can also drastically improve onboarding times. It also connects to our vision on the future of work and employee mobility. Universal Ledger Agent Within our collaborations we faced the problem that partners were using different standards and blockchains. We can also not deny that in the future, multiple solutions will mature and interoperability issues will arise. For users this will be a challenge and we want to avoid that they have to use multiple apps, logins or platforms. Therefore we designed together with one of our partners the Universal Ledger Agent. Plugins can be developed for each specific set of standards (and so blockchains). This section describes our efforts so far. Design The universal ledger agent (ULA) is a component that is implemented by the app and the verifier. The ULA makes it possible to retrieve credentials from issuers, independently which standards and blockchain they use. Also, a verifier can accept and verify credentials from multiple standards. To achieve this interoperability, the ULA uses plugins as shown below. We are currently working on the Ethereum (ERC-780 as well as W3C verifiable credentials) and Sovrin plugin. The ULA is recognizing the standards used in a credential in order to send it to the right plugin. The plugin takes care of all the logic for the specific standard. In the short future we want to encourage others to build plugins for the ULA. Therefore we are now considering to open source the ULA and our plugins. With open sourcing, we also hope to get more involvement from the community. Ledgers Ethereum : We use the public Ethereum blockchain as credential status registry to achieve pure SSI. This credential status registry is a smart contract containing DIDs or hashes. Currently there are still challenges in using a public blockchain like scaling and transaction costs. We follow the developments closely and are seeing potential in the solutions being. Indy (with or without Sovrin) : Despite being a permissioned blockchain, this type of ledger provides less pure SSI. However, if the governance is setup well it might be acceptable. We are following the developments here but it does not have our main focus. X : each ledger could be supported within the ULA in the future by creating plugins. Standards ERC-780 : we started with using uPort's ERC-780 Ethereum claim registry. In this claim registry the issuer, subject, key and value are saved. The value is a hash of the original value with a user's derived public key as salt. Derived keys are also used as subject in order to avoid linkability. We don't use ERC-780 in the current solution (see BIP32 part) but they might be useful for public claims or as trust registry W3C and DIDs : The W3C verifiable credential standard is implemented together with DIDs. This means no hashes are saved on the blockchain, only DIDs. This makes it a more secure and widely supported solution. This plugin also implements proving ownership of DIDs, but we are still doing research on the DID documents Proof request & responses : within the Sovrin plugin there is support for proof request and responses as defined by the Sovrin foundation From proof of concepts to pilot Within Rabobank we did multiple projects with partners to deliver multiple proof of concepts. In the first stages we made the fundamental components more universal. In order to connect data providers or data consumers quickly, we decided to create SSI-as-a-service. With this model, they (or Rabobank) can spin-up a server to issue or verify verifiable credentials. Configurations are e.g. available for their private key, the smart contract being used and on which public keys they rely on (or which trust registry). Currently we are doing PoC's with dummy data and want to move to pilots with real users and data. Therefore we are now researching if we meet the requirements of compliance and legal (e.g. GDPR) and cybersecurity. Besides current collaborations, we are also looking at providing a nationwide solution with governmental partners. Together with government and banks we explore the possible setup of a dedicated entity for SSI. An important question remains who should be the provider and so owner of such an SSI solution. Preferably, a customer does not end up with a set of identity wallets but only one wallet that is interoperable and delivered by a trusted party. Research activities Besides our current activities we are doing research on a wide range of topics. We encourage parties to think along on these topics with us, or as input for the design workshops of RWOT8. * DID documents and the universal resolver : despite we support DIDs, we didn't make the final designs for implementing DID documents and the universal resolver * DID auth / login : using an identity app to sign in using DID auth * Signing documents : signing documents with the app and attach verifiable credentials * Identity hubs : considerations, requirements, designs and implementation of an identity hub or custodian while pursuing a pure SSI solution Disclaimer This short paper is only a high-level overview and a selection of the projects we are doing. If you are interested, we would like to have the conversation with you on any topic regarding SSI. Almost none of the outlined solutions and designs were created solely within Rabobank, therefore we would like to acknowledge and thank our partners for their contributions.","title":"The universal ledger agent: a logical result of Rabobank's journey in blockchain based self-sovereign identity"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#the-universal-ledger-agent-a-logical-result-of-rabobanks-journey-in-blockchain-based-self-sovereign-identity","text":"By David Lamers < david.lamers@rabobank.nl > - v1.2","title":"The universal ledger agent: a logical result of Rabobank's journey in blockchain based self-sovereign identity"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#foreword","text":"Within Rabobank we are doing research on self-sovereign identity using blockchain technology since 2016. Within a wide range of partnerships, we explore together with partners the opportunities for users and businesses as well as the technical standards and solutions. An interesting keynote of our CEO about the importance of SSI can be found on Youtube . Currently we are working on supporting the W3C verifiable credentials (editor's draft January 2019) and DIDs , and have designed the Universal Ledger Agent. With this paper we intended to share our work and to open the discussion on the design and implementation. A demo of the current solution can be found on https://youtu.be/ZV52KpE_yaU","title":"Foreword"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#fundamentals","text":"In this section the fundamental concepts of our SSI solution are explained. Holder, issuer and verifier : We developed a mobile application (using Ionic) for the holder and NodeJS web applications for the issuer and verifier demo\u2019s. For the issuing process, the issuer sends a challenge request to the holder - asking for a DID for each credential so they can be revoked independently whilst maintaining privacy. The holder sends the DID\u2019s back in the form of self-issued verifiable credentials encapsulated in a verifiable presentation . The issuer then issues credentials on those DID\u2019s and sends a verifiable presentation back to the holder. The verifier uses the same process, but the challenge request has different content. So the holder sends a verifiable presentation with credentials retrieved from the issuer in the previous step. Decentralized (pure SSI) : Pure SSI, without any centralized dependencies and providing all functionalities, is probably only possible with blockchain. Blockchain mainly provides functionalities for timestamping, the trust registry and revoking credentials. In each solution being built we maintain the design principles for self-sovereign identity as explained by Christopher Allen . Trust registry/public claims : In order to rely on verifiable credentials, the verifier should be able to verify the signature from the issuer. We foresee a role for the government or a chamber of commerce to maintain these trust registries and maybe also provide public claims about those companies. JSON-LD (signatures) : To create semantic interoperability we use JSON-LD and schema.org definitions when data is exchanged. Signatures are created using the implementation from Digital Bazaar . Personal identity : We use iDIN to retrieve general information of someone\u2019s personal identity in the mobile app. iDIN is supported by all Dutch banks and provides a basic personal identity of the user. Templates : In the past we worked on JSON-LD templates for communication between holder and verifier/issuer. However, over time we wanted to exchange more data which made templates inefficient. Therefore we are now using challenge requests for the issuing and verify processes. The challenge requests contains information about which credentials the holder will receive, but also which credentials the holder must include in the response. This message can contain additional information, for instance the allowed type of verifiable credentials when multiple types are supported (see also universal ledger agent below). Collaboration environments : The fundamentals and use cases have been developed within collaboration environments Rabobank is part of, for example Techruption and the Dutch Blockchain Coalition . The development of SSI should be a common process to create a widely supported and interoperable solution. BIP32 : In the first designs we used Bitcoin's BIP32 to share derived public extended keys with issuers from which a public key was used as salt in hashes to have an high level of entropy. However, hashed personal data (with salt) still has to comply with GDPR regulations. To comply, we changed the design to not use hashes anymore but only use BIP32 to create DIDs. With the possibilities of key derivation, one has to maintain only one master key.","title":"Fundamentals"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#use-cases-within-rabobank","text":"The development of an SSI solution can provide opportunities for the Rabobank within for instance the following business lines. The added value for the business lines, our customers and employees is the reason we work on SSI. KYC : Currently, we have extensive know your customer (KYC) and customer due diligence (CDD) processes for retail and wholesale. With hundreds of possible data points these processes are based on, it's complex to retrieve the data from a (direct) verifiable source. SSI could provide directly verifiable data and as bank we have valuable data for the customer for third parties. Research within Rabobank has shown the added value of SSI within KYC. In the first proof of concept we retrieved data from the chamber of commerce to the identity app. Mortgage : The average mortgage flow requires a lot of time from the customer. Documents from a lot of resources are required and most of them are not verifiable. The application process takes long which can be improved with SSI. By supplying the data using the identity app we can immediately check and verify validity and origin of the data. HR and onboarding of employees : we want to empower employees with their own data. It should be possible to reuse certificates or assessments they achieved or did at Rabobank everywhere else. Therefore we do projects in order to save certificates, diplomas, trainings and employment credentials. This means also supporting open badges within W3C verifiable credentials. If we can receive data this way during onboarding, we can also drastically improve onboarding times. It also connects to our vision on the future of work and employee mobility.","title":"Use cases within Rabobank"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#universal-ledger-agent","text":"Within our collaborations we faced the problem that partners were using different standards and blockchains. We can also not deny that in the future, multiple solutions will mature and interoperability issues will arise. For users this will be a challenge and we want to avoid that they have to use multiple apps, logins or platforms. Therefore we designed together with one of our partners the Universal Ledger Agent. Plugins can be developed for each specific set of standards (and so blockchains). This section describes our efforts so far.","title":"Universal Ledger Agent"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#design","text":"The universal ledger agent (ULA) is a component that is implemented by the app and the verifier. The ULA makes it possible to retrieve credentials from issuers, independently which standards and blockchain they use. Also, a verifier can accept and verify credentials from multiple standards. To achieve this interoperability, the ULA uses plugins as shown below. We are currently working on the Ethereum (ERC-780 as well as W3C verifiable credentials) and Sovrin plugin. The ULA is recognizing the standards used in a credential in order to send it to the right plugin. The plugin takes care of all the logic for the specific standard. In the short future we want to encourage others to build plugins for the ULA. Therefore we are now considering to open source the ULA and our plugins. With open sourcing, we also hope to get more involvement from the community.","title":"Design"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#ledgers","text":"Ethereum : We use the public Ethereum blockchain as credential status registry to achieve pure SSI. This credential status registry is a smart contract containing DIDs or hashes. Currently there are still challenges in using a public blockchain like scaling and transaction costs. We follow the developments closely and are seeing potential in the solutions being. Indy (with or without Sovrin) : Despite being a permissioned blockchain, this type of ledger provides less pure SSI. However, if the governance is setup well it might be acceptable. We are following the developments here but it does not have our main focus. X : each ledger could be supported within the ULA in the future by creating plugins.","title":"Ledgers"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#standards","text":"ERC-780 : we started with using uPort's ERC-780 Ethereum claim registry. In this claim registry the issuer, subject, key and value are saved. The value is a hash of the original value with a user's derived public key as salt. Derived keys are also used as subject in order to avoid linkability. We don't use ERC-780 in the current solution (see BIP32 part) but they might be useful for public claims or as trust registry W3C and DIDs : The W3C verifiable credential standard is implemented together with DIDs. This means no hashes are saved on the blockchain, only DIDs. This makes it a more secure and widely supported solution. This plugin also implements proving ownership of DIDs, but we are still doing research on the DID documents Proof request & responses : within the Sovrin plugin there is support for proof request and responses as defined by the Sovrin foundation","title":"Standards"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#from-proof-of-concepts-to-pilot","text":"Within Rabobank we did multiple projects with partners to deliver multiple proof of concepts. In the first stages we made the fundamental components more universal. In order to connect data providers or data consumers quickly, we decided to create SSI-as-a-service. With this model, they (or Rabobank) can spin-up a server to issue or verify verifiable credentials. Configurations are e.g. available for their private key, the smart contract being used and on which public keys they rely on (or which trust registry). Currently we are doing PoC's with dummy data and want to move to pilots with real users and data. Therefore we are now researching if we meet the requirements of compliance and legal (e.g. GDPR) and cybersecurity. Besides current collaborations, we are also looking at providing a nationwide solution with governmental partners. Together with government and banks we explore the possible setup of a dedicated entity for SSI. An important question remains who should be the provider and so owner of such an SSI solution. Preferably, a customer does not end up with a set of identity wallets but only one wallet that is interoperable and delivered by a trusted party.","title":"From proof of concepts to pilot"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#research-activities","text":"Besides our current activities we are doing research on a wide range of topics. We encourage parties to think along on these topics with us, or as input for the design workshops of RWOT8. * DID documents and the universal resolver : despite we support DIDs, we didn't make the final designs for implementing DID documents and the universal resolver * DID auth / login : using an identity app to sign in using DID auth * Signing documents : signing documents with the app and attach verifiable credentials * Identity hubs : considerations, requirements, designs and implementation of an identity hub or custodian while pursuing a pure SSI solution","title":"Research activities"},{"location":"rwot8/topics-and-advance-readings/universal-ledger-agent/#disclaimer","text":"This short paper is only a high-level overview and a selection of the projects we are doing. If you are interested, we would like to have the conversation with you on any topic regarding SSI. Almost none of the outlined solutions and designs were created solely within Rabobank, therefore we would like to acknowledge and thank our partners for their contributions.","title":"Disclaimer"},{"location":"rwot8/topics-and-advance-readings/universe-is-blockchain/","text":"Is the Universe a blockchain? A cryptographic model of the Universe. Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: crackpotting, fun_with_physics I found it amusing to find analogies between how blockchain and how the Universe work. This is a napkin analysis meant for the entertainment only. Probably it has already been done in much more detail. Anyway, I hope it will amuse someone also. Blockchain Ever increasing in size. Had Big Bang. Have a block time Not editable without impractically massive amount of work Public-private key pairs Have randomness in? Universe Ever expanding. Had genesis block. Have Plank time - smallest possible time period Time is not reversable - not editable Entanglement phenomenon Uncertainty principal Coincidence:) ? Help me with point 5. Find analogy to the uncertainty principal of the universe to something inherently random in blockchain. Is there any randomness, uncertainty in blockchains?","title":"Is the Universe a blockchain?  A cryptographic model of the Universe."},{"location":"rwot8/topics-and-advance-readings/universe-is-blockchain/#is-the-universe-a-blockchain-a-cryptographic-model-of-the-universe","text":"Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: crackpotting, fun_with_physics I found it amusing to find analogies between how blockchain and how the Universe work. This is a napkin analysis meant for the entertainment only. Probably it has already been done in much more detail. Anyway, I hope it will amuse someone also.","title":"Is the Universe a blockchain?  A cryptographic model of the Universe."},{"location":"rwot8/topics-and-advance-readings/universe-is-blockchain/#blockchain","text":"Ever increasing in size. Had Big Bang. Have a block time Not editable without impractically massive amount of work Public-private key pairs Have randomness in?","title":"Blockchain"},{"location":"rwot8/topics-and-advance-readings/universe-is-blockchain/#universe","text":"Ever expanding. Had genesis block. Have Plank time - smallest possible time period Time is not reversable - not editable Entanglement phenomenon Uncertainty principal Coincidence:) ? Help me with point 5. Find analogy to the uncertainty principal of the universe to something inherently random in blockchain. Is there any randomness, uncertainty in blockchains?","title":"Universe"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/","text":"Minimal implementation of verifiable credentials for a community of practice use case on an agent-centric distributed platform Authors Jakub Lanc (jakub.lanc@gmail.com) Keywords Verifiable Credentials; Communities of practice; Agent-centric systems Abstract Organizing distributed peer communities of practice and their interfacing with official institutions comes with numerous challenges \u2013 tracking the education progress of the individuals and sub-communities, maintaining cohesion and coherence of the groups, ensuring the trustworthiness and credibility of the individuals as well as transient sub-groups etc. At the same time, decentralized agent-centric platforms are emerging, allowing for distributed data integrity and implementation of self-sovereignty-friendly models. This prompts us to experiment with implementing a minimal Verifiable Credentials-like system on a distributed agent-centric platform (Holochain), explore its suitability for distributed community of practice use cases, and discuss the specifics of such. Introduction Our aim spans a technical and a conceptual level, which informs the technical experiments. The technical part consists of an experimental implementation of a minimal prototype of a Verifiable Credentials-like conception on an agent-centric distributed platform. The conceptual part is more of a methodological exploration. \u201cIn what ways and to what extent could VCs be used as a tool for facilitating different aspects of functioning of distributed communities of practice?\u201d, we ask. We hope discussing this kind of use cases can also stimulate and enrich the broader discussions related to DID and VC standards and use cases. Since this is a longer term project, we will focus more on the elementary technical implementation issues in this paper, and we\u2019ll describe some of the domain-specific conceptual issues in the appendix. Background context Our prototyping context is a decentralized community of practice / peer network focused on low-barrier mental health support and support in fostering healthy interpersonal relationships and communication within youth. The more obvious domain of VC use in this case is interfacing with official mental health institutions and other educational and professional bodies (and indeed an exemplary use case from this domain might serve as a basis for our initial minimal prototype implementation), but our interest lies in the more \u201cesoteric\u201d aspects of this territory: Continuing long term peer education of this type entails methods from the fields of experiential-reflective learning and psychosocial development. An important part of such process is ensuring quality feedback and tracing individual participants\u2019 learning progress, while assuring selective disclosure and overall autonomy and control over their data. Another important aspect is the patterns of interaction and quality of interpersonal relationships within the community and between sub-groups. \u201cCould VCs be suitable for capturing relevant interpersonal patterns and claims?\u201d, we ask. For us the issue of translating such patterns and signals to a technological system in systemically beneficial ways (preventing undesirable externalities, side effects and unpredictable systems dynamics down the road) is very delicate and important. Further elaborations exceed the scope of this paper, but overall, we consider them essential. We believe interpersonal learning accents of these use cases make for an intriguing borderline exploration - of the potential suitability (or non-suitability) of VCs, but also deeper conceptual (dare say even philosophical) questions. Technical part Agent centric technology \u2013 Holochain We\u2019re still in an exploratory phase, but we believe an agent-centric system allows for a greater flexibility in implementing finer-grained and interpersonally oriented types of claims and credentials, while allowing for sufficient participant autonomy and control over their data. We have chosen the Holochain platform for prototyping, as it currently provides sufficient technological infrastructure and development ecosystem for experimental implementations and testing. We also find their \u201cAutonomous Agent\u201d license model as an important step towards promoting the autonomy and protecting privacy of the users. The Holochain protocol gives individuals high level of control of their actions, identity and data as well as upholding trustworthiness of other agents\u2019 data and actions. Core components are peer validation, and intrinsic data integrity, which is assured by hared rules and tamper-evident journals. Each agent owns a hash-chain, recording his own interactions which are cryptographically signed (or counter-signed in case of multi-party interactions), serving as a proof of authorship. This mean that anyone can make reasonable determinations about data validity without relying on a central authority. We want to start with a minimal, bare-bone implementation of the elementary VC components and then iterate between conceptual exploration and technical prototyping. Implementation on Holochain will be in the Rust language. We will need to define agent roles within the system, relevant data structures and proper validation rules. This presupposes choosing an elementary use case to implement. Appendix: Further background musings We used the phrase \u201cVerifiable Credentials-like\u201d instead of just \u201cVerifiable Credentials\u201d. This stems from our as-of-yet uncertainty about the extent, to which the VC model will be suitable or practical. Here are perfunctory outlines of some of the intended VC functions to provide at least a cursory insight: a) A \u201cmicro credit\u201d system for tracking and facilitating achieving specific learning goals - e.g.: Alice makes an agreement with Bob: she will follow through with a learning goal X as measured by Y in a time frame Z, and Bob will support her through doing W. Cyril and Dan will attest for how this will have been carried out. b) \u201cStaking\u201d - there can be an implicit stake in the form of a \u201clearning integrity score\u201d (see d)), but it might be desirable for the parties to be able to stake also other kinds of resources (e.g.: \u201cAlice has staked XY on following through with her learning goal X.\u201d). How could those kinds of claims be implemented as VCs? c) Motivational and learning feedback - related to a) and b). Some claims may serve more as transient signals for the actor or his close peers themselves than as proofs of competence to outside actors. Alternatively, they could serve as a sort of \u201cintermediate\u201d signals, which could be translated or bridged to \u201cproper\u201d VCs at some point. d) Dynamic \u201cgauges of integrity\u201d - also related to a) and b). E.g.: \u201cHow consistent was Alice in performing specific learning activities or praxes over time?\u201d Furthermore, Alice might or might not want to disclose such partial micro-attestations to different parts of the \u201coutside world\u201d. She might want to be able to delineate time frames to share, or define a \u201cthick black line\u201d, marking a blank slate, new stage. An simplistic example to illustrate the idea: after a difficult life period marked by compromised ability to function well, Alice wants to attest for this divide (\u201clessons learnt\u201d, \u201ccontext has changed\u201d, etc.) together with relevant parties (i.e. a mentor, peers). She might or might not want to make this information visible outside of the community, or even outside their sub-group. e) Interpersonal and relational nature of the claims - we want to be more open to and explicit about the possibility, that in making a claim about a subject, the issuer is also, in a way, making a claim about his own mental models behind the attestation, or the relationship between him and the subject. What could be some specific examples? f) \u201cRupture-and-repair\u201d model - an important model of building trust and relational learning in psychology. To illustrate: In reputation systems associated with mental health services, clients and peers can issue attestations (ratings, reviews, stars etc.) about the provider. So the provider can get \u201cone star\u201d or a \u201cred flag\u201d from the client for his \u201cmis-steps\u201d for example. We don\u2019t endorse such static approach to reputation. Rather we believe in reframing such claims in terms of learning - learning opportunity signals or \u201crepair invitations\u201d for example, inviting one or the other party to initiate and engage in a more receptive interaction, processing the event or conflict either together or with a third party, and then attest about the resolution towards mutual benefit. g) Transient assemblies - temporary dyads, teams or working groups might want to identify themselves, as well as make attestations about their progress. Theoretical background informing our endeavours and framings includes Contextual Behavioral Science, enactive cognitive science or \u201c4E cognitive science\u201d, theories and practices related to experiential-reflective learning as well as frameworks focused on fostering communities of practice. The foreshadowed concerns can also be considered in terms of gamification patterns in education, or even prediction-market-like principles (i.e. staking on learning outcomes). We are aware that some of those functions might be outside the intended scope of VCs as currently defined and intended, but we believe exploration of such borderline causes might stimulate a fertile discussion. Resources Elstad, T. A.; Eide, A. H.; Schumacher, U. (2017). \u201eSocial participation and recovery orientation in a \u201clow threshold\u201d community mental health service: An ethnographic study\u201c. Cogent. Froese, T. & Gallagher, S. (2012). Getting interaction theory (IT) together: Integrating developmental, phenomenological, enactive, and dynamical approaches to social interaction. Interaction Studies, 13(3): 436-468. Zettle, R. D.; Hayes, S.; Barnes-Holmes D.; Biglan, A. (2016). \u201eThe Wiley Handbook of Contextual Behavioral Science\u201c. Wiley-Blackwell. Kim S.; Song K.; Lockee B.; Burton, J. (2018). \u201eGamification in Learning and Education\u201c. Springer. Brock, A.; Harris-Braun E.; Luck, N. (2017). \u201eHolochain: Scalable agent-centric distributed computing\u201c. https://github.com/holochain/holochain-proto/blob/whitepaper/holochain.pdf.","title":"Vcs agent centric"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#minimal-implementation-of-verifiable-credentials-for-a-community-of-practice-use-case-on-an-agent-centric-distributed-platform","text":"","title":"Minimal implementation of verifiable credentials for a community of practice use case on an agent-centric distributed platform"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#authors","text":"Jakub Lanc (jakub.lanc@gmail.com)","title":"Authors"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#keywords","text":"Verifiable Credentials; Communities of practice; Agent-centric systems","title":"Keywords"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#abstract","text":"Organizing distributed peer communities of practice and their interfacing with official institutions comes with numerous challenges \u2013 tracking the education progress of the individuals and sub-communities, maintaining cohesion and coherence of the groups, ensuring the trustworthiness and credibility of the individuals as well as transient sub-groups etc. At the same time, decentralized agent-centric platforms are emerging, allowing for distributed data integrity and implementation of self-sovereignty-friendly models. This prompts us to experiment with implementing a minimal Verifiable Credentials-like system on a distributed agent-centric platform (Holochain), explore its suitability for distributed community of practice use cases, and discuss the specifics of such.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#introduction","text":"Our aim spans a technical and a conceptual level, which informs the technical experiments. The technical part consists of an experimental implementation of a minimal prototype of a Verifiable Credentials-like conception on an agent-centric distributed platform. The conceptual part is more of a methodological exploration. \u201cIn what ways and to what extent could VCs be used as a tool for facilitating different aspects of functioning of distributed communities of practice?\u201d, we ask. We hope discussing this kind of use cases can also stimulate and enrich the broader discussions related to DID and VC standards and use cases. Since this is a longer term project, we will focus more on the elementary technical implementation issues in this paper, and we\u2019ll describe some of the domain-specific conceptual issues in the appendix.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#background-context","text":"Our prototyping context is a decentralized community of practice / peer network focused on low-barrier mental health support and support in fostering healthy interpersonal relationships and communication within youth. The more obvious domain of VC use in this case is interfacing with official mental health institutions and other educational and professional bodies (and indeed an exemplary use case from this domain might serve as a basis for our initial minimal prototype implementation), but our interest lies in the more \u201cesoteric\u201d aspects of this territory: Continuing long term peer education of this type entails methods from the fields of experiential-reflective learning and psychosocial development. An important part of such process is ensuring quality feedback and tracing individual participants\u2019 learning progress, while assuring selective disclosure and overall autonomy and control over their data. Another important aspect is the patterns of interaction and quality of interpersonal relationships within the community and between sub-groups. \u201cCould VCs be suitable for capturing relevant interpersonal patterns and claims?\u201d, we ask. For us the issue of translating such patterns and signals to a technological system in systemically beneficial ways (preventing undesirable externalities, side effects and unpredictable systems dynamics down the road) is very delicate and important. Further elaborations exceed the scope of this paper, but overall, we consider them essential. We believe interpersonal learning accents of these use cases make for an intriguing borderline exploration - of the potential suitability (or non-suitability) of VCs, but also deeper conceptual (dare say even philosophical) questions.","title":"Background context"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#technical-part","text":"","title":"Technical part"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#agent-centric-technology-holochain","text":"We\u2019re still in an exploratory phase, but we believe an agent-centric system allows for a greater flexibility in implementing finer-grained and interpersonally oriented types of claims and credentials, while allowing for sufficient participant autonomy and control over their data. We have chosen the Holochain platform for prototyping, as it currently provides sufficient technological infrastructure and development ecosystem for experimental implementations and testing. We also find their \u201cAutonomous Agent\u201d license model as an important step towards promoting the autonomy and protecting privacy of the users. The Holochain protocol gives individuals high level of control of their actions, identity and data as well as upholding trustworthiness of other agents\u2019 data and actions. Core components are peer validation, and intrinsic data integrity, which is assured by hared rules and tamper-evident journals. Each agent owns a hash-chain, recording his own interactions which are cryptographically signed (or counter-signed in case of multi-party interactions), serving as a proof of authorship. This mean that anyone can make reasonable determinations about data validity without relying on a central authority. We want to start with a minimal, bare-bone implementation of the elementary VC components and then iterate between conceptual exploration and technical prototyping. Implementation on Holochain will be in the Rust language. We will need to define agent roles within the system, relevant data structures and proper validation rules. This presupposes choosing an elementary use case to implement.","title":"Agent centric technology \u2013 Holochain"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#appendix-further-background-musings","text":"We used the phrase \u201cVerifiable Credentials-like\u201d instead of just \u201cVerifiable Credentials\u201d. This stems from our as-of-yet uncertainty about the extent, to which the VC model will be suitable or practical. Here are perfunctory outlines of some of the intended VC functions to provide at least a cursory insight: a) A \u201cmicro credit\u201d system for tracking and facilitating achieving specific learning goals - e.g.: Alice makes an agreement with Bob: she will follow through with a learning goal X as measured by Y in a time frame Z, and Bob will support her through doing W. Cyril and Dan will attest for how this will have been carried out. b) \u201cStaking\u201d - there can be an implicit stake in the form of a \u201clearning integrity score\u201d (see d)), but it might be desirable for the parties to be able to stake also other kinds of resources (e.g.: \u201cAlice has staked XY on following through with her learning goal X.\u201d). How could those kinds of claims be implemented as VCs? c) Motivational and learning feedback - related to a) and b). Some claims may serve more as transient signals for the actor or his close peers themselves than as proofs of competence to outside actors. Alternatively, they could serve as a sort of \u201cintermediate\u201d signals, which could be translated or bridged to \u201cproper\u201d VCs at some point. d) Dynamic \u201cgauges of integrity\u201d - also related to a) and b). E.g.: \u201cHow consistent was Alice in performing specific learning activities or praxes over time?\u201d Furthermore, Alice might or might not want to disclose such partial micro-attestations to different parts of the \u201coutside world\u201d. She might want to be able to delineate time frames to share, or define a \u201cthick black line\u201d, marking a blank slate, new stage. An simplistic example to illustrate the idea: after a difficult life period marked by compromised ability to function well, Alice wants to attest for this divide (\u201clessons learnt\u201d, \u201ccontext has changed\u201d, etc.) together with relevant parties (i.e. a mentor, peers). She might or might not want to make this information visible outside of the community, or even outside their sub-group. e) Interpersonal and relational nature of the claims - we want to be more open to and explicit about the possibility, that in making a claim about a subject, the issuer is also, in a way, making a claim about his own mental models behind the attestation, or the relationship between him and the subject. What could be some specific examples? f) \u201cRupture-and-repair\u201d model - an important model of building trust and relational learning in psychology. To illustrate: In reputation systems associated with mental health services, clients and peers can issue attestations (ratings, reviews, stars etc.) about the provider. So the provider can get \u201cone star\u201d or a \u201cred flag\u201d from the client for his \u201cmis-steps\u201d for example. We don\u2019t endorse such static approach to reputation. Rather we believe in reframing such claims in terms of learning - learning opportunity signals or \u201crepair invitations\u201d for example, inviting one or the other party to initiate and engage in a more receptive interaction, processing the event or conflict either together or with a third party, and then attest about the resolution towards mutual benefit. g) Transient assemblies - temporary dyads, teams or working groups might want to identify themselves, as well as make attestations about their progress. Theoretical background informing our endeavours and framings includes Contextual Behavioral Science, enactive cognitive science or \u201c4E cognitive science\u201d, theories and practices related to experiential-reflective learning as well as frameworks focused on fostering communities of practice. The foreshadowed concerns can also be considered in terms of gamification patterns in education, or even prediction-market-like principles (i.e. staking on learning outcomes). We are aware that some of those functions might be outside the intended scope of VCs as currently defined and intended, but we believe exploration of such borderline causes might stimulate a fertile discussion.","title":"Appendix: Further background musings"},{"location":"rwot8/topics-and-advance-readings/vcs-agent-centric/#resources","text":"Elstad, T. A.; Eide, A. H.; Schumacher, U. (2017). \u201eSocial participation and recovery orientation in a \u201clow threshold\u201d community mental health service: An ethnographic study\u201c. Cogent. Froese, T. & Gallagher, S. (2012). Getting interaction theory (IT) together: Integrating developmental, phenomenological, enactive, and dynamical approaches to social interaction. Interaction Studies, 13(3): 436-468. Zettle, R. D.; Hayes, S.; Barnes-Holmes D.; Biglan, A. (2016). \u201eThe Wiley Handbook of Contextual Behavioral Science\u201c. Wiley-Blackwell. Kim S.; Song K.; Lockee B.; Burton, J. (2018). \u201eGamification in Learning and Education\u201c. Springer. Brock, A.; Harris-Braun E.; Luck, N. (2017). \u201eHolochain: Scalable agent-centric distributed computing\u201c. https://github.com/holochain/holochain-proto/blob/whitepaper/holochain.pdf.","title":"Resources"},{"location":"rwot8/topics-and-advance-readings/verifiable-credentials-primer/","text":"A Verifiable Credentials Primer by Manu Sporny, Digital Bazaar NOTE : \"Verifiable Claims\" are now known as \"Verifiable Credentials\". The W3C Verifiable Claims Working Group's experience with using the term \"Verifiable Claims\" demonstrated that it led to confusion in the marketplace. The group has since found consensus in shifting to use the term \"Verifiable Credentials\", which contain \"Claims\". Introduction It is currently difficult to transmit credentials such as driver's licenses, proofs of age, education qualifications, and healthcare data, via the Internet in a way that is verifiable yet protects individual privacy. Starting in 2013, the W3C Credentials Community Group started to work in earnest on solutions in this space followed shortly thereafter by the Rebooting Web of Trust Community and W3C Verifiable Claims Working Group . These groups, composed of 150+ individuals and organizations, are currently focused on the creation, storage, transmission, and verification of digital credentials via the Internet. This document is a primer for those that want to learn about the Verifiable Credentials initiative, the use cases and ecosystem, a basic overview of the technology, and how to get involved. Use Cases Verifiable Credentials are useful when a person needs to prove that they are: above a certain age, capable of driving a particular motor vehicle, require a particular medication, trained and certified as an electrician, professionally licensed to practice medicine, and cleared to travel internationally. The use cases above are merely a high-level introduction to the problem space. Readers that would like to explore the use cases in more detail are urged to read the Verifiable Claims Working Groups' Use Cases document. Ecosystem The Verifiable Credentials ecosystem is composed of four primary roles: The Issuer , who issues verifiable credentials about a specific Subject . The Holder stores credentials on behalf of a Subject . Holders are typically also the Subject of a credential. The Verifier requests a profile of the Subject . A profile contains a specific set of credentials. The verifier verifies that the credentials provided in the profile are fit-for-purpose. The Identifier Registry is a mechanism that is used to issue identifiers for Subjects . A visual depiction of the ecosystem above is shown below: Claims, Credentials, and Profiles The ecosystem roles exchange data that enables the realization of the previously mentioned use cases. The data that is exchanged differs based on the roles participating, but is fundamentally composed of Claims, Credentials, and Profiles. A claim is statement about a subject, expressed as a subject-property-value relationship: The data model for claims described above is powerful and can be used to express a large variety of statements. For example, whether or not someone is over the age of 21 may be expressed as follows: These claims may be merged together to express a graph of information about a particular subject. The example below extends the data model above by adding claims that state that Pat knows Sam and that Sam is a student. When an Issuer sends data to a Holder, it bundles a set of claims into a data structure called a credential and digitally signs the data structure: When a Verifier asks for data from a Holder, the Holder typically bundles a set of credentials into a data structure called a Profile and digitally signs the data structure: The depictions above are a high-level introduction to the data model and gloss over specifics. Readers that would like to explore the data model in more depth are urged to read the Verifiable Claims Working Groups' Data Model Specification . Participating If you would like to participate in shaping this work, there are multiple ways to participate: If you want weekly updates and are NOT a W3C Member, or want to participate in the more experimental work, you should join the Credentials Community Group . The W3C Credentials Community Group holds weekly calls that are open to the public . If you want weekly updates and are a W3C Member, you should join the Verifiable Claims Working Group . The W3C Verifiable Credentials Working Group holds weekly calls that are open to W3C Members . We hold bi-yearly face-to-face meetings in the spring and fall at Rebooting Web of Trust and once a year in the fall at the W3C Technical Plenary . The groups are very inclusive and welcome input and participation people from all disciplines and levels of expertise.","title":"A Verifiable Credentials Primer"},{"location":"rwot8/topics-and-advance-readings/verifiable-credentials-primer/#a-verifiable-credentials-primer","text":"by Manu Sporny, Digital Bazaar NOTE : \"Verifiable Claims\" are now known as \"Verifiable Credentials\". The W3C Verifiable Claims Working Group's experience with using the term \"Verifiable Claims\" demonstrated that it led to confusion in the marketplace. The group has since found consensus in shifting to use the term \"Verifiable Credentials\", which contain \"Claims\".","title":"A Verifiable Credentials Primer"},{"location":"rwot8/topics-and-advance-readings/verifiable-credentials-primer/#introduction","text":"It is currently difficult to transmit credentials such as driver's licenses, proofs of age, education qualifications, and healthcare data, via the Internet in a way that is verifiable yet protects individual privacy. Starting in 2013, the W3C Credentials Community Group started to work in earnest on solutions in this space followed shortly thereafter by the Rebooting Web of Trust Community and W3C Verifiable Claims Working Group . These groups, composed of 150+ individuals and organizations, are currently focused on the creation, storage, transmission, and verification of digital credentials via the Internet. This document is a primer for those that want to learn about the Verifiable Credentials initiative, the use cases and ecosystem, a basic overview of the technology, and how to get involved.","title":"Introduction"},{"location":"rwot8/topics-and-advance-readings/verifiable-credentials-primer/#use-cases","text":"Verifiable Credentials are useful when a person needs to prove that they are: above a certain age, capable of driving a particular motor vehicle, require a particular medication, trained and certified as an electrician, professionally licensed to practice medicine, and cleared to travel internationally. The use cases above are merely a high-level introduction to the problem space. Readers that would like to explore the use cases in more detail are urged to read the Verifiable Claims Working Groups' Use Cases document.","title":"Use Cases"},{"location":"rwot8/topics-and-advance-readings/verifiable-credentials-primer/#ecosystem","text":"The Verifiable Credentials ecosystem is composed of four primary roles: The Issuer , who issues verifiable credentials about a specific Subject . The Holder stores credentials on behalf of a Subject . Holders are typically also the Subject of a credential. The Verifier requests a profile of the Subject . A profile contains a specific set of credentials. The verifier verifies that the credentials provided in the profile are fit-for-purpose. The Identifier Registry is a mechanism that is used to issue identifiers for Subjects . A visual depiction of the ecosystem above is shown below:","title":"Ecosystem"},{"location":"rwot8/topics-and-advance-readings/verifiable-credentials-primer/#claims-credentials-and-profiles","text":"The ecosystem roles exchange data that enables the realization of the previously mentioned use cases. The data that is exchanged differs based on the roles participating, but is fundamentally composed of Claims, Credentials, and Profiles. A claim is statement about a subject, expressed as a subject-property-value relationship: The data model for claims described above is powerful and can be used to express a large variety of statements. For example, whether or not someone is over the age of 21 may be expressed as follows: These claims may be merged together to express a graph of information about a particular subject. The example below extends the data model above by adding claims that state that Pat knows Sam and that Sam is a student. When an Issuer sends data to a Holder, it bundles a set of claims into a data structure called a credential and digitally signs the data structure: When a Verifier asks for data from a Holder, the Holder typically bundles a set of credentials into a data structure called a Profile and digitally signs the data structure: The depictions above are a high-level introduction to the data model and gloss over specifics. Readers that would like to explore the data model in more depth are urged to read the Verifiable Claims Working Groups' Data Model Specification .","title":"Claims, Credentials, and Profiles"},{"location":"rwot8/topics-and-advance-readings/verifiable-credentials-primer/#participating","text":"If you would like to participate in shaping this work, there are multiple ways to participate: If you want weekly updates and are NOT a W3C Member, or want to participate in the more experimental work, you should join the Credentials Community Group . The W3C Credentials Community Group holds weekly calls that are open to the public . If you want weekly updates and are a W3C Member, you should join the Verifiable Claims Working Group . The W3C Verifiable Credentials Working Group holds weekly calls that are open to W3C Members . We hold bi-yearly face-to-face meetings in the spring and fall at Rebooting Web of Trust and once a year in the fall at the W3C Technical Plenary . The groups are very inclusive and welcome input and participation people from all disciplines and levels of expertise.","title":"Participating"},{"location":"rwot8/topics-and-advance-readings/verifiable-displays-in-HTML/","text":"Verifiable Displays: secure presentation of Verifiable Credentials in HTML Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: Verifiable Displays, Verifiable Credentials, Hashlinks, HTML Abstract This paper describes a way to create tamper-proofed, future-proofed, \"anywhere renderable\" bundles of Verifiable Credentials and their Verifiable Displays, by embedding Verifiable Credentials and their Verifiable Displays into HTML files and linking them with Hashlinks. This paper is a continued exploration of the problem and proposals discussed in the paper - Verifiable Displays . It uses understandings and developments from Resource Integrity Proofs paper and Hashlinks - Cryptographic Hyperlinks specification. Problem Statement The Verifiable Credentials Data Model is an emerging standard for expressing credentials such as driver's licenses, degrees, and passports on the Web in a secure, privacy respecting manner. The \"Verifiable\" part is because they are designed to be cryptographically and machine verifiable. This emphasis on cryptogrphic machine verification is essential, but what\u2019s not in scope of the VC Data Model is scenarios where a human participates in verification. A human person is typically not looking at the content in a raw json format (this content is cryptographically signed/verifiable); instead they are looking at a styled representation of the content: PDF, HTML, SVG, etc. The questions arise: - How does the consumer of Verifiable Credential know that the friendly display they see is what the issuer signed/intended? - How does the issuer know that the consumer will see the intened display, when it went through intermediary hands? - In general, how to know that the display matches the content? - Time-related concerns to credentials display: -- How to ensure, everlasting longevity of the Verifiable Credential display? -- In other words, how to make sure that display of the credential is going to be renderable and verifiable in the distant future? HTML, Verifiable Displays, Verifiable Credentials, Hashlinks - tying all together HTML is a ubiquitous standard used to present data on the Internet. It can be rendered (opened) by all Internet related tools (most importantly by the Internet browsers). Therefore it is the best candidate to be used to display Verifiable Credentials. Verifiable Displays is a visual representation of Verifiable Credentials. It can be created by using any text or image format that can be embedded into HTML and rendered by typical Internet browser. Verifiable Credentials is a JSON based specification to express cryptographically verifiable data. Hashlink is a newly proposed internet standard that provides a universal future-proofed way to reference data and ensure its integrity. Using Hashlink to reference Verifiable Display from Verifiable Credential The example below demonstrates a simple hashlink that provides content integrity protection for the \"http://example.org/hw.txt\" file, which has a content type of \"text/plain\": hl:zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e:zuh8iaLobXC8g9tfma1CSTtYBakXeSTkHrYA5hmD4F7dCLw8XYwZ1GWyJ3zwF Please, see in Hashlink specification the detailed description of how this hashlink is created. TODO: - Provide description of standard metadata in hashlink that references Verifiable Display that is only locally available. For example - \"BundledIntoHTML\", \"BundledIntoSVG\", \"local\". Maybe, better to use hashlink as parametrized URL? - http://BundledIntoHTML?hl=zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e - Provide standard way to add into Verifiable Credential hashlink that references its Verifiable Display. Example: \"reference\": { \"type\":[\"VerifiableDisplay\"], \"hl\":\"http://BundledIntoHTML?hl=zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e\" } Embedding Verifiable Credential and Verifiable Display into HTML file Verifiable Display may be created by using any text or image format that can be embedded into HTML and rendered by typical Internet browser. Store Verifiable Display in first <div> element with attribute data-type = \"Verifiable Display\" , and attribute hl = \"_hash_value_of_hashlink\" Store Verifiable Credential in <head> block, within <script type=\"application/ld+json\"> element. Embedding algorithm: - Create Verifiable Display - Create Hashlink that references this Verifiable Display - Add Hashlink into Verifiable Credential - Create HTML file with Verifiable Credential embedded into <head> block, and Verifiable Display embedded into <div> element with attribute data-type = \"Verifiable Display\" Verification algorithm: - Parse Verifiable Credential from the <head> block - Parse hash value from Hashlink that references correct Verifiable Display - Take content of the div element that is being presented to the user and calculate its hash using the same hash algorithm, that is used in Hashlink taken from Verifiable Credential - Compare calcualted hash value with hash value from Verfiaible Credential. If they are the same Verifiable Display is correct, if they are not the same Verifiable Display has been tempered with See here HTML example and SVG example of HTML files with embedded Verifiable Credentials and their Verifiable Displays. Example implementation: You can see how verification of Verifiable Displays may look like at at: Validbook Statements service . To do this, login as the test user - \u201cJimbo Fry\u201d using this private key (password to private key file: \u201c123456789\u201d). After login go manually to the Validbook Statements service , use Chrome browser.) Conclusion In this paper we proposed the outline of the specification for tamper-proofed, future-proofed, \"anywhere renderable\", \"truly portable\" bundles of Verifiable Credentials and their Verifiable Displays, by embedding them into HTML files and linking them with Hashlinks. Questions How to use Hashlink to reference data that is part of HTML file? (describe hashlink metadata)","title":"Verifiable Displays: secure presentation of Verifiable Credentials in HTML"},{"location":"rwot8/topics-and-advance-readings/verifiable-displays-in-HTML/#verifiable-displays-secure-presentation-of-verifiable-credentials-in-html","text":"Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: Verifiable Displays, Verifiable Credentials, Hashlinks, HTML","title":"Verifiable Displays: secure presentation of Verifiable Credentials in HTML"},{"location":"rwot8/topics-and-advance-readings/verifiable-displays-in-HTML/#abstract","text":"This paper describes a way to create tamper-proofed, future-proofed, \"anywhere renderable\" bundles of Verifiable Credentials and their Verifiable Displays, by embedding Verifiable Credentials and their Verifiable Displays into HTML files and linking them with Hashlinks. This paper is a continued exploration of the problem and proposals discussed in the paper - Verifiable Displays . It uses understandings and developments from Resource Integrity Proofs paper and Hashlinks - Cryptographic Hyperlinks specification.","title":"Abstract"},{"location":"rwot8/topics-and-advance-readings/verifiable-displays-in-HTML/#problem-statement","text":"The Verifiable Credentials Data Model is an emerging standard for expressing credentials such as driver's licenses, degrees, and passports on the Web in a secure, privacy respecting manner. The \"Verifiable\" part is because they are designed to be cryptographically and machine verifiable. This emphasis on cryptogrphic machine verification is essential, but what\u2019s not in scope of the VC Data Model is scenarios where a human participates in verification. A human person is typically not looking at the content in a raw json format (this content is cryptographically signed/verifiable); instead they are looking at a styled representation of the content: PDF, HTML, SVG, etc. The questions arise: - How does the consumer of Verifiable Credential know that the friendly display they see is what the issuer signed/intended? - How does the issuer know that the consumer will see the intened display, when it went through intermediary hands? - In general, how to know that the display matches the content? - Time-related concerns to credentials display: -- How to ensure, everlasting longevity of the Verifiable Credential display? -- In other words, how to make sure that display of the credential is going to be renderable and verifiable in the distant future?","title":"Problem Statement"},{"location":"rwot8/topics-and-advance-readings/verifiable-displays-in-HTML/#html-verifiable-displays-verifiable-credentials-hashlinks-tying-all-together","text":"HTML is a ubiquitous standard used to present data on the Internet. It can be rendered (opened) by all Internet related tools (most importantly by the Internet browsers). Therefore it is the best candidate to be used to display Verifiable Credentials. Verifiable Displays is a visual representation of Verifiable Credentials. It can be created by using any text or image format that can be embedded into HTML and rendered by typical Internet browser. Verifiable Credentials is a JSON based specification to express cryptographically verifiable data. Hashlink is a newly proposed internet standard that provides a universal future-proofed way to reference data and ensure its integrity.","title":"HTML, Verifiable Displays, Verifiable Credentials, Hashlinks  - tying all together"},{"location":"rwot8/topics-and-advance-readings/verifiable-displays-in-HTML/#using-hashlink-to-reference-verifiable-display-from-verifiable-credential","text":"The example below demonstrates a simple hashlink that provides content integrity protection for the \"http://example.org/hw.txt\" file, which has a content type of \"text/plain\": hl:zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e:zuh8iaLobXC8g9tfma1CSTtYBakXeSTkHrYA5hmD4F7dCLw8XYwZ1GWyJ3zwF Please, see in Hashlink specification the detailed description of how this hashlink is created. TODO: - Provide description of standard metadata in hashlink that references Verifiable Display that is only locally available. For example - \"BundledIntoHTML\", \"BundledIntoSVG\", \"local\". Maybe, better to use hashlink as parametrized URL? - http://BundledIntoHTML?hl=zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e - Provide standard way to add into Verifiable Credential hashlink that references its Verifiable Display. Example: \"reference\": { \"type\":[\"VerifiableDisplay\"], \"hl\":\"http://BundledIntoHTML?hl=zQmWvQxTqbG2Z9HPJgG57jjwR154cKhbtJenbyYTWkjgF3e\" }","title":"Using Hashlink to reference Verifiable Display from Verifiable Credential"},{"location":"rwot8/topics-and-advance-readings/verifiable-displays-in-HTML/#embedding-verifiable-credential-and-verifiable-display-into-html-file","text":"Verifiable Display may be created by using any text or image format that can be embedded into HTML and rendered by typical Internet browser. Store Verifiable Display in first <div> element with attribute data-type = \"Verifiable Display\" , and attribute hl = \"_hash_value_of_hashlink\" Store Verifiable Credential in <head> block, within <script type=\"application/ld+json\"> element. Embedding algorithm: - Create Verifiable Display - Create Hashlink that references this Verifiable Display - Add Hashlink into Verifiable Credential - Create HTML file with Verifiable Credential embedded into <head> block, and Verifiable Display embedded into <div> element with attribute data-type = \"Verifiable Display\" Verification algorithm: - Parse Verifiable Credential from the <head> block - Parse hash value from Hashlink that references correct Verifiable Display - Take content of the div element that is being presented to the user and calculate its hash using the same hash algorithm, that is used in Hashlink taken from Verifiable Credential - Compare calcualted hash value with hash value from Verfiaible Credential. If they are the same Verifiable Display is correct, if they are not the same Verifiable Display has been tempered with See here HTML example and SVG example of HTML files with embedded Verifiable Credentials and their Verifiable Displays. Example implementation: You can see how verification of Verifiable Displays may look like at at: Validbook Statements service . To do this, login as the test user - \u201cJimbo Fry\u201d using this private key (password to private key file: \u201c123456789\u201d). After login go manually to the Validbook Statements service , use Chrome browser.)","title":"Embedding Verifiable Credential and Verifiable Display into HTML file"},{"location":"rwot8/topics-and-advance-readings/verifiable-displays-in-HTML/#conclusion","text":"In this paper we proposed the outline of the specification for tamper-proofed, future-proofed, \"anywhere renderable\", \"truly portable\" bundles of Verifiable Credentials and their Verifiable Displays, by embedding them into HTML files and linking them with Hashlinks.","title":"Conclusion"},{"location":"rwot8/topics-and-advance-readings/verifiable-displays-in-HTML/#questions","text":"How to use Hashlink to reference data that is part of HTML file? (describe hashlink metadata)","title":"Questions"},{"location":"rwot8/topics-and-advance-readings/verifiable-postal-address-claims/","text":"Verifiable Claims for Postal Addresses: A Use Case for Decentralized Postal Services using DIDs, VCs and Blockchains By Moses MA Submitted to the 8th Rebooting the Web of Trust Technical Workshop \u2022 March 1-3, 2019, Barcelona Keywords: decentralization, postal services, verified claims, identity, blockchain, decentralized, self-sovereign PROPOSAL This is a proposal to facilitate the collaborative drafting of a technical paper that describes the principles and key design considerations for a use case for verifiable physical address claims. Individuals within the global postal network now seek to understand the \u201cdecentralization revolution\u201d and help to develop game-changing, blockchain-powered new business models for the world. We believe that the active endorsement, support and participation of the global postal industry could provide a tipping point for adoption of DIDs and VCs. This is a first step toward that desired future. We base much of our work on key design considerations for decentralized identity, claims and reputation, developed by C. Allen, M. Sporny, D. Reed, and many others (see references), at previous RWOT design conferences. BACKGROUND The Decentralized ID movement offers a rare and unique opportunity to fix certain deep, systemic flaws in the methods that currently manage online identity, which has caused significant global issues based on surveillance capitalism and the generation of intelligent, targetable, weaponized propaganda. It offers a next step beyond user-centric identity by offering true user control over digital identity, offering robust and meaningful user autonomy. This means control, not just consent, and thus can offer a smooth path to transportable self-sovereign identity services and resources. The goal is to implement a rapid entry for postal authorities into blockchain enabled decentralized identity efforts. This is mean to both strengthen postal industry efforts to address the emerging web of trust, as well as strengthen emerging standards and efforts such as Decentralized Identity, Verifiable Claims, identity.foundation, and others. APPLICATION TO POSTAL SERVICES Self-sovereign identity adheres to a series of guiding principles \u2014 which are meant to ensure the user controls their identity related data. Identity data is a double-edged sword \u2014 usable for both positive and negative purposes. Thus, an identity system must balance transparency, fairness, and support of the commons with protection for the individual. These principles are: Personhood . Users must have an independent existence. Any self-sovereign identity is ultimately based on the ineffable \u201cI\u201d that\u2019s at the heart of identity. It can never exist wholly in digital form. A self-sovereign identity simply makes public and accessible some limited aspects of the \u201cI\u201d that already exists. Control . Users must be able to control their identities. Subject to well-understood and secure algorithms that ensure the continued validity of an identity and its claims, the user is the ultimate authority on their identity. They should always be able to refer to it, update it, or even hide it. They must be able to choose celebrity or privacy as they prefer. Access . Users must have access to their own identity-related data. A user must always be able to retrieve all the claims and other data within his identity. There must be no hidden data and be able to operate without gatekeepers. This does not mean that a user can necessarily modify all the claims associated with his identity, but it does mean they should be aware of them. Transparency . Systems and algorithms must be transparent. The systems used to administer and operate a network of identities must be open, both in how they function and in how they are managed and updated. The algorithms should be free, open-source, well-known, and as independent as possible of any particular architecture; anyone should be able to examine how they work. Persistence . Identities must be long-lived. Preferably, identities should last forever, or at least for as long as the user wishes. This must not contradict a \u201cright to be forgotten\u201d; a user should be able to dispose of an identity if he wishes and claims should be modified or removed as appropriate over time. To do this requires a firm separation between an identity and its claims: they can't be tied forever. Portability . Information and services about identity must be transportable. Identities must not be held by a singular third-party entity, even if it's a trusted entity that is expected to work in the best interest of the user. The problem is that entities can disappear \u2014 and on the Internet, most eventually do. Regimes may change, users may move to different jurisdictions. Transportable identities ensure that the user remains in control of his identity no matter what, and can also improve an identity\u2019s persistence over time. Interoperability . Identities should be as widely usable as possible. Identities are of little value if they only work in limited niches. The goal of a 21st-century digital identity system is to make identity information widely available, crossing international boundaries to create global identities, without losing user control. Consent . Users must agree to the use of their identity. Any identity system is built around sharing that identity and its claims, and an interoperable system increases the amount of sharing that occurs. However, sharing of data must only occur with the consent of the user. Though other users such as an employer, a credit bureau, or a friend might present claims, the user must still offer consent for them to become valid. Minimalization . Disclosure of claims must be minimized. When data is disclosed, that disclosure should involve the minimum amount of data necessary to accomplish the task at hand. For example, if only a minimum age is called for, then the exact age should not be disclosed, and if only an age is requested, then the more precise date of birth should not be disclosed. This principle can be supported with selective disclosure, range proofs, and other zero-knowledge techniques, but non-correlatibility is still a very hard; the best we can do is to use minimalization to support privacy as best as possible. Protection . The rights of users must be protected. When there is a conflict between the needs of the identity network and the rights of individual users, then the network should err on the side of preserving the freedoms and rights of the individuals over the needs of the network. To ensure this, authentication must occur through algorithms that are censorship-resistant and force-resilient and that are run in a decentralized manner. The current embodiment of these principles \u2013 which are essentially in alignment with postal industry principles \u2013 is being encoded into something called the \u201cDecentralized Identity specification\u201d (DID is the acronym for the decentralized identifier). The DID spec is now being developed a working group at the World Wide Web Consortium, known as the Credentials Community Group. DIDs provide a way for individuals and organizations to create permanent, globally unique, verifiable identifiers that are entirely under the identity owner\u2019s control. Unlike a domain name, IP address, or phone number, a DID is not rented from any service provider, and no one can take it away from whomever owns or controls the associated private key. DIDs are the first globally unique verifiable identifiers that require no registration authority. We believe that technical cooperation with the DID community will help the postal industry achieve its strategic objectives. Use of blockchain technology can also help to reduce the \"postal divide\" between industrialized and developing countries and enables the transfer of know-how. And so, some of the next step/deliverables we hope to develop during the workshop include: Develop an understanding for how DIDs and verifiable claims would interoperate with postal services Create a user persona for a typical postal services user to analyze tacit needs Develop a detailed use case for a \u201cpostal address\u201d verifiable claim Develop knowledge about the use of DID within a reference application for financial inclusion Understand social/network interaction functionality between stakeholders and users to map out downstream functionality Develop a pilot development plan (PDP) for the postal industry: a strategy document setting out the pilot implementation of technical cooperation activities Discuss requirements for a multi-year integrated project (MIP) covering expertise, purchase of equipment and training in this arena Open discussion on other issues related to this effort WHY THIS MATTERS Imagine a world where decentralized technology has been deployed and globally adopted. Let us paint a picture for how this might be achieved. Imagine that this approach becomes part of a decentralized identity solution, driven by a robust and active developer community. The systems and functional resources produced would be integrated into postal services that are used in e-commerce, social interaction, low cost banking, healthcare, and so on. Now imagine that mobile telephony companies agree to embed the technology into the operating systems for all smartphones, and the dominant networks \u2013 from social to logistics to financial services \u2013 agree to use postal APIs in their algorithms for driving their applications. This could mean that the postal industry could participate in the beginning of new era for society built on an interconnecting web of trust. This will enable a Cambrian explosion of postal services that will empower e-commerce, financial services, logistics and shipping, and many other areas. It also could mean the end of phishing. The end of spam. All of this is possible via the creation of decentralized trust systems. Therefore, we seek to develop knowledge and expertise in the cost effective integration of blockchain and decentralization technologies to strengthen the applications and offerings or postal organizations, to explore financial inclusion strategies and technologies; to explore and pilot cost effective exploration of advanced technologies to benefit the postal industry. And so, our goal for this working paper is to map out functionality for such a system. We wish to co-author, with members of the Rebooting the Web of Trust community, a position paper that seeks to address these these and related challenges and to produce meaningful solutions. REFERENCES https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2017/blob/master/topics-and-advance-readings/did-primer.md","title":"Verifiable Claims for Postal Addresses: A Use Case for Decentralized Postal Services using DIDs, VCs and Blockchains"},{"location":"rwot8/topics-and-advance-readings/verifiable-postal-address-claims/#verifiable-claims-for-postal-addresses-a-use-case-for-decentralized-postal-services-using-dids-vcs-and-blockchains","text":"","title":"Verifiable Claims for Postal Addresses: A Use Case for Decentralized Postal Services using DIDs, VCs and Blockchains"},{"location":"rwot8/topics-and-advance-readings/verifiable-postal-address-claims/#by-moses-ma","text":"Submitted to the 8th Rebooting the Web of Trust Technical Workshop \u2022 March 1-3, 2019, Barcelona Keywords: decentralization, postal services, verified claims, identity, blockchain, decentralized, self-sovereign PROPOSAL This is a proposal to facilitate the collaborative drafting of a technical paper that describes the principles and key design considerations for a use case for verifiable physical address claims. Individuals within the global postal network now seek to understand the \u201cdecentralization revolution\u201d and help to develop game-changing, blockchain-powered new business models for the world. We believe that the active endorsement, support and participation of the global postal industry could provide a tipping point for adoption of DIDs and VCs. This is a first step toward that desired future. We base much of our work on key design considerations for decentralized identity, claims and reputation, developed by C. Allen, M. Sporny, D. Reed, and many others (see references), at previous RWOT design conferences. BACKGROUND The Decentralized ID movement offers a rare and unique opportunity to fix certain deep, systemic flaws in the methods that currently manage online identity, which has caused significant global issues based on surveillance capitalism and the generation of intelligent, targetable, weaponized propaganda. It offers a next step beyond user-centric identity by offering true user control over digital identity, offering robust and meaningful user autonomy. This means control, not just consent, and thus can offer a smooth path to transportable self-sovereign identity services and resources. The goal is to implement a rapid entry for postal authorities into blockchain enabled decentralized identity efforts. This is mean to both strengthen postal industry efforts to address the emerging web of trust, as well as strengthen emerging standards and efforts such as Decentralized Identity, Verifiable Claims, identity.foundation, and others. APPLICATION TO POSTAL SERVICES Self-sovereign identity adheres to a series of guiding principles \u2014 which are meant to ensure the user controls their identity related data. Identity data is a double-edged sword \u2014 usable for both positive and negative purposes. Thus, an identity system must balance transparency, fairness, and support of the commons with protection for the individual. These principles are: Personhood . Users must have an independent existence. Any self-sovereign identity is ultimately based on the ineffable \u201cI\u201d that\u2019s at the heart of identity. It can never exist wholly in digital form. A self-sovereign identity simply makes public and accessible some limited aspects of the \u201cI\u201d that already exists. Control . Users must be able to control their identities. Subject to well-understood and secure algorithms that ensure the continued validity of an identity and its claims, the user is the ultimate authority on their identity. They should always be able to refer to it, update it, or even hide it. They must be able to choose celebrity or privacy as they prefer. Access . Users must have access to their own identity-related data. A user must always be able to retrieve all the claims and other data within his identity. There must be no hidden data and be able to operate without gatekeepers. This does not mean that a user can necessarily modify all the claims associated with his identity, but it does mean they should be aware of them. Transparency . Systems and algorithms must be transparent. The systems used to administer and operate a network of identities must be open, both in how they function and in how they are managed and updated. The algorithms should be free, open-source, well-known, and as independent as possible of any particular architecture; anyone should be able to examine how they work. Persistence . Identities must be long-lived. Preferably, identities should last forever, or at least for as long as the user wishes. This must not contradict a \u201cright to be forgotten\u201d; a user should be able to dispose of an identity if he wishes and claims should be modified or removed as appropriate over time. To do this requires a firm separation between an identity and its claims: they can't be tied forever. Portability . Information and services about identity must be transportable. Identities must not be held by a singular third-party entity, even if it's a trusted entity that is expected to work in the best interest of the user. The problem is that entities can disappear \u2014 and on the Internet, most eventually do. Regimes may change, users may move to different jurisdictions. Transportable identities ensure that the user remains in control of his identity no matter what, and can also improve an identity\u2019s persistence over time. Interoperability . Identities should be as widely usable as possible. Identities are of little value if they only work in limited niches. The goal of a 21st-century digital identity system is to make identity information widely available, crossing international boundaries to create global identities, without losing user control. Consent . Users must agree to the use of their identity. Any identity system is built around sharing that identity and its claims, and an interoperable system increases the amount of sharing that occurs. However, sharing of data must only occur with the consent of the user. Though other users such as an employer, a credit bureau, or a friend might present claims, the user must still offer consent for them to become valid. Minimalization . Disclosure of claims must be minimized. When data is disclosed, that disclosure should involve the minimum amount of data necessary to accomplish the task at hand. For example, if only a minimum age is called for, then the exact age should not be disclosed, and if only an age is requested, then the more precise date of birth should not be disclosed. This principle can be supported with selective disclosure, range proofs, and other zero-knowledge techniques, but non-correlatibility is still a very hard; the best we can do is to use minimalization to support privacy as best as possible. Protection . The rights of users must be protected. When there is a conflict between the needs of the identity network and the rights of individual users, then the network should err on the side of preserving the freedoms and rights of the individuals over the needs of the network. To ensure this, authentication must occur through algorithms that are censorship-resistant and force-resilient and that are run in a decentralized manner. The current embodiment of these principles \u2013 which are essentially in alignment with postal industry principles \u2013 is being encoded into something called the \u201cDecentralized Identity specification\u201d (DID is the acronym for the decentralized identifier). The DID spec is now being developed a working group at the World Wide Web Consortium, known as the Credentials Community Group. DIDs provide a way for individuals and organizations to create permanent, globally unique, verifiable identifiers that are entirely under the identity owner\u2019s control. Unlike a domain name, IP address, or phone number, a DID is not rented from any service provider, and no one can take it away from whomever owns or controls the associated private key. DIDs are the first globally unique verifiable identifiers that require no registration authority. We believe that technical cooperation with the DID community will help the postal industry achieve its strategic objectives. Use of blockchain technology can also help to reduce the \"postal divide\" between industrialized and developing countries and enables the transfer of know-how. And so, some of the next step/deliverables we hope to develop during the workshop include: Develop an understanding for how DIDs and verifiable claims would interoperate with postal services Create a user persona for a typical postal services user to analyze tacit needs Develop a detailed use case for a \u201cpostal address\u201d verifiable claim Develop knowledge about the use of DID within a reference application for financial inclusion Understand social/network interaction functionality between stakeholders and users to map out downstream functionality Develop a pilot development plan (PDP) for the postal industry: a strategy document setting out the pilot implementation of technical cooperation activities Discuss requirements for a multi-year integrated project (MIP) covering expertise, purchase of equipment and training in this arena Open discussion on other issues related to this effort WHY THIS MATTERS Imagine a world where decentralized technology has been deployed and globally adopted. Let us paint a picture for how this might be achieved. Imagine that this approach becomes part of a decentralized identity solution, driven by a robust and active developer community. The systems and functional resources produced would be integrated into postal services that are used in e-commerce, social interaction, low cost banking, healthcare, and so on. Now imagine that mobile telephony companies agree to embed the technology into the operating systems for all smartphones, and the dominant networks \u2013 from social to logistics to financial services \u2013 agree to use postal APIs in their algorithms for driving their applications. This could mean that the postal industry could participate in the beginning of new era for society built on an interconnecting web of trust. This will enable a Cambrian explosion of postal services that will empower e-commerce, financial services, logistics and shipping, and many other areas. It also could mean the end of phishing. The end of spam. All of this is possible via the creation of decentralized trust systems. Therefore, we seek to develop knowledge and expertise in the cost effective integration of blockchain and decentralization technologies to strengthen the applications and offerings or postal organizations, to explore financial inclusion strategies and technologies; to explore and pilot cost effective exploration of advanced technologies to benefit the postal industry. And so, our goal for this working paper is to map out functionality for such a system. We wish to co-author, with members of the Rebooting the Web of Trust community, a position paper that seeks to address these these and related challenges and to produce meaningful solutions. REFERENCES https://github.com/WebOfTrustInfo/rebooting-the-web-of-trust-fall2017/blob/master/topics-and-advance-readings/did-primer.md","title":"By Moses MA"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/","text":"DRAFT W3C Strong Authentication and Identity Workshop Report December 10th-11th, Redmond, WA This is a draft report from the W3C Strong Authentication and Identity Workshop noting the topics covered during the workshop as well as the outcomes and next steps identified by the workshop participants. 1. Introduction The W3C Strong Authentication and Identity Workshop gathered experts in the space to explore the existing standards landscape, examine existing technology roadmaps, and identify potential future work for how strong identity and strong authentication should work on the web. The workshop explored aligning recent W3C specifications (WebAuthn, Verifiable Claims, Web Payments) and work that is ongoing in the W3C Credentials Community Group (DID, DIDAuth) along with the IETF and ISO, as well as other existing community standards such as IndieAuth, Open ID Connect, OAuth, and SAML. 2. Workshop Agenda Minutes: https://www.w3.org/Security/strong-authentication-and-identity-workshop/minutes.html Time What Presenter Monday, December 10th 8:30 AM Registration / Settling Program Committee / W3C Staff 9:00 AM Welcome and Procedural Wendy Seltzer 9:30 AM Understanding Verifiable Credentials Dan Burnett 9:50 AM Understanding DIDs Kim Duffy 10:10 AM Understanding DID Auth Markus Sabadello 10:30 AM Break 11:00 AM Understanding WebAuthn, CTAP, EAT, FIDO and Authenticators Tony Nadalin Dirk Balfanz Rae Hayward 11:30 AM Understanding JWT/CWT, OpenID, and related ecosystem John Bradley Mike Jones Aaron Parecki 11:50 AM Explanation of Breakout Sessions (Goals/Output) Kaliya Young 12:10 PM Lunch 1:30 PM Breakout #1: Positions on Presented Technologies / Concerns / Potential Work Items All 2:00 PM Breakout #2: Positions on Presented Technologies / Concerns / Potential Work Items All 2:30 PM Report Out to Entire Group on Concerns and Potential Work Items Kaliya Young 3:00 PM Break 3:30 PM Market Verticals - Understanding current and future problems * Government - Peter Watkins * Healthcare - Allen Brown * Supply Chain - Jim Masloski * Legal - Scott David * Once upon a time - John Fontana Industry 4:30 PM Wrap up / Agenda Gardening Wendy Seltzer 5:00 PM Close Afterhours: Zeitgeist of concerns and potential work items Program Committee 6:00 PM Group Dinner On Campus - Tuesday, December 11th 9:00 AM Procedural / Agenda Gardening Wendy Seltzer 9:30 AM Dot voting: Concerns and Potential Work Items Kaliya - All 9:45 AM Exploring Cultural and Economic Perspectives Takashi Minamii Shigeya Suzuki Pindar Wong 10:15 AM Avoiding mistakes and minefields Jeff Hodges Trusted ID Mary Hodder Tom Jones 10:45 AM Morning Break 12:00 PM Lunch 2:15 PM 5 Year Roadmap: DIDs, VCs, and Attestations Christopher Allen Mathias Brossard Greg Kidd 2:45 PM 5 Year Roadmap: Authenticators John Bradley John Callahan Marie Lathi\u00e8re 1:30 PM Entire Group: Discussion Wendy Seltzer - All 2:00 PM Afternoon Break 3:45 PM Breakouts 4:00 PM Finish 3. Presentations 3.2. Understanding Verifiable Credentials A Verifiable Credential is an object a Holder can present to a Verifier showing assertions made by an Issuer. The Verifiable Credential contains one or more claims, metadata, an optional identifier, and a cryptographic proof that makes the digital document tamper-evident. W3C's Verifiable Claims Working Group is preparing to recommend version 1.0 of the Verifiable Credentials Data Model in 2019. slides: Understanding Verifiable Credentials 3.3. Understanding Decentralized Identifiers (DIDs) Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, \"self-sovereign\" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. DID implementations, or \"methods\", must specify create, read, update, and delete operations against a target ledger. DIDs \"resolve\" to DID Documents, which enable operations such as authentication, and DID management through its list of service endpoints and cryptographic key material. The W3C Credentials Community Group has incubated a draft community group report, Data Model and Syntaxes for Decentralized Identifiers (DIDs) . As a next step, the W3C Credentials Community Group has proposed to form a W3C DID Working Group . Related documentation: DID Working Group Charter Proposal DID Use Cases DID Specification (published by W3C CCG) slides: Understanding DIDs 3.4. Understanding DID Authentication In the Decentralized Identifiers (DIDs) community, the term \"DID Auth\" has been frequently used in various contexts, without a clear understanding whether it is meant as a high-level concept, or as a set of concrete protocols. If we consider what DIDs and their Decentralized Public Key Infrastructure (DPKI) inherently provide, it seems that \"proving control of a DID\" for authentication purposes is both very useful and relatively straightforward. In order to prove such control of a DID, the general approach is as follows: 1. A relying party sends a challenge to a DID controller, 2. The DID controller constructs a signed response and sends it to the relying party, 3. The relying party resolves the DID to its DID Document, 4. The relying party verifies the signed response using the public key discovered from the DID Document. Beyond this basic flow however, many different projects are currently implementing the high-level concept in different ways, using different data formats, transport mechanisms, and protocols. In order to introduce some clarity, a report was written by the Rebooting-the-Web-of-Trust community to describe the scope as well as different incarnations of the \"DID Auth\" concept. Given that \"DID Auth\" is at this point still a relatively new and unproven idea, there is now much discussion on how it relates to existing authentication technologies such as OIDC and WebAuthentication, which are mature and proven, and which have strong anti-correlation, anti-phishing, and key management properties. Suggested next steps: Continue to mature the DID and DID Resolution specifications, as a baseline for \"DID Auth\" and other DID-based protocols and applications. Continue to explore how to re-use existing, mature authentication frameworks such as OIDC and WebAuthentication, and how to combine the strengths of these protocols with new features provided by DIDs. slides: Understanding DID-Auth 3.5. Understanding WebAuthentication, CTAP, EAT, FIDO, and Authenticators Modern Authentication: \"FIDO 2\" is an umbrella term for CTAP and WebAuthn. CTAP (client-to authenticator protocol) is developed at FIDO; Web Authentication , an API by which the browser authenticates to a relying party (web application), is a W3C Recommendation. Together, the two enable strong passwordless authentication for the Web. EAT: Entity Attestation Tokens, under discussion at the IETF, provide device attestation about provenance. Such attestation can be used in WebAuthn and FIDO to require proof of authenticator strength. FIDO and Authenticators. FIDO Alliance oversees a certification program to validate the security characteristics of authenticator implementations, at Level 1 through 3+. slides: Understanding WebAuthentication, CTAP, EAT, FIDO, and Authenticators 3.6. Understanding JWT/CWT, OpenID, and IndieWeb JSON Web Token (JWT), representation of claims in JSON; can be signed with JWS. Used by OpenID Connect, among others. OpenID Connect is an identity layer on top of OAuth 2.0. IndieAuth : Bring your own identity, via URL. Prompt the user for identity, discover the authorization endpoint from the URL, send user there for permission, on the redirect back, exchange authorization code for access token and user's canonical URL. slides: Understanding JWT/CWT, OpenID, and IndieWeb 3.7. Current and Future Challenges: 3.7.1 Government Peter Watkins, Government of British Columbia, shared some identity needs and challenges. Multiple levels of jurisdiction, administration and contexts (individual and corporate, legal title, professional credentials, health, education, justice\u2026). \"Damned if we do, damned if we don't\" provide authentication/identity tokens, because there's high value/infrequent usage of a government-specific identity; but there are privacy and accountability concerns with relying on third parties for government identification. slides: Government, Supply Chain, Legal 3.7.2 Healthcare Allen Brown described challenging situations for coalescing health and medical information with identity, including military field hospitals or civilian patients who seek only sporadic emergency care. Identity is needed by payers, providers, patients, and data. slides: Healthcare 3.7.3. Market Verticals: Supply Chain Jim Masloski shared the challenge of a supply chain ledger that requires transparency for some aspects, and confidentiality for others. Used DIDs to identify the brokers, suppliers, and customers, and Verifiable Credentials to describe products and provenance. A challenge was pulling information from legally mandated forms. slides: Government, Supply Chain, Legal 3.7.4 Market Verticals: Legal Scott David shared \"mild\" and \"wild\" perspectives on legal practice relating to DIDs. slides: Government, Supply Chain, Legal 3.7.5 Enterprise: Once Upon a Time John Fontana gave a history of 25 years in tech journalism, covering security, directories, messaging, identity, and the \"wild ride from an LDAP directory to where we are now, and how much has been accomplished.\" He remarked on the tremendous amount of work standards require, and commended the group, saying that collaboration through standards seemed closer now than ever. [conclusion of Day 1] 3.8 Exploring Cultural and Economic Perspectives 3.8.1 Current Situation of Japanese fragmented ID Platforms Takashi Minamii described the fragmentation and siloed identity systems used in Japan, where conglomerates build their own identity platforms and drivers' licenses are the most common know-your-customer method. He identified a strong need for a loose identity federation. 3.8.2 Automatic Identification Standards Shigeya Suzuki described structured identifiers, looking at GS1 standards including UPC bar codes. He suggested an opportunity for intersection between the work in GS1 and DIDs. slides: Cultural and Economic Perspectives 3.8.3 Law and Borders From an Asia Pacific perspective, Pindar Wong invited the group to think about the next billion+, making self-administered identifiers serve netizen expatriates and displaced people whose right to work online is uncertain or unlawful as they have questionable legal standing or non-lawful status. slides: Law and Borders: Self-Administered IDentifiers and NExTPats: NETizen eXpatriates 3.9. Trusted Identity Tom Jones and Mary Hodder spoke of the challenges of providing and using a \"trusted identity,\" one backed with verified claims upon which a recipient can rely. slides: Use Case 3.10. Avoiding Mistakes and Minefields Jeff Hodges shared common challenges on the path from idea through specification and implementation. He noted the errors of inconsistent terminology assumptions and models, and that trust doesn't scale. A principle of \"flexitility was proposed -- build something that is nominally useful yet malleable such that it can evolve to satisfy further use cases.\" slides: Avoiding Mistakes and Minefields 3.11. Roadmap: Attestations Mathias Brossard described attestation as a building-block for IoT security, starting from EAT and RAT. slides: Attestation roadmap 3.13. Roadmap: Decentralized Identifiers and Verifiable Credentials Christopher Allen built a picture of the decentralized identity stack, from Verifiable Credentials (VCs), Decentralized Identifiers (DIDs), DID-Auth, and further potential technologies for future work. He shared the Credentials Community Group's roadmap diagram . slides: DID & VC Architecture roadmap 3.14. Roadmap: Biometrics John Callahan focused on enterprise use of biometrics with proof-of-liveness enabling \"roaming KYC.\" (Know Your Customer) slides: Biometric Authenticators 3.15. Roadmap: Payment Authentication Marie Lathi\u00e8re described European regulations requiring strong customer authentication, proposing that delegating authentication to merchants, with WebAuthn, can enable good security and user experience. slides: the impacts of European regulation 4. Data Produced by Workshop Participants broke-out into discussion groups several times. Some of their outputs were reported in minutes . Other data were captured on index cards and \"dot-voting\" sheets. These materials are linked . 5. Identified Trends 5.1. Integration of WebAuthn with Legacy Systems There exist large legacy authentication systems that provide high assurance deployed and used by major governments and corporations. Two examples include the Common Access Card (CAC) and the Personal Identity Verification (PIV) card systems. There is work being performed to use this existing infrastructure to create \"derived credentials\" for use by newer authentication technologies such as that provided by the WebAuthentication specification, and potentially mechanisms provided via DID-Auth. 5.2. Decentralized Identifiers and Verifiable Credentials A number of technology companies that participated in the workshop are involved with building solutions based on Decentralized Identifiers and Verifiable Credentials and integrating them with more traditional identity and credential issuing systems at large corporations and large governments. In many of these projects, W3C's focus on global interoperability and combating vendor lock-in were identified as key reasons that government and industry funding is being directed toward building an interoperable ecosystem. Many of the proof of concepts focus on interoperability at the Decentralized Identifier, Credential Issuer, Credential Holder (digital wallet), and Credential Verifier roles. While much of the work to date has focused on data models, there is increasing interest in interoperable protocols that move the interoperable data formats from DID Ledger to Issuer to Holder to Verifier. 5.3. Privacy-Enhancing Technologies There is a strong trend towards privacy-enhancing technologies that place primary control of identifiers, credentials, and authenticators into the hands of individuals. There was also an identified desire to move away from centralized control and storage for information related to identifiers, credentials, and authenticators. While this bodes well for addressing a variety of recent data breaches and questions around data sovereignty, the community seemed to agree that there was still a great deal of work that needed to be done to ensure privacy-enhancing technologies were used by default in the future. Many in attendance at the workshop noted that constant vigilance would be required by the W3C community, as well as the greater technical community, to ensure the current trend continues. 6. Community Next Steps The following next steps have been identified by members of the community: The next Rebooting the Web of Trust is March 1-3 in Barcelona. All of these are expected to be active topics there, especially DIDs and the general direction of turning \"DID Auth\" into other existing protocols. The next Internet Identity Workshop is April 30 through May 2 in Mountain View, CA. This too is expected to continue to advance the community dialog. https://www.internetidentityworkshop.com/ W3C Credentials Community Group Roadmap: https://w3c-ccg.github.io/roadmap/diagram.html IETF non-WG EAT list (Entity Attestation Token) https://mailarchive.ietf.org/arch/browse/eat/ IETF non-WG RATS list (Remote ATtestation ProcedureS) https://mailarchive.ietf.org/arch/browse/rats/ and draft charter https://datatracker.ietf.org/doc/charter-ietf-rats/ 7. W3C Next Steps The following next steps are currently under way at W3C: DID charter out for advance-notice review: https://lists.w3.org/Archives/Public/public-new-work/2019Feb/0004.html Verifiable Claims Data Model v1.0 nearing Candidate Recommendation: https://w3c.github.io/vc-data-model/ WebAuthn API is a W3C Recommendation: https://www.w3.org/TR/webauthn/ . The WG is preparing v2 work. Upcoming F2Fs and Meetings The Verifiable Claims Working Group has a face-to-face meeting in Barcelona March 4-5","title":"_DRAFT_ W3C Strong Authentication and Identity Workshop Report"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#draft-w3c-strong-authentication-and-identity-workshop-report","text":"December 10th-11th, Redmond, WA This is a draft report from the W3C Strong Authentication and Identity Workshop noting the topics covered during the workshop as well as the outcomes and next steps identified by the workshop participants.","title":"DRAFT W3C Strong Authentication and Identity Workshop Report"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#1-introduction","text":"The W3C Strong Authentication and Identity Workshop gathered experts in the space to explore the existing standards landscape, examine existing technology roadmaps, and identify potential future work for how strong identity and strong authentication should work on the web. The workshop explored aligning recent W3C specifications (WebAuthn, Verifiable Claims, Web Payments) and work that is ongoing in the W3C Credentials Community Group (DID, DIDAuth) along with the IETF and ISO, as well as other existing community standards such as IndieAuth, Open ID Connect, OAuth, and SAML.","title":"1. Introduction"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#2-workshop-agenda","text":"Minutes: https://www.w3.org/Security/strong-authentication-and-identity-workshop/minutes.html Time What Presenter Monday, December 10th 8:30 AM Registration / Settling Program Committee / W3C Staff 9:00 AM Welcome and Procedural Wendy Seltzer 9:30 AM Understanding Verifiable Credentials Dan Burnett 9:50 AM Understanding DIDs Kim Duffy 10:10 AM Understanding DID Auth Markus Sabadello 10:30 AM Break 11:00 AM Understanding WebAuthn, CTAP, EAT, FIDO and Authenticators Tony Nadalin Dirk Balfanz Rae Hayward 11:30 AM Understanding JWT/CWT, OpenID, and related ecosystem John Bradley Mike Jones Aaron Parecki 11:50 AM Explanation of Breakout Sessions (Goals/Output) Kaliya Young 12:10 PM Lunch 1:30 PM Breakout #1: Positions on Presented Technologies / Concerns / Potential Work Items All 2:00 PM Breakout #2: Positions on Presented Technologies / Concerns / Potential Work Items All 2:30 PM Report Out to Entire Group on Concerns and Potential Work Items Kaliya Young 3:00 PM Break 3:30 PM Market Verticals - Understanding current and future problems * Government - Peter Watkins * Healthcare - Allen Brown * Supply Chain - Jim Masloski * Legal - Scott David * Once upon a time - John Fontana Industry 4:30 PM Wrap up / Agenda Gardening Wendy Seltzer 5:00 PM Close Afterhours: Zeitgeist of concerns and potential work items Program Committee 6:00 PM Group Dinner On Campus - Tuesday, December 11th 9:00 AM Procedural / Agenda Gardening Wendy Seltzer 9:30 AM Dot voting: Concerns and Potential Work Items Kaliya - All 9:45 AM Exploring Cultural and Economic Perspectives Takashi Minamii Shigeya Suzuki Pindar Wong 10:15 AM Avoiding mistakes and minefields Jeff Hodges Trusted ID Mary Hodder Tom Jones 10:45 AM Morning Break 12:00 PM Lunch 2:15 PM 5 Year Roadmap: DIDs, VCs, and Attestations Christopher Allen Mathias Brossard Greg Kidd 2:45 PM 5 Year Roadmap: Authenticators John Bradley John Callahan Marie Lathi\u00e8re 1:30 PM Entire Group: Discussion Wendy Seltzer - All 2:00 PM Afternoon Break 3:45 PM Breakouts 4:00 PM Finish","title":"2. Workshop Agenda"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#3-presentations","text":"","title":"3. Presentations"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#32-understanding-verifiable-credentials","text":"A Verifiable Credential is an object a Holder can present to a Verifier showing assertions made by an Issuer. The Verifiable Credential contains one or more claims, metadata, an optional identifier, and a cryptographic proof that makes the digital document tamper-evident. W3C's Verifiable Claims Working Group is preparing to recommend version 1.0 of the Verifiable Credentials Data Model in 2019. slides: Understanding Verifiable Credentials","title":"3.2. Understanding Verifiable Credentials"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#33-understanding-decentralized-identifiers-dids","text":"Decentralized Identifiers (DIDs) are a new type of identifier for verifiable, \"self-sovereign\" digital identity. DIDs are fully under the control of the DID subject, independent from any centralized registry, identity provider, or certificate authority. DID implementations, or \"methods\", must specify create, read, update, and delete operations against a target ledger. DIDs \"resolve\" to DID Documents, which enable operations such as authentication, and DID management through its list of service endpoints and cryptographic key material. The W3C Credentials Community Group has incubated a draft community group report, Data Model and Syntaxes for Decentralized Identifiers (DIDs) . As a next step, the W3C Credentials Community Group has proposed to form a W3C DID Working Group . Related documentation: DID Working Group Charter Proposal DID Use Cases DID Specification (published by W3C CCG) slides: Understanding DIDs","title":"3.3. Understanding Decentralized Identifiers (DIDs)"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#34-understanding-did-authentication","text":"In the Decentralized Identifiers (DIDs) community, the term \"DID Auth\" has been frequently used in various contexts, without a clear understanding whether it is meant as a high-level concept, or as a set of concrete protocols. If we consider what DIDs and their Decentralized Public Key Infrastructure (DPKI) inherently provide, it seems that \"proving control of a DID\" for authentication purposes is both very useful and relatively straightforward. In order to prove such control of a DID, the general approach is as follows: 1. A relying party sends a challenge to a DID controller, 2. The DID controller constructs a signed response and sends it to the relying party, 3. The relying party resolves the DID to its DID Document, 4. The relying party verifies the signed response using the public key discovered from the DID Document. Beyond this basic flow however, many different projects are currently implementing the high-level concept in different ways, using different data formats, transport mechanisms, and protocols. In order to introduce some clarity, a report was written by the Rebooting-the-Web-of-Trust community to describe the scope as well as different incarnations of the \"DID Auth\" concept. Given that \"DID Auth\" is at this point still a relatively new and unproven idea, there is now much discussion on how it relates to existing authentication technologies such as OIDC and WebAuthentication, which are mature and proven, and which have strong anti-correlation, anti-phishing, and key management properties. Suggested next steps: Continue to mature the DID and DID Resolution specifications, as a baseline for \"DID Auth\" and other DID-based protocols and applications. Continue to explore how to re-use existing, mature authentication frameworks such as OIDC and WebAuthentication, and how to combine the strengths of these protocols with new features provided by DIDs. slides: Understanding DID-Auth","title":"3.4. Understanding DID Authentication"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#35-understanding-webauthentication-ctap-eat-fido-and-authenticators","text":"Modern Authentication: \"FIDO 2\" is an umbrella term for CTAP and WebAuthn. CTAP (client-to authenticator protocol) is developed at FIDO; Web Authentication , an API by which the browser authenticates to a relying party (web application), is a W3C Recommendation. Together, the two enable strong passwordless authentication for the Web. EAT: Entity Attestation Tokens, under discussion at the IETF, provide device attestation about provenance. Such attestation can be used in WebAuthn and FIDO to require proof of authenticator strength. FIDO and Authenticators. FIDO Alliance oversees a certification program to validate the security characteristics of authenticator implementations, at Level 1 through 3+. slides: Understanding WebAuthentication, CTAP, EAT, FIDO, and Authenticators","title":"3.5. Understanding WebAuthentication, CTAP, EAT, FIDO, and Authenticators"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#36-understanding-jwtcwt-openid-and-indieweb","text":"JSON Web Token (JWT), representation of claims in JSON; can be signed with JWS. Used by OpenID Connect, among others. OpenID Connect is an identity layer on top of OAuth 2.0. IndieAuth : Bring your own identity, via URL. Prompt the user for identity, discover the authorization endpoint from the URL, send user there for permission, on the redirect back, exchange authorization code for access token and user's canonical URL. slides: Understanding JWT/CWT, OpenID, and IndieWeb","title":"3.6. Understanding JWT/CWT, OpenID, and IndieWeb"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#37-current-and-future-challenges","text":"","title":"3.7. Current and Future Challenges:"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#371-government","text":"Peter Watkins, Government of British Columbia, shared some identity needs and challenges. Multiple levels of jurisdiction, administration and contexts (individual and corporate, legal title, professional credentials, health, education, justice\u2026). \"Damned if we do, damned if we don't\" provide authentication/identity tokens, because there's high value/infrequent usage of a government-specific identity; but there are privacy and accountability concerns with relying on third parties for government identification. slides: Government, Supply Chain, Legal","title":"3.7.1 Government"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#372-healthcare","text":"Allen Brown described challenging situations for coalescing health and medical information with identity, including military field hospitals or civilian patients who seek only sporadic emergency care. Identity is needed by payers, providers, patients, and data. slides: Healthcare","title":"3.7.2 Healthcare"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#373-market-verticals-supply-chain","text":"Jim Masloski shared the challenge of a supply chain ledger that requires transparency for some aspects, and confidentiality for others. Used DIDs to identify the brokers, suppliers, and customers, and Verifiable Credentials to describe products and provenance. A challenge was pulling information from legally mandated forms. slides: Government, Supply Chain, Legal","title":"3.7.3. Market Verticals: Supply Chain"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#374-market-verticals-legal","text":"Scott David shared \"mild\" and \"wild\" perspectives on legal practice relating to DIDs. slides: Government, Supply Chain, Legal","title":"3.7.4 Market Verticals: Legal"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#375-enterprise-once-upon-a-time","text":"John Fontana gave a history of 25 years in tech journalism, covering security, directories, messaging, identity, and the \"wild ride from an LDAP directory to where we are now, and how much has been accomplished.\" He remarked on the tremendous amount of work standards require, and commended the group, saying that collaboration through standards seemed closer now than ever. [conclusion of Day 1]","title":"3.7.5 Enterprise: Once Upon a Time"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#38-exploring-cultural-and-economic-perspectives","text":"","title":"3.8 Exploring Cultural and Economic Perspectives"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#381-current-situation-of-japanese-fragmented-id-platforms","text":"Takashi Minamii described the fragmentation and siloed identity systems used in Japan, where conglomerates build their own identity platforms and drivers' licenses are the most common know-your-customer method. He identified a strong need for a loose identity federation.","title":"3.8.1 Current Situation of Japanese fragmented ID Platforms"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#382-automatic-identification-standards","text":"Shigeya Suzuki described structured identifiers, looking at GS1 standards including UPC bar codes. He suggested an opportunity for intersection between the work in GS1 and DIDs. slides: Cultural and Economic Perspectives","title":"3.8.2 Automatic Identification Standards"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#383-law-and-borders","text":"From an Asia Pacific perspective, Pindar Wong invited the group to think about the next billion+, making self-administered identifiers serve netizen expatriates and displaced people whose right to work online is uncertain or unlawful as they have questionable legal standing or non-lawful status. slides: Law and Borders: Self-Administered IDentifiers and NExTPats: NETizen eXpatriates","title":"3.8.3 Law and Borders"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#39-trusted-identity","text":"Tom Jones and Mary Hodder spoke of the challenges of providing and using a \"trusted identity,\" one backed with verified claims upon which a recipient can rely. slides: Use Case","title":"3.9. Trusted Identity"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#310-avoiding-mistakes-and-minefields","text":"Jeff Hodges shared common challenges on the path from idea through specification and implementation. He noted the errors of inconsistent terminology assumptions and models, and that trust doesn't scale. A principle of \"flexitility was proposed -- build something that is nominally useful yet malleable such that it can evolve to satisfy further use cases.\" slides: Avoiding Mistakes and Minefields","title":"3.10. Avoiding Mistakes and Minefields"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#311-roadmap-attestations","text":"Mathias Brossard described attestation as a building-block for IoT security, starting from EAT and RAT. slides: Attestation roadmap","title":"3.11. Roadmap: Attestations"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#313-roadmap-decentralized-identifiers-and-verifiable-credentials","text":"Christopher Allen built a picture of the decentralized identity stack, from Verifiable Credentials (VCs), Decentralized Identifiers (DIDs), DID-Auth, and further potential technologies for future work. He shared the Credentials Community Group's roadmap diagram . slides: DID & VC Architecture roadmap","title":"3.13. Roadmap: Decentralized Identifiers and Verifiable Credentials"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#314-roadmap-biometrics","text":"John Callahan focused on enterprise use of biometrics with proof-of-liveness enabling \"roaming KYC.\" (Know Your Customer) slides: Biometric Authenticators","title":"3.14. Roadmap: Biometrics"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#315-roadmap-payment-authentication","text":"Marie Lathi\u00e8re described European regulations requiring strong customer authentication, proposing that delegating authentication to merchants, with WebAuthn, can enable good security and user experience. slides: the impacts of European regulation","title":"3.15. Roadmap: Payment Authentication"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#4-data-produced-by-workshop","text":"Participants broke-out into discussion groups several times. Some of their outputs were reported in minutes . Other data were captured on index cards and \"dot-voting\" sheets. These materials are linked .","title":"4. Data Produced by Workshop"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#5-identified-trends","text":"","title":"5. Identified Trends"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#51-integration-of-webauthn-with-legacy-systems","text":"There exist large legacy authentication systems that provide high assurance deployed and used by major governments and corporations. Two examples include the Common Access Card (CAC) and the Personal Identity Verification (PIV) card systems. There is work being performed to use this existing infrastructure to create \"derived credentials\" for use by newer authentication technologies such as that provided by the WebAuthentication specification, and potentially mechanisms provided via DID-Auth.","title":"5.1. Integration of WebAuthn with Legacy Systems"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#52-decentralized-identifiers-and-verifiable-credentials","text":"A number of technology companies that participated in the workshop are involved with building solutions based on Decentralized Identifiers and Verifiable Credentials and integrating them with more traditional identity and credential issuing systems at large corporations and large governments. In many of these projects, W3C's focus on global interoperability and combating vendor lock-in were identified as key reasons that government and industry funding is being directed toward building an interoperable ecosystem. Many of the proof of concepts focus on interoperability at the Decentralized Identifier, Credential Issuer, Credential Holder (digital wallet), and Credential Verifier roles. While much of the work to date has focused on data models, there is increasing interest in interoperable protocols that move the interoperable data formats from DID Ledger to Issuer to Holder to Verifier.","title":"5.2. Decentralized Identifiers and Verifiable Credentials"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#53-privacy-enhancing-technologies","text":"There is a strong trend towards privacy-enhancing technologies that place primary control of identifiers, credentials, and authenticators into the hands of individuals. There was also an identified desire to move away from centralized control and storage for information related to identifiers, credentials, and authenticators. While this bodes well for addressing a variety of recent data breaches and questions around data sovereignty, the community seemed to agree that there was still a great deal of work that needed to be done to ensure privacy-enhancing technologies were used by default in the future. Many in attendance at the workshop noted that constant vigilance would be required by the W3C community, as well as the greater technical community, to ensure the current trend continues.","title":"5.3. Privacy-Enhancing Technologies"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#6-community-next-steps","text":"The following next steps have been identified by members of the community: The next Rebooting the Web of Trust is March 1-3 in Barcelona. All of these are expected to be active topics there, especially DIDs and the general direction of turning \"DID Auth\" into other existing protocols. The next Internet Identity Workshop is April 30 through May 2 in Mountain View, CA. This too is expected to continue to advance the community dialog. https://www.internetidentityworkshop.com/ W3C Credentials Community Group Roadmap: https://w3c-ccg.github.io/roadmap/diagram.html IETF non-WG EAT list (Entity Attestation Token) https://mailarchive.ietf.org/arch/browse/eat/ IETF non-WG RATS list (Remote ATtestation ProcedureS) https://mailarchive.ietf.org/arch/browse/rats/ and draft charter https://datatracker.ietf.org/doc/charter-ietf-rats/","title":"6. Community Next Steps"},{"location":"rwot8/topics-and-advance-readings/w3c-identity-workshop-report/#7-w3c-next-steps","text":"The following next steps are currently under way at W3C: DID charter out for advance-notice review: https://lists.w3.org/Archives/Public/public-new-work/2019Feb/0004.html Verifiable Claims Data Model v1.0 nearing Candidate Recommendation: https://w3c.github.io/vc-data-model/ WebAuthn API is a W3C Recommendation: https://www.w3.org/TR/webauthn/ . The WG is preparing v2 work. Upcoming F2Fs and Meetings The Verifiable Claims Working Group has a face-to-face meeting in Barcelona March 4-5","title":"7. W3C Next Steps"},{"location":"rwot8/topics-and-advance-readings/why-WOTs-work/","text":"Why webs of trust work, even the PGP one? Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: short_topic_paper_to_open_discussion, PGP, Web of Trust, graph analysis It appears, there is a widespread understanding that webs of trust do not work (based on twitter conversations, email discussions and even the name of this conference). PGP's web of trust is often used as the classic example of WoT's inability to work in the real world. This way of thinking is usually taken unquestioned at the face value. I'd like to challenge it. PGP's WoT actually works! There is a confusion, - PGP's failure to be adopted by wide masses is confused with PGP's WoT failure (inability to work in real life). This is not true. PGP was not widely adopted because it is too difficult to use. For the most of the people PGP's benefit to privacy is not worth mental cost and inconvenience. But for all nerds (sorry:P) who are willing to put effort into using PGP, PGP and it's Web of Trust works! This is especially true now, when PGP WoT is supported by popular social networks (posting PGP address on social profiles). Questions and suggestions: Discuss proofs that PGP WoT works / does not work / would not work if there were more incentives to break it.","title":"Why webs of trust work, even the PGP one?"},{"location":"rwot8/topics-and-advance-readings/why-WOTs-work/#why-webs-of-trust-work-even-the-pgp-one","text":"Authors: Bohdan Andriyiv (bohdan.andriyiv@validbook.org) Keywords: short_topic_paper_to_open_discussion, PGP, Web of Trust, graph analysis It appears, there is a widespread understanding that webs of trust do not work (based on twitter conversations, email discussions and even the name of this conference). PGP's web of trust is often used as the classic example of WoT's inability to work in the real world. This way of thinking is usually taken unquestioned at the face value. I'd like to challenge it. PGP's WoT actually works! There is a confusion, - PGP's failure to be adopted by wide masses is confused with PGP's WoT failure (inability to work in real life). This is not true. PGP was not widely adopted because it is too difficult to use. For the most of the people PGP's benefit to privacy is not worth mental cost and inconvenience. But for all nerds (sorry:P) who are willing to put effort into using PGP, PGP and it's Web of Trust works! This is especially true now, when PGP WoT is supported by popular social networks (posting PGP address on social profiles).","title":"Why webs of trust work, even the PGP one?"},{"location":"rwot8/topics-and-advance-readings/why-WOTs-work/#questions-and-suggestions","text":"Discuss proofs that PGP WoT works / does not work / would not work if there were more incentives to break it.","title":"Questions and suggestions:"}]}